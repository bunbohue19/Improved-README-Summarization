{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /mnt/4TData/vuquang/.cache/huggingface/token\n",
      "Login successful\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /mnt/4TData/vuquang/.netrc\n"
     ]
    }
   ],
   "source": [
    "!sh config-llama3.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/4TData/vuquang/anaconda3/envs/readsum/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from markdown import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_prompt(readme, summary, shots):\n",
    "    if len(shots) == 0:\n",
    "        return f\"\"\"### Instruction: Summarize the following README contents with LESS THAN 30 words. Your answer should be based on the provided README contents only.\n",
    "\n",
    "        ### README contents:\n",
    "        {readme.strip()}\n",
    "\n",
    "        ### Summary:\n",
    "        {summary}\n",
    "        \"\"\".strip()\n",
    "    else:\n",
    "        prompt = \"\"\"### Instruction: Summarize the following README contents with LESS THAN 30 words. Your answer should be based on the provided README contents only.\n",
    "        ### For examples:\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(len(shots)):\n",
    "            prompt += f\"\"\" \n",
    "            ### README contents: \n",
    "            {shots[i]['readme'].strip()}\n",
    "            \n",
    "            ### Summary:\n",
    "            {shots[i]['description'].strip()}            \n",
    "            \"\"\"\n",
    "\n",
    "        prompt += f\"\"\"\n",
    "        ### README contents:\n",
    "        {readme.strip()}\n",
    "\n",
    "        ### Summary:\n",
    "        {summary}\n",
    "        \"\"\".strip()\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "# You need to change this parameter according to your real path.\n",
    "OUTPUT_DIR = \"./llama3-8b_readme_summarization\"\n",
    "train_csv_file = '../dataset/train.csv'\n",
    "val_csv_file = '../dataset/validation.csv'\n",
    "test_csv_file = '../dataset/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "train_df = pd.read_csv(train_csv_file, usecols=['readme', 'description'])\n",
    "val_df = pd.read_csv(val_csv_file, usecols=['readme', 'description'])\n",
    "test_df = pd.read_csv(test_csv_file, usecols=['readme', 'description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Return item and drop from frame. Raise KeyError if not found.\n",
    "\"\"\"\n",
    "def pop(df : pd.DataFrame, idx : int):\n",
    "    readme = df['readme'][idx]\n",
    "    description = df['description'][idx]\n",
    "    result = {'readme' : readme, 'description' : description}\n",
    "    df.at[idx, 'readme'] = np.nan\n",
    "    df.at[idx, 'description'] = np.nan\n",
    "    return result\n",
    "\n",
    "# Function to remove tags\n",
    "def format_entry(md_data) :\n",
    "    html = markdown(md_data)\n",
    "    # parse html content\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    for a in soup.findAll('a', href=True):\n",
    "        a.decompose()\n",
    "    for data in soup(['style', 'script', 'img', 'pre', 'code']):\n",
    "        # Remove tags\n",
    "        data.decompose()\n",
    "    # return data by retrieving the tag content\n",
    "    return ' '.join(soup.stripped_strings)\n",
    "\n",
    "def process_description(s):\n",
    "    if s.endswith('.'):\n",
    "        s = s[:-1]\n",
    "        s = re.sub(r\"\\. \", \", \", s)\n",
    "    return s + '.'\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@[^\\s]+\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"#+\", \" \", text)\n",
    "    return re.sub(r\"\\^[^ ]+\", \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, readme in enumerate(train_df['readme']):\n",
    "    train_df.at[i, 'readme'] = format_entry(readme)\n",
    "\n",
    "for i, readme in enumerate(val_df['readme']):\n",
    "    val_df.at[i, 'readme'] = format_entry(readme)\n",
    "\n",
    "for i, readme in enumerate(test_df['readme']):\n",
    "    test_df.at[i, 'readme'] = format_entry(readme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "shots = []\n",
    "num_of_shots = 0                            # {0, 1, 2, 3} shots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if num_of_shots == 0:\n",
    "    pass\n",
    "elif num_of_shots == 1:\n",
    "    shots.append(pop(test_df, 8))\n",
    "elif num_of_shots == 2:\n",
    "    shots.append(pop(test_df, 8))\n",
    "    shots.append(pop(test_df, 10))\n",
    "elif num_of_shots == 3:\n",
    "    shots.append(pop(test_df, 8))\n",
    "    shots.append(pop(test_df, 10))\n",
    "    shots.append(pop(test_df, 42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_with_prompt(entry):\n",
    "    readme = entry['readme']\n",
    "    readme = clean_text(readme)\n",
    "    description = process_description(entry['description'])\n",
    "    return {\n",
    "        \"formatted_readme\": readme,\n",
    "        \"summary\": description,\n",
    "        \"prompt_text\": generate_training_prompt(readme, description, shots),\n",
    "    }\n",
    "\n",
    "def process_dataset(data: Dataset):\n",
    "    return data.shuffle(seed=42).map(generate_sample_with_prompt).remove_columns(\n",
    "        [\n",
    "            \"readme\",\n",
    "            \"description\",\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5831/5831 [00:01<00:00, 4115.50 examples/s]\n",
      "Map: 100%|██████████| 834/834 [00:00<00:00, 4317.80 examples/s]\n"
     ]
    }
   ],
   "source": [
    "processed_train_dataset = process_dataset(train_dataset)\n",
    "processed_val_dataset = process_dataset(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [02:31<00:00, 37.77s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "    \n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    use_safetensors=True,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "    \n",
    "lora_r = 16\n",
    "lora_alpha = 64\n",
    "lora_dropout = 0.1\n",
    "lora_target_modules = [\n",
    "    \"q_proj\",\n",
    "    \"up_proj\",\n",
    "    \"o_proj\",\n",
    "    \"k_proj\",\n",
    "    \"down_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"v_proj\",\n",
    "]\n",
    "\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    target_modules=lora_target_modules,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    logging_steps=100,\n",
    "    learning_rate=1e-4,\n",
    "    fp16=True,\n",
    "    max_grad_norm=0.3,\n",
    "    num_train_epochs=4,\n",
    "    warmup_ratio=0.05,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    group_by_length=True,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=\"llama3-8b-readsum\",\n",
    "    save_safetensors=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    seed=42,\n",
    "    push_to_hub=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5831/5831 [00:02<00:00, 2358.62 examples/s]\n",
      "Map: 100%|██████████| 834/834 [00:00<00:00, 2623.02 examples/s]\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=processed_train_dataset,\n",
    "    eval_dataset=processed_val_dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"prompt_text\",\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbunbohue1906\u001b[0m (\u001b[33mlocseo\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/4TData/vuquang/Improved-README-Summarization/summarization/wandb/run-20240430_172513-jqhvx8uy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/locseo/huggingface/runs/jqhvx8uy' target=\"_blank\">llama3-8b-readsum</a></strong> to <a href='https://wandb.ai/locseo/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/locseo/huggingface' target=\"_blank\">https://wandb.ai/locseo/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/locseo/huggingface/runs/jqhvx8uy' target=\"_blank\">https://wandb.ai/locseo/huggingface/runs/jqhvx8uy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11503' max='11660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11503/11660 2:20:21 < 01:54, 1.37 it/s, Epoch 3.95/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.929200</td>\n",
       "      <td>1.914178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.995800</td>\n",
       "      <td>1.741249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/bunbohue/llama3-8b_readme_summarization/commit/2770da36791f84dcad17516f41523e77042859b2', commit_message='End of training', commit_description='', oid='2770da36791f84dcad17516f41523e77042859b2', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "readsum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
