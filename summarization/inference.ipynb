{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from markdown import markdown\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_testing_prompt(readme, shots):\n",
    "    if len(shots) == 0:\n",
    "        return f\"\"\"### Instruction: You are a helpful assistant. You need to summarize the following README contents. A good answer should be based on the provided README contents only and LESS THAN 20 words.\n",
    "\n",
    "        ### README contents:\n",
    "        {readme.strip()}\n",
    "\n",
    "        ### Summary:\n",
    "        \"\"\".strip()\n",
    "    else:\n",
    "        prompt = \"\"\"### Instruction: You are a helpful assistant. You need to summarize the following README contents. A good answer should be based on the provided README contents only and LESS THAN 20 words.\n",
    "        ### For examples:\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(len(shots)):\n",
    "            prompt += f\"\"\" \n",
    "            ### README contents: \n",
    "            {shots[i]['readme'].strip()}\n",
    "            \n",
    "            ### Summary:\n",
    "            {shots[i]['description'].strip()}            \n",
    "            \"\"\"\n",
    "\n",
    "        prompt += f\"\"\"\n",
    "        ### README contents:\n",
    "        {readme.strip()}\n",
    "\n",
    "        ### Summary:\n",
    "        \"\"\".strip()\n",
    "        return prompt\n",
    "\n",
    "def format_entry(md_data) :\n",
    "    html = markdown(md_data)\n",
    "    # parse html content\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    for a in soup.findAll('a', href=True):\n",
    "        a.decompose()\n",
    "    for data in soup(['style', 'script', 'img', 'pre', 'code']):\n",
    "        # Remove tags\n",
    "        data.decompose()\n",
    "    # return data by retrieving the tag content\n",
    "    return ' '.join(soup.stripped_strings)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@[^\\s]+\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"#+\", \" \", text)\n",
    "    return re.sub(r\"\\^[^ ]+\", \"\", text)\n",
    "\n",
    "def process_description(s: str) -> str:\n",
    "    if s.endswith('.'):\n",
    "        s = s[:-1]\n",
    "        s = re.sub(r\"\\. \", \", \", s)\n",
    "    return s + '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "checkpoint = \"bunbohue/bart-large-xsum_readme_summarization\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "readme = \"\"\"\n",
    "b'KNOWLEDGE\\n=========\\nStudying goals, curated lists of awesome information, life achievements and more. \\n\\nMotivation\\n----------\\n\"If you cant even clean up your own room, who the hell are you to give advice to the world?\" - Jordan Peterson\\n\\nSoftware Engineering\\n====================\\n\\n\\nData Structures\\n---------------\\n  Linked List\\n  Queue\\n  Stack\\n  Graph\\n  Hash Map\\n  Tree Map\\n  Hash Set\\n  Tree\\n  Array\\n  AVL Tree\\n  Red Black Tree\\n  LRU Cache\\n  Bloom Filter\\n\\nDesign Patterns\\n---------------\\n  Singleton\\n  Factory\\n  Strategy\\n  Observer\\n  Builder\\n  Adapter\\n  State\\n  Bridge\\n  Visitor\\n  Abstract Factory\\n\\nAlgorithms\\n----------\\n  Bubble Sort\\n  Insertion Sort\\n  Selection Sort\\n  Quick Sort\\n  Merge Sort\\n\\nPrinciples\\n----------\\n\\n\\nLanguages\\n---------\\n\\n  C++\\n\\n\\n\\n\\n  Java\\n  Kotlin\\n\\n\\n  Python\\n  Javascript\\n\\n\\n\\n\\n  Go\\n\\n\\n  Functional Programming\\n  Haskell\\n  Scala\\n  Elm\\n\\nShaders\\n-------\\n\\n\\nDevOps\\n======\\n  Kubernetes\\n  Jenkins\\n  Docker\\n\\nCrypto\\n======\\n  Bitcoin\\n  Etherium\\n\\n\\n  Smart Contracts\\n\\n\\nMachine Learning\\n================\\n\\nMathematics\\n===========\\n\\nHardware\\n========\\n\\nMicrocontrollers\\n----------------\\n  Raspberry PI\\n  Arduino\\n  ESP8266\\n\\nGraphic Design\\n=================\\n  Illustrator\\n  Photoshop\\n  Figma\\n\\nLearning Techniques\\n===================\\n  Pomodoro Technique\\n1. Choose a task to be accomplished.\\n2. Set the Pomodoro to 25 minutes (the Pomodoro is the timer).\\n3. Work on the task until the Pomodoro rings, then put a check on your sheet of paper.\\n4. Take a short break (5 minutes is OK).\\n5. Every 4 Pomodoros take a longer break.\\n'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Studying goals, curated lists of awesome information, life achievements and more. \n"
     ]
    }
   ],
   "source": [
    "prefix = \"summarize: \"\n",
    "inputs = tokenizer(prefix + readme, return_tensors=\"pt\", truncation=True).input_ids.to(device)\n",
    "outputs = model.generate(inputs, max_new_tokens=128, do_sample=False)\n",
    "prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with LLM (Llama 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\n",
    "DEVICE = torch.device(\"cuda:0\")\n",
    "OUTPUT_DIR = \"./zero-shot-prompting-llama-2-7b_readsum_29-6-2024\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../dataset/updated_test.csv', usecols=['readme', 'description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "shots = []\n",
    "num_of_shots = 0\n",
    "\n",
    "if num_of_shots == 0:\n",
    "    pass\n",
    "elif num_of_shots == 1:\n",
    "    shots.append(pop(test_df, 8))\n",
    "elif num_of_shots == 2:\n",
    "    shots.append(pop(test_df, 8))\n",
    "    shots.append(pop(test_df, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.16s/it]\n"
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    truncation=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "    \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    use_safetensors=True,\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "    \n",
    "model = PeftModel.from_pretrained(model, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "readme = \"\"\"b'KNOWLEDGE\\n=========\\nStudying goals, curated lists of awesome information, life achievements and more. \\n\\nMotivation\\n----------\\n\"If you cant even clean up your own room, who the hell are you to give advice to the world?\" - Jordan Peterson\\n\\nSoftware Engineering\\n====================\\n\\n\\nData Structures\\n---------------\\n  Linked List\\n  Queue\\n  Stack\\n  Graph\\n  Hash Map\\n  Tree Map\\n  Hash Set\\n  Tree\\n  Array\\n  AVL Tree\\n  Red Black Tree\\n  LRU Cache\\n  Bloom Filter\\n\\nDesign Patterns\\n---------------\\n  Singleton\\n  Factory\\n  Strategy\\n  Observer\\n  Builder\\n  Adapter\\n  State\\n  Bridge\\n  Visitor\\n  Abstract Factory\\n\\nAlgorithms\\n----------\\n  Bubble Sort\\n  Insertion Sort\\n  Selection Sort\\n  Quick Sort\\n  Merge Sort\\n\\nPrinciples\\n----------\\n\\n\\nLanguages\\n---------\\n\\n  C++\\n\\n\\n\\n\\n  Java\\n  Kotlin\\n\\n\\n  Python\\n  Javascript\\n\\n\\n\\n\\n  Go\\n\\n\\n  Functional Programming\\n  Haskell\\n  Scala\\n  Elm\\n\\nShaders\\n-------\\n\\n\\nDevOps\\n======\\n  Kubernetes\\n  Jenkins\\n  Docker\\n\\nCrypto\\n======\\n  Bitcoin\\n  Etherium\\n\\n\\n  Smart Contracts\\n\\n\\nMachine Learning\\n================\\n\\nMathematics\\n===========\\n\\nHardware\\n========\\n\\nMicrocontrollers\\n----------------\\n  Raspberry PI\\n  Arduino\\n  ESP8266\\n\\nGraphic Design\\n=================\\n  Illustrator\\n  Photoshop\\n  Figma\\n\\nLearning Techniques\\n===================\\n  Pomodoro Technique\\n1. Choose a task to be accomplished.\\n2. Set the Pomodoro to 25 minutes (the Pomodoro is the timer).\\n3. Work on the task until the Pomodoro rings, then put a check on your sheet of paper.\\n4. Take a short break (5 minutes is OK).\\n5. Every 4 Pomodoros take a longer break.\\n'\"\"\"\n",
    "prompt = generate_testing_prompt(readme, shots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n        Studying goals, curated lists of awesome information, life achievements and more. . 📚 . 🎓 . 📖 . 🎮 . 🎨 . 📱 . 💻 . 📦 . 📕 . 📑 . 📓 . 📙 . 📜 . �'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(prompt, max_length=4096, truncation=True, return_tensors=\"pt\").to(DEVICE)\n",
    "inputs_length = len(inputs[\"input_ids\"][0])\n",
    "with torch.inference_mode():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=100, temperature=0.0001)\n",
    "prediction = tokenizer.decode(outputs[0][inputs_length:], skip_special_tokens=True)\n",
    "prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "readsum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
