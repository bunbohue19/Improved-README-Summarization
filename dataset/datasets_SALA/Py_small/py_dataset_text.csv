repo_url,description,readme
https://github.com/pytorch/examples,"A set of examples around pytorch in Vision, Text, Reinforcement Learning, etc.","PyTorch Exampleshttps://pytorch.org/examples/ is a repository showcasing examples of using . The goal is to have curated, short, few/no dependencies high quality examples that are substantially different from each other that can be emulated in your existing work.Available modelsAdditionally, a list of good examples hosted in their own repositories:ContributingIf you'd like to contribute your own example or fix a bug please make sure to take a look at ."
https://github.com/welliamcao/OpsManage,自动化运维平台: 代码及应用部署CI/CD、资产管理CMDB、计划任务管理平台、SQL审核|回滚、任务调度、站内WIKI,"OpsManage是什么？一款代码部署、应用部署、计划任务、设备资产管理平台。开源协议：开源声明：欢迎大家star或者fork我的开源项目，如果大家在自己的项目里面需要引用该项目代码，请在项目里面申明协议和版权信息。开发语言与框架：OpsManage环境要求OpsManage功能说明QQ交流群安装环境配置一、安装Python# yum install epel-release -y
# yum install zlib zlib-devel readline-devel sqlite-devel bzip2-devel openssl-devel gdbm-devel libdbi-devel ncurses-libs kernel-devel libxslt-devel libffi-devel python-devel zlib-devel openldap-devel sshpass gcc git rabbitmq-server supervisor -y
# yum localinstall http://dev.mysql.com/get/mysql-community-release-el6-5.noarch.rpm
# yum install mysql-community-server mysql-devel -y
# wget https://www.python.org/ftp/python/3.6.6/Python-3.6.6.tgz  #CentOS 7不用安装python2.7
# tar -xzvf Python-3.6.6.tgz
# cd Python-3.6.6
# ./configure --prefix=/usr/local/python3
# make all
# make install
# make clean
# make distclean  
# ln -s /usr/local/python3/bin/pip3 /usr/bin/pip3
二、安装模块# cd /mnt/
# git clone -b v3 https://github.com/welliamcao/OpsManage.git
# cd /mnt/OpsManage/
# pip3 install -r requirements.txt  #CentOS 7使用pip3
三、安装Redis# wget http://download.redis.io/releases/redis-3.2.8.tar.gz
# tar -xzvf redis-3.2.8.tar.gz
# cd redis-3.2.8
# make
# make install
# vim redis.conf
修改以下配置（不要配置认证）daemonize yes
loglevel warning
logfile ""/var/log/redis.log""
bind 你的服务器ip地址
例如： bind 127.0.0.1 192.168.88.201
# cd ../
# mv redis-3.2.8 /usr/local/redis
# /usr/local/redis/src/redis-server /usr/local/redis/redis.conf
四、安装MySQL# vim /etc/my.cnf
[mysqld]
character_set_server = utf8
添加以上字段
# /etc/init.d/mysqld restart     	#centos 6
# systemctl start mysqld.service 	#centos 7
# mysql -uroot -p  				#初始密码为空，直接回车就行
mysql> create database opsmanage DEFAULT CHARACTER SET utf8 COLLATE utf8_general_ci;
mysql> grant all privileges on opsmanage.* to root@'%' identified by 'password';
mysql>\q
五、配置RabbitMQ# systemctl start rabbitmq-server.service    #启动RabbitMQ
# systemctl enable rabbitmq-server.service   #设置开机自启动RabbitMQ
# rabbitmqctl add_user admin admin           #添加RabbitMQ账户
# rabbitmqctl set_user_tags admin administrator
# rabbitmqctl set_permissions  -p  '/'  admin '.' '.' '.'
# rabbitmqctl list_users   
# rabbitmqctl delete_user guest 			 #删除guest账户（可以不删除）
# rabbitmq-plugins enable rabbitmq_management   #开启Web UI，可以通过http://server_IP:15672/访问
六、配置OpsManage# cd /mnt/OpsManage/conf
# vim opsmanage.ini
根据自己的情况修改配置

七、生成数据表与管理员账户# cd /mnt/OpsManage/
# /usr/local/python3/bin/python3 manage.py makemigrations account
# /usr/local/python3/bin/python3 manage.py makemigrations wiki
# /usr/local/python3/bin/python3 manage.py makemigrations orders
# /usr/local/python3/bin/python3 manage.py makemigrations navbar
# /usr/local/python3/bin/python3 manage.py makemigrations databases
# /usr/local/python3/bin/python3 manage.py makemigrations asset
# /usr/local/python3/bin/python3 manage.py makemigrations deploy
# /usr/local/python3/bin/python3 manage.py makemigrations cicd
# /usr/local/python3/bin/python3 manage.py makemigrations sched
# /usr/local/python3/bin/python3 manage.py makemigrations apply
# /usr/local/python3/bin/python3 manage.py migrate
# /usr/local/python3/bin/python3 manage.py createsuperuser  #创建管理员账户与密码
# 如果出现错误ImportError: cannot import name 'LDAPError'
pip3 uninstall python-ldap
pip3 install --upgrade python-ldap
八、启动部署平台# echo_supervisord_conf > /etc/supervisord.conf
# export PYTHONOPTIMIZE=1
# vim /etc/supervisord.conf
最后添加，/var/log/celery-*.log这些是日志文件，如果有错误请注意查看，directory的值是代码路径
[program:celery-worker-default]
environment=C_FORCE_ROOT=""true"",PYTHONOPTIMIZE=1
command=/usr/local/python3/bin/celery -A OpsManage worker --loglevel=info -E -Q default -n worker-default@%%h
directory=/mnt/OpsManage
stdout_logfile=/var/log/celery-worker-default.log
autostart=true
autorestart=true
redirect_stderr=true
stopsignal=QUIT
numprocs=1

[program:celery-worker-ansible]
environment=C_FORCE_ROOT=""true"",PYTHONOPTIMIZE=1
command=/usr/local/python3/bin/celery -A OpsManage worker --loglevel=info -E -Q ansible -n worker-ansible@%%h
directory=/mnt/OpsManage
stdout_logfile=/var/log/celery-worker-ansible.log
autostart=true
autorestart=true
redirect_stderr=true
stopsignal=QUIT
numprocs=1

[program:celery-beat]
environment=C_FORCE_ROOT=""true"",PYTHONOPTIMIZE=1
command=/usr/local/python3/bin/celery -A OpsManage  beat --loglevel=info --scheduler django_celery_beat.schedulers:DatabaseScheduler
directory=/mnt/OpsManage
stdout_logfile=/var/log/celery-beat.log
autostart=true
autorestart=true
redirect_stderr=true
stopsignal=QUIT
numprocs=1

[program:apply-task]
environment=C_FORCE_ROOT=""true"",PYTHONOPTIMIZE=1
command=/usr/local/python3/bin/python3 manage.py apply_task
directory=/mnt/OpsManage
stdout_logfile=/var/log/apply-task.log
autostart=true
autorestart=true
redirect_stderr=true
stopsignal=QUIT
numprocs=1


[program:opsmanage-web]
command=/usr/local/python3/bin/python3 manage.py runserver 0.0.0.0:8000 --http_timeout 1200
directory=/mnt/OpsManage
stdout_logfile=/var/log/opsmanage-web.log   
stderr_logfile=/var/log/opsmanage-web-error.log
autostart=true
autorestart=true
redirect_stderr=true
stopsignal=QUIT



启动celery
# supervisord -c /etc/supervisord.conf
# supervisorctl status #要检查是否都是running状态，uptime是不是递增


配置nginx（请注意服务器上面是否安装了Nginx）：
# vim /etc/nginx/conf.d/opsmanage.conf 
server {
    listen 80 ;
    server_name 192.168.1.233;

    access_log /var/log/nginx/opsmanage_access.log;
    error_log /var/log/nginx/opsmanage_error.log;

    location / {
        proxy_next_upstream off;
        proxy_set_header    X-Real-IP           $remote_addr;
        proxy_set_header    X-Forwarded-For     $proxy_add_x_forwarded_for;
        proxy_set_header    Host                $host;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection ""upgrade"";
        proxy_pass http://192.168.1.233:8000$request_uri;
    }
    location /static/ {
        expires 30d;
        autoindex on;
        add_header Cache-Control private;
        alias /mnt/OpsManage/static/;
     }
    location /media/navbar/ {
        expires 30d;
        autoindex on;
        add_header Cache-Control private;
        alias /mnt/OpsManage/upload/navbar/;
     }   
	location /media/avatar/ {
        expires 30d;
        autoindex on;
        add_header Cache-Control private;
        alias /mnt/OpsManage/upload/avatar/;
     }     
    location /media/apply/icon/ {
        expires 30d;
        autoindex on;
        add_header Cache-Control private;
        alias /mnt/OpsManage/upload/apply/icon/;
     }     
}
# nginx -t  #检查配置文件
# service start nginx			 #CentOS 6
# systemctl start nginx.service  #CentOS 7
九、使用OpsManage分布式任务调度功能，不使用的话可以不进行下面的步骤# mkdir -p /usr/local/opsched
# cp /mnt/OpsManage/opsched/* /usr/local/opsched/
# vim /usr/local/opsched/sched.conf 
# 注意修改里面secret跟ops_address的值，修改成自己的配置
# /usr/local/opsched/opsched -f /usr/local/opsched/sched.conf -a check      #检查配置看看有没有报错
# /usr/local/opsched/opsched -f /usr/local/opsched/sched.conf -a runserver  #正式运行 
十、demo地址URL: http://42.194.214.22:8000/
UserName：demo
Password：demo
"
https://github.com/nwojke/deep_sort,Simple Online Realtime Tracking with a Deep Association Metric,"Deep SORTIntroductionThis repository contains code for Simple Online and Realtime Tracking with a Deep Association Metric (Deep SORT).We extend the original  algorithm tointegrate appearance information based on a deep appearance descriptor.See the  for more information.DependenciesThe code is compatible with Python 2.7 and 3. The following dependencies areneeded to run the tracker:Additionally, feature generation requires TensorFlow (>= 1.0).InstallationFirst, clone the repository:git clone https://github.com/nwojke/deep_sort.git
Then, download pre-generated detections and the CNN checkpoint file from.NOTE: The candidate object locations of our pre-generated detections aretaken from the following paper:F. Yu, W. Li, Q. Li, Y. Liu, X. Shi, J. Yan. POI: Multiple Object Tracking with
High Performance Detection and Appearance Feature. In BMTT, SenseTime Group
Limited, 2016.
We have replaced the appearance descriptor with a custom deep convolutionalneural network (see below).Running the trackerThe following example starts the tracker on one of thesequences.We assume resources have been extracted to the repository root directory andthe MOT16 benchmark data is in :python deep_sort_app.py \
    --sequence_dir=./MOT16/test/MOT16-06 \
    --detection_file=./resources/detections/MOT16_POI_test/MOT16-06.npy \
    --min_confidence=0.3 \
    --nn_budget=100 \
    --display=True
Check  for an overview of available options.There are also scripts in the repository to visualize results, generate videos,and evaluate the MOT challenge benchmark.Generating detectionsBeside the main tracking application, this repository contains a script togenerate features for person re-identification, suitable to compare the visualappearance of pedestrian bounding boxes using cosine similarity.The following example generates these features from standard MOT challengedetections. Again, we assume resources have been extracted to the repositoryroot directory and MOT16 data is in :python tools/generate_detections.py \
    --model=resources/networks/mars-small128.pb \
    --mot_dir=./MOT16/train \
    --output_dir=./resources/detections/MOT16_train
The model has been generated with TensorFlow 1.5. If you run intoincompatibility, re-export the frozen inference graph to obtain a new that is compatible with your version:python tools/freeze_model.py
The  stores for each sequence of the MOT16 dataseta separate binary file in NumPy native format. Each file contains an array ofshape , where N is the number of detections in the corresponding MOTsequence. The first 10 columns of this array contain the raw MOT detectioncopied over from the input file. The remaining 128 columns store the appearancedescriptor. The files generated by this command can be used as input for the.NOTE: If  raises a TensorFlow error,try passing an absolute path to the  argument. This might help insome cases.Training the modelTo train the deep association metric model we used a novel  approach which is provided as a separate repository.Highlevel overview of source filesIn the top-level directory are executable scripts to execute, evaluate, andvisualize the tracker. The main entry point is in .This file runs the tracker on a MOTChallenge sequence.In package  is the main tracking code:The  expects detections in a custom format, stored in .npyfiles. These can be computed from MOTChallenge detections using. We also provide.Citing DeepSORTIf you find this repo useful in your research, please consider citing the following papers:@inproceedings{Wojke2017simple,
  title={Simple Online and Realtime Tracking with a Deep Association Metric},
  author={Wojke, Nicolai and Bewley, Alex and Paulus, Dietrich},
  booktitle={2017 IEEE International Conference on Image Processing (ICIP)},
  year={2017},
  pages={3645--3649},
  organization={IEEE},
  doi={10.1109/ICIP.2017.8296962}
}

@inproceedings{Wojke2018deep,
  title={Deep Cosine Metric Learning for Person Re-identification},
  author={Wojke, Nicolai and Bewley, Alex},
  booktitle={2018 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  year={2018},
  pages={748--756},
  organization={IEEE},
  doi={10.1109/WACV.2018.00087}
}
"
https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow,"Simple Reinforcement learning tutorials, 莫烦Python 中文AI教学","Reinforcement Learning Methods and TutorialsIn these tutorials for reinforcement learning, it covers from the basic RL algorithms to advanced algorithms developed recent years.If you speak Chinese, visit As many requests about making these tutorials available in English, please find them in this playlist: ()Table of ContentsSome RL NetworksDonationIf this does help you, please consider donating to support me for better tutorials. Any contribution is greatly appreciated!"
https://github.com/lanpa/tensorboardX,"tensorboard for pytorch (and chainer, mxnet, numpy, ...)","tensorboardXWrite TensorBoard events with simple function call.The current release (v2.5) is tested on anaconda3, with PyTorch 1.11.0 / torchvision 0.12 / tensorboard 2.9.0.Installor build from source:You can optionally install  to speed up.Starting from tensorboardX 2.1, You need to install  for the  function (200x speedup).Example# demo.py

import torch
import torchvision.utils as vutils
import numpy as np
import torchvision.models as models
from torchvision import datasets
from tensorboardX import SummaryWriter

resnet18 = models.resnet18(False)
writer = SummaryWriter()
sample_rate = 44100
freqs = [262, 294, 330, 349, 392, 440, 440, 440, 440, 440, 440]

for n_iter in range(100):

    dummy_s1 = torch.rand(1)
    dummy_s2 = torch.rand(1)
    # data grouping by `slash`
    writer.add_scalar('data/scalar1', dummy_s1[0], n_iter)
    writer.add_scalar('data/scalar2', dummy_s2[0], n_iter)

    writer.add_scalars('data/scalar_group', {'xsinx': n_iter * np.sin(n_iter),
                                             'xcosx': n_iter * np.cos(n_iter),
                                             'arctanx': np.arctan(n_iter)}, n_iter)

    dummy_img = torch.rand(32, 3, 64, 64)  # output from network
    if n_iter % 10 == 0:
        x = vutils.make_grid(dummy_img, normalize=True, scale_each=True)
        writer.add_image('Image', x, n_iter)

        dummy_audio = torch.zeros(sample_rate * 2)
        for i in range(x.size(0)):
            # amplitude of sound should in [-1, 1]
            dummy_audio[i] = np.cos(freqs[n_iter // 10] * np.pi * float(i) / float(sample_rate))
        writer.add_audio('myAudio', dummy_audio, n_iter, sample_rate=sample_rate)

        writer.add_text('Text', 'text logged at step:' + str(n_iter), n_iter)

        for name, param in resnet18.named_parameters():
            writer.add_histogram(name, param.clone().cpu().data.numpy(), n_iter)

        # needs tensorboard 0.4RC or later
        writer.add_pr_curve('xoxo', np.random.randint(2, size=100), np.random.rand(100), n_iter)

dataset = datasets.MNIST('mnist', train=False, download=True)
images = dataset.test_data[:100].float()
label = dataset.test_labels[:100]

features = images.view(100, 784)
writer.add_embedding(features, metadata=label, label_img=images.unsqueeze(1))

# export scalar data to JSON for external processing
writer.export_scalars_to_json(""./all_scalars.json"")
writer.close()
ScreenshotsUsing TensorboardX with CometTensorboardX now supports logging directly to . Comet is a free cloud based solution that allows you to automatically track, compare and explain your experiments. It adds a lot of functionality on top of tensorboard such as dataset management, diffing experiments, seeing the code that generated the results and more.This works out of the box and just require an additional line of code. See a full code example in this TweaksTo add more ticks for the slider (show more image history), check https://github.com/lanpa/tensorboardX/issues/44 orhttps://github.com/tensorflow/tensorboard/pull/1138Reference"
https://github.com/rafaelpadilla/Object-Detection-Metrics,Most popular metrics used to evaluate object detection algorithms.,"CitationIf you use this code for your research, please consider citing:@Article{electronics10030279,
AUTHOR = {Padilla, Rafael and Passos, Wesley L. and Dias, Thadeu L. B. and Netto, Sergio L. and da Silva, Eduardo A. B.},
TITLE = {A Comparative Analysis of Object Detection Metrics with a Companion Open-Source Toolkit},
JOURNAL = {Electronics},
VOLUME = {10},
YEAR = {2021},
NUMBER = {3},
ARTICLE-NUMBER = {279},
URL = {https://www.mdpi.com/2079-9292/10/3/279},
ISSN = {2079-9292},
DOI = {10.3390/electronics10030279}
}
Download the paper  or .@INPROCEEDINGS {padillaCITE2020,
    author    = {R. {Padilla} and S. L. {Netto} and E. A. B. {da Silva}},
    title     = {A Survey on Performance Metrics for Object-Detection Algorithms}, 
    booktitle = {2020 International Conference on Systems, Signals and Image Processing (IWSSIP)}, 
    year      = {2020},
    pages     = {237-242},}
Download the paper Attention! A new version of this tool is available The new version includes all COCO metrics, supports other file formats, provides a User Interface (UI) to guide the evaluation process, and presents the STT-AP metric to evaluate object detection in videos. Metrics for object detectionThe motivation of this project is the lack of consensus used by different works and implementations concerning the evaluation metrics of the object detection problem. Although on-line competitions use their own metrics to evaluate the task of object detection, just some of them offer reference code snippets to calculate the accuracy of the detected objects.Researchers who want to evaluate their work using different datasets than those offered by the competitions, need to implement their own version of the metrics. Sometimes a wrong or different implementation can create different and biased results. Ideally, in order to have trustworthy benchmarking among different approaches, it is necessary to have a flexible implementation that can be used by everyone regardless the dataset used.  This project provides easy-to-use functions implementing the same metrics used by the the most popular competitions of object detection. Our implementation does not require modifications of your detection model to complicated input formats, avoiding conversions to XML or JSON files. We simplified the input data (ground truth bounding boxes and detected bounding boxes) and gathered in a single project the main metrics used by the academia and challenges. Our implementation was carefully compared against the official implementations and our results are exactly the same.   In the topics below you can find an overview of the most popular metrics used in different competitions and works, as well as samples showing how to use our code.Table of contents Different competitions, different metricsImportant definitionsIntersection Over Union (IOU)Intersection Over Union (IOU) is a measure based on Jaccard Index that evaluates the overlap between two bounding boxes. It requires a ground truth bounding box  and a predicted bounding box . By applying the IOU we can tell if a detection is valid (True Positive) or not (False Positive).  IOU is given by the overlapping area between the predicted bounding box and the ground truth bounding box divided by the area of union between them:  The image below illustrates the IOU between a ground truth bounding box (in green) and a detected bounding box (in red).True Positive, False Positive, False Negative and True NegativeSome basic concepts used by the metrics:  threshold: depending on the metric, it is usually set to 50%, 75% or 95%.PrecisionPrecision is the ability of a model to identify only the relevant objects. It is the percentage of correct positive predictions and is given by:RecallRecall is the ability of a model to find all the relevant cases (all ground truth bounding boxes). It is the percentage of true positive detected among all relevant ground truths and is given by:MetricsIn the topics below there are some comments on the most popular metrics used for object detection.Precision x Recall curveThe Precision x Recall curve is a good way to evaluate the performance of an object detector as the confidence is changed by plotting a curve for each object class. An object detector of a particular class is considered good if its precision stays high as recall increases, which means that if you vary the confidence threshold, the precision and recall will still be high. Another way to identify a good object detector is to look for a detector that can identify only relevant objects (0 False Positives = high precision), finding all ground truth objects (0 False Negatives = high recall).  A poor object detector needs to increase the number of detected objects (increasing False Positives = lower precision) in order to retrieve all ground truth objects (high recall). That's why the Precision x Recall curve usually starts with high precision values, decreasing as recall increases. You can see an example of the Prevision x Recall curve in the next topic (Average Precision). This kind of curve is used by the PASCAL VOC 2012 challenge and is available in our implementation.  Average PrecisionAnother way to compare the performance of object detectors is to calculate the area under the curve (AUC) of the Precision x Recall curve. As AP curves are often zigzag curves going up and down, comparing different curves (different detectors) in the same plot usually is not an easy task - because the curves tend to cross each other much frequently. That's why Average Precision (AP), a numerical metric, can also help us compare different detectors. In practice AP is the precision averaged across all recall values between 0 and 1.  From 2010 on, the method of computing AP by the PASCAL VOC challenge has changed. Currently, the interpolation performed by PASCAL VOC challenge uses all data points, rather than interpolating only 11 equally spaced points as stated in their . As we want to reproduce their default implementation, our default code (as seen further) follows their most recent application (interpolating all data points). However, we also offer the 11-point interpolation approach. 11-point interpolationThe 11-point interpolation tries to summarize the shape of the Precision x Recall curve by averaging the precision at a set of eleven equally spaced recall levels [0, 0.1, 0.2, ... , 1]:withwhere  is the measured precision at recall .Instead of using the precision observed at each point, the AP is obtained by interpolating the precision only at the 11 levels  taking the maximum precision whose recall value is greater than .Interpolating all pointsInstead of interpolating only in the 11 equally spaced points, you could interpolate through all points  in such way that:withwhere  is the measured precision at recall .In this case, instead of using the precision observed at only few points, the AP is now obtained by interpolating the precision at each level,  taking the maximum precision whose recall value is greater or equal than . This way we calculate the estimated area under the curve.To make things more clear, we provided an example comparing both interpolations.An ilustrated exampleAn example helps us understand better the concept of the interpolated average precision. Consider the detections below:There are 7 images with 15 ground truth objects represented by the green bounding boxes and 24 detected objects represented by the red bounding boxes. Each detected object has a confidence level and is identified by a letter (A,B,...,Y).  The following table shows the bounding boxes with their corresponding confidences. The last column identifies the detections as TP or FP. In this example a TP is considered if IOU  30%, otherwise it is a FP. By looking at the images above we can roughly tell if the detections are TP or FP.In some images there are more than one detection overlapping a ground truth (Images 2, 3, 4, 5, 6 and 7). For those cases, the predicted box with the highest IOU is considered TP (e.g. in image 1 ""E"" is TP while ""D"" is FP because IOU between E and the groundtruth is greater than the IOU between D and the groundtruth). This rule is applied by the PASCAL VOC 2012 metric: ""e.g. 5 detections (TP) of a single object is counted as 1 correct detection and 4 false detections”.The Precision x Recall curve is plotted by calculating the precision and recall values of the accumulated TP or FP detections.  For this, first we need to order the detections by their confidences, then we calculate the precision and recall for each accumulated detection as shown in the table below (Note that for recall computation, the denominator term (""Acc TP + Acc FN"" or ""All ground truths"") is constant at 15 since GT boxes are constant irrespective of detections).: Example computation for the 2nd row (Image 7):  Precision = TP/(TP+FP) = 1/2 = 0.5 and Recall = TP/(TP+FN) = 1/15 = 0.066Plotting the precision and recall values we have the following Precision x Recall curve:As mentioned before, there are two different ways to measure the interpolted average precision: 11-point interpolation and interpolating all points. Below we make a comparisson between them:Calculating the 11-point interpolationThe idea of the 11-point interpolated average precision is to average the precisions at a set of 11 recall levels (0,0.1,...,1). The interpolated precision values are obtained by taking the maximum precision whose recall value is greater than its current recall value as follows: By applying the 11-point interpolation, we have:  Calculating the interpolation performed in all pointsBy interpolating all points, the Average Precision (AP) can be interpreted as an approximated AUC of the Precision x Recall curve. The intention is to reduce the impact of the wiggles in the curve. By applying the equations presented before, we can obtain the areas as it will be demostrated here. We could also visually have the interpolated precision points by looking at the recalls starting from the highest (0.4666) to 0 (looking at the plot from right to left) and, as we decrease the recall, we collect the precision values that are the highest as shown in the image below:Looking at the plot above, we can divide the AUC into 4 areas (A1, A2, A3 and A4):Calculating the total area, we have the AP:        The results between the two different interpolation methods are a little different: 24.56% and 26.84% by the every point interpolation and the 11-point interpolation respectively.  Our default implementation is the same as VOC PASCAL: every point interpolation. If you want to use the 11-point interpolation, change the functions that use the argument  to .   If you want to reproduce these results, see the [<marko.inline.RawText object at 0x000001592FDB1548>].How to use this projectThis project was created to evaluate your detections in a very easy way. If you want to evaluate your algorithm with the most used object detection metrics, you are in the right place.   and  are practical examples demonstrating how to access directly the core functions of this project, providing more flexibility on the usage of the metrics. But if you don't want to spend your time understanding our code, see the instructions below to easily evaluate your detections:  Follow the steps below to start evaluating your detections:Create the ground truth filesIf you prefer, you can also have your bounding boxes in the format:  (see here  how to use it). In this case, your ""2008_000034.txt"" would be represented as:bottle 6 234 39 128
person 1 156 102 180
person 36 111 162 305
person 91 42 247 458
Create your detection filesAlso if you prefer, you could have your bounding boxes in the format: .Optional argumentsOptional arguments:| Argument &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;| Description | Example | Default ||:-------------:|:-----------:|:-----------:|:-----------:|| , |	show help message |  | ||  , | check version |  | || , | folder that contains the ground truth bounding boxes files |  | || , | folder that contains your detected bounding boxes files |  | || , | IOU thershold that tells if a detection is TP or FP |  |  ||  | format of the coordinates of the ground truth bounding boxes  |  |  ||  | format of the coordinates of the detected bounding boxes  |  |  | ||  | reference of the ground truth bounding bounding box coordinates.If the annotated coordinates are relative to the image size (as used in YOLO), set it to .If the coordinates are absolute values, not depending to the image size, set it to  |   |  ||  | reference of the detected bounding bounding box coordinates.If the coordinates are relative to the image size (as used in YOLO), set it to .If the coordinates are absolute values, not depending to the image size, set it to  |  |  ||  | image size in the format  <int,int>.Required if  or  is set to  |  || , | folder where the plots are saved |  |  || , | if present no plot is shown during execution |  | not presented.Therefore, plots are shown |   (*) set  and/or  if format is . Set to  and/or   if format is .References"
https://github.com/fossasia/open-event-server,The Open Event Organizer Server to Manage Events https://test-api.eventyay.com,"Open Event ServerIt offers features for events with several tracks and venues. Event managers can create invitation forms for speakers and build schedules in a drag and drop interface. The event information is stored in a database. The system provides API endpoints to fetch the data, and to modify and update it. Organizers can import and export event data in a standard compressed file format that includes the event data in JSON and binary media files like images and audio.The Open Event Server exposes a well documented  Compliant  that can be used by external services (like the Open Event App generators and the frontend) to access & manipulate the data.API Documentation:CommunicationDemo VersionA demo version is automatically deployed from our repositories:InstallationThe Open Event Server can be easily deployed on a variety of platforms. Detailed platform-specific installation instructions have been provided below.One-click Heroku deployment is also available:Technology StackPlease get familiar with the components of the project in order to be able to contribute.ComponentsNote that open-event-server works with Python 3.8 at the moment.External Service DependenciesOAuth Social AuthenticationOAuth is used to get information from Facebook and Google accounts, that enables users to sign in with their respective credentials:TwitterTwitter feed integration is provided in the public event pages.Required keys can be obtained from InstagramIt is possible to extend the functionality and offer images from Instagram in the event service.Required keys can be obtained from .Google MapsGoogle maps is used to get information about location (info about country, city, latitude and longitude).Required keys can be obtained from .Media Storage - Local/Amazon S3/Google CloudMedia (like audio, avatars and logos) can be stored either Locally, on Amazon S3 or on Google Storage.Emails - SMTP/SendgridThe server can send emails via SMTP or using the sendgrid API.Heroku APIIf the application is deployed on Heroku, we use the heroku API to obtain the latest release and also to display the heroku.The required token can be obtained from .Payment GatewaysFor ticket sales the service integrates payment gateways:Data AccessImport & ExportImport:Open Event server supports multiple formats as a valid source for import.Export:The event data and the sessions can be exported in various formats.RolesThe system has two kinds of role type.Read more .DevelopmentInitial setupPython and Poetry installatioonWe use Python 3.8. If your operating system does not provide Python 3.8 out of thebox, it is best installed using .For Mac users, see  for more info.$ brew install pyenv
$ pyenv init # follow instructions to add run commands to your environment
After editing your environment file, reload your shell and navigate to this repo, then install  to be used locally:$ pyenv install 3.8.17
$ cd ...your../open-event-server/
$ pyenv local 3.8.17
Now the Python version should automatically change when used within open-event-server.We also expect  being available.Package setupChange into the  directory, and execute the following commands:Activate Python 3.8.17 locally$ pyenv local 3.8.17
Install dependencies using poetry$ poetry install --with dev
Activate the pre-commit hook$ poetry run pre-commit install
With that every git commit will be checked/formatted with various tools beforebeing actually committed. Development ModeTo enable development mode (development Flask config), set  environment variable to ""config.DevelopmentConfig"".export APP_CONFIG=config.DevelopmentConfig
Model updates & migrationsWhen writing changes to models. Use migrations. # To generate a migration after doing a model update
 python3 manage.py db migrate

 # To sync Database
 python3 manage.py db upgrade

 # To rollback
 python3 manage.py db downgrade
When checking in code for models, please update migrations as well.API documentationThe api is documented using . First, generate the description/blueprint  file using:npx aglio --input docs/api/api_blueprint_source.apib --compile --output docs/api/api_blueprint.apib # generate the description .apib file

Local changes to the description can be viewed using e.g. the :gem install apiaryio # dependency
apiary preview --path docs/api/api_blueprint.apib # opens browser with generated file
TestingClone the repo and set up the server according to the steps listed. Make sure you have installed  and all the dependencies required for testing by running# Install Poetry
curl -sSL https://install.python-poetry.org | python -
source ~/.profile

# Install Python dependencies
poetry install

# Activate project's virtual environment
poetry shell
Running unit tests./scripts/test_db.sh
And set appropriate value of  in TEST_DATABASE_URL=postgresql://test@localhost:5433/test
pytest tests/
Running robot framework testsrobot -v SERVER:{server_name} -v SUPERUSER_USERNAME:{super_user_email_here} -v SUPERUSER_PASSWORD:{super_user_password} tests/robot
Change all the parameters inside  as per your local server. The final command would look like:robot -v SERVER:localhost:5000 -v SUPERUSER_USERNAME:test@opev.net -v SUPERUSER_PASSWORD:test_password tests/robot
Pre-commits guideGit hook scripts are useful for identifying simple issues before submission to code review.Install the git hook scripts:$ pre-commit install
pre-commit installed at .git/hooks/pre-commit
For configuration, LoggingCertain information is being logged and stored in the database for future reference, resolving conflicts in case of hacks and for maintaining an overview of the system. Read more about .Internationalization (i18n)Open Event is being translated using Weblate, a web tool designed to ease translating for both developers and translators.If you would like to contribute to the translation of Open Event, you need to .Once you have activated your account just proceed to the .Contributions, Bug Reports, Feature RequestsThis is an Open Source project and we would be happy to see contributors who report bugs and file feature requests submitting pull requests as well. Please report issues here https://github.com/fossasia/open-event-server/issues. It is also recommended to go through the  in order to get a basic understanding of the ecosystem.Branch PolicyWe have the following branches :Release PolicyThe tentative release policy, for now (since there is a lot of activity and a lot of bugs), is an alpha release every Monday and Friday (since we see more activity on weekends). So, any bug-fixes will not be reflected at eventyay.com until a new release is made in the master branch.Contributions Best PracticesCommitsFeature Requests and Bug ReportsJoin the developmentLicenseThis project is currently licensed under the [<marko.inline.RawText object at 0x000001592FDB3988>]."
https://github.com/bojone/bert4keras,keras implement of transformers for humans,"bert4keras说明这是笔者重新实现的keras版的transformer模型库，致力于用尽可能清爽的代码来实现结合transformer和keras。本项目的初衷是为了修改、定制上的方便，所以可能会频繁更新。因此欢迎star，但不建议fork，因为你fork下来的版本可能很快就过期了。功能目前已经实现：使用安装稳定版：pip install bert4keras
安装最新版：pip install git+https://www.github.com/bojone/bert4keras.git
使用例子请参考examples目录。之前基于keras-bert给出的例子，仍适用于本项目，只需要将的加载方式换成本项目的。理论上兼容Python2和Python3，兼容tensorflow 1.14+和tensorflow 2.x，实验环境是Python 2.7、Tesorflow 1.14+以及Keras 2.3.1（已经在2.2.4、2.3.0、2.3.1、tf.keras下测试通过）。为了获得最好的体验，建议你使用Tensorflow 1.14 + Keras 2.3.1组合。当然，乐于贡献的朋友如果发现了某些bug的话，也欢迎指出修正甚至Pull Requests～权重目前支持加载的权重：注意事项更新背景之前一直用CyberZHG大佬的keras-bert，如果纯粹只是为了在keras下对bert进行调用和fine tune来说，keras-bert已经足够能让人满意了。然而，如果想要在加载官方预训练权重的基础上，对bert的内部结构进行修改，那么keras-bert就比较难满足我们的需求了，因为keras-bert为了代码的复用性，几乎将每个小模块都封装为了一个单独的库，比如keras-bert依赖于keras-transformer，而keras-transformer依赖于keras-multi-head，keras-multi-head依赖于keras-self-attention，这样一重重依赖下去，改起来就相当头疼了。所以，我决定重新写一个keras版的bert，争取在几个文件内把它完整地实现出来，减少这些依赖性，并且保留可以加载官方预训练权重的特性。鸣谢感谢CyberZHG大佬实现的keras-bert，本实现有不少地方参考了keras-bert的源码，在此衷心感谢大佬的无私奉献。相关：一个跟bert4keras风格很相似的pytorch-based的transofrmer库，使用pytorch的读者可以尝试。引用@misc{bert4keras,
  title={bert4keras},
  author={Jianlin Su},
  year={2020},
  howpublished={\url{https://bert4keras.spaces.ac.cn}},
}
交流QQ交流群：808623966，微信群请加机器人微信号spaces_ac_cn"
https://github.com/ehForwarderBot/ehForwarderBot,An extensible message tunneling chat bot framework. Delivers messages to and from multiple platforms and remotely control your accounts.,"EH Forwarder Bot.. image:: https://img.shields.io/badge/Python->%3D%203.6-blue.svg:alt: Python >= 3.6:target: https://www.python.org/.. image:: https://img.shields.io/badge/GitHub-Discussions-lightgrey?logo=github:alt: GitHub Discussions:target: https://github.com/ehForwarderBot/ehForwarderBot/discussions.. image:: https://img.shields.io/badge/-Telegram-blue.svg?logo=data:image/svg%2Bxml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCAyNCAyNCI%2BPHBhdGggZmlsbD0iI2ZmZiIgZD0iTTkuNzgsMTguNjVMMTAuMDYsMTQuNDJMMTcuNzQsNy41QzE4LjA4LDcuMTkgMTcuNjcsNy4wNCAxNy4yMiw3LjMxTDcuNzQsMTMuM0wzLjY0LDEyQzIuNzYsMTEuNzUgMi43NSwxMS4xNCAzLjg0LDEwLjdMMTkuODEsNC41NEMyMC41NCw0LjIxIDIxLjI0LDQuNzIgMjAuOTYsNS44NEwxOC4yNCwxOC42NUMxOC4wNSwxOS41NiAxNy41LDE5Ljc4IDE2Ljc0LDE5LjM2TDEyLjYsMTYuM0wxMC42MSwxOC4yM0MxMC4zOCwxOC40NiAxMC4xOSwxOC42NSA5Ljc4LDE4LjY1WiIgLz48L3N2Zz4=:alt: Telegram:target: https://telegram.me/efbsupport.. image:: https://readthedocs.org/projects/ehforwarderbot/badge/?version=latest:alt: Documentation:target: https://ehforwarderbot.readthedocs.io/en/latest/.. image:: https://github.com/ehForwarderBot/ehforwarderbot/workflows/Tests/badge.svg:alt: Tests status:target: https://github.com/ehForwarderBot/ehforwarderbot/actions.. image:: https://img.shields.io/pypi/v/ehforwarderbot.svg:alt: PyPI release:target: https://pypi.org/project/ehforwarderbot/.. image:: https://pepy.tech/badge/ehforwarderbot/month:alt: Downloads per month:target: https://pepy.tech/project/ehforwarderbot.. image:: https://img.shields.io/codacy/grade/3b2555f9134844e3b01b00700bc43eeb.svg:alt: Codacy grade:target: https://www.codacy.com/app/blueset/ehForwarderBot.. image:: https://d322cqt584bo4o.cloudfront.net/ehforwarderbot/localized.svg:alt: Translate this project:target: https://crowdin.com/project/ehforwarderbot/.. image:: https://github.com/ehForwarderBot/ehforwarderbot/raw/master/banner.png:alt: BannerCodename EH Forwarder Bot (EFB) is an extensible message tunneling chatbot framework which delivers messages to and from multiple platforms andremotely control your accounts.Read the . For tips, tricks and community contributed... _project wiki: https://efb.1a23.studio/wiki.. a raw:: htmlGetting StartedFeel like contributing?Everyone is welcomed to raise an issue or submit a pull request,just remember to read through and follow thecontribution guideline before you do so.Related articles.. _Idea Group Chat Tunneling (Sync) with EH Forwarder Bot: https://blog.1a23.com/2017/01/28/Idea-Group-Chat-Tunneling-Sync-with-EH-Forwarder-Bot/.. _What’s so new in EH Forwarder Bot 2 (and its modules): https://blog.1a23.com/2018/02/28/What%E2%80%99s-so-new-in-EH-Forwarder-Bot-2-and-its-modules/LicenseEFB framework is licensed under _ orlater versions::EH Forwarder Bot: An extensible message tunneling chat bot framework.
Copyright (C) 2016 - 2020 Eana Hufwe, and the EH Forwarder Bot contributors
All rights reserved.

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU Affero General Public License as
published by the Free Software Foundation, either version 3 of the
License, or any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU Affero General Public License
along with this program.  If not, see <http://www.gnu.org/licenses/>.
Translation supportEFB supports translated user interface prompts,by setting the locale environmental variable (,,  or ) to one of our. Our documentation is also available in different... _supported languages: https://crowdin.com/project/ehforwarderbot/.. _our Crowdin page: https://crowdin.com/project/ehforwarderbot/.. _Install modules: https://ehforwarderbot.readthedocs.io/en/latest/getting-started.html.. _module repository: https://efb-modules.1a23.studio.. _Documentation: https://ehforwarderbot.readthedocs.io/.. _GNU Affero General Public License 3.0: https://www.gnu.org/licenses/agpl-3.0.txt"
https://github.com/satwikkansal/wtfpython,What the f*ck Python? 😱,"Translations:  |  |  |  |  |  | Other modes:  |  | Python, being a beautifully designed high-level and interpreter-based programming language, provides us with many features for the programmer's comfort. But sometimes, the outcomes of a Python snippet may not seem obvious at first sight.Here's a fun project attempting to explain what exactly is happening under the hood for some counter-intuitive snippets and lesser-known features in Python.While some of the examples you see below may not be WTFs in the truest sense, but they'll reveal some of the interesting parts of Python that you might be unaware of. I find it a nice way to learn the internals of a programming language, and I believe that you'll find it interesting too!If you're an experienced Python programmer, you can take it as a challenge to get most of them right in the first attempt. You may have already experienced some of them before, and I might be able to revive sweet old memories of yours! :sweat_smile:PS: If you're a returning reader, you can learn about the new modifications  (the examples marked with asterisk are the ones added in the latest major revision). So, here we go...Table of ContentsStructure of the ExamplesAll the examples are structured like below:Note: All the examples are tested on Python 3.5.2 interactive interpreter, and they should work for all the Python versions unless explicitly specified before the output.UsageA nice way to get the most out of these examples, in my opinion, is to read them in sequential order, and for every example:PS: You can also read WTFPython at the command line using the ,$ pip install wtfpython -U
$ wtfpython
👀 ExamplesSection: Strain your brain!▶ First things first! *For some reason, the Python 3.8's ""Walrus"" operator () has become quite popular. Let's check it out,1# Python version 3.8+

>>> a = ""wtf_walrus""
>>> a
'wtf_walrus'

>>> a := ""wtf_walrus""
File ""<stdin>"", line 1
    a := ""wtf_walrus""
      ^
SyntaxError: invalid syntax

>>> (a := ""wtf_walrus"") # This works though
'wtf_walrus'
>>> a
'wtf_walrus'
2 # Python version 3.8+

>>> a = 6, 9
>>> a
(6, 9)

>>> (a := 6, 9)
(6, 9)
>>> a
6

>>> a, b = 6, 9 # Typical unpacking
>>> a, b
(6, 9)
>>> (a, b = 16, 19) # Oops
  File ""<stdin>"", line 1
    (a, b = 16, 19)
          ^
SyntaxError: invalid syntax

>>> (a, b := 16, 19) # This prints out a weird 3-tuple
(6, 16, 19)

>>> a # a is still unchanged?
6

>>> b
16
💡 ExplanationQuick walrus operator refresherThe Walrus operator () was introduced in Python 3.8, it can be useful in situations where you'd want to assign values to variables within an expression.def some_func():
        # Assume some expensive computation here
        # time.sleep(1000)
        return 5

# So instead of,
if some_func():
        print(some_func()) # Which is bad practice since computation is happening twice

# or
a = some_func()
if a:
    print(a)

# Now you can concisely write
if a := some_func():
        print(a)
Output (> 3.8):5
5
5
This saved one line of code, and implicitly prevented invoking  twice.▶ Strings can be tricky sometimes1>>> a = ""some_string""
>>> id(a)
140420665652016
>>> id(""some"" + ""_"" + ""string"") # Notice that both the ids are same.
140420665652016
2>>> a = ""wtf""
>>> b = ""wtf""
>>> a is b
True

>>> a = ""wtf!""
>>> b = ""wtf!""
>>> a is b
False

3>>> a, b = ""wtf!"", ""wtf!""
>>> a is b # All versions except 3.7.x
True

>>> a = ""wtf!""; b = ""wtf!""
>>> a is b # This will print True or False depending on where you're invoking it (python shell / ipython / as a script)
False
# This time in file some_file.py
a = ""wtf!""
b = ""wtf!""
print(a is b)

# prints True when the module is invoked!
4Output (< Python3.7 )>>> 'a' * 20 is 'aaaaaaaaaaaaaaaaaaaa'
True
>>> 'a' * 21 is 'aaaaaaaaaaaaaaaaaaaaa'
False
Makes sense, right?💡 Explanation:▶ Be careful with chained operations>>> (False == False) in [False] # makes sense
False
>>> False == (False in [False]) # makes sense
False
>>> False == False in [False] # now what?
True

>>> True is False == False
False
>>> False is False is False
True

>>> 1 > 0 < 1
True
>>> (1 > 0) < 1
False
>>> 1 > (0 < 1)
False
💡 Explanation:As per https://docs.python.org/3/reference/expressions.html#comparisonsWhile such behavior might seem silly to you in the above examples, it's fantastic with stuff like  and .▶ How not to use  operatorThe following is a very famous example present all over the internet.1>>> a = 256
>>> b = 256
>>> a is b
True

>>> a = 257
>>> b = 257
>>> a is b
False
2>>> a = []
>>> b = []
>>> a is b
False

>>> a = tuple()
>>> b = tuple()
>>> a is b
True
3Output>>> a, b = 257, 257
>>> a is b
True
Output (Python 3.7.x specifically)>>> a, b = 257, 257
>>> a is b
False
💡 Explanation:The difference between 256When you start up python the numbers from  to  will be allocated. These numbers are used a lot, so it makes sense just to have them ready.Quoting from https://docs.python.org/3/c-api/long.html>>> id(256)
10922528
>>> a = 256
>>> b = 256
>>> id(a)
10922528
>>> id(b)
10922528
>>> id(257)
140084850247312
>>> x = 257
>>> y = 257
>>> id(x)
140084850247440
>>> id(y)
140084850247344
Here the interpreter isn't smart enough while executing  to recognize that we've already created an integer of the value  and so it goes on to create another object in the memory.Similar optimization applies to other immutable objects like empty tuples as well. Since lists are mutable, that's why  will return  and  will return . This explains our second snippet. Let's move on to the third one, Both Output>>> a, b = 257, 257
>>> id(a)
140640774013296
>>> id(b)
140640774013296
>>> a = 257
>>> b = 257
>>> id(a)
140640774013392
>>> id(b)
140640774013488
▶ Hash brownies1some_dict = {}
some_dict[5.5] = ""JavaScript""
some_dict[5.0] = ""Ruby""
some_dict[5] = ""Python""
Output:>>> some_dict[5.5]
""JavaScript""
>>> some_dict[5.0] # ""Python"" destroyed the existence of ""Ruby""?
""Python""
>>> some_dict[5] 
""Python""

>>> complex_five = 5 + 0j
>>> type(complex_five)
complex
>>> some_dict[complex_five]
""Python""
So, why is Python all over the place?💡 Explanation▶ Deep down, we're all the same.class WTF:
  pass
Output:>>> WTF() == WTF() # two different instances can't be equal
False
>>> WTF() is WTF() # identities are also different
False
>>> hash(WTF()) == hash(WTF()) # hashes _should_ be different as well
True
>>> id(WTF()) == id(WTF())
True
💡 Explanation:▶ Disorder within order *from collections import OrderedDict

dictionary = dict()
dictionary[1] = 'a'; dictionary[2] = 'b';

ordered_dict = OrderedDict()
ordered_dict[1] = 'a'; ordered_dict[2] = 'b';

another_ordered_dict = OrderedDict()
another_ordered_dict[2] = 'b'; another_ordered_dict[1] = 'a';

class DictWithHash(dict):
    """"""
    A dict that also implements __hash__ magic.
    """"""
    __hash__ = lambda self: 0

class OrderedDictWithHash(OrderedDict):
    """"""
    An OrderedDict that also implements __hash__ magic.
    """"""
    __hash__ = lambda self: 0
Output>>> dictionary == ordered_dict # If a == b
True
>>> dictionary == another_ordered_dict # and b == c
True
>>> ordered_dict == another_ordered_dict # then why isn't c == a ??
False

# We all know that a set consists of only unique elements,
# let's try making a set of these dictionaries and see what happens...

>>> len({dictionary, ordered_dict, another_ordered_dict})
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: unhashable type: 'dict'

# Makes sense since dict don't have __hash__ implemented, let's use
# our wrapper classes.
>>> dictionary = DictWithHash()
>>> dictionary[1] = 'a'; dictionary[2] = 'b';
>>> ordered_dict = OrderedDictWithHash()
>>> ordered_dict[1] = 'a'; ordered_dict[2] = 'b';
>>> another_ordered_dict = OrderedDictWithHash()
>>> another_ordered_dict[2] = 'b'; another_ordered_dict[1] = 'a';
>>> len({dictionary, ordered_dict, another_ordered_dict})
1
>>> len({ordered_dict, another_ordered_dict, dictionary}) # changing the order
2
What is going on here?💡 Explanation:▶ Keep trying... *def some_func():
    try:
        return 'from_try'
    finally:
        return 'from_finally'

def another_func(): 
    for _ in range(3):
        try:
            continue
        finally:
            print(""Finally!"")

def one_more_func(): # A gotcha!
    try:
        for i in range(3):
            try:
                1 / i
            except ZeroDivisionError:
                # Let's throw it here and handle it outside for loop
                raise ZeroDivisionError(""A trivial divide by zero error"")
            finally:
                print(""Iteration"", i)
                break
    except ZeroDivisionError as e:
        print(""Zero division error occurred"", e)
Output:>>> some_func()
'from_finally'

>>> another_func()
Finally!
Finally!
Finally!

>>> 1 / 0
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ZeroDivisionError: division by zero

>>> one_more_func()
Iteration 0

💡 Explanation:▶ For what?some_string = ""wtf""
some_dict = {}
for i, some_dict[i] in enumerate(some_string):
    i = 10
Output:>>> some_dict # An indexed dict appears.
{0: 'w', 1: 't', 2: 'f'}
💡 Explanation:▶ Evaluation time discrepancy1array = [1, 8, 15]
# A typical generator expression
gen = (x for x in array if array.count(x) > 0)
array = [2, 8, 22]
Output:>>> print(list(gen)) # Where did the other values go?
[8]
2array_1 = [1,2,3,4]
gen_1 = (x for x in array_1)
array_1 = [1,2,3,4,5]

array_2 = [1,2,3,4]
gen_2 = (x for x in array_2)
array_2[:] = [1,2,3,4,5]
Output:>>> print(list(gen_1))
[1, 2, 3, 4]

>>> print(list(gen_2))
[1, 2, 3, 4, 5]
3array_3 = [1, 2, 3]
array_4 = [10, 20, 30]
gen = (i + j for i in array_3 for j in array_4)

array_3 = [4, 5, 6]
array_4 = [400, 500, 600]
Output:>>> print(list(gen))
[401, 501, 601, 402, 502, 602, 403, 503, 603]
💡 Explanation▶  is not >>> 'something' is not None
True
>>> 'something' is (not None)
False
💡 Explanation▶ A tic-tac-toe where X wins in the first attempt!# Let's initialize a row
row = [""""] * 3 #row i['', '', '']
# Let's make a board
board = [row] * 3
Output:>>> board
[['', '', ''], ['', '', ''], ['', '', '']]
>>> board[0]
['', '', '']
>>> board[0][0]
''
>>> board[0][0] = ""X""
>>> board
[['X', '', ''], ['X', '', ''], ['X', '', '']]
We didn't assign three s, did we?💡 Explanation:When we initialize  variable, this visualization explains what happens in the memoryAnd when the  is initialized by multiplying the , this is what happens inside the memory (each of the elements ,  and  is a reference to the same list referred by )We can avoid this scenario here by not using  variable to generate . (Asked in  issue).>>> board = [['']*3 for _ in range(3)]
>>> board[0][0] = ""X""
>>> board
[['X', '', ''], ['', '', ''], ['', '', '']]
▶ Schrödinger's variable *funcs = []
results = []
for x in range(7):
    def some_func():
        return x
    funcs.append(some_func)
    results.append(some_func())  # note the function call here

funcs_results = [func() for func in funcs]
Output (Python version):>>> results
[0, 1, 2, 3, 4, 5, 6]
>>> funcs_results
[6, 6, 6, 6, 6, 6, 6]
The values of  were different in every iteration prior to appending  to , but all the functions return 6 when they're evaluated after the loop completes.>>> powers_of_x = [lambda x: x**i for i in range(10)]
>>> [f(2) for f in powers_of_x]
[512, 512, 512, 512, 512, 512, 512, 512, 512, 512]
💡 Explanation:>>> import inspect
>>> inspect.getclosurevars(funcs[0])
ClosureVars(nonlocals={}, globals={'x': 6}, builtins={}, unbound=set())
Since  is a global value, we can change the value that the  will lookup and return by updating :>>> x = 42
>>> [func() for func in funcs]
[42, 42, 42, 42, 42, 42, 42]
funcs = []
for x in range(7):
    def some_func(x=x):
        return x
    funcs.append(some_func)
Output:>>> funcs_results = [func() for func in funcs]
>>> funcs_results
[0, 1, 2, 3, 4, 5, 6]
It is not longer using the  in the global scope:>>> inspect.getclosurevars(funcs[0])
ClosureVars(nonlocals={}, globals={}, builtins={}, unbound=set())
▶ The chicken-egg problem *1>>> isinstance(3, int)
True
>>> isinstance(type, object)
True
>>> isinstance(object, type)
True
So which is the ""ultimate"" base class? There's more to the confusion by the way,2 >>> class A: pass
>>> isinstance(A, A)
False
>>> isinstance(type, type)
True
>>> isinstance(object, object)
True
3>>> issubclass(int, object)
True
>>> issubclass(type, object)
True
>>> issubclass(object, type)
False
💡 Explanation▶ Subclass relationshipsOutput:>>> from collections.abc import Hashable
>>> issubclass(list, object)
True
>>> issubclass(object, Hashable)
True
>>> issubclass(list, Hashable)
False
The Subclass relationships were expected to be transitive, right? (i.e., if  is a subclass of , and  is a subclass of , the  should a subclass of )💡 Explanation:▶ Methods equality and identityclass SomeClass:
    def method(self):
        pass

    @classmethod
    def classm(cls):
        pass

    @staticmethod
    def staticm():
        pass
Output:>>> print(SomeClass.method is SomeClass.method)
True
>>> print(SomeClass.classm is SomeClass.classm)
False
>>> print(SomeClass.classm == SomeClass.classm)
True
>>> print(SomeClass.staticm is SomeClass.staticm)
True
Accessing  twice, we get an equal object, but not the same one? Let's see what happenswith instances of :o1 = SomeClass()
o2 = SomeClass()
Output:>>> print(o1.method == o2.method)
False
>>> print(o1.method == o1.method)
True
>>> print(o1.method is o1.method)
False
>>> print(o1.classm is o1.classm)
False
>>> print(o1.classm == o1.classm == o2.classm == SomeClass.classm)
True
>>> print(o1.staticm is o1.staticm is o2.staticm is SomeClass.staticm)
True
Accessing  or  twice, creates equal but not same objects for the same instance of .💡 Explanation>>> o1.method
<bound method SomeClass.method of <__main__.SomeClass object at ...>>
>>> SomeClass.method
<function SomeClass.method at ...>
>>> o1.classm
<bound method SomeClass.classm of <class '__main__.SomeClass'>>
>>> SomeClass.classm
<bound method SomeClass.classm of <class '__main__.SomeClass'>>
>>> o1.staticm
<function SomeClass.staticm at ...>
>>> SomeClass.staticm
<function SomeClass.staticm at ...>
▶ All-true-ation *>>> all([True, True, True])
True
>>> all([True, True, False])
False

>>> all([])
True
>>> all([[]])
False
>>> all([[[]]])
True
Why's this True-False alteration?💡 Explanation:▶ The surprising commaOutput (< 3.6):>>> def f(x, y,):
...     print(x, y)
...
>>> def g(x=4, y=5,):
...     print(x, y)
...
>>> def h(x, **kwargs,):
  File ""<stdin>"", line 1
    def h(x, **kwargs,):
                     ^
SyntaxError: invalid syntax

>>> def h(*args,):
  File ""<stdin>"", line 1
    def h(*args,):
                ^
SyntaxError: invalid syntax
💡 Explanation:▶ Strings and the backslashesOutput:>>> print(""\"""")
""

>>> print(r""\"""")
\""

>>> print(r""\"")
File ""<stdin>"", line 1
    print(r""\"")
              ^
SyntaxError: EOL while scanning string literal

>>> r'\'' == ""\\'""
True
💡 Explanation▶ not knot!x = True
y = False
Output:>>> not x == y
True
>>> x == not y
  File ""<input>"", line 1
    x == not y
           ^
SyntaxError: invalid syntax
💡 Explanation:▶ Half triple-quoted stringsOutput:>>> print('wtfpython''')
wtfpython
>>> print(""wtfpython"""""")
wtfpython
>>> # The following statements raise `SyntaxError`
>>> # print('''wtfpython')
>>> # print(""""""wtfpython"")
  File ""<input>"", line 3
    print(""""""wtfpython"")
                        ^
SyntaxError: EOF while scanning triple-quoted string literal
💡 Explanation:▶ What's wrong with booleans?1# A simple example to count the number of booleans and
# integers in an iterable of mixed data types.
mixed_list = [False, 1.0, ""some_string"", 3, True, [], False]
integers_found_so_far = 0
booleans_found_so_far = 0

for item in mixed_list:
    if isinstance(item, int):
        integers_found_so_far += 1
    elif isinstance(item, bool):
        booleans_found_so_far += 1
Output:>>> integers_found_so_far
4
>>> booleans_found_so_far
0
2>>> some_bool = True
>>> ""wtf"" * some_bool
'wtf'
>>> some_bool = False
>>> ""wtf"" * some_bool
''
3def tell_truth():
    True = False
    if True == False:
        print(""I have lost faith in truth!"")
Output (< 3.x):>>> tell_truth()
I have lost faith in truth!
💡 Explanation:▶ Class attributes and instance attributes1class A:
    x = 1

class B(A):
    pass

class C(A):
    pass
Output:>>> A.x, B.x, C.x
(1, 1, 1)
>>> B.x = 2
>>> A.x, B.x, C.x
(1, 2, 1)
>>> A.x = 3
>>> A.x, B.x, C.x # C.x changed, but B.x didn't
(3, 2, 3)
>>> a = A()
>>> a.x, A.x
(3, 3)
>>> a.x += 1
>>> a.x, A.x
(4, 3)
2class SomeClass:
    some_var = 15
    some_list = [5]
    another_list = [5]
    def __init__(self, x):
        self.some_var = x + 1
        self.some_list = self.some_list + [x]
        self.another_list += [x]
Output:>>> some_obj = SomeClass(420)
>>> some_obj.some_list
[5, 420]
>>> some_obj.another_list
[5, 420]
>>> another_obj = SomeClass(111)
>>> another_obj.some_list
[5, 111]
>>> another_obj.another_list
[5, 420, 111]
>>> another_obj.another_list is SomeClass.another_list
True
>>> another_obj.another_list is some_obj.another_list
True
💡 Explanation:▶ yielding Nonesome_iterable = ('a', 'b')

def some_func(val):
    return ""something""
Output (<= 3.7.x):>>> [x for x in some_iterable]
['a', 'b']
>>> [(yield x) for x in some_iterable]
<generator object <listcomp> at 0x7f70b0a4ad58>
>>> list([(yield x) for x in some_iterable])
['a', 'b']
>>> list((yield x) for x in some_iterable)
['a', None, 'b', None]
>>> list(some_func((yield x)) for x in some_iterable)
['a', 'something', 'b', 'something']
💡 Explanation:▶ Yielding from... return! *1def some_func(x):
    if x == 3:
        return [""wtf""]
    else:
        yield from range(x)
Output (> 3.3):>>> list(some_func(3))
[]
Where did the  go? Is it due to some special effect of ? Let's validate that,2def some_func(x):
    if x == 3:
        return [""wtf""]
    else:
        for i in range(x):
          yield i
Output:>>> list(some_func(3))
[]
The same result, this didn't work either.💡 Explanation:▶ Nan-reflexivity *1a = float('inf')
b = float('nan')
c = float('-iNf')  # These strings are case-insensitive
d = float('nan')
Output:>>> a
inf
>>> b
nan
>>> c
-inf
>>> float('some_other_string')
ValueError: could not convert string to float: some_other_string
>>> a == -c # inf==inf
True
>>> None == None # None == None
True
>>> b == d # but nan!=nan
False
>>> 50 / a
0.0
>>> a / a
nan
>>> 23 + b
nan
2>>> x = float('nan')
>>> y = x / x
>>> y is y # identity holds
True
>>> y == y # equality fails of y
False
>>> [y] == [y] # but the equality succeeds for the list containing y
True
💡 Explanation:▶ Mutating the immutable!This might seem trivial if you know how references work in Python.some_tuple = (""A"", ""tuple"", ""with"", ""values"")
another_tuple = ([1, 2], [3, 4], [5, 6])
Output:>>> some_tuple[2] = ""change this""
TypeError: 'tuple' object does not support item assignment
>>> another_tuple[2].append(1000) #This throws no error
>>> another_tuple
([1, 2], [3, 4], [5, 6, 1000])
>>> another_tuple[2] += [99, 999]
TypeError: 'tuple' object does not support item assignment
>>> another_tuple
([1, 2], [3, 4], [5, 6, 1000, 99, 999])
But I thought tuples were immutable...💡 Explanation:▶ The disappearing variable from outer scopee = 7
try:
    raise Exception()
except Exception as e:
    pass
Output (Python 2.x):>>> print(e)
# prints nothing
Output (Python 3.x):>>> print(e)
NameError: name 'e' is not defined
💡 Explanation:▶ The mysterious key type conversionclass SomeClass(str):
    pass

some_dict = {'s': 42}
Output:>>> type(list(some_dict.keys())[0])
str
>>> s = SomeClass('s')
>>> some_dict[s] = 40
>>> some_dict # expected: Two different keys-value pairs
{'s': 40}
>>> type(list(some_dict.keys())[0])
str
💡 Explanation:▶ Let's see if you can guess this?a, b = a[b] = {}, 5
Output:>>> a
{5: ({...}, 5)}
💡 Explanation:▶ Exceeds the limit for integer string conversion>>> # Python 3.10.6
>>> int(""2"" * 5432)

>>> # Python 3.10.8
>>> int(""2"" * 5432)
Output:>>> # Python 3.10.6
222222222222222222222222222222222222222222222222222222222222222...

>>> # Python 3.10.8
Traceback (most recent call last):
   ...
ValueError: Exceeds the limit (4300) for integer string conversion:
   value has 5432 digits; use sys.set_int_max_str_digits()
   to increase the limit.
💡 Explanation:This call to  works fine in Python 3.10.6 and raises a ValueError in Python 3.10.8. Note that Python can still work with large integers. The error is only raised when converting between integers and strings.Fortunately, you can increase the limit for the allowed number of digits when you expect an operation to exceed it. To do this, you can use one of the following: for more details on changing the default limit if you expect your code to exceed this value.Section: Slippery Slopes▶ Modifying a dictionary while iterating over itx = {0: None}

for i in x:
    del x[i]
    x[i+1] = None
    print(i)
Output (Python 2.7- Python 3.5):0
1
2
3
4
5
6
7
Yes, it runs for exactly eight times and stops.💡 Explanation:▶ Stubborn  operationclass SomeClass:
    def __del__(self):
        print(""Deleted!"")
Output:1>>> x = SomeClass()
>>> y = x
>>> del x # this should print ""Deleted!""
>>> del y
Deleted!
Phew, deleted at last. You might have guessed what saved  from being called in our first attempt to delete . Let's add more twists to the example.2>>> x = SomeClass()
>>> y = x
>>> del x
>>> y # check if y exists
<__main__.SomeClass instance at 0x7f98a1a67fc8>
>>> del y # Like previously, this should print ""Deleted!""
>>> globals() # oh, it didn't. Let's check all our global variables and confirm
Deleted!
{'__builtins__': <module '__builtin__' (built-in)>, 'SomeClass': <class __main__.SomeClass at 0x7f98a1a5f668>, '__package__': None, '__name__': '__main__', '__doc__': None}
Okay, now it's deleted :confused:💡 Explanation:▶ The out of scope variable1a = 1
def some_func():
    return a

def another_func():
    a += 1
    return a
2def some_closure_func():
    a = 1
    def some_inner_func():
        return a
    return some_inner_func()

def another_closure_func():
    a = 1
    def another_inner_func():
        a += 1
        return a
    return another_inner_func()
Output:>>> some_func()
1
>>> another_func()
UnboundLocalError: local variable 'a' referenced before assignment

>>> some_closure_func()
1
>>> another_closure_func()
UnboundLocalError: local variable 'a' referenced before assignment
💡 Explanation:▶ Deleting a list item while iteratinglist_1 = [1, 2, 3, 4]
list_2 = [1, 2, 3, 4]
list_3 = [1, 2, 3, 4]
list_4 = [1, 2, 3, 4]

for idx, item in enumerate(list_1):
    del item

for idx, item in enumerate(list_2):
    list_2.remove(item)

for idx, item in enumerate(list_3[:]):
    list_3.remove(item)

for idx, item in enumerate(list_4):
    list_4.pop(idx)
Output:>>> list_1
[1, 2, 3, 4]
>>> list_2
[2, 4]
>>> list_3
[]
>>> list_4
[2, 4]
Can you guess why the output is ?💡 Explanation:Difference between Why the output is ▶ Lossy zip of iterators *>>> numbers = list(range(7))
>>> numbers
[0, 1, 2, 3, 4, 5, 6]
>>> first_three, remaining = numbers[:3], numbers[3:]
>>> first_three, remaining
([0, 1, 2], [3, 4, 5, 6])
>>> numbers_iter = iter(numbers)
>>> list(zip(numbers_iter, first_three)) 
[(0, 0), (1, 1), (2, 2)]
# so far so good, let's zip the remaining
>>> list(zip(numbers_iter, remaining))
[(4, 3), (5, 4), (6, 5)]
Where did element  go from the  list?💡 Explanation:▶ Loop variables leaking out!1for x in range(7):
    if x == 6:
        print(x, ': for x inside loop')
print(x, ': x in global')
Output:6 : for x inside loop
6 : x in global
But  was never defined outside the scope of for loop...2# This time let's initialize x first
x = -1
for x in range(7):
    if x == 6:
        print(x, ': for x inside loop')
print(x, ': x in global')
Output:6 : for x inside loop
6 : x in global
3Output (Python 2.x):>>> x = 1
>>> print([x for x in range(5)])
[0, 1, 2, 3, 4]
>>> print(x)
4
Output (Python 3.x):>>> x = 1
>>> print([x for x in range(5)])
[0, 1, 2, 3, 4]
>>> print(x)
1
💡 Explanation:▶ Beware of default mutable arguments!def some_func(default_arg=[]):
    default_arg.append(""some_string"")
    return default_arg
Output:>>> some_func()
['some_string']
>>> some_func()
['some_string', 'some_string']
>>> some_func([])
['some_string']
>>> some_func()
['some_string', 'some_string', 'some_string']
💡 Explanation:▶ Catching the Exceptionssome_list = [1, 2, 3]
try:
    # This should raise an ``IndexError``
    print(some_list[4])
except IndexError, ValueError:
    print(""Caught!"")

try:
    # This should raise a ``ValueError``
    some_list.remove(4)
except IndexError, ValueError:
    print(""Caught again!"")
Output (Python 2.x):Caught!

ValueError: list.remove(x): x not in list
Output (Python 3.x):  File ""<input>"", line 3
    except IndexError, ValueError:
                     ^
SyntaxError: invalid syntax
💡 Explanation▶ Same operands, different story!1a = [1, 2, 3, 4]
b = a
a = a + [5, 6, 7, 8]
Output:>>> a
[1, 2, 3, 4, 5, 6, 7, 8]
>>> b
[1, 2, 3, 4]
2a = [1, 2, 3, 4]
b = a
a += [5, 6, 7, 8]
Output:>>> a
[1, 2, 3, 4, 5, 6, 7, 8]
>>> b
[1, 2, 3, 4, 5, 6, 7, 8]
💡 Explanation:▶ Name resolution ignoring class scope1x = 5
class SomeClass:
    x = 17
    y = (x for i in range(10))
Output:>>> list(SomeClass.y)[0]
5
2x = 5
class SomeClass:
    x = 17
    y = [x for i in range(10)]
Output (Python 2.x):>>> SomeClass.y[0]
17
Output (Python 3.x):>>> SomeClass.y[0]
5
💡 Explanation▶ Rounding like a banker *Let's implement a naive function to get the middle element of a list:def get_middle(some_list):
    mid_index = round(len(some_list) / 2)
    return some_list[mid_index - 1]
Python 3.x:>>> get_middle([1])  # looks good
1
>>> get_middle([1,2,3])  # looks good
2
>>> get_middle([1,2,3,4,5])  # huh?
2
>>> len([1,2,3,4,5]) / 2  # good
2.5
>>> round(len([1,2,3,4,5]) / 2)  # why?
2
It seems as though Python rounded 2.5 to 2.💡 Explanation:>>> round(0.5)
0
>>> round(1.5)
2
>>> round(2.5)
2
>>> import numpy  # numpy does the same
>>> numpy.round(0.5)
0.0
>>> numpy.round(1.5)
2.0
>>> numpy.round(2.5)
2.0
▶ Needles in a Haystack *I haven't met even a single experience Pythonist till date who has not come across one or more of the following scenarios,1x, y = (0, 1) if True else None, None
Output:>>> x, y  # expected (0, 1)
((0, 1), None)
2t = ('one', 'two')
for i in t:
    print(i)

t = ('one')
for i in t:
    print(i)

t = ()
print(t)
Output:one
two
o
n
e
tuple()
3ten_words_list = [
    ""some"",
    ""very"",
    ""big"",
    ""list"",
    ""that""
    ""consists"",
    ""of"",
    ""exactly"",
    ""ten"",
    ""words""
]
Output>>> len(ten_words_list)
9
4 Not asserting strongly enougha = ""python""
b = ""javascript""
Output:# An assert statement with an assertion failure message.
>>> assert(a == b, ""Both languages are different"")
# No AssertionError is raised
5some_list = [1, 2, 3]
some_dict = {
  ""key_1"": 1,
  ""key_2"": 2,
  ""key_3"": 3
}

some_list = some_list.append(4) 
some_dict = some_dict.update({""key_4"": 4})
Output:>>> print(some_list)
None
>>> print(some_dict)
None
6def some_recursive_func(a):
    if a[0] == 0:
        return
    a[0] -= 1
    some_recursive_func(a)
    return a

def similar_recursive_func(a):
    if a == 0:
        return a
    a -= 1
    similar_recursive_func(a)
    return a
Output:>>> some_recursive_func([5, 0])
[0, 0]
>>> similar_recursive_func(5)
4
💡 Explanation:▶ Splitsies *>>> 'a'.split()
['a']

# is same as
>>> 'a'.split(' ')
['a']

# but
>>> len(''.split())
0

# isn't the same as
>>> len(''.split(' '))
1
💡 Explanation:▶ Wild imports *# File: module.py

def some_weird_name_func_():
    print(""works!"")

def _another_weird_name_func():
    print(""works!"")

Output>>> from module import *
>>> some_weird_name_func_()
""works!""
>>> _another_weird_name_func()
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
NameError: name '_another_weird_name_func' is not defined
💡 Explanation:▶ All sorted? *>>> x = 7, 8, 9
>>> sorted(x) == x
False
>>> sorted(x) == sorted(x)
True

>>> y = reversed(x)
>>> sorted(y) == sorted(y)
False
💡 Explanation:▶ Midnight time doesn't exist?from datetime import datetime

midnight = datetime(2018, 1, 1, 0, 0)
midnight_time = midnight.time()

noon = datetime(2018, 1, 1, 12, 0)
noon_time = noon.time()

if midnight_time:
    print(""Time at midnight is"", midnight_time)

if noon_time:
    print(""Time at noon is"", noon_time)
Output (< 3.5):('Time at noon is', datetime.time(12, 0))
The midnight time is not printed.💡 Explanation:Before Python 3.5, the boolean value for  object was considered to be  if it represented midnight in UTC. It is error-prone when using the  syntax to check if the  is null or some equivalent of ""empty.""Section: The Hidden treasures!This section contains a few lesser-known and interesting things about Python that most beginners like me are unaware of (well, not anymore).▶ Okay Python, Can you make me fly?Well, here you goimport antigravity
Output:Sshh... It's a super-secret.💡 Explanation:▶ , but why?from goto import goto, label
for i in range(9):
    for j in range(9):
        for k in range(9):
            print(""I am trapped, please rescue!"")
            if k == 2:
                goto .breakout # breaking out from a deeply nested loop
label .breakout
print(""Freedom!"")
Output (Python 2.3):I am trapped, please rescue!
I am trapped, please rescue!
Freedom!
💡 Explanation:▶ Brace yourself!If you are one of the people who doesn't like using whitespace in Python to denote scopes, you can use the C-style {} by importing,from __future__ import braces
Output:  File ""some_file.py"", line 1
    from __future__ import braces
SyntaxError: not a chance
Braces? No way! If you think that's disappointing, use Java. Okay, another surprising thing, can you find where's the  raised in  module ?💡 Explanation:▶ Let's meet Friendly Language Uncle For LifeOutput (Python 3.x)>>> from __future__ import barry_as_FLUFL
>>> ""Ruby"" != ""Python"" # there's no doubt about it
  File ""some_file.py"", line 1
    ""Ruby"" != ""Python""
              ^
SyntaxError: invalid syntax

>>> ""Ruby"" <> ""Python""
True
There we go.💡 Explanation:▶ Even Python understands that love is complicatedimport this
Wait, what's this?  is love :heart:Output:The Zen of Python, by Tim Peters

Beautiful is better than ugly.
Explicit is better than implicit.
Simple is better than complex.
Complex is better than complicated.
Flat is better than nested.
Sparse is better than dense.
Readability counts.
Special cases aren't special enough to break the rules.
Although practicality beats purity.
Errors should never pass silently.
Unless explicitly silenced.
In the face of ambiguity, refuse the temptation to guess.
There should be one-- and preferably only one --obvious way to do it.
Although that way may not be obvious at first unless you're Dutch.
Now is better than never.
Although never is often better than *right* now.
If the implementation is hard to explain, it's a bad idea.
If the implementation is easy to explain, it may be a good idea.
Namespaces are one honking great idea -- let's do more of those!
It's the Zen of Python!>>> love = this
>>> this is love
True
>>> love is True
False
>>> love is False
False
>>> love is not True or False
True
>>> love is not True or False; love is love  # Love is complicated
True
💡 Explanation:▶ Yes, it exists!The  One typical example might be:  def does_exists_num(l, to_find):
      for num in l:
          if num == to_find:
              print(""Exists!"")
              break
      else:
          print(""Does not exist"")
Output:>>> some_list = [1, 2, 3, 4, 5]
>>> does_exists_num(some_list, 4)
Exists!
>>> does_exists_num(some_list, -1)
Does not exist
The  An example,try:
    pass
except:
    print(""Exception occurred!!!"")
else:
    print(""Try block executed successfully..."")
Output:Try block executed successfully...
💡 Explanation:▶ Ellipsis *def some_func():
    Ellipsis
Output>>> some_func()
# No output, No Error

>>> SomeRandomString
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
NameError: name 'SomeRandomString' is not defined

>>> Ellipsis
Ellipsis
💡 Explanation▶ InpinityThe spelling is intended. Please, don't submit a patch for this.Output (Python 3.x):>>> infinity = float('infinity')
>>> hash(infinity)
314159
>>> hash(float('-inf'))
-314159
💡 Explanation:▶ Let's mangle1class Yo(object):
    def __init__(self):
        self.__honey = True
        self.bro = True
Output:>>> Yo().bro
True
>>> Yo().__honey
AttributeError: 'Yo' object has no attribute '__honey'
>>> Yo()._Yo__honey
True
2class Yo(object):
    def __init__(self):
        # Let's try something symmetrical this time
        self.__honey__ = True
        self.bro = True
Output:>>> Yo().bro
True

>>> Yo()._Yo__honey__
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: 'Yo' object has no attribute '_Yo__honey__'
Why did  work?3_A__variable = ""Some value""

class A(object):
    def some_func(self):
        return __variable # not initialized anywhere yet
Output:>>> A().__variable
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: 'A' object has no attribute '__variable'

>>> A().some_func()
'Some value'
💡 Explanation:Section: Appearances are deceptive!▶ Skipping lines?Output:>>> value = 11
>>> valuе = 32
>>> value
11
Wut?Note: The easiest way to reproduce this is to simply copy the statements from the above snippet and paste them into your file/shell.💡 ExplanationSome non-Western characters look identical to letters in the English alphabet but are considered distinct by the interpreter.>>> ord('е') # cyrillic 'e' (Ye)
1077
>>> ord('e') # latin 'e', as used in English and typed using standard keyboard
101
>>> 'е' == 'e'
False

>>> value = 42 # latin e
>>> valuе = 23 # cyrillic 'e', Python 2.x interpreter would raise a `SyntaxError` here
>>> value
42
The built-in  function returns a character's Unicode , and different code positions of Cyrillic 'e' and Latin 'e' justify the behavior of the above example.▶ Teleportation# `pip install numpy` first.
import numpy as np

def energy_send(x):
    # Initializing a numpy array
    np.array([float(x)])

def energy_receive():
    # Return an empty numpy array
    return np.empty((), dtype=np.float).tolist()
Output:>>> energy_send(123.456)
>>> energy_receive()
123.456
Where's the Nobel Prize?💡 Explanation:▶ Well, something is fishy...def square(x):
    """"""
    A simple function to calculate the square of a number by addition.
    """"""
    sum_so_far = 0
    for counter in range(x):
        sum_so_far = sum_so_far + x
  return sum_so_far
Output (Python 2.x):>>> square(10)
10
Shouldn't that be 100?Note: If you're not able to reproduce this, try running the file  via the shell.💡 ExplanationSection: Miscellaneous▶  is faster# using ""+"", three strings:
>>> timeit.timeit(""s1 = s1 + s2 + s3"", setup=""s1 = ' ' * 100000; s2 = ' ' * 100000; s3 = ' ' * 100000"", number=100)
0.25748300552368164
# using ""+="", three strings:
>>> timeit.timeit(""s1 += s2 + s3"", setup=""s1 = ' ' * 100000; s2 = ' ' * 100000; s3 = ' ' * 100000"", number=100)
0.012188911437988281
💡 Explanation:▶ Let's make a giant string!def add_string_with_plus(iters):
    s = """"
    for i in range(iters):
        s += ""xyz""
    assert len(s) == 3*iters

def add_bytes_with_plus(iters):
    s = b""""
    for i in range(iters):
        s += b""xyz""
    assert len(s) == 3*iters

def add_string_with_format(iters):
    fs = ""{}""*iters
    s = fs.format(*([""xyz""]*iters))
    assert len(s) == 3*iters

def add_string_with_join(iters):
    l = []
    for i in range(iters):
        l.append(""xyz"")
    s = """".join(l)
    assert len(s) == 3*iters

def convert_list_to_string(l, iters):
    s = """".join(l)
    assert len(s) == 3*iters
Output:# Executed in ipython shell using %timeit for better readability of results.
# You can also use the timeit module in normal python shell/scriptm=, example usage below
# timeit.timeit('add_string_with_plus(10000)', number=1000, globals=globals())

>>> NUM_ITERS = 1000
>>> %timeit -n1000 add_string_with_plus(NUM_ITERS)
124 µs ± 4.73 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
>>> %timeit -n1000 add_bytes_with_plus(NUM_ITERS)
211 µs ± 10.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
>>> %timeit -n1000 add_string_with_format(NUM_ITERS)
61 µs ± 2.18 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
>>> %timeit -n1000 add_string_with_join(NUM_ITERS)
117 µs ± 3.21 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
>>> l = [""xyz""]*NUM_ITERS
>>> %timeit -n1000 convert_list_to_string(l, NUM_ITERS)
10.1 µs ± 1.06 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
Let's increase the number of iterations by a factor of 10.>>> NUM_ITERS = 10000
>>> %timeit -n1000 add_string_with_plus(NUM_ITERS) # Linear increase in execution time
1.26 ms ± 76.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
>>> %timeit -n1000 add_bytes_with_plus(NUM_ITERS) # Quadratic increase
6.82 ms ± 134 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
>>> %timeit -n1000 add_string_with_format(NUM_ITERS) # Linear increase
645 µs ± 24.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
>>> %timeit -n1000 add_string_with_join(NUM_ITERS) # Linear increase
1.17 ms ± 7.25 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
>>> l = [""xyz""]*NUM_ITERS
>>> %timeit -n1000 convert_list_to_string(l, NUM_ITERS) # Linear increase
86.3 µs ± 2 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
💡 Explanation▶ Slowing down  lookups *some_dict = {str(i): 1 for i in range(1_000_000)}
another_dict = {str(i): 1 for i in range(1_000_000)}
Output:>>> %timeit some_dict['5']
28.6 ns ± 0.115 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)
>>> some_dict[1] = 1
>>> %timeit some_dict['5']
37.2 ns ± 0.265 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)

>>> %timeit another_dict['5']
28.5 ns ± 0.142 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)
>>> another_dict[1]  # Trying to access a key that doesn't exist
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
KeyError: 1
>>> %timeit another_dict['5']
38.5 ns ± 0.0913 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)
Why are same lookups becoming slower?💡 Explanation:▶ Bloating instance s *import sys

class SomeClass:
    def __init__(self):
        self.some_attr1 = 1
        self.some_attr2 = 2
        self.some_attr3 = 3
        self.some_attr4 = 4


def dict_size(o):
    return sys.getsizeof(o.__dict__)

Output: (Python 3.8, other Python 3 versions may vary a little)>>> o1 = SomeClass()
>>> o2 = SomeClass()
>>> dict_size(o1)
104
>>> dict_size(o2)
104
>>> del o1.some_attr1
>>> o3 = SomeClass()
>>> dict_size(o3)
232
>>> dict_size(o1)
232
Let's try again... In a new interpreter:>>> o1 = SomeClass()
>>> o2 = SomeClass()
>>> dict_size(o1)
104  # as expected
>>> o1.some_attr5 = 5
>>> o1.some_attr6 = 6
>>> dict_size(o1)
360
>>> dict_size(o2)
272
>>> o3 = SomeClass()
>>> dict_size(o3)
232
What makes those dictionaries become bloated? And why are newly created objects bloated as well?💡 Explanation:▶ Minor Ones *ContributingA few ways in which you can contribute to wtfpython,Please see  for more details. Feel free to create a new  to discuss things.PS: Please don't reach out with backlinking requests, no links will be added unless they're highly relevant to the project.AcknowledgementsThe idea and design for this collection were initially inspired by Denys Dovhan's awesome project . The overwhelming support by Pythonistas gave it the shape it is in right now.Some nice Links!🎓 License&copy; Surprise your friends as well!If you like wtfpython, you can use these quick links to share it with your friends, |  |   Need a pdf version?I've received a few requests for the pdf (and epub) version of wtfpython. You can add your details  to get them as soon as they are finished.That's all folks! For upcoming content like this, you can add your email ."
https://github.com/dbolya/yolact,"A simple, fully convolutional model for real-time instance segmentation.","You Only Look At CoefficienTs    ██╗   ██╗ ██████╗ ██╗      █████╗  ██████╗████████╗
    ╚██╗ ██╔╝██╔═══██╗██║     ██╔══██╗██╔════╝╚══██╔══╝
     ╚████╔╝ ██║   ██║██║     ███████║██║        ██║   
      ╚██╔╝  ██║   ██║██║     ██╔══██║██║        ██║   
       ██║   ╚██████╔╝███████╗██║  ██║╚██████╗   ██║   
       ╚═╝    ╚═════╝ ╚══════╝╚═╝  ╚═╝ ╚═════╝   ╚═╝ 
A simple, fully convolutional model for real-time instance segmentation. This is the code for our papers:YOLACT++ (v1.2) released! ()YOLACT++'s resnet50 model runs at 33.5 fps on a Titan Xp and achieves 34.1 mAP on COCO's  (check out our journal paper ).In order to use YOLACT++, make sure you compile the DCNv2 code. (See )For a real-time demo, check out our ICCV video:Some examples from our YOLACT base model (33.5 fps on a Titan Xp and 29.8 mAP on COCO's ):InstallationEvaluationHere are our YOLACT models (released on April 5th, 2019) along with their FPS on a Titan Xp and mAP on :| Image Size | Backbone      | FPS  | mAP  | Weights                                                                                                              |  ||:----------:|:-------------:|:----:|:----:|----------------------------------------------------------------------------------------------------------------------|--------|| 550        | Resnet50-FPN  | 42.5 | 28.2 |   |  || 550        | Darknet53-FPN | 40.0 | 28.7 |  | | 550        | Resnet101-FPN | 33.5 | 29.8 |       | | 700        | Resnet101-FPN | 23.6 | 31.2 |      | YOLACT++ models (released on December 16th, 2019):| Image Size | Backbone      | FPS  | mAP  | Weights                                                                                                              |  ||:----------:|:-------------:|:----:|:----:|----------------------------------------------------------------------------------------------------------------------|--------|| 550        | Resnet50-FPN  | 33.5 | 34.1 |   |  || 550        | Resnet101-FPN | 27.3 | 34.6 |  | To evalute the model, put the corresponding weights file in the  directory and run one of the following commands. The name of each config is everything before the numbers in the file name (e.g.,  for ).Quantitative Results on COCO# Quantitatively evaluate a trained model on the entire validation set. Make sure you have COCO downloaded as above.
# This should get 29.92 validation mask mAP last time I checked.
python eval.py --trained_model=weights/yolact_base_54_800000.pth

# Output a COCOEval json to submit to the website or to use the run_coco_eval.py script.
# This command will create './results/bbox_detections.json' and './results/mask_detections.json' for detection and instance segmentation respectively.
python eval.py --trained_model=weights/yolact_base_54_800000.pth --output_coco_json

# You can run COCOEval on the files created in the previous command. The performance should match my implementation in eval.py.
python run_coco_eval.py

# To output a coco json file for test-dev, make sure you have test-dev downloaded from above and go
python eval.py --trained_model=weights/yolact_base_54_800000.pth --output_coco_json --dataset=coco2017_testdev_dataset
Qualitative Results on COCO# Display qualitative results on COCO. From here on I'll use a confidence threshold of 0.15.
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --display
Benchmarking on COCO# Run just the raw model on the first 1k images of the validation set
python eval.py --trained_model=weights/yolact_base_54_800000.pth --benchmark --max_images=1000
Images# Display qualitative results on the specified image.
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --image=my_image.png

# Process an image and save it to another file.
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --image=input_image.png:output_image.png

# Process a whole folder of images.
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --images=path/to/input/folder:path/to/output/folder
Video# Display a video in real-time. ""--video_multiframe"" will process that many frames at once for improved performance.
# If you want, use ""--display_fps"" to draw the FPS directly on the frame.
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --video_multiframe=4 --video=my_video.mp4

# Display a webcam feed in real-time. If you have multiple webcams pass the index of the webcam you want instead of 0.
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --video_multiframe=4 --video=0

# Process a video and save it to another file. This uses the same pipeline as the ones above now, so it's fast!
python eval.py --trained_model=weights/yolact_base_54_800000.pth --score_threshold=0.15 --top_k=15 --video_multiframe=4 --video=input_video.mp4:output_video.mp4
As you can tell,  can do a ton of stuff. Run the  command to see everything it can do.python eval.py --help
TrainingBy default, we train on COCO. Make sure to download the entire dataset using the commands above.# Trains using the base config with a batch size of 8 (the default).
python train.py --config=yolact_base_config

# Trains yolact_base_config with a batch_size of 5. For the 550px models, 1 batch takes up around 1.5 gigs of VRAM, so specify accordingly.
python train.py --config=yolact_base_config --batch_size=5

# Resume training yolact_base with a specific weight file and start from the iteration specified in the weight file's name.
python train.py --config=yolact_base_config --resume=weights/yolact_base_10_32100.pth --start_iter=-1

# Use the help option to see a description of all available command line arguments
python train.py --help
Multi-GPU SupportYOLACT now supports multiple GPUs seamlessly during training:LoggingYOLACT now logs training and validation information by default. You can disable this with . A guide on how to visualize these logs is coming soon, but now you can look at  in  for help.Pascal SBDWe also include a config for training on Pascal SBD annotations (for rapid experimentation or comparing with other methods). To train on Pascal SBD, proceed with the following steps:I will automate this all with a script soon, don't worry. Also, if you want the script I used to convert the annotations, I put it in , but you'll have to check how it works to be able to use it because I don't actually remember at this point.If you want to verify our results, you can download our  weights from . This model should get 72.3 mask AP_50 and 56.2 mask AP_70. Note that the ""all"" AP isn't the same as the ""vol"" AP reported in others papers for pascal (they use an averages of the thresholds from  in increments of  instead of what COCO uses).Custom DatasetsYou can also train on your own dataset by following these steps:my_custom_dataset = dataset_base.copy({
    'name': 'My Dataset',

    'train_images': 'path_to_training_images',
    'train_info':   'path_to_training_annotation',

    'valid_images': 'path_to_validation_images',
    'valid_info':   'path_to_validation_annotation',

    'has_gt': True,
    'class_names': ('my_class_id_1', 'my_class_id_2', 'my_class_id_3', ...)
})
Creating a Custom Dataset from ScratchSee  for tips on how to annotate a custom dataset and prepare it for use with YOLACT.CitationIf you use YOLACT or this code base in your work, please cite@inproceedings{yolact-iccv2019,
  author    = {Daniel Bolya and Chong Zhou and Fanyi Xiao and Yong Jae Lee},
  title     = {YOLACT: {Real-time} Instance Segmentation},
  booktitle = {ICCV},
  year      = {2019},
}
For YOLACT++, please cite@article{yolact-plus-tpami2020,
  author  = {Daniel Bolya and Chong Zhou and Fanyi Xiao and Yong Jae Lee},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title   = {YOLACT++: Better Real-time Instance Segmentation}, 
  year    = {2020},
}
ContactFor questions about our paper or code, please contact ."
https://github.com/rwv/chinese-dos-games,🎮 Chinese DOS games collections.,"🎮 中文 DOS 游戏网址： https://dos.lol中文 DOS 游戏合集，目前共有 1898 款游戏。下载游戏文件在根目录下运行 Python 3 脚本python download_data.py
若下载出错请参见 游戏列表参见 https://dos.lol/gamesIPFSIPNS Hash: 网站源代码请参见 版权问题本人明白此项目存在版权上的侵权，如版权方介意的话，请联系 ，本人将立刻删除有关文件。Contributing欢迎提  和  来增加新的游戏!PR 具体参见 Credits"
https://github.com/ethereum/web3.py,A python interface for interacting with the Ethereum blockchain and ecosystem.,"web3.pyA Python library for interacting with Ethereum.Quickstart or of the library.DocumentationFor additional guides, examples, and APIs, see the .Want to help?Want to file a bug, contribute some code, or improve documentation? Excellent! Read up on ourguidelines for ,then check out issues that are labeled.Questions on implementation or usage? Join the conversation on ."
https://github.com/neozhaoliang/pywonderland,A tour in the wonderland of math with python.,"A Tour in the Wonderland of Math with PythonAbout this repoThe purpose of this project is to show the beauty of math with python by rendering high quality images, videos and animations. It consists of several independent projects with each one illustrates a special object/algorithm in math. The current list contains:These topics are chosen largely due to my personal taste:I'll use only popular python libs and build all math stuff by hand (tools like , ,  will not be used here).GalleryThe code for some of the images are not in the master branch, they can be found in the .                                                                        Many more to be comtinued ...How to useAll projects here are implemented in a ready-to-use manner for new comers. You can simply run the examples without tweaking any parameters once you have the dependencies installed correctly.DependenciesThe recommended way to install all dependencies is simply running the bash script .sudo bash install_dependencies.sh
Or you can install the python libs by pip:pip install -r requirements.txt
Open source softwares required:They can all be installed via command-line:sudo apt-get install python3-tk imagemagick ffmpeg povray graphviz inkscape
Note  also requires :sudo apt-get install libgraphviz-dev
In the scripts these softwares are called in command line as , ,  (from ), etc. For Windows users you should add the directories contain these .exe files to the system  environment variables to let the system know what executables these commands refer to. For example on Windows the default location of POV-Ray's exe file is , so you should add  to system  and rename  to , then you can run the scripts without any changes and everything works fine.ThanksI have learned a lot from the following people:Licensesee the LICENSE file."
https://github.com/freqtrade/freqtrade,"Free, open source crypto trading bot","Freqtrade is a free and open source crypto trading bot written in Python. It is designed to support all major exchanges and be controlled via Telegram or webUI. It contains backtesting, plotting and money management tools as well as strategy optimization by machine learning.DisclaimerThis software is for educational purposes only. Do not risk money whichyou are afraid to lose. USE THE SOFTWARE AT YOUR OWN RISK. THE AUTHORSAND ALL AFFILIATES ASSUME NO RESPONSIBILITY FOR YOUR TRADING RESULTS.Always start by running a trading bot in Dry-run and do not engage moneybefore you understand how it works and what profit/loss you shouldexpect.We strongly recommend you to have coding and Python knowledge. Do nothesitate to read the source code and understand the mechanism of this bot.Supported Exchange marketplacesPlease read the  to learn about eventual, special configurations needed for each exchange.Supported Futures Exchanges (experimental)Please make sure to read the , as well as the  documentation before diving in.Community testedExchanges confirmed working by the community:DocumentationWe invite you to read the bot documentation to ensure you understand how the bot is working.Please find the complete documentation on the .FeaturesQuick startPlease refer to the  on how to get started quickly.For further (native) installation methods, please refer to the .Basic UsageBot commandsusage: freqtrade [-h] [-V]
                 {trade,create-userdir,new-config,new-strategy,download-data,convert-data,convert-trade-data,list-data,backtesting,edge,hyperopt,hyperopt-list,hyperopt-show,list-exchanges,list-hyperopts,list-markets,list-pairs,list-strategies,list-timeframes,show-trades,test-pairlist,install-ui,plot-dataframe,plot-profit,webserver}
                 ...

Free, open source crypto trading bot

positional arguments:
  {trade,create-userdir,new-config,new-strategy,download-data,convert-data,convert-trade-data,list-data,backtesting,edge,hyperopt,hyperopt-list,hyperopt-show,list-exchanges,list-hyperopts,list-markets,list-pairs,list-strategies,list-timeframes,show-trades,test-pairlist,install-ui,plot-dataframe,plot-profit,webserver}
    trade               Trade module.
    create-userdir      Create user-data directory.
    new-config          Create new config
    new-strategy        Create new strategy
    download-data       Download backtesting data.
    convert-data        Convert candle (OHLCV) data from one format to
                        another.
    convert-trade-data  Convert trade data from one format to another.
    list-data           List downloaded data.
    backtesting         Backtesting module.
    edge                Edge module.
    hyperopt            Hyperopt module.
    hyperopt-list       List Hyperopt results
    hyperopt-show       Show details of Hyperopt results
    list-exchanges      Print available exchanges.
    list-hyperopts      Print available hyperopt classes.
    list-markets        Print markets on exchange.
    list-pairs          Print pairs on exchange.
    list-strategies     Print available strategies.
    list-timeframes     Print available timeframes for the exchange.
    show-trades         Show trades.
    test-pairlist       Test your pairlist configuration.
    install-ui          Install FreqUI
    plot-dataframe      Plot candles with indicators.
    plot-profit         Generate plot showing profits.
    webserver           Webserver module.

optional arguments:
  -h, --help            show this help message and exit
  -V, --version         show program's version number and exit

Telegram RPC commandsTelegram is not mandatory. However, this is a great way to control your bot. More details and the full command list on the Development branchesThe project is currently setup in two main branches:SupportHelp / DiscordFor any questions not covered by the documentation or for further information about the bot, or to simply engage with like-minded individuals, we encourage you to join the Freqtrade .If you discover a bug in the bot, pleasefirst. If it hasn't been reported, please andensure you follow the template guide so that the team can assist you asquickly as possible.For every  created, kindly follow up and mark satisfaction or reminder to close issue when equilibrium ground is reached.--Maintain github's --Have you a great idea to improve the bot you want to share? Please,first search if this feature was not .If it hasn't been requested, pleaseand ensure you follow the template guide so that it does not get lostin the bug reports.Feel like the bot is missing a feature? We welcome your pull requests!Please read theto understand the requirements before sending your pull-requests.Coding is not a necessity to contribute - maybe start with improving the documentation?Issues labeled  can be good first contributions, and will help get you familiar with the codebase.Note before starting any major new feature work, please open an issue describing what you are planning to do or talk to us on  (please use the #dev channel for this). This will ensure that interested parties can give valuable feedback on the feature, and let others know that you are working on it.Important: Always create your PR against the  branch, not .RequirementsUp-to-date clockThe clock must be accurate, synchronized to a NTP server very frequently to avoid problems with communication to the exchanges.Minimum hardware requiredTo run this bot we recommend you a cloud instance with a minimum of:Software requirements"
https://github.com/Netflix/vmaf,Perceptual video quality assessment based on multi-method fusion.,"VMAF - Video Multi-Method Assessment FusionVMAF is an  perceptual video quality assessment algorithm developed by Netflix. This software package includes a stand-alone C library  and its wrapping Python library. The Python library also provides a set of tools that allows a user to train and test a custom VMAF model.Read  tech blog post for an overview,  post for the tips of best practices, and  post for our latest efforts on speed optimization, new API design and the introduction of a codec evaluation-friendly .Also included in  are implementations of several other metrics: PSNR, PSNR-HVS, SSIM, MS-SSIM and CIEDE2000.NewsDocumentationThere is an  with links to specific pages, covering FAQs, available models and features, software usage guides, and a list of resources.UsageThe software package offers a number of ways to interact with the VMAF implementation.Contribution GuideRefer to the  page. Also refer to this  for an overview contribution guide."
https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix,Image-to-Image Translation in PyTorch,"CycleGAN and pix2pix in PyTorchNew:  Please check out  (CUT), our new unpaired image-to-image translation model that enables fast and memory-efficient training.We provide PyTorch implementations for both unpaired and paired image-to-image translation.The code was written by  and , and supported by .This PyTorch implementation produces results comparable to or better than our original Torch software. If you would like to reproduce the same results as in the papers, check out the original  and  code in Lua/Torch.Note: The current software works well with PyTorch 1.4. Check out the older  that supports PyTorch 0.1-0.3.You may find useful information in  and . To implement custom models and datasets, check out our . To help users better understand and adapt our codebase, we provide an  of the code structure of this repository.CycleGAN: Pix2pix:  [<marko.inline.RawText object at 0x000001592FDC0F08>]If you use this code for your research, please cite:Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks.,  , , . In ICCV 2017. (* equal contributions) Image-to-Image Translation with Conditional Adversarial Networks., , , . In CVPR 2017. Talks and Coursepix2pix slides:  | ,CycleGAN slides:  | CycleGAN course assignment  and  designed by Prof.  for  ""Intro to Neural Networks and Machine Learning"" at University of Toronto. Please contact the instructor if you would like to adopt it in your course.Colab NotebookTensorFlow Core CycleGAN Tutorial:  | TensorFlow Core pix2pix Tutorial:  | PyTorch Colab notebook:  and ZeroCostDL4Mic Colab notebook:  and Other implementationsCycleGANpix2pixPrerequisitesGetting StartedInstallationgit clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix
cd pytorch-CycleGAN-and-pix2pix
CycleGAN train/testbash ./datasets/download_cyclegan_dataset.sh maps
#!./scripts/train_cyclegan.sh
python train.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan
To see more intermediate results, check out .#!./scripts/test_cyclegan.sh
python test.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan
pix2pix train/testbash ./datasets/download_pix2pix_dataset.sh facades
#!./scripts/train_pix2pix.sh
python train.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA
To see more intermediate results, check out  .#!./scripts/test_pix2pix.sh
python test.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA
Apply a pre-trained model (CycleGAN)bash ./scripts/download_cyclegan_model.sh horse2zebra
bash ./datasets/download_cyclegan_dataset.sh horse2zebra
python test.py --dataroot datasets/horse2zebra/testA --name horse2zebra_pretrained --model test --no_dropout
Apply a pre-trained model (pix2pix)Download a pre-trained model with .bash ./scripts/download_pix2pix_model.sh facades_label2photo
bash ./datasets/download_pix2pix_dataset.sh facades
python test.py --dataroot ./datasets/facades/ --direction BtoA --model pix2pix --name facades_label2photo_pretrained
We provide the pre-built Docker image and Dockerfile that can run this code repo. See .Download pix2pix/CycleGAN datasets and create your own datasets.Best practice for training and testing your models.Before you post a new question, please first look at the above Q & A and existing GitHub issues.Custom Model and DatasetIf you plan to implement custom models and dataset for your new applications, we provide a dataset  and a model  as a starting point.To help users better understand and use our code, we briefly overview the functionality and implementation of each package and each module.Pull RequestYou are always welcome to contribute to this repository by sending a .Please run  and  before you commit the code. Please also update the code structure  accordingly if you add or remove files.CitationIf you use this code for your research, please cite our papers.@inproceedings{CycleGAN2017,
  title={Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks},
  author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},
  booktitle={Computer Vision (ICCV), 2017 IEEE International Conference on},
  year={2017}
}


@inproceedings{isola2017image,
  title={Image-to-Image Translation with Conditional Adversarial Networks},
  author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},
  booktitle={Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on},
  year={2017}
}
Other LanguagesRelated Projects[<marko.inline.RawText object at 0x000001592FF18B08>][<marko.inline.RawText object at 0x000001592FDC5808>][<marko.inline.RawText object at 0x000001592FF26348>]Cat Paper CollectionIf you love cats, and love reading cool graphics, vision, and learning papers, please check out the Cat Paper .AcknowledgmentsOur code is inspired by ."
https://github.com/fighting41love/funNLP,"中英文敏感词、语言检测、中外手机/电话归属地/运营商查询、名字推断性别、手机号抽取、身份证抽取、邮箱抽取、中日文人名库、中文缩写库、拆字词典、词汇情感值、停用词、反动词表、暴恐词表、繁简体转换、英文模拟中文发音、汪峰歌词生成器、职业名称词库、同义词库、反义词库、否定词库、汽车品牌词库、汽车零件词库、连续英文切割、各种中文词向量、公司名字大全、古诗词库、IT词库、财经词库、成语词库、地名词库、历史名人词库、诗词词库、医学词库、饮食词库、法律词库、汽车词库、动物词库、中文聊天语料、中文谣言数据、百度中文问答数据集、句子相似度匹配算法集合、bert资源、文本生成&摘要相关工具、cocoNLP信息抽取工具、国内电话号码正则匹配、清华大学XLORE:中英文跨语言百科知识图谱、清华大学人工智能技术系列报告、自然语言生成、NLU太难了系列、自动对联数据及机器人、用户名黑名单列表、罪名法务名词及分类模型、微信公众号语料、cs224n深度学习自然语言处理课程、中文手写汉字识别、中文自然语言处理 语料/数据集、变量命名神器、分词语料库+代码、任务型对话英文数据集、ASR 语音数据集 + 基于深度学习的中文语音识别系统、笑声检测器、Microsoft多语言数字/单位/如日期时间识别包、中华新华字典数据库及api(包括常用歇后语、成语、词语和汉字)、文档图谱自动生成、SpaCy 中文模型、Common Voice语音识别数据集新版、神经网络关系抽取、基于bert的命名实体识别、关键词(Keyphrase)抽取包pke、基于医疗领域知识图谱的问答系统、基于依存句法与语义角色标注的事件三元组抽取、依存句法分析4万句高质量标注数据、cnocr：用来做中文OCR的Python3包、中文人物关系知识图谱项目、中文nlp竞赛项目及代码汇总、中文字符数据、speech-aligner: 从“人声语音”及其“语言文本”产生音素级别时间对齐标注的工具、AmpliGraph: 知识图谱表示学习(Python)库：知识图谱概念链接预测、Scattertext 文本可视化(python)、语言/知识表示工具：BERT & ERNIE、中文对比英文自然语言处理NLP的区别综述、Synonyms中文近义词工具包、HarvestText领域自适应文本挖掘工具（新词发现-情感分析-实体链接等）、word2word：(Python)方便易用的多语言词-词对集：62种语言/3,564个多语言对、语音识别语料生成工具：从具有音频/字幕的在线视频创建自动语音识别(ASR)语料库、构建医疗实体识别的模型（包含词典和语料标注）、单文档非监督的关键词抽取、Kashgari中使用gpt-2语言模型、开源的金融投资数据提取工具、文本自动摘要库TextTeaser: 仅支持英文、人民日报语料处理工具集、一些关于自然语言的基本模型、基于14W歌曲知识库的问答尝试--功能包括歌词接龙and已知歌词找歌曲以及歌曲歌手歌词三角关系的问答、基于Siamese bilstm模型的相似句子判定模型并提供训练数据集和测试数据集、用Transformer编解码模型实现的根据Hacker News文章标题自动生成评论、用BERT进行序列标记和文本分类的模板代码、LitBank：NLP数据集——支持自然语言处理和计算人文学科任务的100部带标记英文小说语料、百度开源的基准信息抽取系统、虚假新闻数据集、Facebook: LAMA语言模型分析，提供Transformer-XL/BERT/ELMo/GPT预训练语言模型的统一访问接口、CommonsenseQA：面向常识的英文QA挑战、中文知识图谱资料、数据及工具、各大公司内部里大牛分享的技术文档 PDF 或者 PPT、自然语言生成SQL语句（英文）、中文NLP数据增强（EDA）工具、英文NLP数据增强工具 、基于医药知识图谱的智能问答系统、京东商品知识图谱、基于mongodb存储的军事领域知识图谱问答项目、基于远监督的中文关系抽取、语音情感分析、中文ULMFiT-情感分析-文本分类-语料及模型、一个拍照做题程序、世界各国大规模人名库、一个利用有趣中文语料库 qingyun 训练出来的中文聊天机器人、中文聊天机器人seqGAN、省市区镇行政区划数据带拼音标注、教育行业新闻语料库包含自动文摘功能、开放了对话机器人-知识图谱-语义理解-自然语言处理工具及数据、中文知识图谱：基于百度百科中文页面-抽取三元组信息-构建中文知识图谱、masr: 中文语音识别-提供预训练模型-高识别率、Python音频数据增广库、中文全词覆盖BERT及两份阅读理解数据、ConvLab：开源多域端到端对话系统平台、中文自然语言处理数据集、基于最新版本rasa搭建的对话系统、基于TensorFlow和BERT的管道式实体及关系抽取、一个小型的证券知识图谱/知识库、复盘所有NLP比赛的TOP方案、OpenCLaP：多领域开源中文预训练语言模型仓库、UER：基于不同语料+编码器+目标任务的中文预训练模型仓库、中文自然语言处理向量合集、基于金融-司法领域(兼有闲聊性质)的聊天机器人、g2pC：基于上下文的汉语读音自动标记模块、Zincbase 知识图谱构建工具包、诗歌质量评价/细粒度情感诗歌语料库、快速转化「中文数字」和「阿拉伯数字」、百度知道问答语料库、基于知识图谱的问答系统、jieba_fast 加速版的jieba、正则表达式教程、中文阅读理解数据集、基于BERT等最新语言模型的抽取式摘要提取、Python利用深度学习进行文本摘要的综合指南、知识图谱深度学习相关资料整理、维基大规模平行文本语料、StanfordNLP 0.2.0：纯Python版自然语言处理包、NeuralNLP-NeuralClassifier：腾讯开源深度学习文本分类工具、端到端的封闭域对话系统、中文命名实体识别：NeuroNER vs. BertNER、新闻事件线索抽取、2019年百度的三元组抽取比赛：“科学空间队”源码、基于依存句法的开放域文本知识三元组抽取和知识库构建、中文的GPT2训练代码、ML-NLP - 机器学习(Machine Learning)NLP面试中常考到的知识点和代码实现、nlp4han:中文自然语言处理工具集(断句/分词/词性标注/组块/句法分析/语义分析/NER/N元语法/HMM/代词消解/情感分析/拼写检查、XLM：Facebook的跨语言预训练语言模型、用基于BERT的微调和特征提取方法来进行知识图谱百度百科人物词条属性抽取、中文自然语言处理相关的开放任务-数据集-当前最佳结果、CoupletAI - 基于CNN+Bi-LSTM+Attention 的自动对对联系统、抽象知识图谱、MiningZhiDaoQACorpus - 580万百度知道问答数据挖掘项目、brat rapid annotation tool: 序列标注工具、大规模中文知识图谱数据：1.4亿实体、数据增强在机器翻译及其他nlp任务中的应用及效果、allennlp阅读理解:支持多种数据和模型、PDF表格数据提取工具 、 Graphbrain：AI开源软件库和科研工具，目的是促进自动意义提取和文本理解以及知识的探索和推断、简历自动筛选系统、基于命名实体识别的简历自动摘要、中文语言理解测评基准，包括代表性的数据集&基准模型&语料库&排行榜、树洞 OCR 文字识别 、从包含表格的扫描图片中识别表格和文字、语声迁移、Python口语自然语言处理工具集(英文)、 similarity：相似度计算工具包，java编写、海量中文预训练ALBERT模型 、Transformers 2.0 、基于大规模音频数据集Audioset的音频增强 、Poplar：网页版自然语言标注工具、图片文字去除，可用于漫画翻译 、186种语言的数字叫法库、Amazon发布基于知识的人-人开放领域对话数据集 、中文文本纠错模块代码、繁简体转换 、 Python实现的多种文本可读性评价指标、类似于人名/地名/组织机构名的命名体识别数据集 、东南大学《知识图谱》研究生课程(资料)、. 英文拼写检查库 、 wwsearch是企业微信后台自研的全文检索引擎、CHAMELEON：深度学习新闻推荐系统元架构 、 8篇论文梳理BERT相关模型进展与反思、DocSearch：免费文档搜索引擎、 LIDA：轻量交互式对话标注工具 、aili - the fastest in-memory index in the East 东半球最快并发索引 、知识图谱车音工作项目、自然语言生成资源大全 、中日韩分词库mecab的Python接口库、中文文本摘要/关键词提取、汉字字符特征提取器 (featurizer)，提取汉字的特征（发音特征、字形特征）用做深度学习的特征、中文生成任务基准测评 、中文缩写数据集、中文任务基准测评 - 代表性的数据集-基准(预训练)模型-语料库-baseline-工具包-排行榜、PySS3：面向可解释AI的SS3文本分类器机器可视化工具 、中文NLP数据集列表、COPE - 格律诗编辑程序、doccano：基于网页的开源协同多语言文本标注工具 、PreNLP：自然语言预处理库、简单的简历解析器，用来从简历中提取关键信息、用于中文闲聊的GPT2模型：GPT2-chitchat、基于检索聊天机器人多轮响应选择相关资源列表(Leaderboards、Datasets、Papers)、(Colab)抽象文本摘要实现集锦(教程 、词语拼音数据、高效模糊搜索工具、NLP数据增广资源集、微软对话机器人框架 、 GitHub Typo Corpus：大规模GitHub多语言拼写错误/语法错误数据集、TextCluster：短文本聚类预处理模块 Short text cluster、面向语音识别的中文文本规范化、BLINK：最先进的实体链接库、BertPunc：基于BERT的最先进标点修复模型、Tokenizer：快速、可定制的文本词条化库、中文语言理解测评基准，包括代表性的数据集、基准(预训练)模型、语料库、排行榜、spaCy 医学文本挖掘与信息提取 、 NLP任务示例项目代码集、 python拼写检查库、chatbot-list - 行业内关于智能客服、聊天机器人的应用和架构、算法分享和介绍、语音质量评价指标(MOSNet, BSSEval, STOI, PESQ, SRMR)、 用138GB语料训练的法文RoBERTa预训练语言模型 、BERT-NER-Pytorch：三种不同模式的BERT中文NER实验、无道词典 - 有道词典的命令行版本，支持英汉互查和在线查询、2019年NLP亮点回顾、 Chinese medical dialogue data 中文医疗对话数据集 、最好的汉字数字(中文数字)-阿拉伯数字转换工具、 基于百科知识库的中文词语多词义/义项获取与特定句子词语语义消歧、awesome-nlp-sentiment-analysis - 情感分析、情绪原因识别、评价对象和评价词抽取、LineFlow：面向所有深度学习框架的NLP数据高效加载器、中文医学NLP公开资源整理 、MedQuAD：(英文)医学问答数据集、将自然语言数字串解析转换为整数和浮点数、Transfer Learning in Natural Language Processing (NLP) 、面向语音识别的中文/英文发音辞典、Tokenizers：注重性能与多功能性的最先进分词器、CLUENER 细粒度命名实体识别 Fine Grained Named Entity Recognition、 基于BERT的中文命名实体识别、中文谣言数据库、NLP数据集/基准任务大列表、nlp相关的一些论文及代码, 包括主题模型、词向量(Word Embedding)、命名实体识别(NER)、文本分类(Text Classificatin)、文本生成(Text Generation)、文本相似性(Text Similarity)计算等，涉及到各种与nlp相关的算法，基于keras和tensorflow 、Python文本挖掘/NLP实战示例、 Blackstone：面向非结构化法律文本的spaCy pipeline和NLP模型通过同义词替换实现文本“变脸” 、中文 预训练 ELECTREA 模型: 基于对抗学习 pretrain Chinese Model 、albert-chinese-ner - 用预训练语言模型ALBERT做中文NER 、基于GPT2的特定主题文本生成/文本增广、开源预训练语言模型合集、多语言句向量包、编码、标记和实现：一种可控高效的文本生成方法、 英文脏话大列表 、attnvis：GPT2、BERT等transformer语言模型注意力交互可视化、CoVoST：Facebook发布的多语种语音-文本翻译语料库，包括11种语言(法语、德语、荷兰语、俄语、西班牙语、意大利语、土耳其语、波斯语、瑞典语、蒙古语和中文)的语音、文字转录及英文译文、Jiagu自然语言处理工具 - 以BiLSTM等模型为基础，提供知识图谱关系抽取 中文分词 词性标注 命名实体识别 情感分析 新词发现 关键词 文本摘要 文本聚类等功能、用unet实现对文档表格的自动检测，表格重建、NLP事件提取文献资源列表 、 金融领域自然语言处理研究资源大列表、CLUEDatasetSearch - 中英文NLP数据集：搜索所有中文NLP数据集，附常用英文NLP数据集 、medical_NER - 中文医学知识图谱命名实体识别 、(哈佛)讲因果推理的免费书、知识图谱相关学习资料/数据集/工具资源大列表、Forte：灵活强大的自然语言处理pipeline工具集 、Python字符串相似性算法库、PyLaia：面向手写文档分析的深度学习工具包、TextFooler：针对文本分类/推理的对抗文本生成模块、Haystack：灵活、强大的可扩展问答(QA)框架、中文关键短语抽取工具","The Most Powerful NLP-Weapon ArsenalNLP民工的乐园: 几乎最全的中文NLP资源库在入门到熟悉NLP的过程中，用到了很多github上的包，遂整理了一下，分享在这里。很多包非常有趣，值得收藏，满足大家的收集癖！如果觉得有用，请分享并star:star:，谢谢！长期不定时更新，欢迎watch和fork！:heart::heart::heart:|  :fire::fire::fire::fire::fire::fire::fire::fire::fire::fire:   &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ||  ----  || *    *   *   *    *   *   *   *   *   *   * |  :eggplant: :cherries: :pear: :tangerine:   &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  |  :sunflower: :strawberry:  :melon: :tomato: :pineapple: &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;||  ----  | ----  || *    *    *     *    *      *     *     *    *   | *     *     *      *     *    *    *    *     *  || *    *   *   *    *    *    *   *   *   *   |  *     *    *   *   *   *   *   *   *   *   *   |类ChatGPT的模型评测对比| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :----   |          :--- ||   ChatALL：可以同时与多个AI聊天机器人（含清华、讯飞的产品）    |    可以同时与多个AI聊天机器人（如ChatGPT、Bing Chat、Bard、Alpaca、Vincuna、Claude、ChatGLM、MOSS、iFlytek Spark、ERNIE等）进行对话的工具。它可以并行发送提示给不同的AI机器人，帮助用户找到最好的回答   |    ||  Chatbot Arena    |  实际场景用Elo rating对 LLM 进行基准测试 - 介绍了 Chatbot Arena，一种针对大型语言模型 (LLM) 的基准平台，采用匿名、随机的方式进行对抗评测，评测方式基于国际象棋等竞技游戏中广泛使用的 Elo rating system。发布了9个流行的开源 LLM 模型的 Elo rating 并推出排行榜。平台采用 FastChat 多模型服务系统，在多个语言下提供交互式界面，数据来源于用户投票。总结了 Chatbot Arena 的优点并计划提供更好的采样算法、排名和服务系统     |       ||   类ChatGPT模型评测总结   |   大型语言模型(LLM)受到广泛关注，这些强大的模型能够理解复杂的信息，并对各种问题提供类人的回应。其中GPT-3和GPT-4表现最好，Flan-t5和Lit-LLaMA表现也不错。但要注意，模型商用可能需要付费和数据共享    |       ||   大型语言模型（LLMs）大盘点   |       |       ||   大模型评测方面的最新研究   |    长文本建模一直是ChaGPT令人惊艳的能力之一，我们以【篇章翻译】为实验场景，对大模型的篇章建模能力进行全面、细粒度的测试。   |       ||中文大模型评测工具&排行榜|C-Eval是一个全面的中文评估套件，适用于基础模型。它包含13948个多项选择题，涵盖52个不同的学科和四个难度级别，具体如下所示。请访问我们的网站或查阅我们的论文获取更多详细信息。|||OpenCompass 大模型评测|OpenCompass 上海人工智能实验室开发的一款开源、高效、全面的评测大模型体系及开放平台，提供完整开源可复现的评测框架，支持大语言模型、多模态模型各类模型的一站式评测。利用分布式技术，即使面对千亿参数模型也能在数小时内完成评测。基于多个不同维度的高认可度数据集开放多样化的评测方式，包括零样本评测、小样本评测和思维链评测，全方位量化模型各个维度能力。|  |类ChatGPT的资料| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :----   |          :--- ||  Open LLMs：可供商业使用的开放大型语言模型(LLM)    |    A list of open LLMs available for commercial use   |       ||LLM Zoo: 大型语言模型的数据、模型和基准集市|LLM Zoo: democratizing ChatGPT - a project that provides data, models, and evaluation benchmark for large language models|||   大型语言模型(LLM)资料合集   |     相关论文列表，包括指导、推理、决策、持续改进和自我提升等方面的研究工作  |       ||DecryptPrompt|总结Prompt&LLM论文，开源数据&模型，AIGC应用|||   SmartGPT    |   旨在为大型语言模型(尤其是GPT-3.5和GPT-4)提供完成复杂任务的能力，通过将它们分解成更小的问题，并使用互联网和其他外部来源收集信息。特点包括模块化设计，易于配置，以及对插件的高度支持。SmartGPT的运作基于""Autos""的概念，包括""Runner""和""Assistant""两种类型，都配有处理计划、推理和任务执行的LLM代理。此外，SmartGPT还具有内存管理系统，以及可以定义各种命令的插件系统     |    || OpenGPT      |   用于创建基于指令的数据集并训练对话领域专家大型语言模型(LLMs)的框架。已经成功应用于训练健康护理对话模型NHS-LLM，利用来自英国国家卫生服务体系(NHS)网站的数据，生成了大量的问答对和独特对话  |           ||   PaLM 2技术报告   |   Google最新发布PaLM 2，一种新的语言模型，具有更好的多语言和推理能力，同时比其前身PaLM更节省计算资源。PaLM 2综合了多项研究进展，包括计算最优的模型和数据规模、更多样化和多语言的数据集、以及更有效的模型架构和目标函数。PaLM 2在多种任务和能力上达到了最先进的性能，包括语言水平考试、分类和问答、推理、编程、翻译和自然语言生成等。PaLM 2还展示了强大的多语言能力，能够处理数百种语言，并在不同语言之间进行翻译和解释。PaLM 2还考虑了负责任的使用问题，包括推理时控制毒性、减少记忆化、评估潜在的伤害和偏见等    |       ||   DB-GPT   |    于vicuna-13b和FastChat的开源实验项目，采用了langchain和llama-index技术进行上下文学习和问答。项目完全本地化部署，保证数据的隐私安全，能直接连接到私有数据库处理私有数据。其功能包括SQL生成、SQL诊断、数据库知识问答等   |        ||  Transformers相关文献资源大列表    |   包含了各种各样的Transformer模型，例如BERT、GPT、Transformer-XL等，这些模型已经在许多自然语言处理任务中得到了广泛应用。此外，该列表还提供了这些模型的相关论文和代码链接，为自然语言处理领域的研究人员和开发者提供了很好的参考资源    |       ||   GPT-4终极指南   |    一份关于如何使用GPT3和GPT4的指南，其中包括100多个资源，可以帮助学习如何用它来提高生活效率。包括如何学习ChatGPT基础知识、如何学习ChatGPT高级知识、如何在语言学习中使用GPT-3、如何在教学中使用GPT-3、如何使用GPT-4等，还提供了如何升级到ChatGPT+计划以使用GPT-4以及如何免费使用GPT-4的方法等内容。同时，还提供了如何在业务、生产力、受益、金钱等方面使用ChatGPT的指南   |       ||  基于LoRA的LLM参数高效微调    |       |       ||  复杂推理：大语言模型的北极星能力     |  在 GPT-4 发布博客中，作者写道：“在一次随意的谈话中，GPT-3.5 和 GPT-4 之间的区别可能是微妙的。当任务的复杂程度达到足够的阈值时，差异就会显现出来。”这意味着复杂任务很可能是大型和小型语言模型的关键差异因素。在这篇文章中，我们将仔细分析讨论如何让大语言模型拥有强大的复杂推理能力。     |       ||   大型语言模型的涌现能力是否是海市蜃楼？   |    大语言模型的涌现能力一直是被大家视作很神奇的现象，似乎是一种大力出奇迹，但这篇论文认为这可能只是一种错觉。   |       ||   大语言模型的概率总结  |   非常详尽的LLM科学解释和总结    |       ||  LLaMA 模型简史    |    LLaMA是Meta发布的语言模型，采用Transformer架构，有多个版本，最大为65B参数。与GPT类似，可用于进一步微调，适用于多种任务。与GPT不同的是，LLaMA是开源的，可以在本地运行。现有的LLaMA模型包括：Alpaca、Vicuna、Koala、GPT4-x-Alpaca和WizardLM。每个模型都有不同的训练数据和性能表现   |       ||  大型语言模型的复杂推理     |   讨论了如何训练具有强大复杂推理能力的语言模型，并探讨了如何有效地提示模型以充分释放其潜力；针对语言模型和编程的训练相似性，提出了三阶段的训练：持续训练、监督微调和强化学习；介绍了评估大型语言模型推理能力的一套任务集合；讨论了如何进行提示工程，通过提供各种学习机会使模型获得更好的学习效果，最终实现智能化    |       ||   大语言模型进化树   |       |       ||李宏毅：穷人如何低资源复刻自己的ChatGPT||||   训练ChatGPT的必备资源：语料、模型和代码库完全指南   |       |       ||  GitHub宝藏库，里面整理了GPT相关的各种开源项目    |       |       ||  ChatGPT中文指南    |       |       ||   探讨了ChatGPT在自然语言处理中的应用、优势、限制以及未来发展方向   |   强调了在使用该技术时的伦理道德考量和提示工程技术。    |       ||大型语言模型相关文献资源列表||||大型语言模型文献综述--中文版||||ChatGPT 相关资源大列表||||Pre-Training to Learn in Context||||Langchain架构图||||LLM开发人员都应该知道的数字||||大语言模型如何构建强大的复杂推理能力||||LLMs九层妖塔|分享打怪(ChatGLM、Chinese-LLaMA-Alpaca、MiniGPT-4、FastChat、LLaMA、gpt4all等)实战与经验||类ChatGPT的开源框架| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :----   |          :--- ||   LLM-As-Chatbot   |   这个项目把市面上有的LLM全部做成了Chatbot，直接可以在google colab运行，不需要自己搭建，非常适用于想体验LLM的朋友们。我刚试了，真的超简单。有些LLM需要的显存比较多，所以最好是要有colab pro订阅。    |       ||   OpenBuddy    |    一款强大的开源多语言聊天机器人模型，目标是全球用户，重点是对话AI和流畅的多语言支持，包括英文、中文等多种语言。基于Facebook的LLAMA模型，进行了微调，包括扩展词汇表、增加常用字符和增强的token embeddings。通过这些改进和多轮对话数据集，OpenBuddy提供了一个强大的模型，能回答问题并在各种语言之间进行翻译任务。OpenBuddy的使命是提供一个免费、开放且可离线使用的AI模型，该模型可以在用户的设备上运行，无论他们的语言或文化背景如何。目前，OpenBuddy-13B的演示版本可以在Discord服务器上找到。其关键功能包括多语言对话AI(包括中文、英文、日文、韩文、法文等)、增强的词汇表和对常见CJK字符的支持，以及两种模型版本：7B和13B   |    ||   Panda: 海外中文开源大语言模型   |    基于 Llama-7B, -13B, -33B, -65B 进行中文领域上的持续预训练，使用了接近15M条数据，并针对推理能力在中文benchmark上进行了评测   |        ||  Dromedary：一个开源的自对齐语言模型，只需少量人工监督即可进行训练    |       |       ||   LaMini-LM 蒸馏的小型、高效的语言模型集合  |   从 ChatGPT 蒸馏的小型、高效的语言模型集合，在2.58 M 指令大规模数据集上进行训练    |       ||   LLaMA-Adapter V2    |   上海人工智能实验室 LLaMA-Adapter V2，仅注入14M参数，1小时时间即可完成训练，对比较果确实很惊艳，且具有多模态功能（对图像进行解释和问答）    |       ||   HuggingChat   |   Hugging Face 推出第一个 ChatGPT 开源替代品：HuggingChat。基于 Open Assistant  大模型搭建，支持中文对话与编写代码，但暂不支持中文回复。应用已上线，无需代理，打开即可访问    |       || Open-Chinese-LLaMA     |   基于 LLaMA-7B 经过 中文数据集增量预训练 产生的 中文大语言模型基座    |       ||   OpenLLaMA   |   LLaMA模型的开源复现，在RedPajama数据集上训练，使用了与LLaMA相同的预处理步骤和超参数，模型结构，上下文长度，训练步骤，学习率调度和优化器。OpenLLaMA的PyTorch和Jax权重可以在Huggingface Hub上获得。OpenLLaMA在各种任务中展现出与LLaMA和GPT-J相似的表现，部分任务表现优异    |       ||  replit-code-v1-3b    |   BY-SA 4.0授权发布，这意味着允许商业使用    |      ||   MOSS   |  MOSS是一个支持中英双语和多种插件的开源对话语言模型，moss-moon系列模型具有160亿参数，在FP16精度下可在单张A100/A800或两张3090显卡运行，在INT4/8精度下可在单张3090显卡运行。MOSS基座语言模型在约七千亿中英文以及代码单词上预训练得到，后续经过对话指令微调、插件增强学习和人类偏好训练具备多轮对话能力及使用多种插件的能力。     |      ||   RedPajama   |   1.2 万亿tokens数据集    |       ||  chinese_llama_alpaca_lora 抽取框架  |       |       ||   Scaling Transformer to 1M tokens and beyond with RMT   |  该论文提出一种名为 RMT 的新技术，或许可将 Transform 的 Token 上限扩展至 100 万，甚至更多。     |       ||   Open Assistant   |   包含大量AI生成的、人工标注的语料库和包括基于LLaMA和基于Pythia的多种模型可选。发布的数据集包括超过161K较高质量的，多达35种语言的人工助手型交互对话语料库    |        ||   ChatGLM Efficient Tuning   |  基于 PEFT 的高效 ChatGLM 微调     |       ||  Dolly介绍    |       |       ||  Baize：一种对自聊天数据进行参数高效调优的开源聊天模型    |   Baize是一个开源的聊天模型，可以进行多轮对话。它是通过使用ChatGPT自我对话生成高质量的多轮聊天语料库，并使用参数高效调整来增强LLaMA（一个开源的大型语言模型）而创建的。Baize模型在具有最小潜在风险的情况下表现出良好的多轮对话性能。它可以在单个GPU上运行，使更广泛的研究人员可以使用它。Baize模型和数据仅用于研究目的。    |       ||   GPTrillion--未找到开源代码   |  包含1.5万亿（1.5T）参数的大模型GPTrillion开源了，号称是目前世界上最大的开源LLM    |       ||Cerebras-GPT-13B(可商用)||||Chinese-ChatLLaMA|中文ChatLLaMA对话模型；预训练/指令微调数据集，基于 TencentPretrain 多模态预训练框架构建，支持简繁体中文、英文、日文等多语言|||Lit-LLaMA|基于Apache 2.0许可证完全开源的LLaMA独立实现，建立在nanoGPT之上，旨在解决原始LLaMA代码采用GPL许可证的限制，以实现更广泛的学术和商业应用|||MosaicML|MPT-7B-StoryWriter，65K tokens，可以把《了不起的盖茨比》都一次性扔进去。|||Langchain|大型语言模型（LLMs）正在成为一项具有变革性的技术，使开发者能够构建以前无法实现的应用程序。然而，仅仅使用这些独立的LLMs通常不足以创建一个真正强大的应用程序 - 真正的力量来自于能够将它们与其他计算或知识来源相结合。|||Guidance|引导能够比传统的提示或链接更有效地控制现代语言模型，并且更高效。引导程序允许您将生成、提示和逻辑控制交错到单一连续流中，与语言模型实际处理文本的方式相匹配。像""Chain of Thought""及其许多变体（例如ART、Auto-CoT等）这样的简单输出结构已被证明能改善语言模型的性能。更强大的语言模型（如GPT-4）的出现使得更丰富的结构成为可能，而引导则使得构建这种结构变得更加容易和经济。|||WizardLM|赋予大型预训练语言模型遵循复杂指令的能力，使用完整进化指令（约300k）训练的WizardLM-7B模型||LLM的训练_推理_低资源_高效训练| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :----   |          :--- ||QLoRA--Guanaco|一种高效的微调方法，可以在单个48GB的GPU上微调一个拥有65B参数的模型，同时保持完整的16位微调任务性能，并通过QLoRA将梯度反向传播通过一个冻结的、4位量化的预训练语言模型到低秩适配器（LoRA）|||Chinese-Guanaco|一个中文低资源的量化训练/部署方案|||   DeepSpeed Chat: 一键式RLHF训练  |       |       ||   LLMTune: 在消费级GPU上微调大型65B+LLM   |   可以在普通消费级GPU上进行4位微调，例如最大的65B LLAMA模型。LLMTune还实现了LoRA算法和GPTQ算法来压缩和量化LLM，并通过数据并行处理大型模型。此外，LLMTune提供了命令行界面和Python库的使用方式    |       ||  基于ChatGLM-6B+LoRA在指令数据集上进行微调    |  基于deepspeed支持多卡微调，速度相比单卡提升8-9倍具体设置可见 微调3 基于DeepSpeed进行Lora微调     |       ||  微软发布RLHF训练工具DeepSpeed Chat    |       |       ||  LlamaChat：Mac上基于LLaMa的聊天机器人    |       |       ||   ChatGPT/GPT4开源“平替”们   |       |       ||训练大型机器学习模型的实用建议和技巧|帮助您训练大型模型（>1B 参数）、避免不稳定性、保存开始失败的实验而不从 0 重新开始|||  Instruction Tuning with GPT-4    |       |       ||   xturing   |   一个Python软件包，用于高效、快速、简单地微调LLM模型，支持LLaMA、GPT-J、GPT-2等多种模型，可使用单GPU和多GPU训练，使用LoRA等高效微调技术可将硬件成本降低高达90%，并在短时间内完成模型训练   |       ||   GPT4All   |    一个允许在Macbook本地运行GPT的开源项目。基于LLaMa-7B大语言模型打造，包括数据、代码和demo都是开源的，对话风格偏向AI助理   |       ||   用Alpaca-LoRA微调ChatGPT类模型   |       |       ||  LMFlow    |   可扩展、方便有效的工具箱，用于微调大型机器学习模型    |       ||闻达：大型语言模型调用平台|目前支持chatGLM-6B、chatRWKV、chatYuan和chatGLM-6B模型下的chatPDF（自建知识库查找）' |||Micro Agent|小型自主智能体开源项目，由LLM(OpenAI GPT-4)提供动力，可以为你编写软件，只需设置一个“目的”，让它自己工作|||Llama-X|开源的学术研究项目，通过社区共同努力，逐步将LLaMA的性能提高到SOTA LLM水平，节省重复工作，共同创造更多、更快的增量|||Chinese-LLaMA-Alpaca|中文LLaMA&Alpaca大语言模型+本地部署 (Chinese LLaMA & Alpaca LLMs) - 开源了经过中文文本数据预训练的中文LLaMA大模型；开源了进一步经过指令精调的中文Alpaca大模型；快速地使用笔记本电脑（个人PC）本地部署和体验量化版大模型|  ||Efficient Alpaca|基于LLaMA实现的开源项目，旨在通过微调 LLaMA-7B模型在资源消耗更少、推理速度更快、更适合研究者使用方面提高Stanford Alpaca的性能|||ChatGLM-6B-Slim|裁减掉20K图片Token的ChatGLM-6B，完全一样的性能，占用更小的显存|  ||Chinese-Vicuna|一个中文低资源的llama+lora方案|  ||Alpaca-LoRA|用LoRA在消费级硬件上复现斯坦福Alpaca的结果|||LLM Accelerator|让基础大模型更聪明的LLM Accelerator来了！基础大模型正在诸多应用中发挥着日益重要的作用。大多数大语言模型的训练都是采取自回归的方式进行生成，虽然自回归模型生成的文本质量有所保证，但却导致了高昂的推理成本和长时间的延迟。由于大模型的参数量巨大、推理成本高，因此如何在大规模部署大模型的过程中降低成本、减小延迟是一个关键课题。针对此问题，微软亚洲研究院的研究员们提出了一种使用参考文本无损加速大语言模型推理的方法 LLM Accelerator，在大模型典型的应用场景中可以取得两到三倍的加速。|||大语言模型（LLM）微调技术笔记||||PyLLMs|简洁的 Python 库，用于连接各种 LLM(OpenAI、Anthropic、Google、AI21、Cohere、Aleph Alpha、HuggingfaceHub)，内置模型性能基准。非常适合快速原型设计和评估不同模型，具有以下特点：通过少量代码连接顶级 LLM；响应元数据包括处理的Token、成本和延迟，对各个模型进行标准化；支持多模型：同时从不同模型获取补全；LLM 基准：评估模型的质量、速度和成本|||用混合精度加速大型语言模型|通过使用低精度浮点数运算，可以将训练和推断速度提升多达3倍，同时不影响模型准确性|||新的LLM训练方法 Federate|杜克大学和微软一起发布了一个新的LLM训练方法 Federated GPT，这个训练方法是将原本中心化的训练方法分散到不同的边缘设备里面（edge device），然后训练完成后，再上传到中心去将各子模型合并。||提示工程| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :----   |          :--- ||   OpenBuprompt-engineering-note  |   提示工程笔记(课程总结)》介绍了面向开发者的 ChatGPT Prompt Engineering Learning Notes 课程，该课程提供了语言模型的工作原理和提示工程实践，并展示了如何将语言模型 API 应用于各种任务的应用程序中。课程包括总结、推断、转换、扩展和打造聊天机器人等方面的内容，并讲述了如何设计好的提示和构建自定义聊天机器人。  |    ||  提示工程指南    |       |       ||  AIGC提示工程学习站 Learn Prompt   |  ChatGPT/Midjourney/Runway     |       ||  Prompts 精选 - ChatGPT 使用指南    |   ChatGPT 使用指南，提升 ChatGPT 可玩性和可用性    |       ||   非官方的ChatGPT资源聚合列表，旨在汇总使用ChatGPT   |    旨在汇总使用ChatGPT的应用、Web应用、浏览器扩展、CLI工具、机器人、集成、软件包、文章等资源   |       || Snack Prompt：ChatGPT Prompt提示分享社区   |       |       ||   ChatGPT提问技巧   |  如何向 ChatGPT 提问以获得高质量答案：提示技巧工程完全指南     |       ||   rompt-Engineering-Guide-Chinese - 提示工程师指南   |     源自英文版，但增加了AIGC的prompt部分  |       ||  OpenPrompt    |  一个开放的共享Prompt社区，大家一起推荐好用的prompt     |       ||  GPT-Prompts    |    教你如何用GPT生成Prompts   |       |类ChatGPT的文档问答| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :----   |          :--- ||   privateGPT    |    基于GPT4All-J的私有化部署文档问答平台，无需联网，能100%保证用户的隐私不泄露。提供了一个API，用户可以使用自己的文档进行交互式问答和生成文本。此外，平台支持自定义训练数据和模型参数，以满足个性化需求    |    ||  Auto-evaluator   |  文档问答的自动评估 ；、     |      ||   PDF GP   |   一个基于 GPT 实现的开源 PDF 文档聊天方案,主要实现以下功能：跟 PDF 文档进行一对一对话；自动切割内容，并使用强大的深度平均网络编码器来生成嵌入；对 PDF 内容执行语义搜索，并将最相关的嵌入传递给 Open AI；自定义逻辑，生成更精确的响应信息，速度要比 OpenAI 的快。    |       ||Redis-LLM-Document-Chat|用LlamaIndex、Redis和OpenAI与PDF文档进行交互，包含一个Jupyter笔记本，演示了如何使用Redis作为向量数据库来存储和检索文档向量，还展示了如何使用LlamaIndex在文档中执行语义搜索，以及如何利用OpenAI提供类似聊天机器人的体验|||doc-chatbot|GPT-4 + Pinecone + LangChain + MongoDB实现的文档聊天机器人，可多文件、多话题和多窗口聊天，聊天历史由MongoDB保存|||document.ai|基于向量数据库与GPT3.5的通用本地知识库方案(A universal local knowledge base solution based on vector database and GPT3.5)|||DocsGPT|DocsGPT是一种尖端的开源解决方案，可以简化在项目文档中查找信息的过程。通过集成强大的GPT模型，开发人员可以轻松地提出关于项目的问题并获得准确的答案。|||ChatGPT Retrieval Plugin|ChatGPT检索插件存储库提供了一种灵活的解决方案，可以使用自然语言查询对个人或组织文档进行语义搜索和检索。|||LamaIndex|lamaIndex（GPT索引）是您的LLM应用程序的数据框架。|||chatWeb|ChatWeb可以爬取任意网页或PDF，DOCX，TXT文件并提取正文，可以生成嵌入式概要，可以根据正文内容回答你的问题。 基于gpt3.5的chatAPI和embeddingAPI，以及向量数据库实现。||类ChatGPT的行业应用| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :----   |          :--- ||   新闻报道进行情感分析    |  用ChatGPT通过对上市公司的新闻报道进行情感分析，在15个月时间内在股票市场(交易期权)产生了500%的回报（在历史数据中测试得出的结果）——探讨了ChatGPT在利用新闻标题的情感分析来预测股市回报方面的潜力。发现ChatGPT的情感分析能力超过了传统的方法，并且与股市回报呈正相关。提出ChatGPT在金融经济领域有很大的价值，并对未来的研究和应用提出了一些启示和建议   |    ||  编程语言生成模型 StarCoder    |   BigCode是 ServiceNow Inc. 和 Hugging Face Inc. 合作成立的。StarCoder 有多个版本。核心版本 StarCoderBase 具有 155 亿个参数，支持80多种编程语言，8192个token的上下文。视频为其vscode插件效果    |       ||  CodeGen2: Lessons for Training LLMs on Programming and Natural Languages    |   code generation    |       ||  MedicalGPT-zh：中文医疗通用语言模型    |   中文医疗通用语言模型，基于28个科室的医疗共识与临床指南文本，提高模型的医疗领域知识与对话能力    |       ||  MagicSlides    |    不少人梦寐以求的AI自作PPT，免费版每月能做3个PPT，支持2500字输入   |       ||   SalesGPT   |   使用LLM实现上下文感知的销售助手，可自动化销售拓展代表的活动，如外呼销售电话    |       ||  华驼(HuaTuo): 基于中文医学知识的LLaMA微调模型    |       |       ||  ai-code-translator    |  帮助你把代码从一种语言翻译成另一种语言，这事对ChatGPT来说简直太擅长了，尤其是GPT-4，翻译质量相当高，而且tokens长度也可以更长。     |       ||   ChatGenTitle   |    使用百万arXiv论文信息在LLaMA模型上进行微调的论文题目生成模型   |       ||   Regex.ai    |    一款所见即所得的，基于 AI 的正则表达式自动生成工具，只需要选择出数据，它就能帮你写正则表达式，并提供多种提取数据的方式   |       ||   ChatDoctor   |  一个基于医学领域知识微调LLaMA的医学聊天模型，其中医学数据包含大约700种疾病的数据、以及大约5000段医生和病人的对话记录     |       ||CodeGPT|提高编程能力的关键在于数据。CodeGPT是通过GPT生成的用于GPT的代码对话数据集。现在公开了32K条中文数据，让模型更擅长编程|||LaWGPT |一系列基于中文法律知识的开源大语言模型|||LangChain-ChatGLM-Webui|受langchain-ChatGLM启发, 利用LangChain和ChatGLM-6B系列模型制作的Webui, 提供基于本地知识的大模型应用.目前支持上传 txt、docx、md、pdf等文本格式文件, 提供包括ChatGLM-6B系列、Belle系列等模型文件以及GanymedeNil/text2vec-large-chinese、nghuyong/ernie-3.0-base-zh、nghuyong/ernie-3.0-nano-zh等Embedding模型.||类ChatGPT的课程资料| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :----   |          :--- ||   Databricks   |   （Dolly模型的作者）在edX发布了两个免费课程程，其中第二个是关于LLM是如何构建的。    |       ||   大语言模型技术分享系列   |   东北大学自然语言处理实验室    |       ||  GPT-4是如何工作的？如何利用GPT-4打造智能程序？    |  哈佛大学CS50公开课     |       ||  提示工程最佳实践：Andrew Ng 提示工程新课摘要+LangChain经验总结    |       |       ||   微调LLM模型   |   如果你对微调LLM模型感兴趣，一定要关注这个油管博主，他把几乎世面上所有的LLM模型都公开了微调的方法。    |   油管博主 Sam Witteveen    ||Transformer的架构解读|通俗易懂的介绍| ||Transformer multi head机制的视频|如果想要真正理解整个Transform的每一个细节，包括里面的数学原理，可以看一下这个视频，真的是剖析地非常详细|||Introduction to Large Language Models | 大语言模型介绍|介绍了大型语言模型（Large Language Models，LLMs）的概念、使用场景、提示调整以及Google的Gen AI开发工具。||LLM的安全问题| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :----   |          :--- ||   LLM模型安全研究   |       |       ||  Chatbot Injections & Exploit    | 收集了一些Chatbot注入和漏洞的例子，以帮助人们了解Chatbot的潜在漏洞和脆弱性。注入和攻击的方式包括命令注入、字符编码、社交工程、表情符号、Unicode等。仓库提供了一些示例，其中一些包括可用于攻击Chatbot的表情符号列表      |       ||   GPTSecurity   |    一个涵盖了前沿学术研究和实践经验分享的社区，集成了生成预训练 Transformer（GPT）、人工智能生成内容（AIGC）以及大型语言模型（LLM）等安全领域应用的知识。在这里，您可以找到关于GPT/AIGC/LLM最新的研究论文、博客文章、实用的工具和预设指令（Prompts）。   |       |多模态LLM| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :----   |          :--- ||  DeepFloyd IF    |  高度逼真且具有语言理解能力的最新开源文本到图像模型，由一个冻结文本编码器和三个连续的像素扩散模块组成，是一个高效的模型，性超越了当前最先进的模型，在COCO数据集上实现了零样本的FID得分为6.66     |       ||  Multi-modal GPT    |   用多模态GPT训练一个能同时接收视觉和语言指令的聊天机器人。基于OpenFlamingo多模态模型，使用各种开放数据集创建各种视觉指导数据，联合训练视觉和语言指导，有效提高模型性能    |       ||   AudioGPT   |   Understanding and Generating Speech, Music, Sound, and Talking Head' by AIGC-Audio    |       ||   text2image-prompt-generator   |    基于GPT-2用25万条Midjourney的promps训练出来的小模型，可以生成高质量的Midjourney  prompt   |       ||  汇总6个Midjourney以外的免费以文生图服务：    |       |            || BARK   |   一个非常强大的TTS（文字转语音）项目，这个项目的特点是，它可以在文字中加入提示词，比如“大笑”。这个提示词会变成笑的声音，然后合成到语音里去。它也可以混合“男声”，“女声”，这样再做就可以不用再做拼接操作了    |       ||  whisper    |   在语音转文字（STT，也称ASR）方面，whisper是我用过的最好的，最快的库。没想到，这么快的模型，还能70x的优化空间。我准备部署这个模型，并开放给大家使用，可以用来转录大的语音文件，和进行翻译。这个模型是多语言的，而且能自动识别是什么语言，真的非常强大    |       ||  OFA-Chinese：中文多模态统一预训练模型    |  transformers结构的中文OFA模型     |       ||文生图开源模型试炼场|可根据输入文字同时用stable-diffusion 1.5、stable-diffusion 2.1、DALL-E、kandinsky-2等模型生成图像，方便测试比较|||LLMScore|LLMScore是一种全新的框架，能够提供具有多粒度组合性的评估分数。它使用大语言模型（LLM）来评估文本到图像生成模型。首先，将图像转化为图像级别和对象级别的视觉描述，然后将评估指令输入到LLM中，以衡量合成图像与文本的对齐程度，并最终生成一个评分和解释。我们的大量分析显示，LLMScore在众多数据集上与人类判断的相关性最高，明显优于常用的文本-图像匹配度量指标CLIP和BLIP。|||VisualGLM-6B|VisualGLM-6B 是一个开源的，支持图像、中文和英文的多模态对话语言模型，语言模型基于 ChatGLM-6B，具有 62 亿参数；图像部分通过训练 BLIP2-Qformer 构建起视觉模型与语言模型的桥梁，整体模型共78亿参数。||LLM的数据集| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :----   |          :--- ||   歧义数据集   |   能否正确的消除歧义是衡量大语言模型的一个重要指标。不过一直没有一个标准化的衡量方法，这篇论文提出了一个包含1,645个具有不同种类歧义的数据集及对应的评估方法。    |       ||   thu指令训练数据   |    设计了一套流程来自动产生多样化高质量的多轮指令对话数据UltraChat，并进行了细致的人工后处理。现已将英文数据全部开源，共计150余万条，是开源社区数量最多的高质量指令数据之一   |       ||   多模态数据集MMC4   |    5.8亿图片，1亿文档，400亿token   |       ||  EleutherAI 数据   |   800g的文本语料给你整合好了免费下载，不知道trian出来的model质量如何，打算试试：    |           ||UltraChat|大规模、信息丰富、多样化的多轮对话数据|||ConvFinQA金融数据问答||||   The botbots dataset   |  一个包含对话内容的数据集，对话内容来自于两个ChatGPT实例(gpt-3.5-turbo)，CLT命令和对话提示来自GPT-4，覆盖多种情境和任务，生成成本约为35美元，可用于研究和训练更小的对话模型(如Alpaca)     |       ||  alpaca_chinese_dataset - 人工精调的中文对话数据集    |       |       ||CodeGPT-data|提高编程能力的关键在于数据。CodeGPT是通过GPT生成的用于GPT的代码对话数据集。现在公开了32K条中文数据，让模型更擅长编程||语料库| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :----   |          :--- ||   人名语料库    |        |    ||   Chinese-Word-Vectors    |  各种中文词向量      |    ||    中文聊天语料    |   该库搜集了包含豆瓣多轮, PTT八卦语料, 青云语料, 电视剧对白语料, 贴吧论坛回帖语料,微博语料,小黄鸡语料     |    ||    中文谣言数据    |     该数据文件中，每一行为一条json格式的谣言数据   |     ||     中文问答数据集   |        |   提取码 2dva  ||    微信公众号语料   |   3G语料，包含部分网络抓取的微信公众号的文章，已经去除HTML，只包含了纯文本。每行一篇，是JSON格式，name是微信公众号名字，account是微信公众号ID，title是题目，content是正文     |     ||    中文自然语言处理 语料、数据集   |        |    ||    任务型对话英文数据集    |     【最全任务型对话数据集】主要介绍了一份任务型对话数据集大全，这份数据集大全涵盖了到目前在任务型对话领域的所有常用数据集的主要信息。此外，为了帮助研究者更好的把握领域进展的脉络，我们以Leaderboard的形式给出了几个数据集上的State-of-the-art实验结果。   |        ||    语音识别语料生成工具    |    从具有音频/字幕的在线视频创建自动语音识别(ASR)语料库    |     ||     LitBankNLP数据集   |   支持自然语言处理和计算人文学科任务的100部带标记英文小说语料     |    ||    中文ULMFiT  |    情感分析 文本分类 语料及模型    |     ||    省市区镇行政区划数据带拼音标注    |        |     ||    教育行业新闻 自动文摘 语料库    |        |     ||    中文自然语言处理数据集    |        |     ||     维基大规模平行文本语料   |  85种语言、1620种语言对、135M对照句  |  |   古诗词库     |        |   |   低内存加载维基百科数据    |     用新版nlp库加载17GB+英文维基语料只占用9MB内存遍历速度2-3 Gbit/s    |    ||    对联数据    |   700,000 couplets, 超过70万对对联     |     ||   《配色辞典》数据集     |        |     ||    42GB的JD客服对话数据(CSDD)    |        |     ||  70万对联数据       |        |    ||   用户名黑名单列表    |        |     ||     依存句法分析语料   |    4万句高质量标注数据    |    ||      人民日报语料处理工具集  |        |     ||  虚假新闻数据集 fake news corpus      |        |     ||    诗歌质量评价/细粒度情感诗歌语料库    |        |     ||    中文自然语言处理相关的开放任务    |  数据集以及当前最佳结果     |     ||    中文缩写数据集    |        |     ||    中文任务基准测评     |    代表性的数据集-基准(预训练)模型-语料库-baseline-工具包-排行榜    |     ||   中文谣言数据库    |        |     ||     CLUEDatasetSearch    |   中英文NLP数据集搜索所有中文NLP数据集，附常用英文NLP数据集     |     ||    多文档摘要数据集    |        |     ||    让人人都变得“彬彬有礼”礼貌迁移任务   |  在保留意义的同时将非礼貌语句转换为礼貌语句，提供包含139M + 实例的数据集       |     ||    粤语/英语会话双语语料库    |        |     ||     中文NLP数据集列表   |        |     ||   类人名/地名/组织机构名的命名体识别数据集     |        |    ||    中文语言理解测评基准    |    包括代表性的数据集&基准模型&语料库&排行榜   |     ||    OpenCLaP多领域开源中文预训练语言模型仓库    |   民事文书、刑事文书、百度百科 |     ||   中文全词覆盖BERT及两份阅读理解数据     |      DRCD数据集：由中国台湾台达研究院发布，其形式与SQuAD相同，是基于繁体中文的抽取式阅读理解数据集。CMRC 2018数据集:哈工大讯飞联合实验室发布的中文机器阅读理解数据。根据给定问题，系统需要从篇章中抽取出片段作为答案，形式与SQuAD相同。|     ||  Dakshina数据集     |    十二种南亚语言的拉丁/本地文字平行数据集合     |     ||    OPUS-100    |   以英文为中心的多语(100种)平行语料     |     ||      中文阅读理解数据集  |        |     ||    中文自然语言处理向量合集    |        |     ||    中文语言理解测评基准    |包括代表性的数据集、基准(预训练)模型、语料库、排行榜       |     ||  NLP数据集/基准任务大列表     |        |     ||   LitBankNLP数据集     |   支持自然语言处理和计算人文学科任务的100部带标记英文小说语料     |     ||70万对联数据||||文言文（古文）-现代文平行语料|短篇章中包括了《论语》、《孟子》、《左传》等篇幅较短的古籍，已和《资治通鉴》合并|||COLDDateset，中文冒犯性语言检测数据集|涵盖了种族、性别和地区等话题内容，数据待论文发表后放出|||GAOKAO-bench：以中国高考题目作为数据集|以中国高考题目作为数据集，评估大语言模型的语言理解能力和逻辑推理能力的测评框架，包含1781道选择题、218道填空题和812道解答题|||zero to nlp - 中文nlp应用数据、模型、训练、推理|||词库及词法工具| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :---  |          :--- ||  textfilter     |    中英文敏感词过滤    |    ||   人名抽取功能    |   中文（现代、古代）名字、日文名字、中文的姓和名、称呼（大姨妈、小姨妈等）、英文->中文名字（李约翰）、成语词典   |    ||   中文缩写库    | 全国人大: 全国 人民 代表大会; 中国: 中华人民共和国;女网赛: 女子/n 网球/n 比赛/vn  |    ||   汉语拆字词典    |  漢字	拆法 (一)	拆法 (二)	拆法 (三) 拆	手 斥	扌 斥	才 斥    |    ||    词汇情感值   |    山泉水:0.400704566541   充沛:	0.37006739587   |    ||   中文词库、停用词、敏感词    |        |    ||   python-pinyin    |   汉字转拼音     |    ||   zhtools   |   中文繁简体互转     |    ||   英文模拟中文发音引擎    |    say wo i ni #说：我爱你    |    ||  chinese_dictionary     |    同义词库、反义词库、否定词库    |    ||   wordninja    |   无空格英文串分割、抽取单词     |   ||   汽车品牌、汽车零件相关词汇    |        |  |     公司名字大全   |        |   |   THU整理的词库   | IT词库、财经词库、成语词库、地名词库、历史名人词库、诗词词库、医学词库、饮食词库、法律词库、汽车词库、动物词库    |    ||   罪名法务名词及分类模型    |    包含856项罪名知识图谱, 基于280万罪名训练库的罪名预测,基于20W法务问答对的13类问题分类与法律资讯问答功能    |         ||   分词语料库+代码    |        |       - 提取码 pea6  ||  基于Bi-LSTM + CRF的中文分词+词性标注     |   keras实现     |    || 基于Universal Transformer + CRF 的中文分词和词性标注    |        |    || 快速神经网络分词包     |    java version     |    ||   chinese-xinhua      |    中华新华字典数据库及api，包括常用歇后语、成语、词语和汉字    |     ||   SpaCy 中文模型     |   包含Parser, NER, 语法树等功能。有一些英文package使用spacy的英文模型的，如果要适配中文，可能需要使用spacy中文模型。     |       ||    中文字符数据    |        |     ||    Synonyms中文近义词工具包    |        |     ||   HarvestText     |   领域自适应文本挖掘工具（新词发现-情感分析-实体链接等）     |      ||    word2word    |    方便易用的多语言词-词对集62种语言/3,564个多语言对    |     ||   多音字词典数据及代码     |        |     ||    汉字、词语、成语查询接口    |        |     ||    103976个英语单词库包    |    （sql版，csv版，Excel版）    |     ||    英文脏话大列表    |        |     ||      词语拼音数据  |        |     ||   186种语言的数字叫法库     |        |     ||    世界各国大规模人名库    |        |     ||   汉字字符特征提取器 (featurizer)     |   提取汉字的特征（发音特征、字形特征）用做深度学习的特征     |     ||     char_featurizer - 汉字字符特征提取工具   |        |     ||   中日韩分词库mecab的Python接口库     |        |     ||    g2pC基于上下文的汉语读音自动标记模块    |        |     ||     ssc, Sound Shape Code   | 音形码 - 基于“音形码”的中文字符串相似度计算方法      |    ||    基于百科知识库的中文词语多词义/义项获取与特定句子词语语义消歧    |        |     ||   Tokenizer快速、可定制的文本词条化库     |        |     ||   Tokenizers     |  注重性能与多功能性的最先进分词器      |    ||    通过同义词替换实现文本“变脸”    |        |     ||    token2index与PyTorch/Tensorflow兼容的强大轻量词条索引库    |        |     ||    繁简体转换    |        |     || 粤语NLP工具|       |   ||领域词典库|涵盖68个领域、共计916万词的专业词典知识库||预训练语言模型&大模型| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :---  |          :--- ||BMList|大模型大列表||| bert论文中文翻译     |        |    ||    bert原作者的slides  |    |    || 文本分类实践     |        |    ||  bert tutorial文本分类教程     |        |  || bert pytorch实现       |        |    ||   bert pytorch实现      |        |    ||  BERT生成句向量，BERT做文本分类、文本相似度计算     |        |    ||  bert、ELMO的图解     |        |    ||  BERT Pre-trained models and downstream applications     |        |    ||  语言/知识表示工具BERT & ERNIE      |        |     ||    Kashgari中使用gpt-2语言模型    |        |     ||     Facebook LAMA   |    用于分析预训练语言模型中包含的事实和常识知识的探针。语言模型分析，提供Transformer-XL/BERT/ELMo/GPT预训练语言模型的统一访问接口    |     ||    中文的GPT2训练代码    |        |     ||   XLMFacebook的跨语言预训练语言模型     |        |     ||    海量中文预训练ALBERT模型    |        |     ||    Transformers 20    |    支持TensorFlow 20 和 PyTorch 的自然语言处理预训练语言模型(BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) 8种架构/33种预训练模型/102种语言   |     ||    8篇论文梳理BERT相关模型进展与反思    |        |     ||    法文RoBERTa预训练语言模型    |    用138GB语料训练的法文RoBERTa预训练语言模型    |     ||     中文预训练 ELECTREA 模型    |    基于对抗学习 pretrain Chinese Model    |     ||   albert-chinese-ner     |   用预训练语言模型ALBERT做中文NER    |     ||    开源预训练语言模型合集    |        |     ||   中文ELECTRA预训练模型     |        |     ||    用Transformers(BERT, XLNet, Bart, Electra, Roberta, XLM-Roberta)预测下一个词(模型比较)    |        |     ||   TensorFlow Hub     |    40+种语言的新语言模型(包括中文)    |     ||   UER     | 基于不同语料、编码器、目标任务的中文预训练模型仓库（包括BERT、GPT、ELMO等）       |      ||    开源预训练语言模型合集    |        |     ||   多语言句向量包     |        |     ||Language Model as a Service (LMaaS)|语言模型即服务|||开源语言模型GPT-NeoX-20B|200亿参数，是目前最大的可公开访问的预训练通用自回归语言模型|||中文科学文献数据集（CSL）|包含 396,209 篇中文核心期刊论文元信息 （标题、摘要、关键词、学科、门类）。CSL 数据集可以作为预训练语料，也可以构建许多NLP任务，例如文本摘要（标题预测）、 关键词生成和文本分类等。|||大模型开发神器|||抽取| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :---  |          :--- ||   时间抽取   |    已集成到 python package 中，欢迎试用    |  ||    神经网络关系抽取 pytorch    |    暂不支持中文     |        ||    基于bert的命名实体识别 pytorch    |  暂不支持中文      |        ||   关键词(Keyphrase)抽取包 pke     |        |        ||    BLINK最先进的实体链接库    |        |     ||   BERT/CRF实现的命名实体识别     |        |     ||    支持批并行的LatticeLSTM中文命名实体识别    |        |     ||    构建医疗实体识别的模型 |   包含词典和语料标注，基于python    |     ||    基于TensorFlow和BERT的管道式实体及关系抽取    |       - Entity and Relation Extraction Based on TensorFlow and BERT 基于TensorFlow和BERT的管道式实体及关系抽取，2019语言与智能技术竞赛信息抽取任务解决方案。Schema based Knowledge Extraction, SKE 2019   |     || 中文命名实体识别NeuroNER vs BertNER       |        |     ||  基于BERT的中文命名实体识别      |        |     ||    中文关键短语抽取工具    |        |     || bert      |     用于中文命名实体识别 tensorflow版本   |     ||   bert-Kashgari     |    基于 keras 的封装分类标注框架 Kashgari，几分钟即可搭建一个分类或者序列标注模型     |    ||    cocoNLP    |  人名、地址、邮箱、手机号、手机归属地 等信息的抽取，rake短语抽取算法。  |   ||    Microsoft多语言数字/单位/如日期时间识别包    |        |     || 百度开源的基准信息抽取系统       |        |     ||    中文地址分词（地址元素识别与抽取），通过序列标注进行NER    |        |     ||    基于依存句法的开放域文本知识三元组抽取和知识库构建    |        |     ||   基于预训练模型的中文关键词抽取方法     |        |     ||  chinese_keyphrase_extractor (CKPE)      |  A tool for chinese keyphrase extraction 一个快速从自然语言文本中提取和识别关键短语的工具    |     ||    简单的简历解析器，用来从简历中提取关键信息    |        |     ||   BERT-NER-Pytorch三种不同模式的BERT中文NER实验    |        |     |知识图谱| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :---  |          :--- ||    清华大学XLORE中英文跨语言百科知识图谱    |   百度、中文维基、英文维基    |       ||    文档图谱自动生成    |        |       ||     基于医疗领域知识图谱的问答系统   |        |    该repo参考了   ||    中文人物关系知识图谱项目    |        |     ||    AmpliGraph 知识图谱表示学习(Python)库知识图谱概念链接预测    |        |      ||    中文知识图谱资料、数据及工具    |        |     ||    基于百度百科的中文知识图谱  |     抽取三元组信息，构建中文知识图谱     |     ||    Zincbase 知识图谱构建工具包    |        |     ||    基于知识图谱的问答系统    |        |     ||    知识图谱深度学习相关资料整理    |        |     ||   东南大学《知识图谱》研究生课程(资料)     |        |     ||    知识图谱车音工作项目    |        |     ||     《海贼王》知识图谱   |        |     ||    132个知识图谱的数据集    |    涵盖常识、城市、金融、农业、地理、气象、社交、物联网、医疗、娱乐、生活、商业、出行、科教    |     ||    大规模、结构化、中英文双语的新冠知识图谱(COKG-19)    |        |     ||    基于依存句法与语义角色标注的事件三元组抽取    |        |       ||     抽象知识图谱  |   目前规模50万，支持名词性实体、状态性描述、事件性动作进行抽象      |     ||    大规模中文知识图谱数据14亿实体    |        |     ||    Jiagu自然语言处理工具     |    以BiLSTM等模型为基础，提供知识图谱关系抽取 中文分词 词性标注 命名实体识别 情感分析 新词发现 关键词 文本摘要 文本聚类等功能    |     ||     medical_NER - 中文医学知识图谱命名实体识别   |        |     ||   知识图谱相关学习资料/数据集/工具资源大列表     |        |     ||    LibKGE面向可复现研究的知识图谱嵌入库    |        |     ||   基于mongodb存储的军事领域知识图谱问答项目    |    包括飞行器、太空装备等8大类，100余小类，共计5800项的军事武器知识库，该项目不使用图数据库进行存储，通过jieba进行问句解析，问句实体项识别，基于查询模板完成多类问题的查询，主要是提供一种工业界的问答思想demo。    |     ||     京东商品知识图谱   |        |     ||    基于远监督的中文关系抽取    |        |     ||  基于医药知识图谱的智能问答系统      |        |    ||    BLINK最先进的实体链接库    |        |     ||   一个小型的证券知识图谱/知识库     |        |     ||   dstlr非结构化文本可扩展知识图谱构建平台     |        |     ||  百度百科人物词条属性抽取    |  用基于BERT的微调和特征提取方法来进行知识图谱      |   ||   新冠肺炎相关数据     |  新冠及其他类型肺炎中文医疗对话数据集；清华大学等机构的开放数据源（COVID-19）   |    ||   DGL-KE 图嵌入表示学习算法     |        |     ||因果关系图谱|| ||基于多领域文本数据集的因果事件对|||文本生成| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :---  |          :--- ||   Texar    |   Toolkit for Text Generation and Beyond     |    ||   Ehud Reiter教授的博客    |        |   北大万小军教授强力推荐，该博客对NLG技术、评价与应用进行了深入的探讨与反思。   ||   文本生成相关资源大列表    |        |      ||  开放域对话生成及在微软小冰中的实践      |   自然语言生成让机器掌握自动创作的本领    |     ||    文本生成控制   |        |      ||    自然语言生成相关资源大列表   |        |     ||    用BLEURT评价自然语言生成   |        |    ||   自动对联数据及机器人    |        |        ||   自动生成评论     |   用Transformer编解码模型实现的根据Hacker News文章标题生成评论     |     ||    自然语言生成SQL语句（英文）    |        |     ||    自然语言生成资源大全    |        |     ||    中文生成任务基准测评    |        |     ||     基于GPT2的特定主题文本生成/文本增广   |        |     ||     编码、标记和实现一种可控高效的文本生成方法   |        |     ||    TextFooler针对文本分类/推理的对抗文本生成模块    |        |     ||    SimBERT     |基于UniLM思想、融检索与生成于一体的BERT模型        |     ||    新词生成及造句    |    不存在的词用GPT-2变体从头生成新词及其定义、例句    |     ||   由文本自动生成多项选择题     |        |     ||     合成数据生成基准   |        |     ||       |        |    |文本摘要| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :---  |          :--- ||   中文文本摘要/关键词提取     |        |     ||    基于命名实体识别的简历自动摘要    |        |     ||    文本自动摘要库TextTeaser     |  仅支持英文      |     ||    基于BERT等最新语言模型的抽取式摘要提取    |        |     ||   Python利用深度学习进行文本摘要的综合指南     |        |     ||   (Colab)抽象文本摘要实现集锦(教程     |        |     |智能问答| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :---  |          :--- ||    中文聊天机器人    |  根据自己的语料训练出自己想要的聊天机器人，可以用于智能客服、在线问答、智能聊天等场景      |     ||   有趣的情趣robot qingyun      |    qingyun 训练出来的中文聊天机器人     |     ||     开放了对话机器人、知识图谱、语义理解、自然语言处理工具及数据   |        |     ||  qa对的机器人      |    Amodel-for-Retrivalchatbot - 客服机器人，Chinese Retreival chatbot（中文检索式机器人）    |    ||  ConvLab开源多域端到端对话系统平台      |        |     ||   基于最新版本rasa搭建的对话系统     |        |     ||   基于金融-司法领域(兼有闲聊性质)的聊天机器人     |        |     ||    端到端的封闭域对话系统    |        |     ||     MiningZhiDaoQACorpus    |    580万百度知道问答数据挖掘项目，百度知道问答语料库，包括超过580万的问题，每个问题带有问题标签。基于该问答语料库，可支持多种应用，如逻辑挖掘    |     ||   用于中文闲聊的GPT2模型GPT2-chitchat     |        |     ||    基于检索聊天机器人多轮响应选择相关资源列表(Leaderboards、Datasets、Papers)    |        |     ||   微软对话机器人框架     |        |     ||      chatbot-list  |   行业内关于智能客服、聊天机器人的应用和架构、算法分享和介绍    |     ||     Chinese medical dialogue data 中文医疗对话数据集   |        |     ||    一个大规模医疗对话数据集    |   包含110万医学咨询，400万条医患对话    |     ||    大规模跨领域中文任务导向多轮对话数据集及模型CrossWOZ    |        |     ||   开源对话式信息搜索平台     |        |     ||      情境互动多模态对话挑战2020(DSTC9 2020)  |        |     ||    用Quora问题对训练的T5问题意译(Paraphrase)    |        |     ||    Google发布Taskmaster-2自然语言任务对话数据集    |        |     ||    Haystack灵活、强大的可扩展问答(QA)框架    |        |     ||    端到端的封闭域对话系统    |        |     ||   Amazon发布基于知识的人-人开放领域对话数据集     |        |     ||    基于百度webqa与dureader数据集训练的Albert Large QA模型    |        |     ||   CommonsenseQA面向常识的英文QA挑战     |        |     ||   MedQuAD(英文)医学问答数据集     |        |     ||    基于Albert、Electra，用维基百科文本作为上下文的问答引擎    |        |     ||   基于14W歌曲知识库的问答尝试    |     功能包括歌词接龙，已知歌词找歌曲以及歌曲歌手歌词三角关系的问答   |     |文本纠错| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :---  |          :--- ||  中文文本纠错模块代码      |        |     ||    英文拼写检查库    |        |     ||  python拼写检查库      |        |     ||    GitHub Typo Corpus大规模GitHub多语言拼写错误/语法错误数据集    |        |     ||    BertPunc基于BERT的最先进标点修复模型    |        |     ||    中文写作校对工具    |        |     ||文本纠错文献列表| Chinese Spell Checking (CSC) and Grammatical Error Correction (GEC)|||文本智能校对大赛冠军方案|已落地应用，来自苏州大学、达摩院团队||多模态| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :---  |          :--- ||中文多模态数据集「悟空」|华为诺亚方舟实验室开源大型，包含1亿图文对|||中文图文表征预训练模型Chinese-CLIP|中文版本CLIP预训练模型，开源多个模型规模，几行代码搞定中文图文表征提取 & 图文检索||语音处理| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :---  |          :--- ||    ASR 语音数据集 + 基于深度学习的中文语音识别系统    |        |      ||   清华大学THCHS30中文语音数据集    |        |     ||    笑声检测器    |        |      ||    Common Voice语音识别数据集新版    |  包括来自42,000名贡献者超过1,400小时的语音样本，涵github     |        ||    speech-aligner    |  从“人声语音”及其“语言文本”，产生音素级别时间对齐标注的工具       |     ||   ASR语音大辞典/词典     |        |     ||     语音情感分析   |        |     ||    masr     | 中文语音识别，提供预训练模型，高识别率       |     ||    面向语音识别的中文文本规范化    |        |     ||      语音质量评价指标(MOSNet, BSSEval, STOI, PESQ, SRMR)  |        |     ||    面向语音识别的中文/英文发音辞典    |        |     ||    CoVoSTFacebook发布的多语种语音-文本翻译语料库    | 包括11种语言(法语、德语、荷兰语、俄语、西班牙语、意大利语、土耳其语、波斯语、瑞典语、蒙古语和中文)的语音、文字转录及英文译文      |     ||    Parakeet基于PaddlePaddle的文本-语音合成    |        |     ||     (Java)准确的语音自然语言检测库   |        |     ||   CoVoSTFacebook发布的多语种语音-文本翻译语料库     |        |     ||   TensorFlow 2 实现的文本语音合成     |        |     ||    Python音频特征提取包    |        |     ||   ViSQOL音频质量感知客观、完整参考指标，分音频、语音两种模式     |        |     ||    zhrtvc    |     好用的中文语音克隆兼中文语音合成系统     |    ||      aukit    |  好用的语音处理工具箱，包含语音降噪、音频格式转换、特征频谱生成等模块       |    ||      phkit    |   好用的音素处理工具箱，包含中文音素、英文音素、文本转拼音、文本正则化等模块     |     ||     zhvoice     |   中文语音语料，语音更加清晰自然，包含8个开源数据集，3200个说话人，900小时语音，1300万字     |     ||   audio面向语音行为检测     |  、二值化、说话人识别、自动语音识别、情感识别等任务的音频标注工具      |     ||     深度学习情感文本语音合成   |        |     ||   Python音频数据增广库     |        |     ||   基于大规模音频数据集Audioset的音频增强     |        |     ||    语声迁移    |        |     |文档处理| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :---  |          :--- ||LayoutLM-v3文档理解模型||||   PyLaia面向手写文档分析的深度学习工具包     |        |     ||    单文档非监督的关键词抽取    |        |     ||      DocSearch免费文档搜索引擎  |        |     ||  fdfgen       |    能够自动创建pdf文档，并填写信息     |    || pdfx       |   自动抽取出引用参考文献，并下载对应的pdf文件 |    ||     invoice2data   |   发票pdf信息抽取     |    ||   pdf文档信息抽取    |        |     ||PDFMiner     |     PDFMiner能获取页面中文本的准确位置，以及字体或行等其他信息。它还有一个PDF转换器，可以将PDF文件转换成其他文本格式(如HTML)。还有一个可扩展的解析器PDF，可以用于文本分析以外的其他用途。   |    ||  PyPDF2      |    PyPDF 2是一个python PDF库，能够分割、合并、裁剪和转换PDF文件的页面。它还可以向PDF文件中添加自定义数据、查看选项和密码。它可以从PDF检索文本和元数据，还可以将整个文件合并在一起。    |     ||   PyPDF2     |     PyPDF 2是一个python PDF库，能够分割、合并、裁剪和转换PDF文件的页面。它还可以向PDF文件中添加自定义数据、查看选项和密码。它可以从PDF检索文本和元数据，还可以将整个文件合并在一起。  |     ||    ReportLab   |      ReportLab能快速创建PDF 文档。经过时间证明的、超好用的开源项目，用于创建复杂的、数据驱动的PDF文档和自定义矢量图形。它是免费的，开源的，用Python编写的。该软件包每月下载5万多次，是标准Linux发行版的一部分，嵌入到许多产品中，并被选中为Wikipedia的打印/导出功能提供动力。  |    ||    SIMPdfPython写的简单PDF文件文字编辑器    |        |     ||pdf-diff |PDF文件diff工具 可显示两个pdf文档的差别| |表格处理| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :---  |          :--- ||  用unet实现对文档表格的自动检测，表格重建      |        |     ||   pdftabextract    |  用于OCR识别后的表格信息解析，很强大      |       || tabula-py    |     直接将pdf中的表格信息转换为pandas的dataframe，有java和python两种版本代码   |     ||   camelot     |   pdf表格解析       |    || pdfplumber      |   pdf表格解析     |    ||   PubLayNet   |     能够划分段落、识别表格、图片   |    ||    从论文中提取表格数据 |        |     ||    用BERT在表格中寻找答案    |        |     ||    表格问答的系列文章    |        |    ||     使用GAN生成表格数据（仅支持英文）   |        |     ||  carefree-learn(PyTorch)      |    表格数据集自动化机器学习(AutoML)包    |     ||   封闭域微调表格检测     |        |     ||   PDF表格数据提取工具     |        |     ||     TaBERT理解表格数据查询的新模型   |        |     || 表格处理 | Awesome-Table-Recognition | |文本匹配| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :---  |          :--- ||    句子、QA相似度匹配MatchZoo    |   文本相似度匹配算法的集合，包含多个深度学习的方法，值得尝试。    |      ||    中文问题句子相似度计算比赛及方案汇总    |        |     ||    similarity相似度计算工具包    |   java编写,用于词语、短语、句子、词法分析、情感分析、语义分析等相关的相似度计算    |     ||    中文词语相似度计算方法    |  综合了同义词词林扩展版与知网（Hownet）的词语相似度计算方法，词汇覆盖更多、结果更准确。      |     ||    Python字符串相似性算法库    |        |     ||    基于Siamese bilstm模型的相似句子判定模型,提供训练数据集和测试数据集    |    提供了10万个训练样本    |     |文本数据增强| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :---  |          :--- ||    中文NLP数据增强（EDA）工具  |        |     ||   英文NLP数据增强工具     |        |     ||    一键中文数据增强工具    |        |    ||    数据增强在机器翻译及其他nlp任务中的应用及效果    |        |     ||    NLP数据增广资源集    |        |     |常用正则表达式| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :---  |          :--- ||   抽取email的正则表达式    |    |  已集成到 python package 中，欢迎试用  ||   抽取phone_number    |     | 已集成到 python package 中，欢迎试用   ||    抽取身份证号的正则表达式   |  IDCards_pattern = r'^([1-9]\d{5}[12]\d{3}(0[1-9]1[012])(0[1-9][12][0-9]3[01])\d{3}[0-9xX])IDs = re.findall(IDCards_pattern, text, flags=0)|IP地址正则表达式|(25[0-5]  2[0-4]\d  [0-1]\d{2}  [1-9]?\d)(25[0-5]  2[0-4]\d  [0-1]\d{2}  [1-9]?\d)(25[0-5]  2[0-4]\d  [0-1]\d{2}  [1-9]?\d)(25[0-5]  2[0-4]\d  [0-1]\d{2}  [1-9]?\d)|||  腾讯QQ号正则表达式     |   1-9]([0-9]{5,11})     |    ||   国内固话号码正则表达式    |      [0-9-()（）]{7,18}  |    ||   用户名正则表达式    |  [A-Za-z0-9_\u4e00-\u9fa5]+      |    ||    国内电话号码正则匹配（三大运营商+虚拟等）    |        |     ||     正则表达式教程   |        |     |文本检索| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :---  |          :--- ||   高效模糊搜索工具     |        |     ||  面向各语种/任务的BERT模型大列表/搜索引擎      |        |     ||    Deepmatch针对推荐、广告和搜索的深度匹配模型库    |        |     ||    wwsearch是企业微信后台自研的全文检索引擎    |        |     ||   aili - the fastest in-memory index in the East 东半球最快并发索引     |        |     ||高效的字符串匹配工具 RapidFuzz|a fast string matching library for Python and C++, which is using the string similarity calculations from FuzzyWuzzy||阅读理解| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :---  |          :--- ||   高效模糊搜索工具     |        |     ||  面向各语种/任务的BERT模型大列表/搜索引擎      |        |     ||    Deepmatch针对推荐、广告和搜索的深度匹配模型库    |        |     ||   allennlp阅读理解支持多种数据和模     |        |     |情感分析| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :---  |          :--- ||     方面情感分析包   |        |     ||    awesome-nlp-sentiment-analysis    |    情感分析、情绪原因识别、评价对象和评价词抽取   |     ||    情感分析技术让智能客服更懂人类情感    |        |     |事件抽取| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :---  |          :--- ||   中文事件抽取    |        |    ||     NLP事件提取文献资源列表   |        |     ||    PyTorch实现的BERT事件抽取(ACE 2005 corpus)    |         |    ||  新闻事件线索抽取      |        |     |机器翻译| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :---  |          :--- ||   无道词典     |  有道词典的命令行版本，支持英汉互查和在线查询      |     ||NLLB|支持200+种语言任意互译的语言模型NLLB|||Easy-Translate|在本地翻译大文本文件的脚本，基于Facebook/Meta AI的 M2M100模型和NLLB200模型，支持200+种语言||数字转换| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :---  |          :--- ||   最好的汉字数字(中文数字)-阿拉伯数字转换工具     |        |     ||  快速转化「中文数字」和「阿拉伯数字」      |        |     ||    将自然语言数字串解析转换为整数和浮点数    |        |     |指代消解| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :---  |          :--- ||    中文指代消解数据    |        |      code a0qq |文本聚类| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :---  |          :--- ||     TextCluster短文本聚类预处理模块 Short text cluster   |        |     |文本分类| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :---  |          :--- ||    NeuralNLP-NeuralClassifier腾讯开源深度学习文本分类工具    |        |     |知识推理| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :---  |          :--- ||   GraphbrainAI开源软件库和科研工具，目的是促进自动意义提取和文本理解以及知识的探索和推断     |        |     ||    (哈佛)讲因果推理的免费书    |        |     |可解释自然语言处理| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :---  |          :--- ||   文本机器学习模型最先进解释器库     |        |     |文本攻击| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :---  |          :--- ||     TextAttack自然语言处理模型对抗性攻击框架   |        |     ||OpenBackdoor: 文本后门攻防工具包|       OpenBackdoor基于Python和PyTorch开发，可用于复现、评估和开发文本后门攻防的相关算法     |    |文本可视化| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :---  |          :--- ||     Scattertext 文本可视化(python)   |        |     ||     whatlies词向量交互可视化   |        |   ||   PySS3面向可解释AI的SS3文本分类器机器可视化工具     |        |     ||     用记事本渲染3D图像   |        |     ||    attnvisGPT2、BERT等transformer语言模型注意力交互可视化    |        |     ||    Texthero文本数据高效处理包    |   包括预处理、关键词提取、命名实体识别、向量空间分析、文本可视化等     |     |文本标注工具| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :---  |          :--- ||   NLP标注平台综述     |        |     ||   brat rapid annotation tool 序列标注工具     |        |     ||     Poplar网页版自然语言标注工具   |        |     ||   LIDA轻量交互式对话标注工具     |        |     ||    doccano基于网页的开源协同多语言文本标注工具    |        |     ||     Datasaurai 在线数据标注工作流管理工具   |        |     |语言检测| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :---  |          :--- ||  langid     |   97种语言检测     |    ||   langdetect    |   语言检测     |    |综合工具| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :---  |          :--- ||   jieba    |        |    ||  hanlp     |        |    ||    nlp4han    |  中文自然语言处理工具集(断句/分词/词性标注/组块/句法分析/语义分析/NER/N元语法/HMM/代词消解/情感分析/拼写检    |     ||    仇恨言论检测进展    |        |     ||   基于Pytorch的Bert应用    |    包括命名实体识别、情感分析、文本分类以及文本相似度等    |     ||    nlp4han中文自然语言处理工具集    |   断句/分词/词性标注/组块/句法分析/语义分析/NER/N元语法/HMM/代词消解/情感分析/拼写检查     |     ||    一些关于自然语言的基本模型    |        |     ||     用BERT进行序列标记和文本分类的模板代码   |        |  ||   jieba_fast 加速版的jieba     |        |     ||    StanfordNLP     |   纯Python版自然语言处理包     |     ||     Python口语自然语言处理工具集(英文)   |        |     ||    PreNLP自然语言预处理库    |        |     ||    nlp相关的一些论文及代码    |  包括主题模型、词向量(Word Embedding)、命名实体识别(NER)、文本分类(Text Classificatin)、文本生成(Text Generation)、文本相似性(Text Similarity)计算等，涉及到各种与nlp相关的算法，基于keras和tensorflow      |     ||  Python文本挖掘/NLP实战示例      |        |     ||   Forte灵活强大的自然语言处理pipeline工具集     |        |     ||   stanza斯坦福团队NLP工具     |  可处理六十多种语言   |     ||   Fancy-NLP用于建设商品画像的文本知识挖掘工具     |        |     ||    全面简便的中文 NLP 工具包    |        |     ||   工业界常用基于DSSM向量化召回pipeline复现     |        |     ||    Texthero文本数据高效处理包    |   包括预处理、关键词提取、命名实体识别、向量空间分析、文本可视化等     |     ||    nlpgnn图神经网络自然语言处理工具箱    |        |     ||    Macadam   |  以Tensorflow(Keras)和bert4keras为基础，专注于文本分类、序列标注和关系抽取的自然语言处理工具包     |     ||    LineFlow面向所有深度学习框架的NLP数据高效加载器    |        |     ||Arabica：Python文本数据探索性分析工具包||||Python 压力测试工具：SMSBoom|||有趣搞笑工具| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :---  |          :--- ||   汪峰歌词生成器    |        |    ||  女友 情感波动分析     |        |    ||   NLP太难了系列    |        |     ||  变量命名神器     |        |     ||     图片文字去除，可用于漫画翻译   |        |     ||     CoupletAI - 对联生成   |   基于CNN+Bi-LSTM+Attention 的自动对对联系统     |     ||   用神经网络符号推理求解复杂数学方程     |        |     ||   基于14W歌曲知识库的问答机器人    |     功能包括歌词接龙，已知歌词找歌曲以及歌曲歌手歌词三角关系的问答   |     ||    COPE - 格律诗编辑程序    |        |     ||Paper2GUI | 一款面向普通人的AI桌面APP工具箱，免安装即开即用，已支持18+AI模型，内容涵盖语音合成、视频补帧、视频超分、目标检测、图片风格化、OCR识别等领域 |    ||礼貌程度估算器（使用新浪微博数据训练）||  ||草蟒（Python 中文版）入门指南|中文编程语言|  |课程报告面试等| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :---  |          :--- ||   自然语言处理报告     |        |    ||    知识图谱报告   |        |    ||   数据挖掘报告   |        |    ||   自动驾驶报告    |        |    ||   机器翻译报告    |        |    ||    区块链报告   |        |    ||   机器人报告    |        |    ||   计算机图形学报告    |        |    ||  3D打印报告    |        |    ||   人脸识别报告    |        |    ||   人工智能芯片报告    |        |    ||   cs224n深度学习自然语言处理课程    |        |   课程中模型的pytorch实现    ||   面向深度学习研究人员的自然语言处理实例教程     |        |    ||   《Natural Language Processing》by Jacob Eisenstein     |        |     ||     ML-NLP    | 机器学习(Machine Learning)、NLP面试中常考到的知识点和代码实现       |     ||      NLP任务示例项目代码集  |        |     ||     2019年NLP亮点回顾   |        |     ||   nlp-recipes微软出品--自然语言处理最佳实践和范例     |        |     ||    面向深度学习研究人员的自然语言处理实例教程    |        |     ||   Transfer Learning in Natural Language Processing (NLP)     |        |     ||《机器学习系统》图书|  |     |比赛| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :---  |          :--- || NLPer-Arsenal | NLP竞赛，含当前赛事信息、过往竞赛方案等，持续更新中 |  ||    复盘所有NLP比赛的TOP方案    |        |     ||   2019年百度的三元组抽取比赛，“科学空间队”源码(第7名)     |        |     |金融自然语言处理| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :---  |          :--- ||   BDCI2019金融负面信息判定     |        |     ||     开源的金融投资数据提取工具    |        |     ||    金融领域自然语言处理研究资源大列表    |        |     ||   基于金融-司法领域(兼有闲聊性质)的聊天机器人     |        |     ||小型金融知识图谱构流程示范| ||医疗自然语言处理| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :---  |          :--- ||   中文医学NLP公开资源整理     |        |     ||    spaCy 医学文本挖掘与信息提取    |        |     ||    构建医疗实体识别的模型 |   包含词典和语料标注，基于python    |     ||     基于医疗领域知识图谱的问答系统   |        |    该repo参考了   ||     Chinese medical dialogue data 中文医疗对话数据集   |        |     ||    一个大规模医疗对话数据集    |   包含110万医学咨询，400万条医患对话    |     ||   新冠肺炎相关数据     |  新冠及其他类型肺炎中文医疗对话数据集；清华大学等机构的开放数据源（COVID-19）   |    |法律自然语言处理| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :---  |          :--- ||    Blackstone面向非结构化法律文本的spaCy pipeline和NLP模型    |        |     ||   法务智能文献资源列表     |        |     ||   基于金融-司法领域(兼有闲聊性质)的聊天机器人     |        |     ||   罪名法务名词及分类模型    |    包含856项罪名知识图谱, 基于280万罪名训练库的罪名预测,基于20W法务问答对的13类问题分类与法律资讯问答功能    |         ||法律NLP相关资源大列表|||文本生成图像| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :---  |          :--- || Dalle-mini|根据文本提示生成图片的迷你版DALL·E||其他| 资源名（Name）      | 描述（Description） | 链接     || :---        |    :---  |          :--- ||  phone     |     中国手机归属地查询    |    ||   phone    |    国际手机、电话归属地查询    |    ||    ngender   |   根据名字判断性别     |    ||    中文对比英文自然语言处理NLP的区别综述  |        |     ||  各大公司内部里大牛分享的技术文档 PDF 或者 PPT      |        |     ||   comparxiv 用于比较arXiv上两提交版本差异的命令     |        |     ||     CHAMELEON深度学习新闻推荐系统元架构   |        |     ||    简历自动筛选系统    |        |     ||    Python实现的多种文本可读性评价指标    |        |     |"
https://github.com/Kr1s77/Python-crawler-tutorial-starts-from-zero,"python爬虫教程，带你从零到一，包含js逆向，selenium, tesseract OCR识别,mongodb的使用，以及scrapy框架",分布式爬虫从零开始目录实例目录文献整理http知识Chrome控制台Requests使用JSONXML
https://github.com/zzw922cn/Automatic_Speech_Recognition,End-to-end Automatic Speech Recognition for Madarian and English in Tensorflow ,"Automatic-Speech-RecognitionEnd-to-end automatic speech recognition system implemented in TensorFlow.Recent UpdatesRecommendationIf you want to replace feed dict operation with Tensorflow multi-thread and fifoqueue input pipeline, you can refer to my repo  for more example codes. My own practices prove that fifoqueue input pipeline would improve the training speed in some time.If you want to look the history of speech recognition, I have collected the significant papers since 1981 in the ASR field. You can read awesome paper list in my repo , all download links of papers are provided. I will update it every week to add new papers, including speech recognition, speech synthesis and language modelling. I hope that we won't miss any important papers in speech domain.All my public repos will be updated in future, thanks for your stars!Install and UsageCurrently only python 3.5 is supported.This project depends on scikit.audiolab, for which you need to have  installed in your system.Clone the repository to your preferred directory and install using:To use, simply run the following command:Instead of configuration in command line, you can also set the arguments above in  in practice.Besides, you can also run  for both training and testing simultaneously! See  for details.PerformancePER based dynamic BLSTM on TIMIT database, with casual tuning because time it limitedLibriSpeech recognition result without LMLabel:it was about noon when captain waverley entered the straggling village or rather hamlet of tully veolan close to which was situated the mansion of the proprietorPrediction:it was about noon when captain wavraly entered the stragling bilagor of rather hamlent of tulevallon close to which wi situated the mantion of the propriaterLabel:the english it is evident had they not been previously assured of receiving the king would never have parted with so considerable a sum and while they weakened themselves by the same measure have strengthened a people with whom they must afterwards have so material an interest to discussPrediction:the onglish it is evident had they not being previously showed of receiving the king would never have parted with so considerable a some an quile they weakene themselves by the same measure haf streigth and de people with whom they must afterwards have so material and interest to discussLabel:one who writes of such an era labours under a troublesome disadvantagePrediction:one how rights of such an er a labours onder a troubles hom disadvantageLabel:then they started on again and two hours later came in sight of the house of doctor piptPrediction:then they started on again and two hours laytor came in sight of the house of doctor pipdLabel:what does he wantPrediction:whit daes he wantLabel:there just in frontPrediction:there just infrontLabel:under ordinary circumstances the abalone is tough and unpalatable but after the deft manipulation of herbert they are tender and make a fine dish either fried as chowder or a la newbergPrediction:under ordinary circumstancesi the abl ony is tufgh and unpelitable but after the deftominiculation of hurbourt and they are tender and make a fine dish either fride as choder or alanuburgLabel:by degrees all his happiness all his brilliancy subsided into regret and uneasiness so that his limbs lost their power his arms hung heavily by his sides and his head drooped as though he was stupefiedPrediction:by degrees all his happiness ill his brilliancy subsited inter regret and aneasiness so that his limbs lost their power his arms hung heavily by his sides and his head druped as though he was stupifiedLabel:i am the one to go after walt if anyone has to i'll go down mister thomasPrediction:i have the one to go after walt if ety wod hastu i'll go down mister thommasLabel:i had to read it over carefully as the text must be absolutely correctPrediction:i had to readit over carefully as the tex must be absolutely correctLabel:with a shout the boys dashed pell mell to meet the pack train and falling in behind the slow moving burros urged them on with derisive shouts and sundry resounding slaps on the animals flanksPrediction:with a shok the boy stash pale mele to meek the pecktrait ane falling in behind the slow lelicg burs ersh tlan with deressive shouts and sudery resounding sleps on the animal slankesLabel:i suppose though it's too early for them then came the explosionPrediction:i suppouse gho waths two early for them then came the explosionContentThis is a powerful library for automatic speech recognition, it is implemented in TensorFlow and support training with CPU/GPU. This library contains followings models you can choose to train your own model:Implementation DetailsData preprocessingTIMIT corpusThe original TIMIT database contains 6300 utterances, but we find the 'SA' audio files occurs many times, it will lead bad bias for our speech recognition system. Therefore, we removed the all 'SA' files from the original dataset and attain the new TIMIT dataset, which contains only 5040 utterances including 3696 standard training set and 1344 test set.Automatic Speech Recognition transcribes a raw audio file into character sequences; the preprocessing stage converts a raw audio file into feature vectors of several frames. We first split each audio file into 20ms Hamming windows with an overlap of 10ms, and then calculate the 12 mel frequency ceptral coefficients, appending an energy variable to each frame. This results in a vector of length 13. We then calculate the delta coefficients and delta-delta coefficients, attaining a total of 39 coefficients for each frame. In other words, each audio file is split into frames using the Hamming windows function, and each frame is extracted to a feature vector of length 39 (to attain a feature vector of different length, modify the settings in the file .In folder data/mfcc, each file is a feature matrix with size timeLength39 of one audio file; in folder data/label, each file is a label vector according to the mfcc file.If you want to set your own data preprocessing, you can edit  or .The original TIMIT dataset contains 61 phonemes, we use 61 phonemes for training and evaluation, but when scoring, we mappd the 61 phonemes into 39 phonemes for better performance. We do this mapping according to the paper . The mapping details are as follows:| Original Phoneme(s) | Mapped Phoneme || :------------------  | :-------------------: || iy | iy || ix, ih | ix || eh | eh || ae | ae || ax, ah, ax-h | ax || uw, ux | uw || uh | uh || ao, aa | ao || ey | ey || ay | ay || oy | oy || aw | aw || ow | ow || er, axr | er || l, el | l || r | r || w | w || y | y || m, em | m || n, en, nx | n || ng, eng | ng || v | v || f | f || dh | dh || th | th || z | z || s | s || zh, sh | zh || jh | jh || ch | ch || b | b || p | p || d | d || dx | dx || t | t || g | g || k | k || hh, hv | hh || bcl, pcl, dcl, tcl, gcl, kcl, q, epi, pau, h# | h# |LibriSpeech corpusLibriSpeech is a corpus of approximately 1000 hours of 16kHz read English speech. It can be downloaded from In order to preprocess LibriSpeech data, download the dataset from the above mentioned link, extract it and run the following:The processed data will be saved in the ""save"" path. To train the model, run the following:where the ""datadir"" is the ""save"" path used in preprocess stage.Wall Street Journal corpusTODOCore FeaturesFuture WorkLicenseMITContact UsIf this program is helpful to you, please give us a star or fork to encourage us to keep updating. Thank you! Besides, any issues or pulls are appreciated.Collaborators: "
https://github.com/OpenNMT/OpenNMT-py,Open Source Neural Machine Translation and (Large) Language Models in PyTorch,"OpenNMT-py: Open-Source Neural Machine Translation and (Large) Language ModelsOpenNMT-py is the  version of the  project, an open-source (MIT) neural machine translation (and beyond!) framework. It is designed to be research friendly to try out new ideas in translation, language modeling, summarization, and many other NLP tasks. Some companies have proven the code to be production ready.We love contributions! Please look at issues marked with the  tag.Before raising an issue, make sure you read the requirements and the  examples.Unless there is a bug, please use the  or  to ask questions.For beginners:There is a step-by-step and explained tuto (Thanks to Yasmin Moslem): Please try to read and/or follow before raising newbies issues.Otherwise you can just have a look at the  stepsNew:For all usecases including NMT, you can now use Multiquery instead of Multihead attention (faster at training and inference) and remove biases from all Linear (QKV as well as FeedForward modules).If you used previous versions of OpenNMT-py, you can check the  or the Tutorials:SetupOpenNMT-py requires:Install  from :pip install OpenNMT-py
or from the sources:git clone https://github.com/OpenNMT/OpenNMT-py.git
cd OpenNMT-py
pip install -e .
Note: if you encounter a  during installation, try to use  with .(Optional) Some advanced features (e.g. working pretrained models or specific transforms) require extra packages, you can install them with:pip install -r requirements.opt.txt
Manual installation of some dependenciesApex is highly recommended to have fast performance (especially the legacy fusedadam optimizer and FusedRMSNorm)git clone https://github.com/NVIDIA/apex
cd apex
pip3 install -v --no-build-isolation --config-settings --build-option=""--cpp_ext --cuda_ext --deprecated_fused_adam --xentropy --fast_multihead_attn"" ./
cd ..
Flash attention:As of Oct. 2023 flash attention 1 has been upstreamed to pytorch v2 but it is recommended to use flash attention 2 with v2.3.1 for sliding window attention support.When using regular  or Rotary with  OpenNMT-py will try to use an optimized dot-product path.if you want to use  then you need to manually install it first:pip install flash-attn --no-build-isolation
if flash attention 2 is not installed, then we will use  from pytorch 2.xWhen using  or Alibi  OpenNMT-py will use its legacy code for matrix multiplications.flash attention and  are a bit faster and saves some GPU memory.Documentation & FAQsAcknowledgementsOpenNMT-py is run as a collaborative open-source project.Project was incubated by Systran and Harvard NLP in 2016 in Lua and ported to Pytorch in 2017.Current maintainers (since 2018): (Seedfall)CitationIf you are using OpenNMT-py for academic work, please cite the initial  published in ACL 2017:@misc{klein2018opennmt,
      title={OpenNMT: Neural Machine Translation Toolkit}, 
      author={Guillaume Klein and Yoon Kim and Yuntian Deng and Vincent Nguyen and Jean Senellart and Alexander M. Rush},
      year={2018},
      eprint={1805.11462},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
"
https://github.com/threat9/routersploit,Exploitation Framework for Embedded Devices,"RouterSploit - Exploitation Framework for Embedded DevicesThe RouterSploit Framework is an open-source exploitation framework dedicated to embedded devices.It consists of various modules that aid penetration testing operations:InstallationRequirementsRequired:Optional:Installation on Kali Linuxapt-get install python3-pip
git clone https://www.github.com/threat9/routersploit
cd routersploit
python3 -m pip install -r requirements.txt
python3 rsf.py
Bluetooth Low Energy support:apt-get install libglib2.0-dev
python3 -m pip install bluepy
python3 rsf.py
Installation on Ubuntu 20.04sudo apt-get install git python3-pip
git clone https://github.com/threat9/routersploit
cd routersploit
python3 -m pip install -r requirements.txt
python3 rsf.py
Bluetooth Low Energy support:sudo apt-get install libglib2.0-dev
python3 -m pip install bluepy
python3 rsf.py
Installation on Ubuntu 18.04 & 17.10sudo add-apt-repository universe
sudo apt-get install git python3-pip
git clone https://www.github.com/threat9/routersploit
cd routersploit
python3 -m pip install setuptools
python3 -m pip install -r requirements.txt
python3 rsf.py
Bluetooth Low Energy support:apt-get install libglib2.0-dev
python3 -m pip install bluepy
python3 rsf.py
Installation on OSXgit clone https://www.github.com/threat9/routersploit
cd routersploit
sudo python3 -m pip install -r requirements.txt
python3 rsf.py
Running on Dockergit clone https://www.github.com/threat9/routersploit
cd routersploit
docker build -t routersploit .
docker run -it --rm routersploit
UpdateUpdate RouterSploit Framework often. The project is under heavy development and new modules are shipped almost every day.cd routersploit
git pull
Build your ownTo our surprise, people started to fork not because they wereinterested in the security of embedded devices but simply because they want toleverage our interactive shell logic and build their tools using similarconcept. All these years they must have said: ""There must be a better way!""and they were completely right, the better way is called. allows you to easily wrap yourapplication inside a tailored interactive shell. Common chores regardingbuilding REPLs was factored out and being taken care of so you canfocus on specific domain logic of your application.LicenseThe RouterSploit Framework is under a BSD license.Please see  for more details.Acknowledgments"
https://github.com/PySimpleGUI/PySimpleGUI,"Launched in 2018. It's 2023 and PySimpleGUI is actively developed & supported. Create complex windows simply. Supports tkinter, Qt, WxPython, Remi (in browser). Create GUI applications trivially with a full set of widgets. Multi-Window applications are also simple. 3.4 to 3.11 supported. 325+ Demo programs & Cookbook for rapid start. Extensive docs","Transforms the tkinter, Qt, WxPython, and Remi (browser-based) GUI frameworks into a simpler interface.  The window definition is simplified by using Python core data types understood by beginners (lists and dictionaries). Further simplification happens by changing event handling from a callback-based model to a message passing one.  Your code is not required to have an object oriented architecture which makes the package usable by a larger audience. While the architecture is simple to understand, it does not necessarily limit you to only simple problems.  Some programs are not well-suited for PySimpleGUI however.  By definition, PySimpleGUI implements a subset of the underlying GUI frameworks' capabilities.  It's difficult to define exactly which programs are well suited for PySimpleGUI and which are not.  It depends on the details of your program.  Duplicating Excel in every detail is an example of something not well suited for PySimpleGUI..PySimpleGUI needs your support.  If you find PySimpleGUI useful, please consider sponsoring the project on GitHub or BuyMeACoffee.  It's expensive working full-time on PySimpleGUI and also paying for ongoing expenses (domains, artists, software, consultants, sponsoring open source projects).  Statistics  📈PyPI Statistics & Versions| TK | TK 2.7 | Qt| WxPython | Web (Remi) || -- | -- | -- | -- | -- ||  |  |  |  |  ||  |  |  |  |  ||  |  |  |  |  ||    |    |  |  |  |GitHub Statistics|  Issues | Commit Activity | Stars | Docs || -- | -- | -- | -- ||   |  |  |  ||   |   |  |What Is PySimpleGUI ❓PySimpleGUI is a Python package that enables Python programmers of all levels to create GUIs.  You specify your GUI window using a ""layout"" which contains widgets (they're called ""Elements"" in PySimpleGUI).  Your layout is used to create a window using one of the 4 supported frameworks to display and interact with your window.  Supported frameworks include tkinter, Qt, WxPython, or Remi. The term ""wrapper"" is sometimes used for these kinds of packages.Your PySimpleGUI code is simpler and shorter than writing directly using the underlying framework because PySimpleGUI implements much of the ""boilerplate code"" for you.  Additionally, interfaces are simplified to require as little code as possible to get the desired result.  Depending on the program and framework used, a PySimpleGUI program may require 1/2 to 1/10th amount of code to create an identical window using one of the frameworks directly.While the goal is to encapsulate/hide the specific objects and code used by the GUI framework you are running on top of, if needed you can access the frameworks' dependent widgets and windows directly. If a setting or feature is not yet exposed or accessible using the PySimpleGUI APIs, you are not walled off from the framework. You can expand capabilities without directly modifying the PySimpleGUI package itself.Bridging the ""GUI Gap""Python has brought a large number of people into the programming community. The number of programs and the range of areas it touches is mindboggling.  But more often than not, these technologies are out of reach of all but a handful of people.  The majority of Python programs are ""command line"" based. This isn't a problem for programmer-types as we're all used to interacting with computers through a text interface.  While programmers don't have a problem with command-line interfaces, most ""normal people"" do.  This creates a digital divide, a ""GUI Gap"".Adding a GUI to a program opens that program up to a wider audience. It becomes more approachable. GUIs can also make interacting with some programs easier, even for those that are comfortable with a command-line interface.  And finally, some problems require a GUI.  Recognition of Open Source UseIn the Demo Programs or one of the PySimpleGUI Account's Repos these packages were used at least one time. Some of your are the goodies on the right of the GUI gap.  If you use Open Source software in your project, be sure and supply information about the packages you used.  LPLG3 as an ExampleThe licensing terms in the LLC3 Licensing, it states:Since the above packages each have a similar license clause, I'm listing them here, in what I would consider a ""prominent notice"" location, that I'm using the fine works of these groups or individuals.  They are used in the Demo Programs most likely or one of the Repos that are under this account as this list is all inclusive.You all have my respect and admiration.  You're enabling bigger things. What a special kind of thing to make. Who knows what you've enabled. I believe more people are getting over to your creations and getting to experience them.tkinter team - PySimpleGUI would be nowhere without your lengthy work & continuous dedication. ONE GUI API for 3 different OS's? Really? With no code changes to move between? That's a huge accomplishment. You're #1 to me.  Thank you for your hard work.Getting Over ""The Bar""It's been said by some that ""the bar is pretty high"" when it comes to learning GUI programming in Python.  This is one of the questions that the PySimpleGUI project has tried to answer.  Here's a humorous look at what's been a not funny situation.The results have been fascinating to witness and it's been touching to read the accounts of the journeys of users. Nothing prepared me for the emails that started to arrive soon after the first release of PySimpleGUI. They are heartwarming and heartbreaking tales of life-long dreams of creating a program that required a GUI. Some made a few attempts, giving up each time. Others never started once they started to research what was required. After recounting the varied and long road to finding PySimpleGUI, the stories became similar. They each found success and expressed joy and gratitude. The joy expressed in these messages was unlike anything I had encountered in the entirety of career in the computing field.It's been these emails and the messages of gratitude seen here in the GitHub Issues that made dedicating my life to his project a non-decision.Subscribing to Announcements 📢If you click the ""Subscribe"" button in , then you'll be notified when project news is published.  This Issue is the official location to get the latest PySimpleGUI information.  Information is posted frequently including release contents, tips & tricks, documentation updates, etc. There have been over 1,000 posts since the project started.About Me 👋Hi there!  I'm Mike.  You'll find me right here, on the PySimpleGUI GitHub, solving problems and continuously pushing PySimpleGUI forward.  I've dedicated my days, nights, and weekends to the project and PySimpleGUI users.  Our successes are ultimately shared.  I'm successful when you're successful.  While I'm a relative newcomer to Python, I've been writing software since the 70s.  The majority of my career was spent creating products in Silicon Valley. I bring to PySimpleGUI the same professionalism and dedication as I did to the corporate products I developed. You are my customers now.Project Goals 🥅Two of the most important goals of the PySimpleGUI project:Fun as a goal on a serious project sounds odd, but it's a serious goal. I find writing these GUI programs to be a lot of fun. One reason is how little time it takes to write a complete solution. If we're not enjoying the process then someone's going to give up. There is a significant amount of documentation, a cookbook, 100's of demo programs to get you immediately running, a detailed call reference, YouTube videos, online Trinket demos, and more... all working to create... a fun experience.Your Success is a shared goal.  PySimpleGUI was built for developers. You're my peeps. It's been an unexpected reward to see the results of the combined effort of users and PySimpleGUI. Use the documentation & other materials to help build your application.  If you run into trouble, help is available by opening on .  Take a look at the section on Support below.Educational Resources 📚PySimpleGUI now has an official Udemy course!  Check the header of this readme and the PySimpleGUI documentation for a coupon code.  The course can be found at .  This course is currently the only income source for the PySimpleGUI project other than sponsorships and donations.is easy to remember and is where the documentation is located. You'll find tabs across the top that represent several different documents. The documentation is located on ""Read The Docs"" so that there is a table of contents for each document and they are easy to search.There are 100s of pages of written documentation and 100s of example programs that will help you be effective very quickly.  Rather than requiring days or weeks of investment to learn a single GUI package, you may be able to complete your project in a single afternoon when using PySimpleGUI.Example 1 - The One-Shot WindowThis type of program is called a ""one-shot"" window because the window is displayed one time, the values collected, and then it is closed.  It doesn't remain open for a long time like you would in a Word Processor.Anatomy of a Simple PySimpleGUI ProgramThere are 5 sections to a PySimpleGUI programimport PySimpleGUI as sg                        # Part 1 - The import

# Define the window's contents
layout = [  [sg.Text(""What's your name?"")],     # Part 2 - The Layout
            [sg.Input()],
            [sg.Button('Ok')] ]

# Create the window
window = sg.Window('Window Title', layout)      # Part 3 - Window Defintion
                                                
# Display and interact with the Window
event, values = window.read()                   # Part 4 - Event loop or Window.read call

# Do something with the information gathered
print('Hello', values[0], ""! Thanks for trying PySimpleGUI"")

# Finish up by removing from the screen
window.close()                                  # Part 5 - Close the Window
The code produces this windowExample 2 - Interactive WindowIn this example, our window will remain on the screen until the user closes the window or clicks the Quit button.  The main difference between the one-shot window you saw earlier and an interactive window is the addition of an ""Event Loop"". The Event Loop reads events and inputs from your window.  The heart of your application lives in the event loop.import PySimpleGUI as sg

# Define the window's contents
layout = [[sg.Text(""What's your name?"")],
          [sg.Input(key='-INPUT-')],
          [sg.Text(size=(40,1), key='-OUTPUT-')],
          [sg.Button('Ok'), sg.Button('Quit')]]

# Create the window
window = sg.Window('Window Title', layout)

# Display and interact with the Window using an Event Loop
while True:
    event, values = window.read()
    # See if user wants to quit or window was closed
    if event == sg.WINDOW_CLOSED or event == 'Quit':
        break
    # Output a message to the window
    window['-OUTPUT-'].update('Hello ' + values['-INPUT-'] + ""! Thanks for trying PySimpleGUI"")

# Finish up by removing from the screen
window.close()
This is the window that Example 2 produces.And here's what it looks like after you enter a value into the Input field and click the Ok button.Let's take a quick look at some of the differences between this example and the one-shot window.First, you'll notice differences in the layout.  Two changes in particular are important.  One is the addition of the  parameter to the  element and one of the  elements.  A  is like a name for an element.  Or, in Python terms, it's like a dictionary key.  The  element's key will be used as a dictionary key later in the code.Another difference is the addition of this  element:          [sg.Text(size=(40,1), key='-OUTPUT-')],
There are 2 parameters, the  we already covered.  The  parameter defines the size of the element in characters.  In this case, we're indicating that this  element is 40 characters wide, by 1 character high.  Notice that there is no text string specified which means it'll be blank.  You can easily see this blank row in the window that's created.We also added a button,  ""Quit"".The Event Loop has our familiar  call. Following the read is this if statement:    if event == sg.WINDOW_CLOSED or event == 'Quit':
        break
This code is checking to see if the user closed the window by clicking the ""X"" or if they clicked the ""Quit"" button.  If either of these happens, then the code will break out of the event loop.If the window wasn't closed nor the Quit button clicked, then execution continues.  The only thing that could have happened is the user clicked the ""Ok"" button.  The last statement in the Event Loop is this one:    window['-OUTPUT-'].update('Hello ' + values['-INPUT-'] + ""! Thanks for trying PySimpleGUI"")
This statement updates the  element that has the key  with a string.   finds the element with the key .  That key belongs to our blank  element.  Once that element is returned from the lookup, then its  method is called.  Nearly all elements have an  method.  This method is used to change the value of the element or to change some configuration of the element. If we wanted the text to be yellow, then that can be accomplished by adding a  parameter to the  method so that it reads:    window['-OUTPUT-'].update('Hello ' + values['-INPUT-'] + ""! Thanks for trying PySimpleGUI"",
                              text_color='yellow')
After adding the  parameter, this is our new resulting window:The parameters available for each element are documented in both the  as well as the docstrings.  PySimpleGUI has extensive documentation to help you understand all of the options available to you. If you lookup the  method for the  element, you'll find this definition for the call:As you can see several things can be changed for a  element.  The call reference documentation is a valuable resource that will make programming in PySimpleGUI, uhm, simple.Jump Start! Get the Demo Programs & Demo Browser 🔎The over 300 Demo Programs will give you a jump-start and provide many design patterns for you to learn how to use PySimpleGUI and how to integrate PySimpleGUI with other packages.  By far the best way to experience these demos is using the Demo Browser.  This tool enables you to search, edit and run the Demo Programs.  To get them installed quickly along with the Demo Browser, use  to install :    or if you're in Linux, Mac, etc, that uses  instead of  to launch Python:  Once installed, launch the demo browser by typing  from the command line""    Layouts Are Funny LOL! 😆Your window's layout is a ""list of lists"" (LOL).  Windows are broken down into ""rows"".  Each row in your window becomes a list in your layout.  Concatenate together all of the lists and you've got a layout...a list of lists.Here is the same layout as before with an extra  element added to each row so that you can more easily see how rows are defined:layout = [  [sg.Text('Row 1'), sg.Text(""What's your name?"")],
            [sg.Text('Row 2'), sg.Input()],
            [sg.Text('Row 3'), sg.Button('Ok')] ]
Each row of this layout is a list of elements that will be displayed on that row in your window.Using lists to define your GUI has some huge advantages over how GUI programming is done using other frameworks.  For example, you can use Python's list comprehension to create a grid of buttons in a single line of code.These 3 lines of code:import PySimpleGUI as sg

layout = [[sg.Button(f'{row}, {col}') for col in range(4)] for row in range(4)]

event, values = sg.Window('List Comprehensions', layout).read(close=True)
produces this window which has a 4 x 4 grid of buttons:Recall how ""fun"" is one of the goals of the project.  It's fun to directly apply Python's powerful basic capabilities to GUI problems. Instead of pages of code to create a GUI, it's a few (or often 1) lines of code.Collapsing CodeIt's possible to condense a window's code down to a single line of code.  The layout definition, window creation, display, and data collection can all be written in this line of code:event, values = sg.Window('Window Title', [[sg.Text(""What's your name?"")],[sg.Input()],[sg.Button('Ok')]]).read(close=True)
The same window is shown and returns the same values as the example showing the sections of a PySimpleGUI program.  Being able to do so much with so little enables you to quickly and easily add GUIs to your Python code.  If you want to display some data and get a choice from your user, it can be done in a line of code instead of a page of code.By using short-hand aliases, you can save even more space in your code by using fewer characters.  All of the Elements have one or more shorter names that can be used.  For example, the  element can be written simply as . The  element can be written as  and the  as .  Your single-line window code thus becomes:event, values = sg.Window('Window Title', [[sg.T(""What's your name?"")],[sg.I()],[sg.B('Ok')]]).read(close=True)
Code PortabilityPySimpleGUI is currently capable of running on 4 Python GUI Frameworks.  The framework to use is specified using the import statement.  Change the import and you'll change the underlying GUI framework.  For some programs, no other changes are needed than the import statement to run on a different GUI framework.  In the example above, changing the import from  to , ,  will change the framework.| Import Statement | Resulting Window ||--|--|| PySimpleGUI |   || PySimpleGUIQt |   || PySimpleGUIWx |   || PySimpleGUIWeb |   |Porting GUI code from one framework to another (e.g. moving your code from tkinter to Qt) usually requires a rewrite of your code.  PySimpleGUI is designed to enable you to have easy movement between the frameworks.  Sometimes some changes are required of you, but the goal is to have highly portable code with minimal changes.  Some features, like a System Tray Icon, are not available on all of the ports.  The System Tray Icon feature is available on the Qt and WxPython ports.  A simulated version is available on tkinter.  There is no support for a System Tray icon in the PySimpleGUIWeb port.Runtime Environments| Environment | Supported ||--|--|| Python |  Python 3.4+ || Operating Systems | Windows, Linux, Mac || Hardware | Desktop PCs, Laptops, Raspberry Pi, Android devices running PyDroid3 || Online | repli.it, Trinket.com (both run tkinter in a browser) || GUI Frameworks | tkinter, pyside2, WxPython, Remi |IntegrationsAmong the more than 200 ""Demo Programs"", you'll find examples of how to integrate many popular Python packages into your GUI.Want to embed a Matplotlib drawing into your window?  No problem, copy the demo code and instantly have a Matplotlib drawing of your dreams into your GUI.  These packages and more are ready for you to put into your GUI as there are demo programs or a demo repo available for each:Package | Description ||--|--|Matplotlib | Many types of graphs and plots |OpenCV | Computer Vision (often used for AI) |VLC | Video playback |pymunk | Physics engine|psutil | System environment statistics |prawn | Reddit API |json | PySimpleGUI wraps a special API to store ""User Settings"" |weather | Integrates with several weather APIs to make weather apps |mido | MIDI playback |beautiful soup | Web Scraping (GitHub issue watcher example) |Installing  💾Two common ways of installing PySimpleGUI:Pip Installing & UpgradingThe current suggested way of invoking the  command is by running it as a module using Python.  Previously the command  or  was directly onto a command-line / shell.  The suggested way Initial install for Windows:Initial install for Linux and MacOS:To upgrade using , you simply add 2 parameters to the line .  Upgrade installation on Windows:Upgrade for Linux and MacOS:Single File InstallingPySimpleGUI was created as a single .py file so that it would be very easy for you to install it, even on systems that are not connected to the internet like a Raspberry Pi.  It's as simple as placing the PySimpleGUI.py file into the same folder as your application that imports it. Python will use your local copy when performing the import.When installing using just the .py file, you can get it from either PyPI or if you want to run the most recent unreleased version then you'll download it from GitHub.To install from PyPI, download either the wheel or the .gz file and unzip the file.  If you rename the .whl file to .zip you can open it just like any normal zip file.  You will find the PySimpleGUI.py file in one of the folders.  Copy this file to your application's folder and you're done.The PyPI link for the tkinter version of PySimpleGUI is:https://pypi.org/project/PySimpleGUI/#filesThe GitHub repo's latest version can be found here:https://raw.githubusercontent.com/PySimpleGUI/PySimpleGUI/master/PySimpleGUI.pyNow some of you are thinking, ""yea, but, wait, having a single huge source file is a terrible idea"".  And, yea, sometimes it can be a terrible idea.  In this case, the benefits greatly outweighed the downside.  Lots of concepts in computer science are tradeoffs or subjective.  As much as some would like it to be, not everything is black and white.  Many times the answer to a question is ""it depends"".Galleries 🎨Work on a more formal gallery of user-submitted GUIs as well as those found on GitHub is underway but as of this writing it's not complete.  There are currently 2 places you can go to see some screenshots in a centralized way.  Hopefully, a Wiki or other mechanism can be released soon to do justice to the awesome creations people are making.User Submitted GalleryThe first is a  located on the GitHub.  It's an informal way for people to show off what they've made.  It's not ideal, but it was a start.Massive Scraped GitHub ImagesThe second is a  scraped from 1,000 projects on GitHub that are reportedly using PySimpleGUI.  It's not been hand-filtered and there are plenty of old screenshots that were used in the early documentation.  But, you may find something in there that sparks your imagination.Uses for PySimpleGUI 🔨The following sections showcase a fraction of the uses for PySimpleGUI.  There are over 1,000 projects on GitHub alone that use PySimpleGUI.  It's truly amazing how possibilities have opened up for so many people.  Many users have spoken about previously attempting to create a GUI in Python and failing, but finally achieving their dreams when they tried PySimpleGUI.Your First GUIOf course one of the best uses of PySimpleGUI is getting you into making GUIs for your Python projects.  You can start as small as requesting a filename.  For this, you only need to make a single call to one of the ""high-level functions"" called .  There are all kinds of popups, some collect information. on itself makes a window to display information.  You can pass multiple parameters just like a print.  If you want to get information, then you will call functions that start with  such as .Adding a single line to get a filename instead of specifying a filename on the command line can transform your program into one that ""normal people"" will feel comfortable using.import PySimpleGUI as sg

filename = sg.popup_get_file('Enter the file you wish to process')
sg.popup('You entered', filename)
This code will display 2 popup windows.  One to get the filename, which can be browsed to or pasted into the input box.  The other window will output what is collected.Rainmeter-Style WindowsThe default settings for GUI frameworks don't tend to produce the nicest looking windows.  However, with some attention to detail, you can do several things to make windows look attractive.  PySimpleGUI makes it easier to manipulate colors and features like removing the title bar.  The result is windows that don't look like your typical tkinter windows.Here is an example of how you can create windows that don't look like your typical tkinter in windows.  In this example, the windows have their titlebars removed.  The result is windows that look much like those found when using Rainmeter, a desktop widget program.You can easily set the transparency of a window as well.  Here are more examples of desktop widgets in the same Rainmeter style.  Some are dim appearing because they are semi-transparent.Both of these effects; removing the titlebar and making a window semi-transparent, are achieved by setting 2 parameters when creating the window.  This is an example of how PySimpleGUI enables easy access to features.  And because PySimpleGUI code is portable across the GUI frameworks, these same parameters work for the other ports such as Qt.Changing the Window creation call in Example 1 to this line of code produces a similar semi-transparent window:window = sg.Window('My window', layout, no_titlebar=True, alpha_channel=0.5)
GamesWhile not specifically written as a game development SDK, PySimpleGUI makes the development of some games quite easy.This Chess program not only plays chess, but it integrates with the Stockfish chess-playing AI.Several variants of Minesweeper have been released by users.Card games work well with PySimpleGUI as manipulating images is simple when using the PySimpleGUI  element.While not specifically written as a game development SDK, PySimpleGUI makes development of some games quite easy.Media Capture and PlaybackCapturing and displaying video from your webcam in a GUI is 4 lines of PySimpleGUI code.  Even more impressive is that these 4 lines of code work with the tkinter, Qt, and Web ports.  You can display your webcam, in realtime, in a browser using the same code that displays the image using tkinter.Media playback, audio and video, can also be achieved using the VLC player.  A demo application is provided to you so that you have a working example to start from. Everything you see in this readme is available to you as a starting point for your own creations.Artificial IntelligenceAI and Python have long been a recognized superpower when the two are paired together. What's often missing however is a way for users to interact with these AI algorithms familiarly, using a GUI.These YOLO demos are a great example of how a GUI can make a tremendous difference in interacting with AI algorithms.  Notice two sliders at the bottom of these windows.  These 2 sliders change a couple of the parameters used by the YOLO algorithm.  If you were tuning your YOLO demo using only the command line, you would need to set the parameters, once, when you launch the application, see how they perform, stop the application, change the parameters, and finally restart the application with the new parameters.Contrast those steps against what can be done using a GUI.  A GUI enables you to modify these parameters in real-time.  You can immediately get feedback on how they are affecting the algorithm.There are SO many AI programs that have been published that are command-line driven.  This in itself isn't a huge hurdle, but it's enough of a ""pain in the ass"" to type/paste the filename you want to colorize on the command line, run the program, then open the resulting output file in a file viewer.GUIs have the power to change the user experience, to fill the ""GUI Gap"".  With this colorizer example, the user only needs to supply a folder full of images, and then click on an image to both colorize and display the result.  The program/algorithm to do the colorization was freely available, ready to use.  What was missing is the ease of use that a GUI could bring.GraphingDisplaying and interacting with data in a GUI is simple with PySimpleGUI.  You have several options.  You can use the built-in drawing/graphing capabilities to produce custom graphs.  This CPU usage monitor uses the  elementMatplotlib is a popular choice with Python users.  PySimpleGUI can enable you to embed Matplotlib graphs directly into your GUI window.  You can even embed the interactive controls into your window if you want to retain the Matplotlib interactive features.Using PySimpleGUI's color themes, you can produce graphs that are a notch above default graphs that most people create in Matplotlib.Front-endsThe ""GUI Gap"" mentioned earlier can be easily solved using PySimpleGUI.  You don't even need to have the source code to the program you wish to add a GUI onto.  A ""front-end"" GUI is one that collects information that is then passed to a command-line application.  Front-end GUIs are a fantastic way for a programmer to distribute an application that users were reluctant to use previously because they didn't feel comfortable using a command-line interface.  These GUIs are your only choice for command-line programs that you don't have access to the source code for.This example is a front-end for a program called ""Jump Cutter"".  The parameters are collected via the GUI, a command-line is constructed using those parameters, and then the command is executed with the output from the command-line program being routed to the GUI interface.  In this example, you can see in yellow the command that was executed.Raspberry PiBecause PySimpleGUI is compatible back to Python 3.4, it is capable of creating a GUI for your Raspberry Pi projects.  It works particularly well when paired with a touchscreen.  You can also use PySimpleGUIWeb to control your Pi if it doesn't have a monitor attached.Easy Access to Advanced FeaturesBecause it's very easy to access many of the underlying GUI frameworks' features, it's possible to piece together capabilities to create applications that look nothing like those produced using the GUI framework directly.For example, it's not possible to change the color/look-and-feel of a titlebar using tkinter or the other GUI packages, but with PySimpleGUI it's easy to create windows that appear as if they have a custom titlebar.Unbelievably, this window is using tkinter to achieve what appears to be something like a screensaver.On windows, tkinter can completely remove the background from your application.  Once again, PySimpleGUI makes accessing these capabilities trivial.  Creating a transparent window requires adding a single parameter to the call that creates your .  One parameter change can result in a simple application with this effect:You can interact with everything on your desktop, clicking through a full-screen window.ThemesTired of the default grey GUIs?  PySimpleGUI makes it trivial for your window to look nice by making a single call to the  function.  There are over 150 different color themes available for you to choose:With most GUI frameworks, you must specify the color for every widget you create.  PySimpleGUI takes this chore from you and will automatically color the Elements to match your chosen theme.To use a theme, call the  function with the name of the theme before creating your window. You can add spaces for readability.  To set the theme to ""Dark Grey 9"":import PySimpleGUI as sg

sg.theme('dark grey 9')
This single line of code changes the window's appearance entirely:The theme changed colors of the background, text, input background, input text, and button colors.  In other GUI packages, to change color schemes like this, you would need to specify the colors of each widget individually, requiring numerous changes to your code.DistributionWant to share your PySimpleGUI program with friends and family that don't have Python installed on their computer?  Try the GUI front-end for PyInstaller that you'll find in the  project.Support 💪Your first stop should be the  and .   Be sure and install the Demo Browser (instructions in the Cookbook) so that you can search and run the 100s of demo programs.If you still have a question or need help...  no problem... help is available to you, at no cost. Simply  on the PySimpleGUI GitHub repo and you'll get help.  Nearly all software companies have a form that accompanies bug reports.  It's not a bad trade... fill in the form, get free software support.  This information helps get you an answer efficiently.In addition to requesting information such as the version numbers of PySimpleGUI and underlying GUI frameworks, you're also given a checklist of items that may help you solve your problem.[<marko.inline.RawText object at 0x000001592FF20388>]  It may feel pointless to you.  It may feel painful, despite it taking just a moment.  It helps get you a solution faster.  If it wasn't useful and necessary information to help you get a speedy reply and fix, you wouldn't be asked to fill it out.  ""Help me help you"".Supporting 	Financial support for the project is greatly appreciated.  To be honest, financial help is needed.  It's expensive just keeping the lights on.  The domain name registrations, a long list of subscriptions for things like Trinket, consulting help, etc., quickly add up to a sizable recurring cost.  PySimpleGUI wasn't inexpensive to create. While a labor of love, it was very laborious over several years, and quite a bit was invested, and continues to be invested, in creating what you see today.PySimpleGUI has an open-source license and it would be great if it could remain that way.  If you or your company (especially if you're using PySimpleGUI in a company) are benefiting financially by using PySimpleGUI, you have the capability of extending the life of the project for you and other users.Buy Me A CoffeeBuy Me a Coffee is a great way to publicly support developers.  It's quick, easy, and your contribution is recorded so that others can see that you're a supporter of PySimpleGUI.  You can also choose to make your donation private.GitHub SponsoringThe  is how you can sponsor the project at varying levels of support on an ongoing basis.  It's how many Open Source developers are able to receive corporate level sponsorship.Your help in financially contributing to the project would be greatly appreciated. Being an Open Source developer is financially challenging.  YouTube video creators are able to make a living creating videos.  It's not so easy yet for Open Source developers.Thank you for the Thank You'sTo everyone that's helped, in whatever fashion, I'm very very grateful.Even taking a moment to say ""thank you"" helps, and a HUGE number of you have done that.  It's been an amazing number actually.  I value these thanks and find inspiration in the words alone.  Every message is a little push forward.  It adds a little bit of energy and keeps the whole project's momentum.  I'm so very grateful to everyone that's helped in whatever form it's been.Contributing  👷While PySimpleGUI is currently licensed under an open-source license, the project itself is structured like a proprietary product.  Pull Requests are not accepted.One of the best ways for you to contribute code is to write and publish applications.  Users are inspired by seeing what other users build.  Here's a simple set of steps you can take - Create a GitHub repo, post the code, and include a screenshot in your repo's readme file.  Then come back to the PySimpleGUI repo and post a screenshot in Issue #10 or in the project's WIKI.If there is a feature missing that you need or you have an enhancement to suggest, then Special Thanks 🙏The PySimpleGUI team is tiny and they're all superstars. Every week I've been stunned by what they do. Dream-team is an understatement.  Simply put , ,  and Mike [<marko.inline.RawText object at 0x000001592FDC5548>] PySimpleGUI.This version of the PySimpleGUI readme wouldn't have come together without the help from . He's a fantastic developer and has been a PySimpleGUI supporter since the project's launch.    is another long-term supporter and has written several PySimpleGUI programs that pushed the envelope of the package's capabilities.  The unique minesweeper that uses an image for the board was created by Israel.   surprised many when he published the first card game using PySimpleGUI that you see pictured above as well as the first minesweeper game made with PySimpleGUI.   is the youngest developer I've worked with, ever, on projects. This kid shocks me on a regular basis.  Ask for a capability, such as the PySimpleGUI GitHub Issues form error checking bot, and it simply happens regardless of the technologies involved. I'm fortunate that we were introduced. Someday he's going to be whisked away, but until then we're all benefiting from his talent.  made the Udemy course happen. It wouldn't have been 1/4th of what it is without his amazing skills in video production, course design, marketing brilliance, and web programming. The Japanese version of the readme was greatly improved with help from  .  has had a very large impact on the project, also getting involved with PySimpleGUI in the first year of initial release.  He wrote a designer, came up with the familiar window[key] lookup syntax, wrote the tools that create the documentation, designed the first set of doc strings as well as tools that generate the online documenation using the PySimpleGUI code itself.  PySimpleGUI would not be where it is today were it not for the help of these individuals.The more than 4,000 GitHub repos that use PySimpleGUI are owed a ""Thank You"" as well, for it is you that has been the inspiration that fuels this project's engine.  The overseas users that post on Twitter overnight are the spark that starts the day's work on PySimpleGUI. They've been a source of positive energy that gets the development engine started and ready to run every day. As a token of appreciation, this readme file has been translated into .PySimpleGUI users have been the best user community an Open Source developer could hope for. I've never seen expressions of gratitude and joy like PySimpleGUI users show on a daily basis.  © Copyright 2021, 2022 PySimpleGUI"
https://github.com/Asabeneh/30-Days-Of-Python,"30 days of Python programming challenge is a step-by-step guide to learn the Python programming language in 30 days. This challenge may take more than100 days, follow your own pace.  These videos may help too: https://www.youtube.com/channel/UC7PNRuno1rzYPb1xLa4yktw","🐍 30 Days Of Python|# Day | Topics                                                    ||------|:---------------------------------------------------------:|| 01  |  || 02  |  || 03  |  || 04  |  || 05  |  || 06  |  || 07  |  || 08  |  || 09  |  || 10  |  || 11  |  || 12  |  || 13  |  || 14  |  || 15  |  || 16 |   || 17 |  || 18 |  || 19 |  || 20 |  || 21 |  || 22 |  || 23 |  || 24 |  || 25 |  || 26 |  || 27 |  || 28 |  || 29 |  || 30 |  |🧡🧡🧡 HAPPY CODING 🧡🧡🧡Author:Asabeneh Yetayeh Second Edition: July, 2021📘 Day 1WelcomeCongratulations for deciding to participate in a 30 days of Python programming challenge . In this challenge you will learn everything you need to be a python programmer and the whole concept of programming. In the end of the challenge you will get a 30DaysOfPython programming challenge certificate.If you would like to actively engage in the challenge, you may join the  telegram group.  IntroductionPython is a high-level programming language for general-purpose programming. It is an open source, interpreted, objected-oriented programming language. Python was created by a Dutch programmer, Guido van Rossum. The name of Python programming language was derived from a British sketch comedy series, Monty Python's Flying Circus.  The first version was released on February 20, 1991. This 30 days of Python challenge will help you learn the latest version of Python, Python 3 step by step. The topics are broken down into 30 days, where each day contains several topics with easy-to-understand explanations, real-world examples, many hands on exercises and projects.This challenge is designed for beginners and professionals who want to learn python programming language. It may take 30 to 100 days to complete the challenge, people who actively participate on the telegram group have a high probability of completing the challenge.This challenge is easy to read, written in conversational English, engaging, motivating and at the same time, it is very demanding. You need to allocate much time to finish this challenge. If you are a visual learner, you may get the video lesson on  Washera YouTube channel. You may start from . Subscribe the channel, comment and ask questions on YouTube vidoes and be proactive, the author will eventually notice you.The author likes to hear your opinion about the challenge, share the author by expressing your thoughts about the 30DaysOfPython challenge. You can leave your testimonial on this Why Python ?It is a programming language which is very close to human language and because of that it is easy to learn and use.Python is used by various industries and companies (including Google). It has been used to develop web applications, desktop applications, system adminstration, and machine learning libraries. Python is highly embraced language in the data science and machine learning community. I hope this is enough to convince you to start learning Python. Python is eating the world and you are killing it before it eats you.Environment SetupInstalling PythonTo run a python script you need to install python. Let's  python.If your are a windows user. Click the button encircled in red.If you are a macOS user. Click the button encircled in red.To check if python is installed write the following command on your device terminal.python --version
As you can see from the terminal, I am using Python 3.7.5 version at the moment. Your version of Python might be different from mine by but it should be 3.6 or above. If you mange to see the python version, well done. Python has been installed on your machine. Continue to the next section.Python ShellPython is an interpreted scripting language, so it does not need to be compiled. It means it executes the code line by line. Python comes with a Python Shell (Python Interactive Shell). It is used to execute a single python command and get the result.Python Shell waits for the Python code from the user. When you enter the code, it interprets the code and shows the result in the next line.Open your terminal or command prompt(cmd) and write:python
The Python interactive shell is opened and it is waiting for you to write Python code(Python script). You will write your Python script next to this symbol >>> and then click Enter.Let us write our very first script on the Python scripting shell.Well done, you wrote your first Python script on Python interactive shell. How do we close the Python interactive shell ?To close the shell, next to this symbol >> write exit() command and press Enter.Now, you know how to open the Python interactive shell and how to exit from it.Python will give you results if you write scripts that Python understands, if not it returns errors. Let's make a deliberate mistake and see what Python will return.As you can see from the returned error, Python is so clever that it knows the mistake we made and which was Syntax Error: invalid syntax. Using x as multiplication in Python is a syntax error because (x) is not a valid syntax in Python. Instead of (x) we use asterisk (*) for multiplication. The returned error clearly shows what to fix.The process of identifying and removing errors from a program is called debugging. Let us debug it by putting * in place of x.Our bug was fixed, the code ran and we got a result we were expecting. As a programmer you will see such kind of errors on daily basis. It is good to know how to debug. To be good at debugging you should understand what kind of errors you are facing. Some of the Python errors you may encounter are SyntaxError, IndexError, NameError, ModuleNotFoundError, KeyError, ImportError, AttributeError, TypeError, ValueError, ZeroDivisionError etc. We will see more about different Python [<marko.inline.RawText object at 0x000001592FF3AA48>] in later sections.Let us practice more how to use Python interactive shell. Go to your terminal or command prompt and write the word python.The Python interactive shell is opened. Let us do some basic mathematical operations (addition, subtraction, multiplication, division, modulus,  exponential).Let us do some maths first before we write any Python code:In python we have the following additional operations:Let us change the above mathematical expressions to Python code. The Python shell has been opened and let us write a comment at the very beginning of the shell.A comment is a part of the code which is not executed by python. So we can leave some text in our code to make our code more readable. Python does not run the comment part. A comment in python starts with hash(#) symbol.This is how you write a comment in python # comment starts with hash
 # this is a python comment, because it starts with a (#) symbol
Before we move on to the next section, let us practice more on the Python interactive shell. Close the opened shell by writing exit() on the shell and open it again and let us practice how to write text on the Python shell.Installing Visual Studio CodeThe Python interactive shell is good to try and test small script codes but it will not be for a big project. In real work environment, developers use different code editors to write codes. In this 30 days of Python programming challenge we will use visual studio code. Visual studio code is a very popular open source text editor. I am a fan of vscode and I would recommend to  visual studio code, but if you are in favor of other editors, feel free to follow with what you have.If you installed visual studio code, let us see how to use it.If you prefer a video, you can follow this Visual Studio Code for Python How to use visual studio codeOpen the visual studio code by double clicking the visual studio icon. When you open it you will get this kind of interface. Try to interact with the labeled icons.Create a folder named 30DaysOfPython on your desktop. Then open it using visual studio code.After opening it you will see shortcuts for creating files and folders inside of 30DaysOfPython project's directory. As you can see below, I have created the very first file, helloworld.py. You can do the same.After a long day of coding, you want to close your code editor, right? This is how you will close the opened project.Congratulations, you have finished setting up the development environment. Let us start coding.Basic PythonPython SyntaxA Python script can be written in Python interactive shell or in the code editor. A Python file has an extension .py.Python IndentationAn indentation is a white space in a text. Indentation in many languages is used to increase code readability, however Python uses indentation to create block of codes. In other programming languages curly brackets are used to create blocks of codes instead of indentation. One of the common bugs when writing python code is wrong indentation.CommentsComments are very important to make the code more readable and to leave remarks in our code. Python does not run comment parts of our code.Any text starting with hash(#) in Python is a comment.Example: Single Line Comment    # This is the first comment
    # This is the second comment
    # Python is eating the world
Example: Multiline CommentTriple quote can be used for multiline comment if it is not assigned to a variable""""""This is multiline comment
multiline comment takes multiple lines.
python is eating the world
""""""
Data typesIn Python there are several types of data types. Let us get started with the most common ones. Different data types will be covered in detail in other sections. For the time being, let us just go through the different data types and get familiar with them. You do not have to have a clear understanding now.NumberStringA collection of one or more characters under a single or double quote. If a string is more than one sentence then we use a triple quote.Example:'Asabeneh'
'Finland'
'Python'
'I love teaching'
'I hope you are enjoying the first day of 30DaysOfPython Challenge'
BooleansA boolean data type is either a True or False value. T and F should be always uppercase.Example:    True  #  Is the light on? If it is on, then the value is True
    False # Is the light on? If it is off, then the value is False
ListPython list is an ordered collection which allows to store different data type items. A list is similar to an array in JavaScript.Example:[0, 1, 2, 3, 4, 5]  # all are the same data types - a list of numbers
['Banana', 'Orange', 'Mango', 'Avocado'] # all the same data types - a list of strings (fruits)
['Finland','Estonia', 'Sweden','Norway'] # all the same data types - a list of strings (countries)
['Banana', 10, False, 9.81] # different data types in the list - string, integer, boolean and float
DictionaryA Python dictionary object is an unordered collection of data in a key value pair format.Example:{
'first_name':'Asabeneh',
'last_name':'Yetayeh',
'country':'Finland', 
'age':250, 
'is_married':True,
'skills':['JS', 'React', 'Node', 'Python']
}
TupleA tuple is an ordered collection of different data types like list but tuples can not be modified once they are created. They are immutable.Example:('Asabeneh', 'Pawel', 'Brook', 'Abraham', 'Lidiya') # Names
('Earth', 'Jupiter', 'Neptune', 'Mars', 'Venus', 'Saturn', 'Uranus', 'Mercury') # planets
SetA set is a collection of data types similar to list and tuple. Unlike list and tuple, set is not an ordered collection of items. Like in Mathematics, set in Python stores only unique items.In later sections, we will go in detail about each and every Python data type.Example:{2, 4, 3, 5}
{3.14, 9.81, 2.7} # order is not important in set
Checking Data typesTo check the data type of certain data/variable we use the type function. In the following terminal you will see different python data types:Python FileFirst open your project folder, 30DaysOfPython. If you don't have this folder, create a folder name called 30DaysOfPython. Inside this folder, create a file called helloworld.py. Now, let's do what we did on python interactive shell using visual studio code.The Python interactive shell was printing without using print but on visual studio code to see our result we should use a built in function _print(). The print() built-in function takes one or more arguments as follows print('arument1', 'argument2', 'argument3'). See the examples below.Example:The file name is helloworld.py# Day 1 - 30DaysOfPython Challenge

print(2 + 3)             # addition(+)
print(3 - 1)             # subtraction(-)
print(2 * 3)             # multiplication(*)
print(3 / 2)             # division(/)
print(3 ** 2)            # exponential(**)
print(3 % 2)             # modulus(%)
print(3 // 2)            # Floor division operator(//)

# Checking data types
print(type(10))          # Int
print(type(3.14))        # Float
print(type(1 + 3j))      # Complex number
print(type('Asabeneh'))  # String
print(type([1, 2, 3]))   # List
print(type({'name':'Asabeneh'})) # Dictionary
print(type({9.8, 3.14, 2.7}))    # Set
print(type((9.8, 3.14, 2.7)))    # Tuple
To run the python file check the image below. You can run the python file either by running the green button on Visual Studio Code or by typing python helloworld.py in the terminal .🌕  You are amazing. You have just completed day 1 challenge and you are on your way to greatness. Now do some exercises for your brain and muscles.💻 Exercises - Day 1Exercise: Level 1Exercise: Level 2Exercise: Level 3🎉 CONGRATULATIONS ! 🎉"
https://github.com/prakhar1989/Algorithms,:computer: Data Structures and Algorithms in Python,"Algorithms in PythonImplementations of a few algorithms and datastructures for fun and profit!CompletedTestspython -m tests.graph_test
python -m tests.digraph_test
python -m tests.graph_algorithms_test
python -m tests.heap_test
python -m tests.unionfind_test
python -m tests.singly_linked_list_test
python -m tests.modular_exponentiation_test
python -m tests.modular_multiplicative_inverse_test
"
https://github.com/0xAX/linux-insides,A little bit about a linux kernel,"linux-insidesA book-in-progress about the linux kernel and its insides.The goal is simple - to share my modest knowledge about the insides of the linux kernel and help people who are interested in linux kernel insides, and other low-level subject matter. Feel free to go through the book Questions/Suggestions: Feel free about any questions or suggestions by pinging me at twitter , adding an  or just drop me an .Generating eBooks and PDFs - Mailing ListWe have a Google Group mailing list for learning the kernel source code. Here are some instructions about how to use it.JoinSend an email with any subject/content to . Then you will receive a confirmation email. Reply it with any content and then you are done.Send emails to mailing listJust send emails to . The basic usage is the same as other mailing lists powered by mailman.Archiveshttps://groups.google.com/forum/#!forum/kernelhackingOn other languagesDockerIn order to run your own copy of the book with gitbook within a local container:ContributionsFeel free to create issues or pull-requests if you have any problems.Please read AuthorLICENSELicensed ."
https://github.com/joerick/pyinstrument,🚴 Call stack profiler for Python. Shows you why your code is slow!,"pyinstrumentPyinstrument is a Python profiler. A profiler is a tool to help you optimizeyour code - make it faster. To get the biggest speed increase you should.Pyinstrument helps you find it!Installationpip install pyinstrument
Pyinstrument supports Python 3.7+.DocumentationTo learn how to use pyinstrument, or to check the reference, head to the.Known issuesChangelogv4.6.012 October 2023v4.5.37 September 2023v4.5.21 September 2023v4.5.122 July 2023v4.5.05 June 2023v4.4.05 November 2022v4.3.021 August 2022v4.2.0v4.1.1v4.1.0v4.0.4v4.0.3v4.0.2v4.0.0v3.4.2v3.4.1v3.4.0v3.3.0v3.2.0v3.1.2v3.1.1v3.1.0v3.0.3v3.0.2v3.0.1v3.0.0Yikes! See #49 for the gory details. I hope you like it.v2.3.0v2.2.1v2.2.0v2.1.0v2.0.4v2.0.3v2.0.2v2.0.1v2.0.0v0.13v0.12ContributingTo setup a dev environment:virtualenv --python=python3 env
. env/bin/activate
pip install --upgrade pip
pip install -r requirements-dev.txt
pre-commit install --install-hooks
To get some sample output:pyinstrument examples/wikipedia_article_word_count.py
To run the tests:pytest
To run linting checks locally:pre-commit run --all-files
Some of the pre-commit checks, like  or , will auto-fixthe problems they find. So if the above command returns an error, tryrunning it again, it might succeed the second time :)Running all the checks can be slow, so you can also run checksindividually, e.g., to format source code that fails  or checks:pre-commit run --all-files isort
pre-commit run --all-files black
To diagnose why  checks are failing:pre-commit run --all-files pyright
The HTML renderer Vue.js appThe HTML renderer works by embedding a JSON representation of the sample witha Javascript 'bundle' inside an HTML file that can be viewed in any webbrowser.To edit the html renderer style, do:cd html_renderer
npm ci
npm run serve
When launched without a top-level  object, it willfetch a sample profile so you can work with it.To compile the JS app and bundle it back into the pyinstrument python tool:bin/build_js_bundle.py [--force]
"
https://github.com/Guake/guake,Drop-down terminal for GNOME,"==============Guake 3 README|actions-badge|_ |bountysource-badge|_ |docs-badge|_ |translation-badge|_.. |actions-badge| image:: https://github.com/Guake/guake/actions/workflows/ci.yml/badge.svg.. _actions-badge: https://github.com/Guake/guake/actions.. |bountysource-badge| image:: https://img.shields.io/bountysource/team/guake/activity.svg.. _bountysource-badge: https://www.bountysource.com/teams/guake.. |docs-badge| image:: https://readthedocs.org/projects/guake/badge/?version=stable.. _docs-badge: https://guake.readthedocs.io/en/stable/?badge=stable.. |translation-badge| image:: https://hosted.weblate.org/widgets/guake/-/guake/svg-badge.svg.. _translation-badge: https://hosted.weblate.org/projects/guake/guake/IntroductionGuake is a python based dropdown terminal made for the GNOME desktop environment. Guake's style of window isbased on an FPS game, and one of its goals is to be easy to reach.Quick Installation GuidePlease refer to _What it looks like ?.. image:: https://i.ibb.co/s97cJWZ/guake.png:alt: Guake Screenshot:class: with-shadowDrop down terminal on pressing Note when compiling from sourceDo NOT use the Tarball packages automatically generated by GitHub on the Release pages. Theywon't work because one of the main components of Guake build system, PBR, requires the full Githistory to be available when building from source. Note this does not impact source distributionpackages you can download from Pypi.Build from sources instructions are described on_.Please read this carefully before opening an issue!Bugs? Information?Important note: Do NOT use the domain guake.org, it has been registered by someone outsidethe team. We cannot be held responsible for the content on that web site.This project was originally created by Gabriel Falcão, see: https://sourceforge.net/projects/guake-gnome-vte/"
https://github.com/Dman95/SASM,"SASM - simple crossplatform IDE for NASM, MASM, GAS and FASM assembly languages","﻿SASM (SimpleASM) - простая кроссплатформенная среда разработки для языков ассемблера NASM, MASM, GAS, FASM с подсветкой синтаксиса и отладчиком. В SASM Вы можете легко разрабатывать и выполнять программы, написанные на языках ассемблера NASM, MASM, GAS, FASM. Вводите код в форму и запускайте приложение. Программа работает ""из коробки"" и хорошо подойдет для начинающих изучать язык ассемблера.Основана на Qt. Распространяется по свободной лицензии GNU GPL v3.0.SASM (SimpleASM) - simple Open Source crossplatform IDE for NASM, MASM, GAS, FASM assembly languages.SASM has syntax highlighting and debugger. The program works out of the box and is great for beginners to learn assembly language. SASM is translated into Russian, English, Turkish (thanks Ali Goren), Chinese (thanks Ahmed Zetao Yang), German (thanks Sebastian Fischer), Italian (thanks Carlo Dapor), Polish (thanks Krzysztof Rossa), Hebrew (thanks Elian Kamal), Spanish (thanks Mariano Cordoba).Licensed under the GNU GPL v3.0. Based on the Qt.===========================================================================How to build and run SASM:You need:On Windows:For building:C++ compiler (e.g. gcc from MinGW)make (e.g. mingw32-make from MinGW)Qt 5For running:Everything needed is included.On Linux:For building:build-essentialqtbase5-devqtbase5-dev-toolsqt5-qmakeqtchooserFor running:gcc-multilib (x64 OS) or gcc (x86 OS)gdbnasmDownload sources and unpack their.Go to directory with their: ""cd ""Further print commands:Also you can download already compiled packagesfrom site https://dman95.github.io/SASM/ orfrom OBS repository https://download.opensuse.org/repositories/home:/Dman95/More help info in file help.htmlAlso SASM supports doxygen: run ""doxygen configfile"" to generate documentation. In this documentation you can also find a small developer guide which includes information about adding new assemblers and languages support.Copyright © 2013 Dmitriy Manushin"
https://github.com/benoitc/gunicorn,"gunicorn 'Green Unicorn' is a WSGI HTTP Server for UNIX, fast clients and sleepy applications.","Gunicorn.. image:: https://img.shields.io/pypi/v/gunicorn.svg?style=flat:alt: PyPI version:target: https://pypi.python.org/pypi/gunicorn.. image:: https://img.shields.io/pypi/pyversions/gunicorn.svg:alt: Supported Python versions:target: https://pypi.python.org/pypi/gunicorn.. image:: https://github.com/benoitc/gunicorn/actions/workflows/tox.yml/badge.svg:alt: Build Status:target: https://github.com/benoitc/gunicorn/actions/workflows/tox.yml.. image:: https://github.com/benoitc/gunicorn/actions/workflows/lint.yml/badge.svg:alt: Lint Status:target: https://github.com/benoitc/gunicorn/actions/workflows/lint.ymlGunicorn 'Green Unicorn' is a Python WSGI HTTP Server for UNIX. It's a pre-forkworker model ported from Ruby's Unicorn_ project. The Gunicorn server is broadlycompatible with various web frameworks, simply implemented, light on serverresource usage, and fairly speedy.Feel free to join us in _ on _.DocumentationThe documentation is hosted at https://docs.gunicorn.org.InstallationGunicorn requires Python 3.x >= 3.5.Install from PyPI::$ pip install gunicorn
UsageBasic usage::$ gunicorn [OPTIONS] APP_MODULE
Where  is of the pattern . Themodule name can be a full dotted path. The variable name refers to a WSGIcallable that should be found in the specified module.Example with test app::$ cd examples
$ gunicorn --workers=2 test:app
ContributingSee _ for more details.LicenseGunicorn is released under the MIT License. See the LICENSE_ file for moredetails... _Unicorn: https://bogomips.org/unicorn/.. _: https://web.libera.chat/?channels=#gunicorn.. _: https://libera.chat/.. _LICENSE: https://github.com/benoitc/gunicorn/blob/master/LICENSE"
https://github.com/madmaze/pytesseract,A Python wrapper for Google Tesseract,"Python Tesseract.. image:: https://img.shields.io/pypi/pyversions/pytesseract.svg:target: https://pypi.python.org/pypi/pytesseract:alt: Python versions.. image:: 	https://img.shields.io/github/release/madmaze/pytesseract.svg:target: https://github.com/madmaze/pytesseract/releases:alt: Github release.. image:: https://img.shields.io/pypi/v/pytesseract.svg?color=blue:target: https://pypi.python.org/pypi/pytesseract:alt: PyPI release.. image:: https://img.shields.io/conda/vn/conda-forge/pytesseract.svg?color=blue:target: https://anaconda.org/conda-forge/pytesseract:alt: Conda release.. image:: https://results.pre-commit.ci/badge/github/madmaze/pytesseract/master.svg:target: https://results.pre-commit.ci/latest/github/madmaze/pytesseract/master:alt: Pre-commit CI status.. image:: https://github.com/madmaze/pytesseract/workflows/CI/badge.svg?branch=master:target: https://github.com/madmaze/pytesseract/actions?query=workflow%3ACI:alt: CI workflow statusPython-tesseract is an optical character recognition (OCR) tool for python.That is, it will recognize and ""read"" the text embedded in images.Python-tesseract is a wrapper for _.It is also useful as a stand-alone invocation script to tesseract, as it can read all image typessupported by the Pillow and Leptonica imaging libraries, including jpeg, png, gif, bmp, tiff,and others. Additionally, if used as a script, Python-tesseract will print the recognizedtext instead of writing it to a file.USAGEQuickstartNote: Test images are located in the  folder of the Git repo.Library usage:.. code-block:: pythonfrom PIL import Image

import pytesseract

# If you don't have tesseract executable in your PATH, include the following:
pytesseract.pytesseract.tesseract_cmd = r'<full_path_to_your_tesseract_executable>'
# Example tesseract_cmd = r'C:\Program Files (x86)\Tesseract-OCR\tesseract'

# Simple image to string
print(pytesseract.image_to_string(Image.open('test.png')))

# In order to bypass the image conversions of pytesseract, just use relative or absolute image path
# NOTE: In this case you should provide tesseract supported images or tesseract will return error
print(pytesseract.image_to_string('test.png'))

# List of available languages
print(pytesseract.get_languages(config=''))

# French text image to string
print(pytesseract.image_to_string(Image.open('test-european.jpg'), lang='fra'))

# Batch processing with a single file containing the list of multiple image file paths
print(pytesseract.image_to_string('images.txt'))

# Timeout/terminate the tesseract job after a period of time
try:
    print(pytesseract.image_to_string('test.jpg', timeout=2)) # Timeout after 2 seconds
    print(pytesseract.image_to_string('test.jpg', timeout=0.5)) # Timeout after half a second
except RuntimeError as timeout_error:
    # Tesseract processing is terminated
    pass

# Get bounding box estimates
print(pytesseract.image_to_boxes(Image.open('test.png')))

# Get verbose data including boxes, confidences, line and page numbers
print(pytesseract.image_to_data(Image.open('test.png')))

# Get information about orientation and script detection
print(pytesseract.image_to_osd(Image.open('test.png')))

# Get a searchable PDF
pdf = pytesseract.image_to_pdf_or_hocr('test.png', extension='pdf')
with open('test.pdf', 'w+b') as f:
    f.write(pdf) # pdf type is bytes by default

# Get HOCR output
hocr = pytesseract.image_to_pdf_or_hocr('test.png', extension='hocr')

# Get ALTO XML output
xml = pytesseract.image_to_alto_xml('test.png')

# getting multiple types of output with one call to save compute time
# currently supports mix and match of the following: txt, pdf, hocr, box, tsv
text, boxes = pytesseract.run_and_get_multiple_output('test.png', extensions=['txt', 'box'])
Support for OpenCV image/NumPy array objects.. code-block:: pythonimport cv2

img_cv = cv2.imread(r'/<path_to_image>/digits.png')

# By default OpenCV stores images in BGR format and since pytesseract assumes RGB format,
# we need to convert from BGR to RGB format/mode:
img_rgb = cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB)
print(pytesseract.image_to_string(img_rgb))
# OR
img_rgb = Image.frombytes('RGB', img_cv.shape[:2], img_cv, 'raw', 'BGR', 0, 0)
print(pytesseract.image_to_string(img_rgb))
If you need custom configuration like /, use the config keyword... code-block:: python# Example of adding any additional options
custom_oem_psm_config = r'--oem 3 --psm 6'
pytesseract.image_to_string(image, config=custom_oem_psm_config)

# Example of using pre-defined tesseract config file with options
cfg_filename = 'words'
pytesseract.run_and_get_output(image, extension='txt', config=cfg_filename)
Add the following config, if you have tessdata error like: ""Error opening data file..."".. code-block:: python# Example config: r'--tessdata-dir ""C:\Program Files (x86)\Tesseract-OCR\tessdata""'
# It's important to add double quotes around the dir path.
tessdata_dir_config = r'--tessdata-dir ""<replace_with_your_tessdata_dir_path>""'
pytesseract.image_to_string(image, lang='chi_sim', config=tessdata_dir_config)
FunctionsParametersCLI usage:.. code-block:: bashpytesseract [-l lang] image_file
INSTALLATIONPrerequisites:| Installing via pip:Check the _ for more information... code-block:: bashpip install pytesseract
| Or if you have git installed:.. code-block:: bashpip install -U git+https://github.com/madmaze/pytesseract.git
| Installing from source:.. code-block:: bashgit clone https://github.com/madmaze/pytesseract.git
cd pytesseract && pip install -U .
| Install with conda (via _):.. code-block:: bashconda install -c conda-forge pytesseract
TESTINGTo run this project's test suite, install and run . Ensure that you have installed and in your PATH... code-block:: bashpip install tox
tox
LICENSECheck the LICENSE file included in the Python-tesseract repository/distribution.As of Python-tesseract 0.3.1 the license is Apache License Version 2.0CONTRIBUTORS"
https://github.com/flask-restful/flask-restful,Simple framework for creating REST APIs,Flask-RESTfulFlask-RESTful provides the building blocks for creating a great REST API.User GuideYou'll find the user guide and all documentation 
https://github.com/robotframework/robotframework,Generic automation framework for acceptance testing and RPA,"Robot Framework.. contents:::local:Introduction_ |r| is a generic open sourceautomation framework for acceptance testing, acceptance test drivendevelopment (ATDD), and robotic process automation (RPA). It has simple plaintext syntax and it can be extended easily with generic and custom libraries.Robot Framework is operating system and application independent. It isimplemented using _ which is also the primarylanguage to extend it. The framework has a rich ecosystem around it consistingof various generic libraries and tools that are developed as separate projects.For more information about Robot Framework and the ecosystem, seehttp://robotframework.org.Robot Framework project is hosted on GitHub_ where you can find source code,an issue tracker, and some further documentation. Downloads are hosted on PyPI_.Robot Framework development is sponsored by non-profit _. If you are using the frameworkand benefiting from it, consider joining the foundation to help maintainingthe framework and developing it further... _GitHub: https://github.com/robotframework/robotframework.. _PyPI: https://pypi.python.org/pypi/robotframework.. image:: https://img.shields.io/pypi/v/robotframework.svg?label=version:target: https://pypi.python.org/pypi/robotframework:alt: Latest version.. image:: https://img.shields.io/pypi/l/robotframework.svg:target: http://www.apache.org/licenses/LICENSE-2.0.html:alt: LicenseInstallationIf you already have Python_ with _ installed,you can simply run::pip install robotframework
For more detailed installation instructions, including installing Python, see__.Robot Framework requires Python 3.6 or newer and runs also on . or_, you can use __.__ https://github.com/robotframework/robotframework/tree/v4.1.3#readmeExampleBelow is a simple example test case for testing login to some system.You can find more examples with links to related demo projects fromhttp://robotframework.org... code:: robotframework*** Settings ***
Documentation     A test suite with a single test for valid login.
...
...               This test has a workflow that is created using keywords in
...               the imported resource file.
Resource          login.resource

*** Test Cases ***
Valid Login
    Open Browser To Login Page
    Input Username    demo
    Input Password    mode
    Submit Credentials
    Welcome Page Should Be Open
    [Teardown]    Close Browser
UsageTests (or tasks) are executed from the command line using the command or by executing the  module directly like  .The basic usage is giving a path to a test (or task) file or directory as anargument with possible command line options before the path::robot tests.robot
robot --variable BROWSER:Firefox --outputdir results path/to/tests/
Additionally, there is the  tool for combining results and otherwisepost-processing outputs::rebot --name Example output1.xml output2.xml
Run  and  for more information about the commandline usage. For a complete reference manual see _.DocumentationSupport and ContactContributingInterested to contribute to Robot Framework? Great! In that case it is a goodstart by looking at the . If you and __ labels.Remember also that there are many other tools and libraries in the wider_ that you cancontribute to!__ https://github.com/robotframework/robotframework/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22__ https://github.com/robotframework/robotframework/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22License and TrademarkRobot Framework is open source software provided under the . license. Most libraries and toolsin the ecosystem are also open source, but they may use different licenses.Robot Framework trademark is owned by _.__ http://apache.org/licenses/LICENSE-2.0__ http://creativecommons.org/licenses/by/3.0.. |r| unicode:: U+00AE"
https://github.com/douban/dpark,"Python clone of Spark,  a MapReduce alike framework in Python","DPark|pypi status| |ci status| |gitter|DPark is a Python clone of Spark, MapReduce(R) alike computing frameworksupporting iterative computation.Installation.. code:: bash## Due to the use of C extensions, some libraries need to be installed first.

$ sudo apt-get install libtool pkg-config build-essential autoconf automake
$ sudo apt-get install python-dev
$ sudo apt-get install libzmq-dev

## Then just pip install dpark (``sudo`` maybe needed if you encounter permission problem).

$ pip install dpark
Examplefor word counting ():.. code:: python from dpark import DparkContext
 ctx = DparkContext()
 file = ctx.textFile(""/tmp/words.txt"")
 words = file.flatMap(lambda x:x.split()).map(lambda x:(x,1))
 wc = words.reduceByKey(lambda x,y:x+y).collectAsMap()
 print wc
This script can run locally or on a Mesos cluster without anymodification, just using different command-line arguments:.. code:: bash$ python wc.py
$ python wc.py -m process
$ python wc.py -m host[:port]
See examples/ for more use cases.ConfigurationDPark can run with Mesos 0.9 or higher.If a  environment variable is set, you can use ashortcut and run DPark with Mesos just by typing.. code:: bash$ python wc.py -m mesos
 can be any scheme of Mesos master, such as.. code:: bash$ export MESOS_MASTER=zk://zk1:2181,zk2:2181,zk3:2181/mesos_master
In order to speed up shuffling, you should deploy Nginx at port 5055 foraccessing data in  (default is ), suchas:.. code:: bash        server {
                listen 5055;
                server_name localhost;
                root /tmp/dpark/;
        }
UI2 DAGs:UI when running
Just open the url from log like ``start listening on Web UI http://server_01:40812`` .


UI after running
UI examples for features

show sharing shuffle map output

.. code:: python


   rdd = DparkContext().makeRDD([(1,1)]).map(m).groupByKey()
   rdd.map(m).collect()
   rdd.map(m).collect()


.. image:: images/share_mapoutput.png


combine nodes iff with same lineage,  form a logic tree inside stage, then each node contain a PIPELINE of rdds.


.. code:: python


   rdd1 = get_rdd()
   rdd2 = dc.union([get_rdd() for i in range(2)])
   rdd3 = get_rdd().groupByKey()
   dc.union([rdd1, rdd2, rdd3]).collect()


.. image:: images/unions.png


More docs (in Chinese)
-------------------------

https://dpark.readthedocs.io/zh_CN/latest/

https://github.com/jackfengji/test\_pro/wiki

Mailing list: dpark-users@googlegroups.com
(http://groups.google.com/group/dpark-users)


.. |pypi status| image:: https://img.shields.io/pypi/v/DPark.svg
   :target: https://pypi.python.org/pypi/DPark

.. |gitter| image:: https://badges.gitter.im/douban/dpark.svg
   :alt: Join the chat at https://gitter.im/douban/dpark
   :target: https://gitter.im/douban/dpark?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge

.. |ci status| image:: https://travis-ci.org/douban/dpark.svg
   :target: https://travis-ci.org/douban/dpark
"
https://github.com/shidenggui/easytrader,"提供同花顺客户端/国金/华泰客户端/雪球的基金、股票自动程序化交易以及自动打新，支持跟踪 joinquant /ricequant 模拟交易 和 实盘雪球组合, 量化交易组件","easytrader微信群以及公众号欢迎大家扫码关注公众号「食灯鬼」，一起交流。进群可通过菜单加我好友，备注量化。若二维码因 Github 网络无法打开，请点击直接打开图片。Authoreasytrader © , Released under the  License.相关支持券商模拟交易使用文档作者其他作品"
https://github.com/GoogleCloudPlatform/python-docs-samples,Code samples used on cloud.google.com,Google Cloud Platform Python SamplesPython samples for .    Google Cloud SamplesCheck out some of the samples found on this repository on the  page.SetupHow to run a sampleContributingContributions welcome! See the .
https://github.com/coleifer/peewee,"a small, expressive orm -- supports postgresql, mysql, sqlite and cockroachdb",".. image:: https://media.charlesleifer.com/blog/photos/peewee3-logo.pngpeeweePeewee is a simple and small ORM. It has few (but expressive) concepts, making it easy to learn and intuitive to use.New to peewee? These may help:ExamplesDefining models is similar to Django or SQLAlchemy:.. code-block:: pythonfrom peewee import *
import datetime


db = SqliteDatabase('my_database.db')

class BaseModel(Model):
    class Meta:
        database = db

class User(BaseModel):
    username = CharField(unique=True)

class Tweet(BaseModel):
    user = ForeignKeyField(User, backref='tweets')
    message = TextField()
    created_date = DateTimeField(default=datetime.datetime.now)
    is_published = BooleanField(default=True)
Connect to the database and create tables:.. code-block:: pythondb.connect()
db.create_tables([User, Tweet])
Create a few rows:.. code-block:: pythoncharlie = User.create(username='charlie')
huey = User(username='huey')
huey.save()

# No need to set `is_published` or `created_date` since they
# will just use the default values we specified.
Tweet.create(user=charlie, message='My first tweet')
Queries are expressive and composable:.. code-block:: python# A simple query selecting a user.
User.get(User.username == 'charlie')

# Get tweets created by one of several users.
usernames = ['charlie', 'huey', 'mickey']
users = User.select().where(User.username.in_(usernames))
tweets = Tweet.select().where(Tweet.user.in_(users))

# We could accomplish the same using a JOIN:
tweets = (Tweet
          .select()
          .join(User)
          .where(User.username.in_(usernames)))

# How many tweets were published today?
tweets_today = (Tweet
                .select()
                .where(
                    (Tweet.created_date >= datetime.date.today()) &
                    (Tweet.is_published == True))
                .count())

# Paginate the user table and show me page 3 (users 41-60).
User.select().order_by(User.username).paginate(3, 20)

# Order users by the number of tweets they've created:
tweet_ct = fn.Count(Tweet.id)
users = (User
         .select(User, tweet_ct.alias('ct'))
         .join(Tweet, JOIN.LEFT_OUTER)
         .group_by(User)
         .order_by(tweet_ct.desc()))

# Do an atomic update (for illustrative purposes only, imagine a simple
# table for tracking a ""count"" associated with each URL). We don't want to
# naively get the save in two separate steps since this is prone to race
# conditions.
Counter.update(count=Counter.count + 1).where(Counter.url == request.url)
Check out the _.Learning moreCheck the _ for more examples.Specific question? Come hang out in the #peewee channel on irc.libera.chat, or post to the mailing list, http://groups.google.com/group/peewee-orm . If you would like to report a bug, _ on GitHub.Still want more info?.. image:: https://media.charlesleifer.com/blog/photos/wat.jpgI've written a number of blog posts about building applications and web-services with peewee (and usually Flask). If you'd like to see some real-life applications that use peewee, the following resources may be useful:"
https://github.com/anishathalye/git-remote-dropbox,A transparent bridge between Git and Dropbox - use a Dropbox (shared) folder as a Git remote! 🎁,"git-remote-dropbox  git-remote-dropbox is a transparent bidirectional bridge between Git andDropbox. It lets you use a Dropbox folder or a shared folder as a Git remote!This Git remote helper makes Dropbox act like a true Git remote. It maintainsall guarantees that are provided by a traditional Git remote while usingDropbox as a backing store. This means that it works correctly even when thereare multiple people operating on the repository at once, making it possible touse a Dropbox shared folder as a Git remote for collaboration.Once the helper is installed, using it is as simple as adding a remote like.To clone repositories in folders or shared folders mounted in your Dropbox, youcan run:git clone ""dropbox:///path/to/repo""
To add a remote to an existing local repository, you can run:git remote add origin ""dropbox:///path/to/repo""
The repository directory will be created automatically the first time you push.After adding the remote, you can treat it just like a regular Git remote. TheDropbox-backed remote supports all operations that regular remotes support, andit provides identical guarantees in terms of atomicity even when there areconcurrent operations, even when using a shared folder.SetupInstall git-remote-dropboxLog in to DropboxRun  and follow the instructions to authenticate with OAuthand log in to your Dropbox account.SharingThe above gives you a way to create a Git repository on Dropbox and use it from multiple machines that you own. In other words, it's a convenient way to share a remote with your laptop and your desktop.If you want to share with other people, you should explicitly share (e.g. via the Dropbox website) the root folder of the repo with your collaborators. Then they should also install git-remote-dropbox and log in with their own account.Multiple Accountsgit-remote-dropbox supports using multiple Dropbox accounts. You can have namedaccounts with . These usernames are unrelatedYou can tell git-remote-dropbox to use a particular account by setting the gitremote URL appropriately, specifying a username like:.Repository ManagerIn addition to the git remote helper, git-remote-dropbox comes with anadditional tool to manage your logins and repositories on Dropbox. This toolcan be invoked as .The tool supports the following commands:NotesFAQWhy shouldn't I keep my Git repository in Dropbox and let the client syncThere seem to be a lot of articles on the Internet recommending this as a goodworkflow. However, this is not a good idea! The desktop client is not awareof how Git manages it's on-disk format, so if there are concurrent changes ordelays in syncing, it's possible to have conflicts that result in a corruptedGit repository. This may be uncommon with the way the timing works out in thesingle user case, but it's still not safe!Why shouldn't I keep a bare Git repository in a Dropbox shared folder, use itThere seem to be some articles on the Internet suggesting that this is a goodidea. It's not. Using the desktop client to sync a bare Git repository is notsafe. Concurrent changes or delays in syncing can result in a corrupted Gitrepository.How can I access / recover my repository from Dropbox without using theBecause git-remote-dropbox uses an on-disk format that's compatible with Git,accessing your repository without using the helper is easy:How do I use git-remote-dropbox from behind a proxy server?You can use git-remote-dropbox from behind a proxy server by setting the and  environment variables. See formore details.How do I use git-remote-dropbox with submodules?You can allow this by settingto :git config --global --add protocol.dropbox.allow always
DesignTo read about the design of git-remote-dropbox, see .This could be especially useful if you're thinking about contributing to theproject.ContributingDo you have ideas on how to improve git-remote-dropbox? Have a feature request,bug report, or patch? Great! See  forinformation on what you can do about that.LicenseCopyright (c) Anish Athalye. Released under the MIT License. See for details."
https://github.com/sloria/TextBlob,"Simple, Pythonic, text processing--Sentiment analysis, part-of-speech tagging, noun phrase extraction, translation, and more.","TextBlob: Simplified Text Processing.. image:: https://badgen.net/pypi/v/TextBlob:target: https://pypi.org/project/textblob/:alt: Latest version.. image:: https://badgen.net/travis/sloria/TextBlob/dev:target: https://travis-ci.org/sloria/TextBlob:alt: Travis-CIHomepage: _ is a Python (2 and 3) library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more... code-block:: pythonfrom textblob import TextBlob

text = '''
The titular threat of The Blob has always struck me as the ultimate movie
monster: an insatiably hungry, amoeba-like mass able to penetrate
virtually any safeguard, capable of--as a doomed doctor chillingly
describes it--""assimilating flesh on contact.
Snide comparisons to gelatin be damned, it's a concept with the most
devastating of potential consequences, not unlike the grey goo scenario
proposed by technological theorists fearful of
artificial intelligence run rampant.
'''

blob = TextBlob(text)
blob.tags           # [('The', 'DT'), ('titular', 'JJ'),
                    #  ('threat', 'NN'), ('of', 'IN'), ...]

blob.noun_phrases   # WordList(['titular threat', 'blob',
                    #            'ultimate movie monster',
                    #            'amoeba-like mass', ...])

for sentence in blob.sentences:
    print(sentence.sentiment.polarity)
# 0.060
# -0.341
TextBlob stands on the giant shoulders of _ and _, and plays nicely with both.FeaturesGet it now::$ pip install -U textblob
$ python -m textblob.download_corpora
ExamplesSee more examples at the _... _: https://textblob.readthedocs.io/en/latest/quickstart.html#quickstartDocumentationFull documentation is available at https://textblob.readthedocs.io/.RequirementsProject LinksLicenseMIT licensed. See the bundled _ file for more details... _pattern: https://github.com/clips/pattern/.. _NLTK: http://nltk.org/"
https://github.com/vnpy/vnpy,基于Python的开源量化交易平台开发框架,"VeighNa - By Traders, For Traders.💬 Want to read this in english ? Go VeighNa是一套基于Python的开源量化交易系统开发框架，在开源社区持续不断的贡献下一步步成长为多功能量化交易平台，自发布以来已经积累了众多来自金融机构或相关领域的用户，包括私募基金、证券公司、期货公司等。:rocket: :rocket: :rocket: 面向专业交易员的【VeighNa Elite量化终端】已经正式发布，针对专业交易员群体在海量策略并发、智能移仓换月、算法拆单执行、多账户交易支持等方面的需求提供了完善支持。了解更详细的信息请扫描下方二维码关注后，点击菜单栏的【社区交流 -> Elite会员服务】即可：在使用VeighNa进行二次开发（策略、模块等）的过程中有任何疑问，请查看，如果无法解决请前往的【提问求助】板块寻求帮助，也欢迎在【经验分享】板块分享你的使用心得！针对VeighNa的金融机构用户，创建了一个专门的【VeighNa机构用户群】（QQ群号：676499931），主要分享机构应用方面相关的问题，如：银行间市场接入、资管O32系统、分布式部署等内容。请注意本群只对金融机构用户开放，加群时请注明：姓名-机构-部门。功能特点注：以上关于功能特点的说明为根据说明文档发布时情况罗列，后续可能存在更新或调整。若功能描述同实际存在出入，欢迎通过Issue联系进行调整。环境准备安装步骤在下载Release发布版本，解压后运行以下命令安装：Windowsinstall.bat
Ubuntubash install.sh
Macosbash install_osx.sh
注意：setup.cfg中列举了VeighNa框架安装所需的依赖库，requirements.txt中给出了这些依赖库的推荐安装版本。使用指南注意：脚本运行除了基于VeighNa Station的图形化启动方式外，也可以在任意目录下创建run.py，写入以下示例代码：from vnpy.event import EventEngine
from vnpy.trader.engine import MainEngine
from vnpy.trader.ui import MainWindow, create_qapp

from vnpy_ctp import CtpGateway
from vnpy_ctastrategy import CtaStrategyApp
from vnpy_ctabacktester import CtaBacktesterApp


def main():
    """"""Start VeighNa Trader""""""
    qapp = create_qapp()

    event_engine = EventEngine()
    main_engine = MainEngine(event_engine)
    
    main_engine.add_gateway(CtpGateway)
    main_engine.add_app(CtaStrategyApp)
    main_engine.add_app(CtaBacktesterApp)

    main_window = MainWindow(main_engine, event_engine)
    main_window.showMaximized()

    qapp.exec()


if __name__ == ""__main__"":
    main()
在该目录下打开CMD（按住Shift->点击鼠标右键->在此处打开命令窗口/PowerShell）后运行下列命令启动VeighNa Trader：python run.py
贡献代码VeighNa使用Github托管其源代码，如果希望贡献代码请使用github的PR（Pull Request）的流程:在提交代码的时候，请遵守以下规则，以提高代码质量：其他内容版权说明MIT"
https://github.com/XX-net/XX-Net,A proxy tool to bypass GFW.,:rocket: XX-Net (翻墙VPN)这是一个可靠的翻墙系统，已经连续运行 8 年！我们不去研究墙有什么缺陷，因为所有的缺陷都会被慢慢的补上。我们的策略是化身为普通流量，完全无法区分，最终隐身在茫茫的网络连接中。。。:electric_plug: 功能特性官网下载: Telegram: Twitter:  &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;最新公告：2023-08-19提示：
https://github.com/toml-lang/toml,"Tom's Obvious, Minimal Language","TOMLTom's Obvious, Minimal Language.By Tom Preston-Werner, Pradyun Gedam, et al.ObjectivesTOML aims to be a minimal configuration file format that's easy to read due toobvious semantics. TOML is designed to map unambiguously to a hash table. TOMLshould be easy to parse into data structures in a wide variety of languages.Example# This is a TOML document.

title = ""TOML Example""

[owner]
name = ""Tom Preston-Werner""
dob = 1979-05-27T07:32:00-08:00 # First class dates

[database]
server = ""192.168.1.1""
ports = [ 8000, 8001, 8002 ]
connection_max = 5000
enabled = true

[servers]

  # Indentation (tabs and/or spaces) is allowed but not required
  [servers.alpha]
  ip = ""10.0.0.1""
  dc = ""eqdc10""

  [servers.beta]
  ip = ""10.0.0.2""
  dc = ""eqdc10""

[clients]
data = [ [""gamma"", ""delta""], [1, 2] ]

# Line breaks are OK when inside arrays
hosts = [
  ""alpha"",
  ""omega""
]
Comparison with Other FormatsTOML shares traits with other file formats used for application configurationand data serialization, such as YAML and JSON. TOML and JSON both are simple anduse ubiquitous data types, making them easy to code for or parse with machines.TOML and YAML both emphasize human readability features, like comments that makeit easier to understand the purpose of a given line. TOML differs in combiningthese, allowing comments (unlike JSON) but preserving simplicity (unlike YAML).Because TOML is explicitly intended as a configuration file format, parsing itis easy, but it is not intended for serializing arbitrary data structures. TOMLalways has a hash table at the top level of the file, which can easily have datanested inside its keys, but it doesn't permit top-level arrays or floats, so itcannot directly serialize some data. There is also no standard identifying thestart or end of a TOML file, which can complicate sending it through a stream.These details must be negotiated on the application layer.INI files are frequently compared to TOML for their similarities in syntax anduse as configuration files. However, there is no standardized format for INI andthey do not gracefully handle more than one or two levels of nesting.Further reading:Get InvolvedDocumentation, bug reports, pull requests, and all other contributions arewelcome!WikiWe have an  thatcatalogs the following:Please take a look if you'd like to view or add to that list. Thanks for being apart of the TOML community!"
https://github.com/paralax/awesome-honeypots,an awesome list of honeypot resources,"Awesome Honeypots A curated list of awesome honeypots, plus related components and much more, divided into categories such as Web, services, and others, with a focus on free and open source projects.There is no pre-established order of items in each category, the order is for contribution. If you want to contribute, please read the .Discover more awesome lists at .ContentsRelated ListsHoneypotsHoneyd ToolsNetwork and Artifact AnalysisData ToolsGuides"
https://github.com/openimages/dataset,The Open Images dataset,"Open Images DatasetAs of V4, ."
https://github.com/programthink/zhao,【编程随想】整理的《太子党关系网络》，专门揭露赵国的权贵,= 俺整理的《太子党关系网络》 === 简介 ==此项目创建于2016年2月，专门用来揭露天朝的权贵（也就是传说中的“赵家人”）。俺把这几年收集整理的数据开源到 GitHub，便于多人协作——大伙儿群策群力，一起来曝光权贵家族。初次上传的数据包括：700多个数据文件（ '''对应700多人，130多个家族''' ），另有200多张图片（人物头像）。随着俺不断完善，数据会越来越多。对这个项目，俺会【持续更新】。比如朝廷每次换届的时候，俺都会补充新的素材。为了确保数据的可信度，俺主要参考“维基百科”以及一些国际权威媒体的报道（比如《纽约时报》、《华尔街日版》、《金融时报》等等）。另外，对于某些客观事实（比如：生卒年月、简历、亲戚关系），俺也参考了天朝政府的官方网站，以及墙内的“百度百科”。== 下载说明 ==GitHub 提供了“下载整个项目”的功能，但是会比较大。如果你仅仅想看《太子党关系网络》这份文档，只需在首页上方点击进入 '''download''' 这个目录。该目录下有 '''pdf''' 和 '''jpg''' 两个子目录，分别存放对应的 '''【文件类型】''' 。你想要看哪一种文件格式，就进入哪个子目录里面。进入【文件类型】的子目录之后，会看到一个文件列表（目前有13个文件）。先点击你想要的某个文件，会进入该文件的页面。然后在【右上方】你会看到一个 '''Raw 按钮''' ，在这个按钮上点【右键】，在【右键菜单】里面选“保存”或“另存为”，就可以把这个文件下载到你本机。== 多人协作说明 ==俺非常希望有更多的网友参与该项目，大伙儿一起来完善天朝权贵家族的资料。想要参与的同学，可以通过如下方式：（后面两种方式，你需要有 GitHub 的帐号）== 数据格式说明 ==本项目的数据文件，全部采用[https://zh.wikipedia.org/wiki/YAML YAML 格式]。这种格式非常简洁明了，有利于完全不懂技术的网友参与编辑。而且俺在每一个 YAML 格式的文件中都写了详细的注释，便于其他网友修改。== 目录说明 ===== data 目录 ===data 目录用来保存数据文件，该目录下另有如下三个子目录：这个目录存放个人的资料，每个人一个目录，目录名就是人名。对于偶尔有同名的情况，在目录名末尾追加数字序号来区分。每个目录下都有一个 brief.yaml 文件，包含此人的简介。有些目录下还有一个 portrait.png 文件，对应此人的头像。这个目录存放与太子党有关的公司或组织机构。目录结构与 person 类似。这个目录存放家族关系的信息。每个家族是一个 yaml 格式的文件。=== bin 目录 ===该目录存放编译脚本。该脚本的使用参见下面的章节。=== download 目录 ===该目录存放制作好的文件，目前先提供 jpg 和 pdf 两种格式。如果你需要其它格式，可以用 bin 目录下的编译脚本自行搞定（编译脚本的使用，参见下面的章节）。== 编译脚本使用说明 ==（俺是在 Linux 上编写该脚本，尚未在 Windows 上进行测试）如果你在 Windows 上使用碰到问题，可以到[https://program-think.blogspot.com/ 俺博客]留言进行反馈。也可以在[https://github.com/programthink/zhao/issues 本项目发一个 issue]。=== 脚本的命令行参数 ===俺使用 python 作为编译脚本，该脚本位于 bin 目录下。通过该脚本可以把原始数据生成为 dot 语言的脚本。然后再调用 Graphviz 把 dot 脚本生成各种格式（比如：pdf、jpeg）。要使用该脚本，先在命令行模式下进入 bin 目录，然后运行如下命令：（生成 pdf 格式的示例）'''python make.py pdf'''（生成 jpg 格式的示例）'''python make.py jpg'''=== 依赖的软件 ===要使用上述脚本，你需要事先安装相关的软件（如下）因为俺用的是 Python 脚本，所以你需要先安装 python 软件。目前 Python 有两种大版本——python2 和 python3——俺的编译脚本 '''【同时兼容】''' 这两种 Python 的大版本。对于 Python 的小版本，俺本人在 '''2.7''' 和 '''3.5''' 上测试通过。2.6 和 3.4 估计也可以。这是一个基于 python 开发的软件包，专门用来处理 YAML 格式的文件。你需要在你的 python 环境中安装该软件包。其官方链接如下：[http://pyyaml.org/wiki/PyYAML PyYAML 的官网的 wiki][https://pypi.python.org/pypi/PyYAML Python 官网的 PYPI]这个软件是用来生成【关系图】的。关于该这个软件，俺已经写了一篇扫盲教程：《[https://program-think.blogspot.com/2016/02/opensource-review-graphviz.html 开源项目：【自动】绘图工具Graphviz——《太子党关系网络》就是用它制作]》== 致“反对此项目的墙内程序员” ==本项目上线第二天，就收获 363 个 star 兼 88 个 fork，甚至还挤进 GitHub 的“当日 Trending”——俺很荣幸，也很高兴有这么多人给俺捧场。但是在[https://github.com/programthink/zhao/issues 本项目的 issue 列表]中也看到好几个反对此项目的程序员（应该都来自墙内），他们担心这个项目导致 GitHub 被 GFW 封杀。这几年来，类似的言论俺已经看了不少。就好比强盗拿刀杀人，围观者不但没有谴责强盗，反而去谴责卖刀的店家——这就是传说中的“斯德哥尔摩综合症”。有兴趣的同学，可以看俺之前的博文——《[https://program-think.blogspot.com/2012/06/stockholm-syndrome.html 天朝民众的心理分析：斯德哥尔摩综合症]》
https://github.com/Zulko/moviepy,Video editing with Python,"MoviePy.. image:: https://badge.fury.io/py/moviepy.svg:target: PyPI_:alt: MoviePy page on the Python Package Index.. image:: https://img.shields.io/gitter/room/movie-py/gitter?color=46BC99&logo=gitter:target: Gitter_:alt: Discuss MoviePy on Gitter.. image:: https://img.shields.io/github/actions/workflow/status/Zulko/moviepy/test_suite.yml?logo=github:target: https://github.com/Zulko/moviepy/actions/workflows/test_suite.yml:alt: Build status on gh-actions.. image:: https://img.shields.io/coveralls/github/Zulko/moviepy/master?logo=coveralls:target: https://coveralls.io/github/Zulko/moviepy?branch=master:alt: Code coverage from coveralls.ioMoviePy (full documentation_) is a Python library for video editing: cutting, concatenations, title insertions, video compositing (a.k.a. non-linear editing), video processing, and creation of custom effects. See the gallery_ for some examples of use.MoviePy can read and write all the most common audio and video formats, including GIF, and runs on Windows/Mac/Linux, with Python 3.6+. Here it is in action in an IPython notebook:.. image:: https://raw.githubusercontent.com/Zulko/moviepy/master/docs/demo_preview.jpeg:alt: [logo]:align: centerExampleIn this example we open a video file, select the subclip between t=50s and t=60s, add a title at the center of the screen, and write the result to a new file:.. code:: pythonfrom moviepy import *

video = VideoFileClip(""myHolidays.mp4"").subclip(50,60)

# Make the text. Many more options are available.
txt_clip = ( TextClip(""My Holidays 2013"",fontsize=70,color='white')
             .with_position('center')
             .with_duration(10) )

result = CompositeVideoClip([video, txt_clip]) # Overlay text on video
result.write_videofile(""myHolidays_edited.webm"",fps=25) # Many options...
Note: This example uses the new 2.x API, for MoviePy 1.0.3, currently on PyPI, see _.Maintainers wanted!As there are more and more people seeking support (270 open issues as of Jan. 2021!) and all the MoviePy maintainers seem busy, we'd love to hear about developers interested in giving a hand and solving some of the issues (especially the ones that affect you) or reviewing pull requests. Open an issue or contact us directly if you are interested. Thanks!InstallationMoviePy depends on the Python modules NumPy_, Imageio_, Decorator_, and Proglog_, which will be automatically installed during MoviePy's installation. The software FFMPEG should be automatically downloaded/installed (by imageio) during your first use of MoviePy (installation will take a few seconds). If you want to use a specific version of FFMPEG, follow the instructions in . In case of trouble, provide feedback.Installation by hand: download the sources, either from PyPI_ or, if you want the development version, from GitHub_, unzip everything into one folder, open a terminal and type:.. code:: bash$ (sudo) python setup.py install
Installation with pip: if you have  installed, just type this in a terminal:.. code:: bash$ (sudo) pip install moviepy
If you have neither  nor  installed, the command above will fail. In this case type this before installing:.. code:: bash$ (sudo) pip install setuptools
Optional but useful dependencies
You can install ``moviepy`` with all dependencies via:

.. code:: bash

    $ (sudo) pip install moviepy[optional]

ImageMagick_ is not strictly required, but needed if you want to incorporate texts. It can also be used as a backend for GIFs, though you can also create GIFs with MoviePy without ImageMagick.

Once you have installed ImageMagick, MoviePy will try to autodetect the path to its executable. If it fails, you can still configure it by setting environment variables (see the documentation).

PyGame_ is needed for video and sound previews (not relevant if you intend to work with MoviePy on a server but essential for advanced video editing by hand).

For advanced image processing, you will need one or several of the following packages:

- The Python Imaging Library (PIL) or, even better, its branch Pillow_.
- Scipy_ (for tracking, segmenting, etc.) can be used to resize video clips if PIL and OpenCV are not installed.
- `Scikit Image`_ may be needed for some advanced image manipulation.
- `OpenCV 2.4.6`_ or a more recent version (one that provides the package ``cv2``) may be needed for some advanced image manipulation.
- `Matplotlib`_

For instance, using the method ``clip.resize`` requires that at least one of Scipy, PIL, Pillow or OpenCV is installed.


Documentation
-------------

Building the documentation has additional dependencies that require installation.

.. code:: bash

    $ (sudo) pip install moviepy[doc]

The documentation can be generated and viewed via:

.. code:: bash

    $ python setup.py build_docs

You can pass additional arguments to the documentation build, such as clean build:

.. code:: bash

    $ python setup.py build_docs -E

More information is available from the `Sphinx`_ documentation.

New in 1.0.0: Progress bars and messages with Proglog
-------------------------------------------------------

Non-backwards-compatible changes were introduced in 1.0.0 to
manage progress bars and messages using
`Proglog <https://github.com/Edinburgh-Genome-Foundry/Proglog>`_, which
enables to display nice progress bars in the console as well as in
a Jupyter notebook or any user interface, like a website.

To display notebook friendly progress bars, first install IPyWidgets:

.. code::

    sudo pip install ipywidgets
    sudo jupyter nbextension enable --py --sys-prefix widgetsnbextension

Then at the beginning of your notebook enter:

.. code:: python

    import proglog
    proglog.notebook()

Have a look at the Proglog project page for more options.

Contribute
----------

MoviePy is open-source software originally written by Zulko_ and released under the MIT licence. The project is hosted on GitHub_, where everyone is welcome to contribute, ask for help or simply give feedback. Please read our `Contributing Guidelines`_ for more information about how to contribute!

You can also discuss the project on Reddit_ or Gitter_. These are preferred over GitHub issues for usage questions and examples.


Maintainers
-----------

- Zulko_ (owner)
- `@tburrows13`_
- `@mgaitan`_
- `@earney`_
- `@mbeacom`_
- `@overdrivr`_
- `@keikoro`_
- `@ryanfox`_
- `@mondeja`_


.. MoviePy links
.. _gallery: https://zulko.github.io/moviepy/gallery.html
.. _documentation: https://zulko.github.io/moviepy/
.. _`download MoviePy`: https://github.com/Zulko/moviepy
.. _`Label Wiki`: https://github.com/Zulko/moviepy/wiki/Label-Wiki
.. _Contributing Guidelines: https://github.com/Zulko/moviepy/blob/master/CONTRIBUTING.md

.. Websites, Platforms
.. _Reddit: https://www.reddit.com/r/moviepy/
.. _PyPI: https://pypi.python.org/pypi/moviepy
.. _GitHub: https://github.com/Zulko/moviepy
.. _Gitter: https://gitter.im/movie-py/Lobby

.. Software, Tools, Libraries
.. _Pillow: https://pillow.readthedocs.org/en/latest/
.. _Scipy: https://www.scipy.org/
.. _`OpenCV 2.4.6`: https://github.com/skvark/opencv-python
.. _Pygame: https://www.pygame.org/download.shtml
.. _Numpy: https://www.scipy.org/install.html
.. _imageio: https://imageio.github.io/
.. _`Scikit Image`: https://scikit-image.org/docs/stable/install.html
.. _Decorator: https://pypi.python.org/pypi/decorator
.. _proglog: https://github.com/Edinburgh-Genome-Foundry/Proglog
.. _ffmpeg: https://www.ffmpeg.org/download.html
.. _ImageMagick: https://www.imagemagick.org/script/index.php
.. _`Matplotlib`: https://matplotlib.org/
.. _`Sphinx`: https://www.sphinx-doc.org/en/master/setuptools.html

.. People
.. _Zulko: https://github.com/Zulko
.. _`@mgaitan`: https://github.com/mgaitan
.. _`@tburrows13`: https://github.com/tburrows13
.. _`@earney`: https://github.com/earney
.. _`@mbeacom`: https://github.com/mbeacom
.. _`@overdrivr`: https://github.com/overdrivr
.. _`@keikoro`: https://github.com/keikoro
.. _`@ryanfox`: https://github.com/ryanfox
.. _`@mondeja`: https://github.com/mondeja
"
https://github.com/IDSIA/sacred,"Sacred is a tool to help you configure, organize, log and reproduce experiments developed at IDSIA.","Sacred| *Every experiment is sacred*
| *Every experiment is great*
| *If an experiment is wasted*
| *God gets quite irate*
|pypi| |py_versions| |license| |rtfd| |doi||build| |coverage| |code_quality| |black|Sacred is a tool to help you configure, organize, log and reproduce experiments.It is designed to do all the tedious overhead work that you need to do aroundyour actual experiment in order to:Sacred achieves this through the following main mechanisms:Example+------------------------------------------------+--------------------------------------------+| Script to train an SVM on the iris dataset | The same script as a Sacred experiment |+------------------------------------------------+--------------------------------------------+| .. code:: python                               | .. code:: python                           ||                                                |                                            ||  from numpy.random import permutation          |   from numpy.random import permutation     ||  from sklearn import svm, datasets             |   from sklearn import svm, datasets        ||                                                |   from sacred import Experiment            ||                                                |   ex = Experiment('iris_rbf_svm')          ||                                                |                                            ||                                                |   @ex.config                               ||                                                |   def cfg():                               ||  C = 1.0                                       |     C = 1.0                                ||  gamma = 0.7                                   |     gamma = 0.7                            ||                                                |                                            ||                                                |   @ex.automain                             ||                                                |   def run(C, gamma):                       ||  iris = datasets.load_iris()                   |     iris = datasets.load_iris()            ||  perm = permutation(iris.target.size)          |     per = permutation(iris.target.size)    ||  iris.data = iris.data[perm]                   |     iris.data = iris.data[per]             ||  iris.target = iris.target[perm]               |     iris.target = iris.target[per]         ||  clf = svm.SVC(C=C, kernel='rbf',              |     clf = svm.SVC(C=C, kernel='rbf',       ||          gamma=gamma)                          |             gamma=gamma)                   ||  clf.fit(iris.data[:90],                       |     clf.fit(iris.data[:90],                ||          iris.target[:90])                     |             iris.target[:90])              ||  print(clf.score(iris.data[90:],               |     return clf.score(iris.data[90:],       ||                  iris.target[90:]))            |                      iris.target[90:])     |+------------------------------------------------+--------------------------------------------+DocumentationThe documentation is hosted at _.InstallingYou can directly install it from the Python Package Index with pip:pip install sacred
Or if you want to do it manually you can checkout the current version from gitand install it yourself:| git clone https://github.com/IDSIA/sacred.git| cd sacred| python setup.py installYou might want to also install the  and the  packages. They areoptional dependencies but they offer some cool features:pip install numpy pymongo
TestsThe tests for sacred use the _ package.You can execute them by running  in the sacred directory like this:pytest
There is also a config file for _ so youcan automatically run the tests for various python versions like this:tox
Update pytest version+++++++++++++++++++++If you update or change the pytest version, the following files need to be changed:ContributingIf you find a bug, have a feature request or want to discuss something general you are welcome to open an. If you have a specific question related. We value documentationa lot. If you find something that should be included in the documentation pleasedocument it or let us know whats missing. If you are using Sacred in one of your projects and want to shareyour code with others, put your repo in the >_ list.Pull requests are highly welcome!FrontendsAt this point there are three frontends to the database entries created by sacred (that I'm aware of).They are developed externally as separate projects._+++++++++++++++++++++++++++++++++++++++++++++++++++++++++.. image:: docs/images/omniboard-table.png.. image:: docs/images/omniboard-metric-graphs.pngOmniboard is a web dashboard that helps in visualizing the experiments and metrics / logs collected by sacred.Omniboard is written with React, Node.js, Express and Bootstrap._+++++++++++++++++++++++++++++++++++++++++++++++++++++++++.. image:: docs/images/incense-artifact.png.. image:: docs/images/incense-metric.pngIncense is a Python library to retrieve runs stored in a MongoDB and interactively display metrics and artifactsin Jupyter notebooks._+++++++++++++++++++++++++++++++++++++++++++++++++++++++++.. image:: docs/images/sacredboard.pngSacredboard is a web-based dashboard interface to the sacred runs stored in aMongoDB._+++++++++++++++++++++++++++++++++++++++++++++++++++++++++.. image:: docs/images/neptune-compare.png.. image:: docs/images/neptune-collaboration.pngNeptune is a metadata store for MLOps, built for teams that run a lot of experiments.It gives you a single place to log, store, display, organize, compare, and query all your model-building metadata via API available for both Python and R programming languages:.. image:: docs/images/neptune-query-api.pngIn order to log your sacred experiments to Neptune, all you need to do is add an observer:.. code-block:: pythonfrom neptune.new.integrations.sacred import NeptuneObserver
ex.observers.append(NeptuneObserver(api_token='<YOUR_API_TOKEN>',
                                    project='<YOUR_WORKSPACE/YOUR_PROJECT>'))
For more info, check the _._+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++.. image:: docs/images/sacred_browser.pngSacredBrowser is a PyQt4 application to browse the MongoDB entries created bysacred experiments.Features include custom queries, sorting of the results,access to the stored source-code, and many more.No installation is required and it can connect to a localdatabase or over the network._+++++++++++++++++++++++++++++++++++++++++++++++Prophet is an early prototype of a webinterface to the MongoDB entries created bysacred experiments, that is discontinued.It requires you to run _ to access the database.Related Projects_++++++++++++++++++++++++++++++++++++++++++++++| Sumatra is a tool for managing and tracking projects based on numerical| simulation and/or analysis, with the aim of supporting reproducible research.| It can be thought of as an automated electronic lab notebook for| computational projects.Sumatra takes a different approach by providing commandline tools to initializea project and then run arbitrary code (not just python).It tracks information about all runs in a SQL database and even provides a nice browser tool.It integrates less tightly with the code to be run, which makes it easilyapplicable to non-python experiments.But that also means it requires more setup for each experiment andconfiguration needs to be done using files.Use this project if you need to run non-python experiments, or are ok with the additional setup/configuration overhead._++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++| FGLab is a machine learning dashboard, designed to make prototyping| experiments easier. Experiment details and results are sent to a database,| which allows analytics to be performed after their completion. The server| is FGLab, and the clients are FGMachines.Similar to Sumatra, FGLab is an external tool that can keep track of runs fromany program. Projects are configured via a JSON schema and the program needs toaccept these configurations via command-line options.FGLab also takes the role of a basic scheduler by distributing runs over severalmachines.LicenseThis project is released under the terms of the _.Citing Sacred_... |pypi| image:: https://img.shields.io/pypi/v/sacred.svg:target: https://pypi.python.org/pypi/sacred:alt: Current PyPi Version.. |py_versions| image:: https://img.shields.io/pypi/pyversions/sacred.svg:target: https://pypi.python.org/pypi/sacred:alt: Supported Python Versions.. |license| image:: https://img.shields.io/badge/license-MIT-blue.png:target: http://choosealicense.com/licenses/mit/:alt: MIT licensed.. |rtfd| image:: https://readthedocs.org/projects/sacred/badge/?version=latest&style=flat:target: https://sacred.readthedocs.io/en/stable/:alt: ReadTheDocs.. |doi| image:: https://zenodo.org/badge/doi/10.5281/zenodo.16386.svg:target: http://dx.doi.org/10.5281/zenodo.16386:alt: DOI for this release.. |build| image:: https://github.com/IDSIA/sacred/actions/workflows/test.yml/badge.svg:target: https://github.com/IDSIA/sacred/actions/workflows/test.yml/badge.svg:alt: Github Actions PyTest.. |coverage| image:: https://coveralls.io/repos/IDSIA/sacred/badge.svg:target: https://coveralls.io/r/IDSIA/sacred:alt: Coverage Report.. |code_quality| image:: https://scrutinizer-ci.com/g/IDSIA/sacred/badges/quality-score.png?b=master:target: https://scrutinizer-ci.com/g/IDSIA/sacred/:alt: Code Scrutinizer Quality.. |black| image:: https://img.shields.io/badge/code%20style-black-000000.svg:target: https://github.com/ambv/black:alt: Code style: black"
https://github.com/LonamiWebs/Telethon,"Pure Python 3 MTProto API Telegram client library, for bots too!","Telethon.. epigraph::⭐️ Thanks everyone who has starred the project, it means a lot!|logo| Telethon is an asyncio_ Python 3MTProto_ library to interact with Telegram_'s APIas a user or through a bot account (bot API alternative)... important::If you have code using Telethon before its 1.0 version, you must
read `Compatibility and Convenience`_ to learn how to migrate.
As with any third-party library for Telegram, be careful not to
break `Telegram's ToS`_ or `Telegram can ban the account`_.
What is this?Telegram is a popular messaging application. This library is meantto make it easy for you to write Python programs that can interactwith Telegram. Think of it as a wrapper that has already done theheavy job for you, so you can focus on developing an application.Installing.. code-block:: shpip3 install telethonCreating a client.. code-block:: pythonfrom telethon import TelegramClient, events, sync

# These example values won't work. You must get your own api_id and
# api_hash from https://my.telegram.org, under API Development.
api_id = 12345
api_hash = '0123456789abcdef0123456789abcdef'

client = TelegramClient('session_name', api_id, api_hash)
client.start()
Doing stuff.. code-block:: pythonprint(client.get_me().stringify())

client.send_message('username', 'Hello! Talking to you from Telethon')
client.send_file('username', '/home/myself/Pictures/holidays.jpg')

client.download_profile_photo('me')
messages = client.get_messages('username')
messages[0].download_media()

@client.on(events.NewMessage(pattern='(?i)hi|hello'))
async def handler(event):
    await event.respond('Hey!')
Next stepsDo you like how Telethon looks? Check out _ for a morein-depth explanation, with examples, troubleshooting issues, and moreuseful information... _asyncio: https://docs.python.org/3/library/asyncio.html.. _MTProto: https://core.telegram.org/mtproto.. _Telegram: https://telegram.org.. _Compatibility and Convenience: https://docs.telethon.dev/en/stable/misc/compatibility-and-convenience.html.. _Telegram's ToS: https://core.telegram.org/api/terms.. _Telegram can ban the account: https://docs.telethon.dev/en/stable/quick-references/faq.html#my-account-was-deleted-limited-when-using-the-library.. _Read The Docs: https://docs.telethon.dev.. |logo| image:: logo.svg:width: 24pt:height: 24pt"
https://github.com/fail2ban/fail2ban,Daemon to ban hosts that cause multiple authentication errors,"                     __      _ _ ___ _               
                    / _|__ _(_) |_  ) |__  __ _ _ _  
                   |  _/ _` | | |/ /| '_ \/ _` | ' \ 
                   |_| \__,_|_|_/___|_.__/\__,_|_||_|
                   v1.1.0.dev1            20??/??/??
Fail2Ban: ban hosts that cause multiple authentication errorsFail2Ban scans log files like  and bans IP addresses conductingtoo many failed login attempts. It does this by updating system firewall rulesto reject new connections from those IP addresses, for a configurable amountof time. Fail2Ban comes out-of-the-box ready to read many standard log files,such as those for sshd and Apache, and is easily configured to read any logfile of your choosing, for any error you wish.Though Fail2Ban is able to reduce the rate of incorrect authenticationattempts, it cannot eliminate the risk presented by weak authentication.Set up services to use only two factor, or public/private authenticationmechanisms if you really want to protect services. | Since v0.10 fail2ban supports the matching of IPv6 addresses.------|------This README is a quick introduction to Fail2Ban. More documentation, FAQ, and HOWTOsto be found on fail2ban(1) manpage, ,and the website: https://www.fail2ban.orgInstallation:Fail2Ban is likely already packaged for your Linux distribution and .If your distribution is not listed, you can install from GitHub:Required:Optional:To install:tar xvfj fail2ban-master.tar.bz2
cd fail2ban-master
sudo python setup.py install
Alternatively, you can clone the source from GitHub to a directory of Your choice, and do the install from there. Pick the correct branch, for example, master or 0.11git clone https://github.com/fail2ban/fail2ban.git
cd fail2ban
sudo python setup.py install 
This will install Fail2Ban into the python library directory. The executablescripts are placed into , and configuration in .Fail2Ban should be correctly installed now. Just type:fail2ban-client -h
to see if everything is alright. You should always use fail2ban-client andnever call fail2ban-server directly.You can verify that you have the correct version installed with fail2ban-client version
Please note that the system init/service script is not automatically installed.To enable fail2ban as an automatic service, simply copy the script for yourdistro from the  directory to . Example (on a Debian-basedsystem):cp files/debian-initd /etc/init.d/fail2ban
update-rc.d fail2ban defaults
service fail2ban start
Configuration:You can configure Fail2Ban using the files in . It is possible toconfigure the server using commands sent to it by . Theavailable commands are described in the fail2ban-client(1) manpage.  Also seefail2ban(1) and jail.conf(5)  manpages for further references.Code status:Contact:Bugs, feature requests, discussions?See You just appreciate this program:Send kudos to the original author ()or better to the since Fail2Ban is ""community-driven"" for years now.Thanks:See  file.License:Fail2Ban is free software; you can redistribute it and/or modify it under theterms of the GNU General Public License as published by the Free SoftwareFoundation; either version 2 of the License, or (at your option) any laterversion.Fail2Ban is distributed in the hope that it will be useful, but WITHOUT ANYWARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR APARTICULAR PURPOSE. See the GNU General Public License for more details.You should have received a copy of the GNU General Public License along withFail2Ban; if not, write to the Free Software Foundation, Inc., 51 FranklinStreet, Fifth Floor, Boston, MA 02110, USA"
https://github.com/mopidy/mopidy,Mopidy is an extensible music server written in Python,"Mopidy_ is an extensible music server written in Python.Mopidy plays music from local disk, Spotify, SoundCloud, Google Play Music, andmore. You edit the playlist from any phone, tablet, or computer using a varietyof MPD and web clients.Stream music from the cloudVanilla Mopidy only plays music from files and radio streams.  Through_, Mopidy can play music from cloud services like Spotify,SoundCloud, and Google Play Music.With Mopidy's extension support, backends for new music sources can be easilyadded.Mopidy is just a serverMopidy is a Python application that runs in a terminal or in the background onLinux computers or Macs that have network connectivity and audio output.Out of the box, Mopidy is an HTTP server. If you install the _extension, it becomes an MPD server too. Many additional frontends forcontrolling Mopidy are available as extensions.Pick your favorite clientYou and the people around you can all connect their favorite MPD or web clientto the Mopidy server to search for music and manage the playlist together.With a browser or MPD client, which is available for all popular operatingsystems, you can control the music from any phone, tablet, or computer.Mopidy on Raspberry PiThe _ is a popular device to run Mopidy on, either usingRaspbian, Ubuntu, or Arch Linux.Pimoroni recommends Mopidy for use with their _ audio gear forRaspberry Pi.Mopidy is also a significant building block in the _ integratedaudio jukebox system for Raspberry Pi.Mopidy is hackableMopidy's extension support and Python, JSON-RPC, and JavaScript APIs makeMopidy a perfect base for your projects.In one hack, a Raspberry Pi was embedded in an old cassette player. The buttonsand volume control are wired up with GPIO on the Raspberry Pi, and are used tocontrol playback through a custom Mopidy extension. The cassettes have NFC tagsused to select playlists from Spotify... _Mopidy: https://mopidy.com/.. _extensions: https://mopidy.com/ext/.. _Mopidy-MPD: https://mopidy.com/ext/mpd/.. _Raspberry Pi: https://www.raspberrypi.org/.. _Pirate Audio: https://shop.pimoroni.com/collections/pirate-audio.. _Pi Musicbox: https://www.pimusicbox.com/Getting startedTo get started with Mopidy, begin by reading the_.ContributingBegin by reading the_section of our documentation.If you are a developer, please also read_and/or_.We welcome all kinds of help with bug fixing, testing, documentation, and supporting other users.Project resources.. image:: https://img.shields.io/pypi/v/Mopidy.svg?style=flat:target: https://pypi.python.org/pypi/Mopidy/:alt: Latest PyPI version.. image:: https://img.shields.io/github/actions/workflow/status/mopidy/mopidy/ci.yml?branch=develop:target: https://github.com/mopidy/mopidy/actions/workflows/ci.yml:alt: CI build status.. image:: https://img.shields.io/readthedocs/mopidy.svg:target: https://docs.mopidy.com/:alt: Read the Docs build status.. image:: https://img.shields.io/codecov/c/github/mopidy/mopidy/develop.svg:target: https://codecov.io/gh/mopidy/mopidy:alt: Test coverage.. image:: https://img.shields.io/badge/chat-on%20zulip-brightgreen:target: https://mopidy.zulipchat.com/:alt: Chat on Zulip"
https://github.com/lmacken/pyrasite,Inject code into running Python processes,".. image:: http://pyrasite.com/logo.pngpyrasite.. image:: https://api.travis-ci.org/lmacken/pyrasite.png?branch=develop:target: http://travis-ci.org/lmacken/pyrasite.. image:: https://coveralls.io/repos/lmacken/pyrasite/badge.png?branch=develop:target: https://coveralls.io/r/lmacken/pyrasite?branch=develop.. image:: https://pypip.in/v/pyrasite/badge.png:target: https://crate.io/packages/pyrasite.. image:: https://pypip.in/d/pyrasite/badge.png:target: https://crate.io/packages/pyrasite.. split hereTools for injecting arbitrary code into running Python processes.:homepage: http://pyrasite.com:documentation: http://pyrasite.rtfd.org:download: http://pypi.python.org/pypi/pyrasite:source: http://github.com/lmacken/pyrasite:screenshots: http://readthedocs.org/docs/pyrasite/en/latest/GUI.html:mailing list: https://fedorahosted.org/mailman/listinfo/pyrasite:jenkins: http://ci.csh.rit.edu/view/Pyrasite:irc: #pyrasite on FreenodeRequirements
 * `gdb <https://www.gnu.org/s/gdb>`_ (version 7.3+ (or RHEL5+))
 
On OS X you will need to have a codesigned gdb - see https://sourceware.org/gdb/wiki/BuildingOnDarwin
if you get errors while running with --verbose which mention codesigning.

Compatibility
Pyrasite works with Python 2.4 and newer. Injection works between versionsas well, so you can run Pyrasite under Python 3 and inject into 2, andvice versa.pyrasite-gui
The graphical interface can be found here: https://github.com/lmacken/pyrasite-gui

.. image:: http://lewk.org/img/pyrasite/pyrasite-info-thumb.png

Authors
~~~~~~~

Created by `Luke Macken <http://twitter.com/lmacken>`_ with the help of
`David Malcolm <http://dmalcolm.livejournal.com>`_ and many other
`contributors <https://github.com/lmacken/pyrasite/contributors>`_.
Logo by `Adam Saunders <https://fedorahosted.org/design-team/ticket/214>`_.

Licenses
~~~~~~~~

Code
^^^^

.. image:: https://www.gnu.org/graphics/gplv3-127x51.png
   :target: https://www.gnu.org/licenses/gpl.txt

Logo
^^^^

.. image:: https://creativecommons.org/images/deed/nolaw.png
   :target: https://creativecommons.org/publicdomain/zero/1.0/
"
https://github.com/richzhang/colorization,"Automatic colorization using deep neural networks. ""Colorful Image Colorization."" In ECCV, 2016.","Colorful Image Colorization  , , . In .+ automatic colorization functionality for Real-Time User-Guided Image Colorization with Learned Deep Priors, SIGGRAPH 2017![Sept20 Update] Since it has been 3-4 years, I converted this repo to support minimal test-time usage in PyTorch. I also added our SIGGRAPH 2017 (it's an interactive method but can also do automatic). See the  for the original release.Clone the repository; install dependenciesgit clone https://github.com/richzhang/colorization.git
pip install requirements.txt
Colorize! This script will colorize an image. The results should match the images in the  folder.python demo_release.py -i imgs/ansel_adams3.jpg
Model loading in Python The following loads pretrained colorizers. See  for some details on how to run the model. There are some pre and post-processing steps: convert to Lab space, resize to 256x256, colorize, and concatenate to the original full resolution, and convert to RGB.import colorizers
colorizer_eccv16 = colorizers.eccv16().eval()
colorizer_siggraph17 = colorizers.siggraph17().eval()
Original implementation (Caffe branch)The original implementation contained train and testing, our network and AlexNet (for representation learning tests), as well as representation learning tests. It is in Caffe and is no longer supported. Please see the  branch for it.CitationIf you find these models useful for your resesarch, please cite with these bibtexs.@inproceedings{zhang2016colorful,
  title={Colorful Image Colorization},
  author={Zhang, Richard and Isola, Phillip and Efros, Alexei A},
  booktitle={ECCV},
  year={2016}
}

@article{zhang2017real,
  title={Real-Time User-Guided Image Colorization with Learned Deep Priors},
  author={Zhang, Richard and Zhu, Jun-Yan and Isola, Phillip and Geng, Xinyang and Lin, Angela S and Yu, Tianhe and Efros, Alexei A},
  journal={ACM Transactions on Graphics (TOG)},
  volume={9},
  number={4},
  year={2017},
  publisher={ACM}
}
MiscContact Richard Zhang at rich.zhang at eecs.berkeley.edu for any questions or comments."
https://github.com/ajenti/ajenti,Ajenti Core and stock plugins,"Ajenti is a Linux & BSD modular server admin panel. Ajenti 2 provides a new interface and a better architecture, developed with  and .Feature highlightsScreenshotsSee https://ajenti.org for more informationContributors ✨Thanks goes to these wonderful people ():This project follows the  specification. Contributions of any kind welcome!ContributingHelpers are welcome ! There are many ways to help in Ajenti Project : RoadmapActually big changes are planned and under development. A global roadmap is : "
https://github.com/tensorflow/models,Models and examples built with TensorFlow,"Welcome to the Model Garden for TensorFlowThe TensorFlow Model Garden is a repository with a number of differentimplementations of state-of-the-art (SOTA) models and modeling solutions forTensorFlow users. We aim to demonstrate the best practices for modeling so thatTensorFlow users can take full advantage of TensorFlow for their research andproduct development.To improve the transparency and reproducibility of our models, training logs on are also provided for models to theextent possible though not all models are suitable.| Directory | Description ||-----------|-------------||  | • A collection of example implementations for SOTA models using the latest TensorFlow 2's high-level APIs• Officially maintained, supported, and kept up to date with the latest TensorFlow 2 APIs by TensorFlow• Reasonably optimized for fast performance while still being easy to read For more details on the capabilities, check the guide on the ||  | • A collection of research model implementations in TensorFlow 1 or 2 by researchers• Maintained and supported by researchers ||  | • A curated list of the GitHub repositories with machine learning models and implementations powered by TensorFlow 2 ||  | • A flexible and lightweight library that users can easily use or fork when writing customized training loop code in TensorFlow 2.x. It seamlessly integrates with  and supports running on different device types (CPU, GPU, and TPU). |InstallationTo install the current release of tensorflow-models, please follow any one of the methods described below.Method 1: Install the TensorFlow Model Garden pip packagetf-models-official is the stable Model Garden package. Please check out the  to see what are available modules.pip3 will install all models and dependencies automatically.pip3 install tf-models-official
Please check out our examples:Note that tf-models-official may not include the latest changes in the master branch of thisgithub repo. To include latest changes, you may install tf-models-nightly,which is the nightly Model Garden package created daily automatically.pip3 install tf-models-nightly
Method 2: Clone the sourcegit clone https://github.com/tensorflow/models.git
export PYTHONPATH=$PYTHONPATH:/path/to/models
If you are using in a Windows environment, you may need to use the following command with PowerShell:$env:PYTHONPATH += "":\path\to\models""
If you are using a Colab notebook, please set the Python path with os.environ.import os
os.environ['PYTHONPATH'] += "":/path/to/models""
pip3 install --user -r models/official/requirements.txt
Finally, if you are using nlp packages, please also installtensorflow-text-nightly:pip3 install tensorflow-text-nightly
AnnouncementsPlease check  for recent announcements.ContributionsIf you want to contribute, please review the .LicenseCiting TensorFlow Model GardenIf you use TensorFlow Model Garden in your research, please cite this repository.@misc{tensorflowmodelgarden2020,
  author = {Hongkun Yu, Chen Chen, Xianzhi Du, Yeqing Li, Abdullah Rashwan, Le Hou, Pengchong Jin, Fan Yang,
            Frederick Liu, Jaeyoun Kim, and Jing Li},
  title = {{TensorFlow Model Garden}},
  howpublished = {\url{https://github.com/tensorflow/models}},
  year = {2020}
}
"
https://github.com/swisskyrepo/PayloadsAllTheThings,A list of useful payloads and bypass for Web Application Security and Pentest/CTF,"Payloads All The ThingsA list of useful payloads and bypasses for Web Application Security.Feel free to improve with your payloads and techniques !I :heart: pull requests :)You can also contribute with a :beers: IRL, or using the sponsor button An alternative display version is available at .📖 DocumentationEvery section contains the following files, you can use the  folder to create a new chapter:You might also like the  folder :You want more ? Check the  and  selections.👨‍💻 ContributionsBe sure to read Thanks again for your contribution! :heart:🧙‍♂️ SponsorsThis project is proudly sponsored by these companies. "
https://github.com/google-research/text-to-text-transfer-transformer,"Code for the paper ""Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer""","T5: Text-To-Text Transfer TransformerAs of July 2022, we recommend using T5X: is the new and improved implementation of T5 (and more) in JAX and Flax.T5 on Tensorflow with MeshTF is no longer actively developed. If you are newto T5, we recommend starting with .The  library serves primarily as code for reproducing the experiments in . In the paper, we demonstrate how to achieve state-of-the-art results on multiple NLP tasks using a text-to-text transformer pre-trained on a large text corpus.The bulk of the code in this repository is used for loading, preprocessing, mixing, and evaluating datasets.It also provides a way to fine-tune the  released alongside the publication.The  library can be used for future model development by providing useful modules for training and fine-tuning (potentially huge) models on mixtures of text-to-text tasks.Table of ContentsLibraryt5.data is a package for defining  objects that provide s.Each  is made up of:Additionally, you may optionally provide:The data source can be an arbitrary function that provides a , but we also provide simpler wrappers for datasets available in  (a ) or stored as text files with one example per line (a ).The text preprocessor converts the examples in the source dataset into the appropriate format for a text-to-text model with fields for  and .  For example, the predefined  preprocessor converts inputs in the form{'de': 'Das ist gut.', 'en': 'That is good.'}
to the form{'inputs': 'translate German to English: Das ist gut.', 'targets': 'That is good.'}
In addition to text preprocessing, you can also use one or more token preprocessors to modify the inputs post-tokenization. We implemented our unsupervised pre-training objectives using these token preprocessors.We provide many predefined preprocessors in , but you may also define your own.The SentencePiece model is used to tokenize the input strings and decode the output tokens. You can create your own model with the  library, or use our default one at . If you create your own, you must use the flags  with  to be compatible with our model code.The metric function returns a score given the target and prediction from the model. You may also define a postprocess function to convert the target and prediction text to another format before calling the metric. We provide some predefined metrics in .Finally,  contains a  class that can be instantiated to combine multiple  datasets for multi-task training using various functions for specifying the mixture rates.t5.evaluation contains two core components:t5.models contains shims for connecting T5  and  to a model implementation for training, evaluation, and inference.Currently there are two shims available: One for the  that we used in our paper and another for the .The Hugging Face API is currently experimental and subject to change, but provides a simple and easy way to load, fine-tune, and evaluate our pre-trained models using PyTorch on a single GPU.If you want to use our largest models on TPUs and/or reproduce the results in our paper, you should use the  API and the  binary.If you are interested fine-tuning our models on a GPU in PyTorch, you should try the  API.Since the HfPyTorchModel is experimental, the remainder of this README assumes usage of the MtfModel and its associated binary.A usage example of HfPyTorchModel is available .UsageThe easiest way to try out T5 is with a free TPU in our .Below we provide examples for how to pre-train, fine-tune, evaluate, and decode from a model from the command-line with our codebase. You can use these instructions to reproduce our results, fine-tune one of our released checkpoints with your own data and/or hyperparameters, or pre-train a model from scratch.Dataset PreparationYou may either use a new or pre-existing , or you may load examples from a preprocessed TSV file.Using a Depending on your data source (see ), you will need to prepare your data appropriately.If using a vanilla task, just make sure any file(s) loaded by your  are accessible to the TPU (i.e., are in a GCS bucket), and you should be good to go!Most of our predefined s use  as their data source. When you run our training binary (see instructions ) with a , the dataset will automatically be downloaded and prepared on its first use. After preparation is complete, the dataset is cached to your local storage to avoid this overhead in future runs.  If working in the cloud, we recommend you set the  flag to point to a persistent storage location, such as a . This is a requirement when training on TPU.C4The  dataset we created for unsupervised pre-training is available in TensorFlow Datasets, but it requires a significant amount of bandwidth for downloading the raw  scrapes (~7 TB) and compute for its preparation (~335 CPU-days). We suggest you take advantage of the  support in TFDS, which enables distributed preprocessing of the dataset and can be run on . With 500 workers, the job should complete in ~16 hours.After defining  and  appropriately, you can build the dataset in DataFlow from GCP using the following commands:pip install tfds-nightly[c4]
echo 'tfds-nightly[c4]' > /tmp/beam_requirements.txt
python -m tensorflow_datasets.scripts.download_and_prepare \
  --datasets=c4/en \
  --data_dir=gs://$MY_BUCKET/tensorflow_datasets \
  --beam_pipeline_options=""project=$MY_PROJECT,job_name=c4,staging_location=gs://$MY_BUCKET/binaries,temp_location=gs://$MY_BUCKET/temp,runner=DataflowRunner,requirements_file=/tmp/beam_requirements.txt,experiments=shuffle_mode=service,region=$MY_REGION""
Read more in the .A  is useful when your data source is a text file (or files) with one example per line. You can then use a text preprocessor to convert each line into a dictionary of inputs and targets.Make sure your files are accessible to the TPU (i.e., are in a GCS bucket), and you should be good to go!Using a TSV File DirectlyInstead of defining a new , you may use a TSV file (or files) directly as your dataset where each line is formatted as .However, there are a couple of caveats:If you need any of these features, you must define a new , , or .Similar to the above cases, your TSV file(s) must be accessible to the TPU (i.e., are in a GCS bucket).InstallationTo install the T5 package, simply run:pip install t5[gcp]
Setting up TPUs on GCPYou will first need to launch a Virtual Machine (VM) on Google Cloud. Details about launching the VM can be found at the .In order to run training or eval on Cloud TPUs, you must set up the following variables based on your project, zone and GCS bucket appropriately. Please refer to the  guide for more details.export PROJECT=your_project_name
export ZONE=your_project_zone
export BUCKET=gs://yourbucket/
export TPU_NAME=t5-tpu
export TPU_SIZE=v3-8
export DATA_DIR=""${BUCKET}/your_data_dir""
export MODEL_DIR=""${BUCKET}/your_model_dir""
Please use the following command to create a TPU device in the Cloud VM.ctpu up --name=$TPU_NAME --project=$PROJECT --zone=$ZONE --tpu-size=$TPU_SIZE \
        --tpu-only --noconf
TrainingIn the command below, we train a model on the  MRPC task from scratch. You can change the  gin parameter to use any of the tasks or mixtures provided in our package.t5_mesh_transformer  \
  --tpu=""${TPU_NAME}"" \
  --gcp_project=""${PROJECT}"" \
  --tpu_zone=""${ZONE}"" \
  --model_dir=""${MODEL_DIR}"" \
  --t5_tfds_data_dir=""${DATA_DIR}"" \
  --gin_file=""dataset.gin"" \
  --gin_file=""models/bi_v1.gin"" \
  --gin_param=""utils.tpu_mesh_shape.model_parallelism = 1"" \
  --gin_param=""utils.tpu_mesh_shape.tpu_topology = '${TPU_SIZE}'"" \
  --gin_param=""MIXTURE_NAME = 'glue_mrpc_v002'""
The full list of tasks and mixtures can be obtained by running:python -c ""import t5; print(t5.data.MixtureRegistry.names())""
You may also define additional tasks and mixtures in a new file and import it using the  flag.Alternatively, you could train with a TSV file where each line is formatted as  (see ).Fine-tuningIn order to fine-tune one of our , you need to pass the operative config of the pre-trained model to the training script. The operative config should be passed in as a  flag. It specifies the model architecture and other hyperparameters. In addition, you need to specify the mixture to fine-tune on. For example, to fine-tune the T5-small model on the  mixture, please run:t5_mesh_transformer  \
  --tpu=""${TPU_NAME}"" \
  --gcp_project=""${PROJECT}"" \
  --tpu_zone=""${ZONE}"" \
  --model_dir=""${MODEL_DIR}"" \
  --t5_tfds_data_dir=""${DATA_DIR}"" \
  --gin_file=""dataset.gin"" \
  --gin_param=""utils.tpu_mesh_shape.model_parallelism = 1"" \
  --gin_param=""utils.tpu_mesh_shape.tpu_topology = '${TPU_SIZE}'"" \
  --gin_param=""MIXTURE_NAME = 'glue_mrpc_v002'"" \
  --gin_file=""gs://t5-data/pretrained_models/small/operative_config.gin""
The correct pre-trained checkpoint path is included in the operative config.You may also define additional tasks and mixtures in a new file and import it using the  flag.Alternatively, you could fine-tune with a TSV file where each line is formatted as  (see ). For example, you could try one of the paired translation datasets from WMT '19  training set(e.g., ). When using a TSV file, you would replace the  flag with:--gin_param=""utils.run.train_dataset_fn = @t5.models.mesh_transformer.tsv_dataset_fn""
--gin_param=""tsv_dataset_fn.filename = 'gs:/path/to/tsv'""
To fine-tune with the same hyperparameters we used in the  (using a constant learning rate of 0.001), you can pass in this gin file which is included in the T5 package:--gin_file=""learning_rate_schedules/constant_0_001.gin""
The operative config for the pre-trained models are set so that there is effectively no limit on the number of train steps. If you'd like to train for a specific number of steps, you'll need to pass that in. Since the pre-trained model has already been trained for 1,000,000 steps, you should specify the total number of steps after pre-training and fine-tuning. For example, if you want to fine-tune for an additional 10,000 steps, you should pass--gin_param=""run.train_steps = 1010000""
You can also use a different batch size for fine-tuning. We set the batch size according to the total number of tokens in a batch. By default, a batch uses a sequence length of 512. To set the number of tokens in a batch, you should set--gin_param = ""tokens_per_batch=1048576""
EvalIn order to evaluate a model in the T5 framework, you need to use the  file, specify the model directory, decoding method, and which checkpoint step(s) to evaluate. So, to evaluate on the GLUE MRPC task using beam search on all checkpoints, use the following command:t5_mesh_transformer \
  --tpu=""${TPU_NAME}"" \
  --gcp_project=""${PROJECT}"" \
  --tpu_zone=""${ZONE}"" \
  --model_dir=""${MODEL_DIR}"" \
  --gin_file=""${MODEL_DIR}/operative_config.gin"" \
  --t5_tfds_data_dir=${DATA_DIR} \
  --gin_file=""eval.gin"" \
  --gin_file=""beam_search.gin"" \
  --gin_param=""run.dataset_split = 'validation'"" \
  --gin_param=""utils.tpu_mesh_shape.tpu_topology = '${TPU_SIZE}'"" \
  --gin_param=""MIXTURE_NAME = 'glue_mrpc_v002'"" \
  --gin_param=""eval_checkpoint_step = 'all'""
To evaluate a specific checkpoint, simply set the  parameter to appropriate checkpoint.--gin_param=""eval_checkpoint_step = 100000""
You can also use  or  instead of  in the command above.DecodeIn order to produce predictions from a model in the T5 framework, you need to specify the model directory, decoding method, and which checkpoint step(s) to use for decoding. Assuming you have a text file of input sequences stored at , an example command would be:t5_mesh_transformer \
  --tpu=""${TPU_NAME}"" \
  --gcp_project=""${PROJECT}"" \
  --tpu_zone=""${ZONE}"" \
  --model_dir=""${MODEL_DIR}"" \
  --gin_file=""${MODEL_DIR}/operative_config.gin"" \
  --gin_file=""infer.gin"" \
  --gin_file=""sample_decode.gin"" \
  --gin_param=""input_filename = '/path/to/inputs.txt'""\
  --gin_param=""output_filename = '/tmp/outputs.txt'""\
  --gin_param=""utils.tpu_mesh_shape.tpu_topology = '${TPU_SIZE}'""\
  --gin_param=""infer_checkpoint_step = 'all'""
To predict with a specific checkpoint, simply set the  parameter to appropriate checkpoint.--gin_param=""infer_checkpoint_step = 100000""
You can also use  or  instead of  in the command above.ExportYou may also want to export a , which is useful for serving your trained model, (e.g., when deploying with  or in a  image).t5_mesh_transformer \
  --gcp_project=""${PROJECT}"" \
  --tpu_zone=""${ZONE}"" \
  --model_dir=""${MODEL_DIR}"" \
  --use_model_api \
  --mode=""export_predict"" \
  --export_dir=""/path/to/export/dir""
The command above exports the latest checkpoint in the model directory. To export a particular checkpoint, add the following flags:  --checkpoint_mode=""specific"" \
  --checkpoint_steps=1000000
The  demonstrates exporting a  and packaging it in a  image for serving.GPU UsageIf you would like to use GPU instead of TPUs, you can modify the above commands by removing TPU-specific flags (, , ) and setting the gin params for  and  based on your desired setup.For example, if your machine has access to 6 GPUs and you'd like to do 3-way model parallelism and 2-way data parallelism, the fine-tuning command above would become:t5_mesh_transformer  \
  --model_dir=""${MODEL_DIR}"" \
  --t5_tfds_data_dir=""${DATA_DIR}"" \
  --gin_file=""dataset.gin"" \
  --gin_param=""utils.run.mesh_shape = 'model:3,batch:2'"" \
  --gin_param=""utils.run.mesh_devices = ['gpu:0','gpu:1','gpu:2','gpu:3','gpu:4','gpu:5']"" \
  --gin_param=""MIXTURE_NAME = 'glue_mrpc_v002'"" \
  --gin_file=""gs://t5-data/pretrained_models/small/operative_config.gin""
With a single GPU, the command is:t5_mesh_transformer  \
  --model_dir=""${MODEL_DIR}"" \
  --t5_tfds_data_dir=""${DATA_DIR}"" \
  --gin_file=""dataset.gin"" \
  --gin_param=""utils.run.mesh_shape = 'model:1,batch:1'"" \
  --gin_param=""utils.run.mesh_devices = ['gpu:0']"" \
  --gin_param=""MIXTURE_NAME = 'glue_mrpc_v002'"" \
  --gin_file=""gs://t5-data/pretrained_models/small/operative_config.gin""
Reproducing our experimentsWe provide operative configs for all of the experiments in the  in .The  folder has different subdirectories corresponding to the different sections in our paper.For example,  contains the experiments from Section 3.3 (""Unsupervised objectives"").Each subdirectory of the  folder contains operative configs for some particular experiment (where loosely speaking an ""experiment"" is one of the rows in one of the tables in our paper).Let's say you want to reproduce the results for the ""Prefix language modeling"" objective (the first row in Table 4).The operative configs for that experiment live in .In the base directory, there is an operative config for pre-training the model ().Then, there are subdirectories for each of the downstream fine-tuning mixtures we consider, each of which has its own operative config (for example, ).To run this experiment, first pre-train a model with the pre-training operative config:export PRETRAIN_MODEL_DIR=""${BUCKET}/obj-prefix_lm""
t5_mesh_transformer  \
  --tpu=""${TPU_NAME}"" \
  --gcp_project=""${PROJECT}"" \
  --tpu_zone=""${ZONE}"" \
  --model_dir=""${PRETRAIN_MODEL_DIR}"" \
  --gin_file=""gs://t5-data/experiments/objectives/obj-prefix_lm/operative_config.gin"" \
  --gin_param=""utils.tpu_mesh_shape.model_parallelism = 1"" \
  --gin_param=""utils.tpu_mesh_shape.tpu_topology = '${TPU_SIZE}'""
Then, you can fine-tune the pre-trained model on CNN/Daily Mail like so:export FINETUNE_MODEL_DIR=""${BUCKET}/obj-prefix_lm/cnn_dailymail_v002""
t5_mesh_transformer  \
  --tpu=""${TPU_NAME}"" \
  --gcp_project=""${PROJECT}"" \
  --tpu_zone=""${ZONE}"" \
  --model_dir=""${FINETUNE_MODEL_DIR}"" \
  --gin_file=""gs://t5-data/experiments/objectives/obj-prefix_lm/cnn_dailymail_v002/operative_config.gin"" \
  --gin_param=""init_checkpoint = '${PRETRAIN_MODEL_DIR}/model.ckpt-524288'"" \
  --gin_param=""utils.tpu_mesh_shape.model_parallelism = 1"" \
  --gin_param=""utils.tpu_mesh_shape.tpu_topology = '${TPU_SIZE}'""
Useful OptionsSome training variants need multiple flags to be set at the same time. For eachof the below variants, add the group of flags to.Deterministic training  --train_gin_param=""mesh_train_dataset_fn.seed=${SEED}"" \
  --train_gin_param=""utils.run.skip_seen_data = True"" \
Language model  --objective=""lm"" \
  --train_gin_param=""utils.run.model_type = \""lm\"""" \
Released Model CheckpointsWe have released the following checkpoints for pre-trained models described in our :See  for a list of additional experimental pre-trained model checkpoints.How to CiteIf you extend or use this work, please cite the  where it was introduced:@article{2020t5,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1-67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}
"
https://github.com/junyanz/iGAN,Interactive Image Generation via Generative Adversarial Networks,"iGAN: Interactive Image Generation via Generative Adversarial Networks |   |    Recent projects:: Torch implementation for learning a mapping from input images to output images.: Torch implementation for learning an image-to-image translation (i.e., pix2pix) without input-output pairs.: PyTorch implementation for both unpaired and paired image-to-image translation.OverviewiGAN (aka. interactive GAN) is the author's implementation of interactive image generation interface described in:""Generative Visual Manipulation on the Natural Image Manifold"", , , In European Conference on Computer Vision (ECCV) 2016Given a few user strokes, our system could produce photo-realistic samples that best satisfy the user edits in real-time. Our system is based on deep generative models such as Generative Adversarial Networks () and . The system serves the following two purposes:Please cite our paper if you find this code useful in your research. (Contact: Jun-Yan Zhu, junyanz at mit dot edu)Getting startedgit clone https://github.com/junyanz/iGAN
cd iGAN
bash ./models/scripts/download_dcgan_model.sh outdoor_64
THEANO_FLAGS='device=gpu0, floatX=float32, nvcc.fastmath=True' python iGAN_main.py --model_name outdoor_64
RequirementsThe code is written in Python2 and requires the following 3rd party libraries:sudo apt-get install python-opencv
sudo pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git
sudo apt-get install python-qt4
sudo pip install qdarkstyle
sudo pip install dominate
Python3For  users, you need to replace  with :sudo apt-get install python3-pyqt4
Interface:See  at 2:18s for the interactive image generation demos.  LayoutUser interactionModel Zoo:Download the Theano DCGAN model (e.g., outdoor_64). Before using our system, please check out the random real images vs. DCGAN generated samples to see which kind of images that a model can produce.bash ./models/scripts/download_dcgan_model.sh outdoor_64
We provide a simple script to generate samples from a pre-trained DCGAN model. You can run this script to test if Theano, CUDA, cuDNN are configured properly before running our interface.THEANO_FLAGS='device=gpu0, floatX=float32, nvcc.fastmath=True' python generate_samples.py --model_name outdoor_64 --output_image outdoor_64_dcgan.png
Command line arguments:Type  for a complete list of the arguments. Here we discuss some important arguments:THEANO_FLAGS='device=gpu0, floatX=float32, nvcc.fastmath=True' python iGAN_main.py --model_name hed_shoes_64 --shadow --average
Dataset and TrainingSee more details Projecting an Image onto Latent SpaceWe provide a script to project an image into latent space (i.e., ):bash models/scripts/download_alexnet.sh conv4
THEANO_FLAGS='device=gpu0, floatX=float32, nvcc.fastmath=True' python iGAN_predict.py --model_name shoes_64 --input_image ./pics/shoes_test.png --solver cnn_opt
Script without UIWe also provide a standalone script that should work without UI. Given user constraints (i.e., a color map, a color mask, and an edge map), the script generates multiple images that mostly satisfy the user constraints. See  for more details.THEANO_FLAGS='device=gpu0, floatX=float32, nvcc.fastmath=True' python iGAN_script.py --model_name outdoor_64
Citation@inproceedings{zhu2016generative,
  title={Generative Visual Manipulation on the Natural Image Manifold},
  author={Zhu, Jun-Yan and Kr{\""a}henb{\""u}hl, Philipp and Shechtman, Eli and Efros, Alexei A.},
  booktitle={Proceedings of European Conference on Computer Vision (ECCV)},
  year={2016}
}
Cat Paper CollectionIf you love cats, and love reading cool graphics, vision, and learning papers, please check out our Cat Paper Collection: Acknowledgement"
https://github.com/victoresque/pytorch-template,PyTorch deep learning projects made easy.,"PyTorch Template ProjectPyTorch deep learning project made easy.RequirementsFeaturesFolder Structurepytorch-template/
│
├── train.py - main script to start training
├── test.py - evaluation of trained model
│
├── config.json - holds configuration for training
├── parse_config.py - class to handle config file and cli options
│
├── new_project.py - initialize new project with template files
│
├── base/ - abstract base classes
│   ├── base_data_loader.py
│   ├── base_model.py
│   └── base_trainer.py
│
├── data_loader/ - anything about data loading goes here
│   └── data_loaders.py
│
├── data/ - default directory for storing input data
│
├── model/ - models, losses, and metrics
│   ├── model.py
│   ├── metric.py
│   └── loss.py
│
├── saved/
│   ├── models/ - trained models are saved here
│   └── log/ - default logdir for tensorboard and logging output
│
├── trainer/ - trainers
│   └── trainer.py
│
├── logger/ - module for tensorboard visualization and logging
│   ├── visualization.py
│   ├── logger.py
│   └── logger_config.json
│  
└── utils/ - small utility functions
    ├── util.py
    └── ...
UsageThe code in this repo is an MNIST example of the template.Try  to run code.Config file formatConfig files are in  format:{
  ""name"": ""Mnist_LeNet"",        // training session name
  ""n_gpu"": 1,                   // number of GPUs to use for training.
  
  ""arch"": {
    ""type"": ""MnistModel"",       // name of model architecture to train
    ""args"": {

    }                
  },
  ""data_loader"": {
    ""type"": ""MnistDataLoader"",         // selecting data loader
    ""args"":{
      ""data_dir"": ""data/"",             // dataset path
      ""batch_size"": 64,                // batch size
      ""shuffle"": true,                 // shuffle training data before splitting
      ""validation_split"": 0.1          // size of validation dataset. float(portion) or int(number of samples)
      ""num_workers"": 2,                // number of cpu processes to be used for data loading
    }
  },
  ""optimizer"": {
    ""type"": ""Adam"",
    ""args"":{
      ""lr"": 0.001,                     // learning rate
      ""weight_decay"": 0,               // (optional) weight decay
      ""amsgrad"": true
    }
  },
  ""loss"": ""nll_loss"",                  // loss
  ""metrics"": [
    ""accuracy"", ""top_k_acc""            // list of metrics to evaluate
  ],                         
  ""lr_scheduler"": {
    ""type"": ""StepLR"",                  // learning rate scheduler
    ""args"":{
      ""step_size"": 50,          
      ""gamma"": 0.1
    }
  },
  ""trainer"": {
    ""epochs"": 100,                     // number of training epochs
    ""save_dir"": ""saved/"",              // checkpoints are saved in save_dir/models/name
    ""save_freq"": 1,                    // save checkpoints every save_freq epochs
    ""verbosity"": 2,                    // 0: quiet, 1: per epoch, 2: full
  
    ""monitor"": ""min val_loss""          // mode and metric for model performance monitoring. set 'off' to disable.
    ""early_stop"": 10	                 // number of epochs to wait before early stop. set 0 to disable.
  
    ""tensorboard"": true,               // enable tensorboard visualization
  }
}
Add addional configurations if you need.Using config filesModify the configurations in  config files, then run:python train.py --config config.json
Resuming from checkpointsYou can resume from a previously saved checkpoint by:python train.py --resume path/to/checkpoint
Using Multiple GPUYou can enable multi-GPU training by setting  argument of the config file to larger number.If configured to use smaller number of gpu than available, first n devices will be used by default.Specify indices of available GPUs by cuda environmental variable.python train.py --device 2,3 -c config.json
This is equivalent toCUDA_VISIBLE_DEVICES=2,3 python train.py -c config.py
CustomizationProject initializationUse the  script to make your new project directory with template files. then a new project folder named 'NewProject' will be made.This script will filter out unneccessary files like cache, git files or readme file. Custom CLI optionsChanging values of config file is a clean, safe and easy way of tuning hyperparameters. However, sometimesit is better to have command line options if some values need to be changed too often or quickly.This template uses the configurations stored in the json file by default, but by registering custom options as followsyou can change some of them using CLI flags.# simple class-like object having 3 attributes, `flags`, `type`, `target`.
CustomArgs = collections.namedtuple('CustomArgs', 'flags type target')
options = [
    CustomArgs(['--lr', '--learning_rate'], type=float, target=('optimizer', 'args', 'lr')),
    CustomArgs(['--bs', '--batch_size'], type=int, target=('data_loader', 'args', 'batch_size'))
    # options added here can be modified by command line flags.
]
 argument should be sequence of keys, which are used to access that option in the config dict. In this example, for the learning rate option is  because  points to the learning rate. runs training with options given in  except for the which is increased to 256 by command line options.Data LoaderTrainerModelLossCustom loss functions can be implemented in 'model/loss.py'. Use them by changing the name given in ""loss"" in config file, to corresponding name.MetricsMetric functions are located in 'model/metric.py'.You can monitor multiple metrics by providing a list in the configuration file, e.g.:""metrics"": [""accuracy"", ""top_k_acc""],
Additional loggingIf you have additional information to be logged, in  of your trainer class, merge them with  as shown below before returning:additional_log = {""gradient_norm"": g, ""sensitivity"": s}
log.update(additional_log)
return log
TestingYou can test trained model by running  passing path to the trained checkpoint by  argument.Validation dataTo split validation data from a data loader, call , then it will return a data loader for validation of size specified in your config file.The  can be a ratio of validation set per total data(0.0 <= float < 1.0), or the number of samples (0 <= int < ).Note: the  method will modify the original data loaderNote:  will return  if  is set to CheckpointsYou can specify the name of the training session in config files:""name"": ""MNIST_LeNet"",
The checkpoints will be saved in , with timestamp in mmdd_HHMMSS format.A copy of config file will be saved in the same folder.Note: checkpoints contain:{
  'arch': arch,
  'epoch': epoch,
  'state_dict': self.model.state_dict(),
  'optimizer': self.optimizer.state_dict(),
  'monitor_best': self.mnt_best,
  'config': self.config
}
Tensorboard VisualizationThis template supports Tensorboard visualization by using either   or .By default, values of loss and metrics specified in config file, input images, and histogram of model parameters will be logged.If you need more visualizations, use , , etc in the  method. methods in this template are basically wrappers for those of  and  modules. Note: You don't have to specify current steps, since  class defined at  will track current steps.ContributionFeel free to contribute any kind of function or enhancement, here the coding style follows PEP8Code should pass the  check before committing.TODOsLicenseThis project is licensed under the MIT License. See  LICENSE for more detailsAcknowledgementsThis project is inspired by the project  by "
https://github.com/yuanxiaosc/DeepNude-an-Image-to-Image-technology,"DeepNude's algorithm and general image generation theory and practice research, including pix2pix, CycleGAN, UGATIT, DCGAN, SinGAN, ALAE, mGANprior, StarGAN-v2 and VAE models (TensorFlow2 implementation). DeepNude的算法以及通用生成对抗网络（GAN,Generative Adversarial Network）图像生成的理论与实践研究。","DeepNude-an-Image-to-Image-technology | This repository contains the pix2pixHD algorithms(proposed by NVIDIA) of , and more importantly, the general image generation theory and practice behind DeepNude.This resource includes the TensorFlow2 (Pytorch | PaddlePaddle) implementation of image generation models such as , , UGATIT, , SinGAN, , ALAE, mGANprior and StarGAN-v2, which can be used to systematically learn to Generating Adversarial Network (GAN).Content of this resourceWhat is DeepNude?DeepNude uses a slightly modified version of the  GAN architecture, quoted from deepnude_official. pix2pixHD is a general-purpose Image2Image technology proposed by NVIDIA. Obviously, DeepNude is the wrong application of artificial intelligence technology, but it uses Image2Image technology for researchers and developers working in other fields such as fashion, film and visual effects.Fake Image Generation DemoThis section provides a fake image generation demo that you can use as you wish. They are fake images generated by StyleGAN without any copyright issues. Note: Each time you refresh the page, a new fake image will be generated, pay attention to save!Image-to-Image DemoThis section provides a demo of Image-to-Image Demo: Black and white stick figures to colorful faces, cats, shoes, handbags. DeepNude software mainly uses Image-to-Image technology, which theoretically converts the images you enter into any image you want. You can experience Image-to-Image technology in your browser by clicking Image-to-Image Demo below.An example of using this demo is as follows：In the left side box, draw a cat as you imagine, and then click the process button, you can output a model generated cat.:underage: DeepNude Algorithm content::+1: NSFWNSFW(Not Safe/Suitable For Work) is a large-scale image dataset containing five categories of images [porn, hentai, sexy, natural, drawings]. Here, CycleGAN is used to convert different types of images, such as porn->natural.Image Generation Theoretical ResearchThis section describes DeepNude-related AI/Deep Learning theory (especially computer vision) research. If you like to read the paper and use the latest papers, enjoy it.1. Pix2PixResult is a general solution for the use of conditional confrontation networks as an image-to-image conversion problem proposed by the University of Berkeley.2. Pix2PixHDResultGet high resolution images from the semantic map. The semantic graph is a color picture. The different color blocks on the map represent different kinds of objects, such as pedestrians, cars, traffic signs, buildings, and so on. Pix2PixHD takes a semantic map as input and produces a high-resolution, realistic image. Most of the previous techniques can only produce rough, low-resolution images that don't look real. This research has produced images with a resolution of 2k by 1k, which is very close to full HD photos.3. CycleGANResultCycleGAN uses a cycle consistency loss to enable training without the need for paired data. In other words, it can translate from one domain to another without a one-to-one mapping between the source and target domain. This opens up the possibility to do a lot of interesting tasks like photo-enhancement, image colorization, style transfer, etc. All you need is the source and the target dataset.4. UGATITResultUGATIT is a novel method for unsupervised image-to-image translation, which incorporates a new attention module and a new learnable normalization function in an end-to-end manner. UGATIT can do both image conversions that require Holistic Changes, and image conversions that require Large Shape Changes. It can be seen as an enhanced version of CycleGAN, a more efficient general image conversion framework.5. StyleGANResultSource A + Source B (Style) = ?StyleGAN can not only generate fake images source A and source B, but also combine the content of source A and source B from different strengths, as shown in the following table.|Style level (from source b)|Source A|Source B||-|-|-||High level (coarse) | all colors (eyes, hair, light) and details facial features from Source A| inherit advanced facial features from Source B, such as posture, general hair style, facial shape and glasses||Medium level | posture, general facial shape and glasses come from source a| inherits the middle level facial features of source B, such as hair style, open / closed eyes||High level (fine) | the main facial content comes from source a| inherits the advanced facial features of source B, such as color scheme and microstructure|StyleGAN2Without increasing the amount of calculation of StyleGAN, while solving the image artifacts generated by StyleGAN and obtaining high-quality images with better details, StyleGAN2 implements a new SOTA for unconditional image modeling tasks.6. Image InpaintingResultIn the image interface of  video, you only need to use tools to simply smear the unwanted content in the image. Even if the shape is very irregular, NVIDIA's model can “restore” the image with very realistic The picture fills the smeared blank. It can be described as a one-click P picture, and ""no ps traces."" The study was based on a team from Nvidia's Guilin Liu et al. who published a deep learning method that could edit images or reconstruct corrupted images, even if the images were worn or lost pixels. This is the current 2018 state-of-the-art approach.7. SinGANICCV2019 Best paper - Marr prizeResultWe introduce SinGAN, an unconditional generative model that can be learned from a single natural image. Our model is trained to capture the internal distribution of patches within the image, and is then able to generate high quality, diverse samples that carry the same visual content as the image. SinGAN contains a pyramid of fully convolutional GANs, each responsible for learning the patch distribution at a different scale of the image. This allows generating new samples of arbitrary size and aspect ratio, that have significant variability, yet maintain both the global structure and the fine textures of the training image. In contrast to previous single image GAN schemes, our approach is not limited to texture images, and is not conditional (i.e. it generates samples from noise). User studies confirm that the generated samples are commonly confused to be real images. We illustrate the utility of SinGAN in a wide range of image manipulation tasks.8. ALAEResultAlthough studied extensively, the issues of whether they have the same generative power of GANs, or learn disentangled representations, have not been fully addressed. We introduce an autoencoder that tackles these issues jointly, which we call Adversarial Latent Autoencoder (ALAE). It is a general architecture that can leverage recent improvements on GAN training procedures.9. mGANpriorResultDespite the success of Generative Adversarial Networks (GANs) in image synthesis, applying trained GAN models to real image processing remains challenging. Previous methods typically invert a target image back to the latent space either by back-propagation or by learning an additional encoder. However, the reconstructions from both of the methods are far from ideal. In this work, we propose a novel approach, called mGANprior, to incorporate the well-trained GANs as effective prior to a variety of image processing tasks.10. StarGAN v2ResultA good image-to-image translation model should learn a mapping between different visual domains while satisfying the following properties: 1) diversity of generated images and 2) scalability over multiple domains. Existing methods address either of the issues, having limited diversity or multiple models for all domains. We propose StarGAN v2, a single framework that tackles both and shows significantly improved results over the baselines.11. DeepFaceDrawingResultRecent deep image-to-image translation techniques allow fast generation of face images from freehand sketches. However, existing solutions tend to overfit to sketches, thus requiring professional sketches or even edge maps as input. To address this issue, our key idea is to implicitly model the shape space of plausible face images and synthesize a face image in this space to approximate an input sketch.Image Generation Practice ResearchThis section explains DeepNude-related AI/Deep Learning (especially computer vision) code practices, and if you like to experiment, enjoy them.1. Pix2PixUse the Pix2Pix model (Conditional Adversarial Networks) to implement black and white stick figures to color graphics, flat houses to stereoscopic houses and aerial maps to maps.2. Pix2PixHDUnder development... First you can use the 3. CycleGANThe CycleGAN neural network model is used to realize the four functions of photo style conversion, photo effect enhancement, landscape season change, and object conversion.4. DCGANDCGAN is used to achieve random number to image generation tasks, such as face generation.5. Variational Autoencoder (VAE)VAE is used to achieve random number to image generation tasks, such as face generation.6. Neural style transferUse VGG19 to achieve image style migration effects, such as photo changes to oil paintings and comics...........................................................................If you are a user of , you can refer to the .DeepFakes (Promotion of DeepNude)Speaker's Video + Any Image = Fake VideoRealistic Speech-Driven Facial Animation with GANsOne photo + One audio = Composite VideoWe propose a temporal GAN capable of producing animated faces using only a still image of a person and an audio clip containing speech. The videos generated using this model do not only produce lip movements that are synchronized with the audio but also exhibit characteristic facial expressions such as blinks, brow raises etc. This extends our previous model by separately dealing with audio-visual synchronization and expression generation. Our improved model works on ""in-the-wild"" unseen faces and is capable of capturing the emotion of the speaker and reflecting it in the facial expression.Interested in DeepFakes?Future"
https://github.com/keon/algorithms,Minimal examples of data structures and algorithms in Python,"Pythonic Data Structures and AlgorithmsMinimal and clean example implementations of data structures and algorithms in Python 3.ContributingThanks for your interest in contributing! There are many ways to contribute to this project. TestsUse unittestFor running all tests write down:$ python3 -m unittest discover tests
For running some specific tests you can do this as following (Ex: sort):$ python3 -m unittest tests.test_sort
Use pytestFor running all tests write down:$ python3 -m pytest tests
InstallIf you want to use the API algorithms in your code, it is as simple as:$ pip3 install algorithms
You can test by creating a python file: (Ex: use  in )from algorithms.sort import merge_sort

if __name__ == ""__main__"":
    my_list = [1, 8, 3, 5, 6]
    my_list = merge_sort(my_list)
    print(my_list)
UninstallIf you want to uninstall algorithms, it is as simple as:$ pip3 uninstall -y algorithms
List of ImplementationsContributorsThanks to who helped in building the repo."
https://github.com/avinashpaliwal/Super-SloMo,PyTorch implementation of Super SloMo by Jiang et al.,"Super-SloMo PyTorch implementation of ""Super SloMo: High Quality Estimation of Multiple Intermediate Frames for Video Interpolation"" by Jiang H., Sun D., Jampani V., Yang M., Learned-Miller E. and Kautz J.  Check out ResultsResults on UCF101 dataset using the  provided by paper's author. The  script was used. It uses motions masks when calculating PSNR, SSIM and IE.| Method | PSNR | SSIM | IE ||------|:-----:|:-----:|:-----:|| DVF | 29.37 | 0.861 | 16.37 ||  - L_1 | 30.18 | 0.875 | 15.54 ||  - L_F | 30.03 | 0.869 | 15.78 || SuperSloMo_Adobe240fps | 29.80 | 0.870 | 15.68 || pretrained mine | 29.77 | 0.874 | 15.58 || SuperSloMo | 30.22 | 0.880 | 15.18 |PrerequisitesThis codebase was developed and tested with pytorch 0.4.1 and CUDA 9.2 and Python 3.6.Install:For GPU, runconda install pytorch=0.4.1 cuda92 torchvision==0.2.0 -c pytorch
For CPU, runconda install pytorch-cpu=0.4.1 torchvision-cpu==0.2.0 cpuonly -c pytorch
TrainingPreparing training dataIn order to train the model using the provided code, the data needs to be formatted in a certain manner.The create_dataset.py script uses  to extract frames from videos.Adobe240fpsFor adobe240fps, , unzip it and then run the following commandpython data\create_dataset.py --ffmpeg_dir path\to\folder\containing\ffmpeg --videos_folder path\to\adobe240fps\videoFolder --dataset_folder path\to\dataset --dataset adobe240fps
CustomFor custom dataset, run the following commandpython data\create_dataset.py --ffmpeg_dir path\to\folder\containing\ffmpeg --videos_folder path\to\adobe240fps\videoFolder --dataset_folder path\to\dataset
The default train-test split is 90-10. You can change that using command line argument .Run the following commmand for help / more infopython data\create_dataset.py --h
TrainingIn the , set the parameters (dataset path, checkpoint directory, etc.) and run all the cells.or to train from terminal, run:python train.py --dataset_root path\to\dataset --checkpoint_dir path\to\save\checkpoints
Run the following commmand for help / more options like continue from checkpoint, progress frequency etc.python train.py --h
TensorboardTo get visualization of the training, you can run tensorboard from the project directory using the command:tensorboard --logdir log --port 6007
and then go to .EvaluationPretrained modelYou can download the pretrained model trained on adobe240fps dataset .Video ConverterYou can convert any video to a slomo or high fps video (or both) using . Use the command# Windows
python video_to_slomo.py --ffmpeg path\to\folder\containing\ffmpeg --video path\to\video.mp4 --sf N --checkpoint path\to\checkpoint.ckpt --fps M --output path\to\output.mkv

# Linux
python video_to_slomo.py --video path\to\video.mp4 --sf N --checkpoint path\to\checkpoint.ckpt --fps M --output path\to\output.mkv
If you want to convert a video from 30fps to 90fps set  to 90 and  to 3 (to get 3x frames than the original video).Run the following commmand for help / more infopython video_to_slomo.py --h
You can also use  if you do not want to use ffmpeg. You will instead need to install  using pip for video IO.A sample usage would be:python eval.py data/input.mp4 --checkpoint=data/SuperSloMo.ckpt --output=data/output.mp4 --scale=4
Use  for more detailsMore info TBAReferences:Parts of the code is based on "
https://github.com/0xHJK/music-dl,search and download music 从网易云音乐、QQ音乐、酷狗音乐、百度音乐、虾米音乐、咪咕音乐等搜索和下载歌曲,"Music-dl: Listen to what you want[<marko.inline.RawText object at 0x000001592FE76688>] is a command line tool which helps you search and download music from multiple sources.Support for QQ music, Netease music, Xiami music, Kugou music and Baidu music. See .Python3 Only. Python 3.5+ Recommended. | 中文文档[<marko.inline.RawText object at 0x000001592FDFD2C8>]，API是从公开的网络中获得，不是破解版，也听不了付费歌曲。禁止将本工具用于商业用途，如产生法律纠纷与本人无关，如有侵权，请联系我删除。微博：QQ群：最近API封杀有点多，个人有点维护不过来，需要大家帮忙更新。查看 功能安装使用pip安装（推荐，注意前面有一个）：$ pip3 install pymusic-dl
手动安装（最新）：$ git clone https://github.com/0xHJK/music-dl.git
$ cd music-dl
$ python3 setup.py install
不安装直接运行：$ git clone https://github.com/0xHJK/music-dl.git
$ cd music-dl
$ pip3 install -r requirements.txt
$ ./music-dl

# 或 python3 music-dl
在以下环境测试通过：| 系统名称 | 系统版本       | Python版本 || -------- | -------------- | ---------- || macOS    | 10.14          | 3.7.0      || macOS    | 10.13          | 3.7.0      || Windows  | Windows 7 x64  | 3.7.2      || Windows  | Windows 10 x64 | 3.7.2      || Ubuntu   | 16.04 x64      | 3.5.2      |使用方式v3.0预览版命令有较大的改变，建议先查看帮助$ music-dl --help
Usage: music-dl [OPTIONS]

  Search and download music from netease, qq, kugou, baidu and xiami.
  Example: music-dl -k ""周杰伦""

Options:
  --version             Show the version and exit.
  -k, --keyword TEXT    搜索关键字，歌名和歌手同时输入可以提高匹配（如 空帆船 朴树）
  -u, --url TEXT        通过指定的歌曲URL下载音乐
  -p, --playlist TEXT   通过指定的歌单URL下载音乐
  -s, --source TEXT     Supported music source: qq netease kugou baidu
  -n, --number INTEGER  Number of search results
  -o, --outdir TEXT     Output directory
  -x, --proxy TEXT      Proxy (e.g. http://127.0.0.1:1087)
  -v, --verbose         Verbose mode
  --lyrics              同时下载歌词
  --cover               同时下载封面
  --nomerge             不对搜索结果列表排序和去重
  --help                Show this message and exit.
示例：支持的音乐源列表| 音乐源     | 缩写    | 网址                      | 有效 | 无损 | 320K | 封面 | 歌词 | 歌单 | 单曲 || ---------- | ------- | ------------------------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- || QQ音乐     | qq      |        | ✓     | -    | -    | ✕    | ✓     | ✕    | ✕    || 酷狗音乐   | kugou   |    | ✓     | -    | -    | -    | ✕    | -    | ✕    || 网易云音乐 | netease |   | ✓    | -    | ✓    | ✓    | ✓    | ✓    | ✓    || 咪咕音乐   | migu    |      | ✓    | ✓    | ✓    | ✓    | ✓    | ✕    | ✕    || 百度音乐   | baidu   |  | ✓    | -    | ✓    | ✓    | ✓    | ✕    | ✕    || 虾米音乐   | xiami   |   | ✕    | -    | -    | -    | -    | ✕    | ✕    |欢迎提交插件支持更多音乐源！插件写法参考中的文件更新记录提Issues说明Credits 致谢本项目受以下项目启发，参考了其中一部分思路，向这些开发者表示感谢。用爱发电维护不易，欢迎扫描恰饭二维码LICENSE"
https://github.com/OWASP/CheatSheetSeries,The OWASP Cheat Sheet Series was created to provide a concise collection of high value information on specific application security topics.,"Welcome to the OWASP Cheat Sheet SeriesWelcome to the official repository for the Open Web Application Security Project® (OWASP) Cheat Sheet Series project. The project focuses on providing good security practices for builders in order to secure their applications.In order to read the cheat sheets and reference them, use the project . The project details can be viewed on the  without the cheat sheets.:triangular_flag_on_post: Markdown files are the working sources and aren't intended to be referenced in any external documentation, books or websites.Cheat Sheet Series TeamProject LeadersCore TeamChat With UsWe're easy to find on Slack:Feel free to ask questions, suggest ideas, or share your best recipes.Contributions, Feature Requests, and FeedbackWe are actively inviting new contributors! To start, please read the .This project is only possible thanks to the work of many dedicated volunteers. Everyone is encouraged to help in ways large and small. Here are a few ways you can help:Automated BuildThis  allows you to download a build (ZIP archive) of the offline website.Local build The OWASP Cheat Sheet Series website can be built and tested locally by issuing the following commands:make install-python-requirements
make generate-site
make serve  # Binds port 8000
ContributorsSpecial thanksA special thank you to the following people for their help provided during the migration:Open Web Application Security Project and OWASP are registered trademarks of the OWASP Foundation, Inc."
https://github.com/reinderien/mimic,[ab]using Unicode to create tragedy,"mimic[ab]using Unicode to create tragedyIntroductionmimic provokes:It's inspired by this terrible idea floating around:There are many more characters in the Unicode character set that look, to some extent or another, like others – homoglyphs. Mimic substitutes common ASCII characters for obscure homoglyphs.Fun games to play with mimic:ResultsObserve the mayhem:""BUT WHY?""Or, if you've been mimicked a little harder,DiscussionPeople have noticed how terrible this is.Further Reading"
https://github.com/google/nogotofail,An on-path blackbox network traffic security testing tool,"nogotofailNogotofail is a network security testing tool designed to help developers andsecurity researchers spot and fix weak TLS/SSL connections and sensitivecleartext traffic on devices and applications in a flexible, scalable, powerful way.It includes testing for common SSL certificate verification issues, HTTPS and TLS/SSLlibrary bugs, SSL and STARTTLS stripping issues, cleartext issues, and more.DesignNogotofail is composed of an on-path network MiTM and optional clients for the devices being tested.See  for the overview and design goals of nogotofail.DependenciesNogotofail depends only on Python 2.7 and pyOpenSSL>=0.13. The MiTM is designed to work on Linuxmachines and the transparent traffic capture modes are Linux specific and require iptables as well.Additionally the Linux client depends on .Getting startedSee  for setup and a walkthrough of nogotofail.DiscussionFor discussion please use our ."
https://github.com/benfred/implicit,Fast Python Collaborative Filtering for Implicit Feedback Datasets,"ImplicitFast Python Collaborative Filtering for Implicit Datasets.This project provides fast Python implementations of several different popular recommendation algorithms forimplicit feedback datasets:All models have multi-threaded training routines, using Cython and OpenMP to fit the models inparallel among all available CPU cores.  In addition, the ALS and BPR models both have custom CUDAkernels - enabling fitting on compatible GPU's. Approximate nearest neighbours libraries such as , and  can also be used by Implicit to .InstallationImplicit can be installed from pypi with:pip install implicit
Installing with pip will use prebuilt binary wheels on x86_64 Linux, Windowsand OSX. These wheels include GPU support on Linux.Implicit can also be installed with conda:# CPU only package
conda install -c conda-forge implicit

# CPU+GPU package
conda install -c conda-forge implicit implicit-proc=*=gpu
Basic Usageimport implicit

# initialize a model
model = implicit.als.AlternatingLeastSquares(factors=50)

# train the model on a sparse matrix of user/item/confidence weights
model.fit(user_item_data)

# recommend items for a user
recommendations = model.recommend(userid, user_item_data[userid])

# find related items
related = model.similar_items(itemid)
The examples folder has a program showing how to use this to .For more information see the .Articles about ImplicitThese blog posts describe the algorithms that power this library:There are also several other articles about using Implicit to build recommendation systems:RequirementsThis library requires SciPy version 0.16 or later and Python version 3.6 or later.GPU Support requires at least version 11 of the .This library is tested with Python 3.7, 3.8, 3.9, 3.10 and 3.11 on Ubuntu, OSX and Windows.BenchmarksSimple benchmarks comparing the ALS fitting time versus .Optimal ConfigurationI'd recommend configuring SciPy to use Intel's MKL matrix libraries. One easy way of doing this is by installing the Anaconda Python distribution.For systems using OpenBLAS, I highly recommend setting 'export OPENBLAS_NUM_THREADS=1'. Thisdisables its internal multithreading ability, which leads to substantial speedups for thispackage. Likewise for Intel MKL, setting 'export MKL_NUM_THREADS=1' should also be set.Released under the MIT License"
https://github.com/listen1/listen1,one for all free music in china (origin edition),"Listen 1注意：本项目已经停止维护，而发展为两个新项目。Listen1插件版请前往，Listen1桌面版请前往Listen 1 让你用一个网页就能听到多个网站的在线音乐（现已包括网易云音乐，QQ音乐，虾米音乐，豆瓣音乐）。你可以非常的简单的访问和收听在线音乐，而不用受到单个音乐网站资源不全的限制了。它不仅能搜索多家在线音乐提供商的资源，还能方便的整理你喜欢的音乐，制作自己的歌单。尽兴的享受音乐吧！支持浏览器：IE 11, Chrome, FireFox, Safari安装Windows 环境Windows安装包（点击下载）：注意：可能误触发杀毒软件的警报，请忽略。Mac 环境Mac安装包（点击下载）：调试开发后台基于tornado开发，可以用Python环境直接运行。打包打包注意事项请参考和。致谢在开发过程中，参考了很多音乐网站API的分析代码和文章，感谢这些开发者的努力。（具体项目网址参考源码）LicenseMIT版权声明“QQ”、“QQ音乐”及企鹅形象等文字、图形和商业标识，其著作权或商标权归腾讯公司所有。QQ音乐享有对其平台授权音乐的版权，请勿随意下载，复制版权内容。具体内容请参考QQ音乐用户协议。"
https://github.com/SublimeText-Markdown/MarkdownEditing,Powerful Markdown package for Sublime Text with better syntax understanding and good color schemes.,MarkdownEditingMarkdown plugin for Sublime Textwith...Please visit  to learn more about the full set of features and how to use them.FeaturesFolding and NavigationHeadingsListsBlockquotesCritic MarkupLinks and ReferencesText FormattingContributingSee .CreditsMarkdownEditing was originally created by  and has become a community project with the goal of consolidating the best features from the varied collection of Markdown packages for Sublime Text. Related blog posts from Brett:Development was headed by  and  until early 2021.Current development is headed up by .This plugin contains portions of code from .Footnote commands were submitted by  and originated at .LicenseMarkdownEditing is released under the .
https://github.com/ethereon/caffe-tensorflow,Caffe models in TensorFlow,Caffe to TensorFlowConvert  models to .UsageRun  to convert an existing Caffe model to TensorFlow.Make sure you're using the latest Caffe format (see the notes section for more info).The output consists of two files:ExamplesSee the  folder for more details.VerificationThe following converted models have been verified on the ILSVRC2012 validation set using.| Model                                                 | Top 5 Accuracy ||:------------------------------------------------------|---------------:||          |         92.92% ||          |         92.63% ||           |         92.02% ||               |         89.88% ||            |         89.06% ||   |         81.21% ||             |         79.93% ||                        |         79.84% |Notes
https://github.com/trustedsec/ptf,The Penetration Testers Framework (PTF) is a way for modular support for up-to-date tools.,"The PenTesters Framework (PTF)A TrustedSec Project - Copyright 2022Written by: David Kennedy (@HackingDave)https://www.trustedsec.comTwitter: @TrustedSec, @HackingDaveThe PenTesters Framework (PTF) is a Python script designed for Debian/Ubuntu/ArchLinux based distributions to create a similar and familiar distribution for Penetration Testing. As pentesters, we've been accustom to the /pentest/ directories or our own toolsets that we want to keep up-to-date all of the time. We have those ""go to"" tools that we use on a regular basis, and using the latest and greatest is important.PTF attempts to install all of your penetration testing tools (latest and greatest), compile them, build them, and make it so that you can install/update your distribution on any machine. Everything is organized in a fashion that is cohesive to the Penetration Testing Execution Standard (PTES) and eliminates a lot of things that are hardly used. PTF simplifies installation and packaging and creates an entire pentest framework for you. Since this is a framework, you can configure and add as you see fit. We commonly see internally developed repos that you can use as well as part of this framework. It's all up to you.The ultimate goal is for community support on this project. We want new tools added to the github repository. Submit your modules. It's super simple to configure and add them and only takes a few minute.InstallationPTF requires python-pexpect in order to work appropriately. Run the following command below:pip install -r requirements.txt
./ptf
Instructions:First check out the config/ptf.config file which contains the base location of where to install everything. By default this will install in the /pentest directory. Once you have that configured, move to running PTF by typing  (or python ptf).This will put you in a Metasploitesque type shell which has a similar look and feel for consistency. Show modules, use , etc. are all accepted commands. First things first, always type help or  to see a full list of commands.For a video tutorial on how to use PTF, check out our Vimeo page here: https://vimeo.com/137133837Update EVERYTHING!If you want to install and/or update everything, simply do the following:./ptf
use modules/install_update_all
yes
This will install all of the tools inside of PTF. If they are already installed, this will iterate through and update everything for you automatically.You can also individually install each module, then use the  use modules/update_installed which will only update what you've previously installed.For example:./ptf
use modules/update_installed
This will only update previous ones you've installed.You can also show options to change information about the modules.If you only want to install only for example exploitation tools, you can run:./ptf
use modules/exploitation/install_update_all
This will only install the exploitation modules. You can do this for any module category.Customize your own installed toolsYou can install only the tools you want to by going to the modules/custom_list/list.txt section. Modify the list.txt file and add the tools you only want to install or update.Example list.txt file:modules/exploitation/metasploitmodules/post-exploitation/unicornThen when in PTF:./ptf
use modules/custom_list/list
yes
This allows you to carry your module configuration over and only install the tools that you want and keep them updated.You can also simply specify a module without using the category:./ptf
use trevorc2
yes
Modules:First, head over to the modules/ directory, inside of there are sub directories based on the Penetration Testing Execution Standard (PTES) phases. Go into those phases and look at the different modules. As soon as you add a new one, for example testing.py, it will automatically be imported next time you launch PTF. There are a few key components when looking at a module that must be completed.Below is a sample moduleAUTHOR=""David Kennedy (ReL1K)""

DESCRIPTION=""This module will install/update the Browser Exploitation Framework (BeEF)""

INSTALL_TYPE=""GIT""

REPOSITORY_LOCATION=""https://github.com/beefproject/beef""

X64_LOCATION=""https://github.com/something_thats_x64_instead_of_x86

INSTALL_LOCATION=""beef""

DEBIAN=""ruby1.9.3,sqlite3,ruby-sqlite3""

ARCHLINUX = ""arch-module,etc""

BYPASS_UPDATE=""NO""

AFTER_COMMANDS=""cd {INSTALL_LOCATION},ruby install-beef""

LAUNCHER=""beef""

TOOL_DEPEND=""modules/exploitation/metasploit""
Module Development:All of the fields are pretty easy, on the repository locations, you can use GIT, SVN FILE, OR TAGS. Fill in the depends, and where you want the install location to be. PTF will take where the python file is located (for example exploitation) and move it to what you specify in the PTF config (located under config). By default it installs all your tools to Note in modules, you can specify after commands . This will append where you want the install location to go when using after commands.You can also specify  which will pull the base path for your PTF installation.You also have the ability for repository locations to specify both a 32 bit and 64 bit location. Repository location should always be the x86 download path. To add a 64 bit path for a tool, specify X64_LOCATION and give it a URL. When PTF launches it will automatically detect the architecture and attempt to use the x64 link instead of the x86.Note that ArchLinux packages are also supported, it needs to be specified for both DEBIAN and ARCH in order for it to be properly installed on either platform in the moduleWhen using the TAGS mode, this will allow you to use a github project that utilizes tags to pull the latest version (usually compiled applications) and automatically download. In order to use the TAGS method, take a look at the structure under modules/intelligence-gathering/teamfiltration.py. In this example, there is no need for a repository_location, but you will need to know the project owner, project name/repo, and the filename to download. In the example of TeamFiltration, it is located at: https://github.com/Flangvik/TeamFiltration. The owner would be Flangvik, the project/tool would be TeamFiltration. If you navigate to releases: https://github.com/Flangvik/TeamFiltration/releases/, we can see here that the name of the file we want to download is ""TeamFiltration_Linux"". These are under the OWNER, REPOHOME, and FILENAME. Specifying these, PTF will automatically detect the latest release of the tool and install them.GITLAB SupportYou can create your own modules and PTF also supports gitlab access. Instead of specifying git, wget, etc., simply specify gitlab and point to your own internal gitlab tools for modules.BYPASS UPDATES:When using traditional git or svn as a main method, what will happen after a module is installed is it will just go and grab the latest version of the tool. With after commands, normally when installing, you may need to run the after commands after each time you update. If you specify bypass updates to YES (), each time the tool is run, it will check out the latest version and still run after commands. If this is marked to no, it will only git pull the latest version of the system. For  options, it is recommended to always use  so that it will overwrite the files each time.After Commands:After commands are commands that you can insert after an installation. This could be switching to a directory and kicking off additional commands to finish the installation. For example in the BEEF scenario, you need to run ruby install-beef afterwards. Below is an example of after commands using the  flag.AFTER_COMMANDS=""cp config/dict/rockyou.txt {INSTALL_LOCATION}""
For  that do self install (don't need user interaction).Automatic LaunchersThe flag LAUNCHER= in modules is optional. If you add  for example, PTF will automatically create a launcher for the tool under . In the setoolkit example, when run - PTF will automatically create a file under  so you can launch SET from anywhere by simply typing setoolkit. All files will still be installed under the appropriate categories, for example  however an automatic launcher will be created.You can have multiple launchers for an application. For example, for Metasploit you may want msfconsole, msfvenom, etc. In order to add multiple launchers, simply put a  between them. For example . This would create launchers for both.Automatic Command LineYou can also just run  and it will automatically update everything for you without having to go into the framework.Running UnattendedIf you're running  in an automatic build, you can use a  so you don't have to interactively type the modules you wish to install. Example:./ptf <<EOF
use modules/exploitation/metasploit
run
use modules/password-recovery/johntheripper
run
EOF
TOOL DEPENDSSome tools such as Veil, SET, etc. require tools such as the Metasploit Framework. You can add in the module  and multiple other tools if there is a tool required to be installed prior to installing the tool. This will force PTF to install the required tool first, then install the module that requires it. Example:This will install Metasploit first or ensured its installed first prior to installing the application.IGNORE Modules or CategoriesThe  config option can be found under config/ptf.config in the PTF root directory. This will ignore modules and not install them - everything is comma separated and based on name - example:  or entire module categories, like IGNORE Modules from Update/Install AllThe  config option can be found under config/ptf.config in the PTF root directory. This will ignore modules only when doing install_update_all which are used when you want to install all tools. This could be for large applications that take substantial time, ones that require user interaction, or open up a number of ports and protocols on the system. This works very similar in the IGNORE_THESE_MODULES, except that they can be manually installed and updated through the modules/update_installed. These are comma deliminated, so for example modules/exploitation/tool1,modules/exploitation/tool2, when running install_update_all, this would not install the tools unless you went to use modules/exploitation/tool1 and installed via that method. INCLUDE_ONLY_THESE_MODULESThe  in the config option under config/ptf.config will only install and include specific modules that are specified here. This is good for baselining the tools that you want and install only them.LAUNCH PTF WITH NO BANNERYou can launch PTF with no banner message if you want. Simply specify:./ptf --no-banner

or 

./ptf -nb
CHECK FOR INSTALLED PROGRAMS THROUGH PTFYou can check to see what applications you've already installed through PTF by typing the following:ptf>show installed
"
https://github.com/maciejkula/spotlight,Deep recommender models using PyTorch.,".. image:: docs/_static/img/spotlight.png.. inclusion-marker-do-not-remove.. image:: https://travis-ci.org/maciejkula/spotlight.svg?branch=master:target: https://travis-ci.org/maciejkula/spotlight.. image:: https://ci.appveyor.com/api/projects/status/jq5e76a7a08ra2ji/branch/master?svg=true:target: https://ci.appveyor.com/project/maciejkula/spotlight/branch/master.. image:: https://badges.gitter.im/gitterHQ/gitter.png:target: https://gitter.im/spotlight-recommendations/Lobby.. image:: https://anaconda.org/maciejkula/spotlight/badges/version.svg:target: https://anaconda.org/maciejkula/spotlight.. image:: https://img.shields.io/badge/docs-latest-brightgreen.svg?style=flat:target: https://maciejkula.github.io/spotlight/.. image:: https://img.shields.io/badge/progress%20tracker-trello-brightgreen.svg:target: https://trello.com/b/G5iFgS1W/spotlight|Spotlight uses _ to build both deep and shallowrecommender models. By providing both a slew of building blocks for loss functions(various pointwise and pairwise ranking losses), representations (shallowfactorization representations, deep sequence models), and utilities for fetching(or generating) recommendation datasets, it aims to be a tool for rapid explorationand prototyping of new recommender models.See the full _ for details.Installation
.. code-block:: python

   conda install -c maciejkula -c pytorch spotlight


Usage
~~~~~

Factorization models
====================

To fit an explicit feedback model on the MovieLens dataset:

.. code-block:: python

    from spotlight.cross_validation import random_train_test_split
    from spotlight.datasets.movielens import get_movielens_dataset
    from spotlight.evaluation import rmse_score
    from spotlight.factorization.explicit import ExplicitFactorizationModel

    dataset = get_movielens_dataset(variant='100K')

    train, test = random_train_test_split(dataset)

    model = ExplicitFactorizationModel(n_iter=1)
    model.fit(train)

    rmse = rmse_score(model, test)



To fit an implicit ranking model with a BPR pairwise loss on the MovieLens dataset:

.. code-block:: python

    from spotlight.cross_validation import random_train_test_split
    from spotlight.datasets.movielens import get_movielens_dataset
    from spotlight.evaluation import mrr_score
    from spotlight.factorization.implicit import ImplicitFactorizationModel

    dataset = get_movielens_dataset(variant='100K')

    train, test = random_train_test_split(dataset)

    model = ImplicitFactorizationModel(n_iter=3,
                                       loss='bpr')
    model.fit(train)

    mrr = mrr_score(model, test)




Sequential models
=================

Recommendations can be seen as a sequence prediction task: given the items a user
has interacted with in the past, what will be the next item they will interact
with? Spotlight provides a range of models and utilities for fitting next item
recommendation models, including

- pooling models, as in `YouTube recommendations <https://pdfs.semanticscholar.org/bcdb/4da4a05f0e7bc17d1600f3a91a338cd7ffd3.pdf>`_,
- LSTM models, as in `Session-based recommendations... <https://arxiv.org/pdf/1511.06939>`_, and
- causal convolution models, as in `WaveNet <https://arxiv.org/pdf/1609.03499>`_.

.. code-block:: python

    from spotlight.cross_validation import user_based_train_test_split
    from spotlight.datasets.synthetic import generate_sequential
    from spotlight.evaluation import sequence_mrr_score
    from spotlight.sequence.implicit import ImplicitSequenceModel

    dataset = generate_sequential(num_users=100,
                                  num_items=1000,
                                  num_interactions=10000,
                                  concentration_parameter=0.01,
                                  order=3)

    train, test = user_based_train_test_split(dataset)

    train = train.to_sequence()
    test = test.to_sequence()

    model = ImplicitSequenceModel(n_iter=3,
                                  representation='cnn',
                                  loss='bpr')
    model.fit(train)

    mrr = sequence_mrr_score(model, test)


  

Datasets
========

Spotlight offers a slew of popular datasets, including Movielens 100K, 1M, 10M, and 20M.
It also incorporates utilities for creating synthetic datasets. For example, `generate_sequential`
generates a Markov-chain-derived interaction dataset, where the next item a user chooses is
a function of their previous interactions:

.. code-block:: python

    from spotlight.datasets.synthetic import generate_sequential

    # Concentration parameter governs how predictable the chain is;
    # order determins the order of the Markov chain.
    dataset = generate_sequential(num_users=100,
                                  num_items=1000,
                                  num_interactions=10000,
                                  concentration_parameter=0.01,
                                  order=3)




Examples
~~~~~~~~

1. `Rating prediction on the Movielens dataset <https://github.com/maciejkula/spotlight/tree/master/examples/movielens_explicit>`_.
2. `Using causal convolutions for sequence recommendations <https://github.com/maciejkula/spotlight/tree/master/examples/movielens_sequence>`_.
3. `Bloom embedding layers <https://github.com/maciejkula/spotlight/tree/master/examples/bloom_embeddings>`_.


How to cite
~~~~~~~~~~~

Please cite Spotlight if it helps your research. You can use the following BibTeX entry:

.. code-block::

   @misc{kula2017spotlight,
     title={Spotlight},
     author={Kula, Maciej},
     year={2017},
     publisher={GitHub},
     howpublished={\url{https://github.com/maciejkula/spotlight}},
   }


Contributing
Spotlight is meant to be extensible: pull requests are welcome. Development progress is tracked on _: have a look at the outstanding tickets to get an idea of what would be a useful contribution.We accept implementations of new recommendation models into the Spotlight model zoo: if you've just published a paper describing your new model, or have an implementation of a model from the literature, make a PR!"
https://github.com/momosecurity/aswan,陌陌风控系统静态规则引擎，零基础简易便捷的配置多种复杂规则，实时高效管控用户异常行为。,"陌陌风控系统静态规则引擎加入社区欢迎加入QQ群沟通！群号：663227548架构介绍本项目的主分支仅支持Python3，目前通过Python3.7.3的版本测试，如果需要python2.7的版本，请使用tag: last-support-Python2.7 的代码.快速启动手动    # 为了简单可以使用docker安装
    # docker安装文档地址(以ubuntu为例): https://docs.docker.com/install/linux/docker-ce/ubuntu/
    mongo: docker run -d --name mongo -v $HOME/docker_volumes/mongodb:/data/db  -p 27017:27017 mongo:latest
    mysql: docker run -d --name mysql -e MYSQL_ROOT_PASSWORD=root -v $HOME/docker_volumes/mysql:/var/lib/mysql -v $HOME/docker_volumes/conf/mysql:/etc/mysql/conf.d -p 3306:3306 mysql:5.6
    redis: docker run -d --name redis -p 6379:6379  -v $HOME/docker_volumes/redis:/var/lib/redis redis:latest
    docker exec -it mysql mysql -h 127.0.0.1 -u root -p # 后续需输入密码 若以上述方式安装mysql，密码为root.
    CREATE DATABASE risk_control CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci; # 创建数据库时指定编码格式，规避乱码问题(注意: 此编码格式在mysql低版本上可能有兼容性问题)
    # 在www目录下
    python manage.py makemigrations && python manage.py migrate
    # 创建管理员账户  此处详见  其它操作--增加用户
    python manage.py createsuperuser # 后续 依次输入用户名、密码、邮箱 即可创建一个管理员账号
    # 如果希望对系统有一个直观的感受，可以使用如下指令来预注入一些数据
    python manage.py init_risk_data
    # 在aswan下以nohup的方式启动服务进程、管理后台、拦截日志消费进程
    bash start.sh
docker此处不再赘述docker-compose -f deploy/docker-compose.yaml up --build

# 如果需要初始化数据及账号，可登录到具体的实例上执行下述命令
# 创建管理员账户  此处详见  其它操作--增加用户
python manage.py createsuperuser # 后续 依次输入用户名、密码、邮箱 即可创建一个管理员账号
# 如果希望对系统有一个直观的感受，可以使用如下指令来预注入一些数据
python manage.py init_risk_data
后台介绍调用样例curl 127.0.0.1:50000/query/ -X POST -d '{""rule_id"": ""1"", ""user_id"": ""10000""}' -H ""Content-Type:application/json""
curl 127.0.0.1:50000/report/ -X POST -d '{""source_name"": ""test"", ""user_id"": ""10000"", ""ip"": ""127.0.0.1"", ""uid"": ""abcabc112333222"", ""timestamp"": 1559049606}' -H ""Content-Type:application/json""
内置函数的扩展其它增加用户考虑到企业用户大多数为域账户登录，因此推荐使用LDAP认证模块直接集成。但考虑到大家的场景不一样，因此也可以手动增加用户，样例代码如下:# coding=utf-8
from django.contrib.auth.models import User

username = 'username'
password = 'password'
email = 'email@momo.com'
first_name = '测'
last_name = '试'
# 普通用户
User.objects.create_user(username=username, password=password, email=email, first_name=first_name, last_name=last_name)
# 管理员账户
User.objects.create_superuser(username=username, password=password, email=email, first_name=first_name, last_name=last_name)
添加完成后，让用户登录，然后管理员配置权限即可。权限管理目前的权限模型包含如下元素，可在对应的页面进行配置。|元素名称 | 元素含义 | 配置方式 | 注 || :-----| ----: | :----: | :----: || uri | 风控管理后台的一个独立uri | 开发时自动产生 | 此处uri为相对路径，例如: /permissions/groups/ || uri组 | 多个相互关联的uri可以被放置到一个uri组中 | /permissions/uri_groups/ | - || 权限组 | 多个uri组可以被分配到一个权限组中 | /permissions/groups/ | - || 用户 | 用户即为独立的个人/员工 | /permissions/users/ | 1. 本系统在界面上不提供添加用户的功能；2. 用户可以被分配到某个权限组中，也可以直接配置uri组 || 管理员 | 即为系统的拥有者，默认拥有所有权限 | 手动配置 | - |具体图示如下:配置相关目前Django部分的配置均存放于 www/settings 目录，非Django部分的配置均位于 config 目录下。为了在不同环境加载不同的配置，我们使用了RISK_ENV这个环境变量，系统在运行时会自动通过这个环境变量的值加载对应的配置文件。为了方便项目启动，在未设置这个值时，系统默认会加载 develop 环境的配置。而在执行测试时(python manage.py test)时，RISK_ENV的值必须是 test 。aswan系统的工作流例子现在有一个打地鼠的活动，打地鼠成功可获得小礼物一个规则业务侧规则为：风控侧规则为：请求示例以一个正常用户执行多次动作(假设每次执行时间间隔均为5s)来举例:第一次：用户打地鼠前，调用query接口，由于未命中策略，得到pass，但用户运气不好(业务侧规则)，打地鼠失败，则直接结束第二次：用户打地鼠前，调用query接口，由于未命中策略，得到pass，且用户运气不错(业务侧规则)，打地鼠成功，则调用report上报数据第三次：用户打地鼠前，调用query接口，命中时段频控型策略，得到deny，此次行为被拦截，用户打地鼠失败，结束...在某个时刻，此用户由于某些规则被判定为异常用户...第四次：用户打地鼠前，调用query接口，命中bool型策略(异常用户)，得到deny，此次行为被拦截，用户打地鼠失败，结束项目代码测试$ pip install coverage
$ export RISK_ENV=test
$ python www/manage.py test
$ cd tests && python run_test.py
关于我们Website：https://security.immomo.comWeChat:"
https://github.com/openvinotoolkit/open_model_zoo,Pre-trained Deep Learning models and demos (high quality and extremely fast)," - Open Model Zoo repositoryThis repository includes optimized deep learning models and a set of demos to expedite development of high-performance deep learning inference applications. Use these free pre-trained models instead of training your own models to speed-up the development and production deployment process.Intel is committed to the respect of human rights and avoiding complicity in human rights abuses, a policy reflected in the . Accordingly, by accessing the Intel material on this platform you agree that you will not use the material in a product or application that causes or contributes to a violation of an internationally recognized human right.Repository Components:LicenseOpen Model Zoo is licensed under .TelemetryOpenVINO™ collects software performance and usage data for the purpose of improving OpenVINO™ tools. This data is collected directly by OpenVINO™ or through the use of Google Analytics 4.You can opt-out at any time by running the command:opt_in_out --opt_out
Online DocumentationOther Usage ExamplesHow to ContributeWe welcome community contributions to the Open Model Zoo repository. If you have an idea how to improve the product, please share it with us doing the following steps:You can find additional information about model contribution .We will review your contribution and, if any additional fixes or modifications are needed, may give you feedback to guide you. When accepted, your pull request will be merged into the GitHub* repositories.Open Model Zoo is licensed under Apache License, Version 2.0. By contributing to the project, you agree to the license and copyright terms therein and release your contribution under these terms.SupportPlease report questions, issues and suggestions using: Other names and brands may be claimed as the property of others."
https://github.com/eriklindernoren/Keras-GAN,Keras implementations of Generative Adversarial Networks.,"This repository has gone stale as I unfortunately do not have the time to maintain it anymore. If you would like to continue the development of it as a collaborator send me an email at eriklindernoren@gmail.com.Keras-GANCollection of Keras implementations of Generative Adversarial Networks (GANs) suggested in research papers. These models are in some cases simplified versions of the ones ultimately described in the papers, but I have chosen to focus on getting the core ideas covered instead of getting every layer configuration right. Contributions and suggestions of GAN varieties to implement are very welcomed.See also: Table of ContentsInstallation$ git clone https://github.com/eriklindernoren/Keras-GAN
$ cd Keras-GAN/
$ sudo pip3 install -r requirements.txt
ImplementationsAC-GANImplementation of Auxiliary Classifier Generative Adversarial Network.Paper: https://arxiv.org/abs/1610.09585Example$ cd acgan/
$ python3 acgan.py
Adversarial AutoencoderImplementation of Adversarial Autoencoder.Paper: https://arxiv.org/abs/1511.05644Example$ cd aae/
$ python3 aae.py
BiGANImplementation of Bidirectional Generative Adversarial Network.Paper: https://arxiv.org/abs/1605.09782Example$ cd bigan/
$ python3 bigan.py
BGANImplementation of Boundary-Seeking Generative Adversarial Networks.Paper: https://arxiv.org/abs/1702.08431Example$ cd bgan/
$ python3 bgan.py
CC-GANImplementation of Semi-Supervised Learning with Context-Conditional Generative Adversarial Networks.Paper: https://arxiv.org/abs/1611.06430Example$ cd ccgan/
$ python3 ccgan.py
CGANImplementation of Conditional Generative Adversarial Nets.Paper:https://arxiv.org/abs/1411.1784Example$ cd cgan/
$ python3 cgan.py
Context EncoderImplementation of Context Encoders: Feature Learning by Inpainting.Paper: https://arxiv.org/abs/1604.07379Example$ cd context_encoder/
$ python3 context_encoder.py
CoGANImplementation of Coupled generative adversarial networks.Paper: https://arxiv.org/abs/1606.07536Example$ cd cogan/
$ python3 cogan.py
CycleGANImplementation of Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks.Paper: https://arxiv.org/abs/1703.10593Example$ cd cyclegan/
$ bash download_dataset.sh apple2orange
$ python3 cyclegan.py
DCGANImplementation of Deep Convolutional Generative Adversarial Network.Paper: https://arxiv.org/abs/1511.06434Example$ cd dcgan/
$ python3 dcgan.py
DiscoGANImplementation of Learning to Discover Cross-Domain Relations with Generative Adversarial Networks.Paper: https://arxiv.org/abs/1703.05192Example$ cd discogan/
$ bash download_dataset.sh edges2shoes
$ python3 discogan.py
DualGANImplementation of DualGAN: Unsupervised Dual Learning for Image-to-Image Translation.Paper: https://arxiv.org/abs/1704.02510Example$ cd dualgan/
$ python3 dualgan.py
GANImplementation of Generative Adversarial Network with a MLP generator and discriminator.Paper: https://arxiv.org/abs/1406.2661Example$ cd gan/
$ python3 gan.py
InfoGANImplementation of InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets.Paper: https://arxiv.org/abs/1606.03657Example$ cd infogan/
$ python3 infogan.py
LSGANImplementation of Least Squares Generative Adversarial Networks.Paper: https://arxiv.org/abs/1611.04076Example$ cd lsgan/
$ python3 lsgan.py
Pix2PixImplementation of Image-to-Image Translation with Conditional Adversarial Networks.Paper: https://arxiv.org/abs/1611.07004Example$ cd pix2pix/
$ bash download_dataset.sh facades
$ python3 pix2pix.py
PixelDAImplementation of Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks.Paper: https://arxiv.org/abs/1612.05424MNIST to MNIST-M ClassificationTrains a classifier on MNIST images that are translated to resemble MNIST-M (by performing unsupervised image-to-image domain adaptation). This model is compared to the naive solution of training a classifier on MNIST and evaluating it on MNIST-M. The naive model manages a 55% classification accuracy on MNIST-M while the one trained during domain adaptation gets a 95% classification accuracy.$ cd pixelda/
$ python3 pixelda.py
| Method       | Accuracy  || ------------ |:---------:|| Naive        | 55%       || PixelDA      | 95%       |SGANImplementation of Semi-Supervised Generative Adversarial Network.Paper: https://arxiv.org/abs/1606.01583Example$ cd sgan/
$ python3 sgan.py
SRGANImplementation of Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network.Paper: https://arxiv.org/abs/1609.04802Example$ cd srgan/
<follow steps at the top of srgan.py>
$ python3 srgan.py
WGANImplementation of Wasserstein GAN (with DCGAN generator and discriminator).Paper: https://arxiv.org/abs/1701.07875Example$ cd wgan/
$ python3 wgan.py
WGAN GPImplementation of Improved Training of Wasserstein GANs.Paper: https://arxiv.org/abs/1704.00028Example$ cd wgan_gp/
$ python3 wgan_gp.py
"
https://github.com/stanfordnlp/stanza,"Stanford NLP Python library for tokenization, sentence segmentation, NER, and parsing of many human languages","The Stanford NLP Group's official Python NLP library. It contains support for running various accurate natural language processing tools on 60+ languages and for accessing the Java Stanford CoreNLP software from Python. For detailed information please visit our .🔥 &nbsp;A new collection of biomedical and clinical English model packages are now available, offering seamless experience for syntactic analysis and named entity recognition (NER) from biomedical literature text and clinical notes. For more information, check out our .ReferencesIf you use this library in your research, please kindly cite our :@inproceedings{qi2020stanza,
    title={Stanza: A {Python} Natural Language Processing Toolkit for Many Human Languages},
    author={Qi, Peng and Zhang, Yuhao and Zhang, Yuhui and Bolton, Jason and Manning, Christopher D.},
    booktitle = ""Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations"",
    year={2020}
}
If you use our biomedical and clinical models, please also cite our :@article{zhang2021biomedical,
    author = {Zhang, Yuhao and Zhang, Yuhui and Qi, Peng and Manning, Christopher D and Langlotz, Curtis P},
    title = {Biomedical and clinical {E}nglish model packages for the {S}tanza {P}ython {NLP} library},
    journal = {Journal of the American Medical Informatics Association},
    year = {2021},
    month = {06},
    issn = {1527-974X}
}
The PyTorch implementation of the neural pipeline in this repository is due to  (@qipeng),  (@yuhaozhang), and  (@yuhui-zh15), with help from  (@j38),  (@tdozat) and  (@AngledLuffa). Maintenance of this repo is currently led by .If you use the CoreNLP software through Stanza, please cite the CoreNLP software package and the respective modules as described  (""Citing Stanford CoreNLP in papers""). The CoreNLP client is mostly written by , and  spearheaded merging the two projects together.If you use the Semgrex or Ssurgeon part of CoreNLP, please cite :@inproceedings{bauer-etal-2023-semgrex,
    title = ""Semgrex and Ssurgeon, Searching and Manipulating Dependency Graphs"",
    author = ""Bauer, John  and
      Kiddon, Chlo{\'e}  and
      Yeh, Eric  and
      Shan, Alex  and
      D. Manning, Christopher"",
    booktitle = ""Proceedings of the 21st International Workshop on Treebanks and Linguistic Theories (TLT, GURT/SyntaxFest 2023)"",
    month = mar,
    year = ""2023"",
    address = ""Washington, D.C."",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2023.tlt-1.7"",
    pages = ""67--73"",
    abstract = ""Searching dependency graphs and manipulating them can be a time consuming and challenging task to get right. We document Semgrex, a system for searching dependency graphs, and introduce Ssurgeon, a system for manipulating the output of Semgrex. The compact language used by these systems allows for easy command line or API processing of dependencies. Additionally, integration with publicly released toolkits in Java and Python allows for searching text relations and attributes over natural text."",
}
Issues and Usage Q&ATo ask questions, report issues or request features 🤔, please use the . Before creating a new issue, please make sure to search for existing issues that may solve your problem, or visit the  on our website.Contributing to StanzaWe welcome community contributions to Stanza in the form of bugfixes 🛠️ and enhancements 💡! If you want to contribute, please first read .InstallationpipStanza supports Python 3.6 or later. We recommend that you install Stanza via , the Python package manager. To install, simply run:pip install stanza
This should also help resolve all of the dependencies of Stanza, for instance  1.3.0 or above.If you currently have a previous version of  installed, use:pip install stanza -U
AnacondaTo install Stanza via Anaconda, use the following conda command:conda install -c stanfordnlp stanza
Note that for now installing Stanza via Anaconda does not work for Python 3.10. For Python 3.10 please use pip installation.From SourceAlternatively, you can also install from source of this git repository, which will give you more flexibility in developing on top of Stanza. For this option, rungit clone https://github.com/stanfordnlp/stanza.git
cd stanza
pip install -e .
Running StanzaGetting Started with the neural pipelineTo run your first Stanza pipeline, simply following these steps in your Python interactive interpreter:>>> import stanza
>>> stanza.download('en')       # This downloads the English models for the neural pipeline
>>> nlp = stanza.Pipeline('en') # This sets up a default neural pipeline in English
>>> doc = nlp(""Barack Obama was born in Hawaii.  He was elected president in 2008."")
>>> doc.sentences[0].print_dependencies()
If you encounter , please try to use a proxy:>>> import stanza
>>> proxies = {'http': 'http://ip:port', 'https': 'http://ip:port'}
>>> stanza.download('en', proxies=proxies)  # This downloads the English models for the neural pipeline
>>> nlp = stanza.Pipeline('en')             # This sets up a default neural pipeline in English
>>> doc = nlp(""Barack Obama was born in Hawaii.  He was elected president in 2008."")
>>> doc.sentences[0].print_dependencies()
The last command will print out the words in the first sentence in the input string (or , as it is represented in Stanza), as well as the indices for the word that governs it in the Universal Dependencies parse of that sentence (its ""head""), along with the dependency relation between the words. The output should look like:('Barack', '4', 'nsubj:pass')
('Obama', '1', 'flat')
('was', '4', 'aux:pass')
('born', '0', 'root')
('in', '6', 'case')
('Hawaii', '4', 'obl')
('.', '4', 'punct')
See  for more details.Accessing Java Stanford CoreNLP softwareAside from the neural pipeline, this package also includes an official wrapper for accessing the Java Stanford CoreNLP software with Python code.There are a few initial setup steps.We provide  in our documentation that show how one can use CoreNLP through Stanza and extract various annotations from it.Online Colab NotebooksTo get your started, we also provide interactive Jupyter notebooks in the  folder. You can also open these notebooks and run them interactively on . To view all available notebooks, follow these steps:Trained Models for the Neural PipelineWe currently provide models for all of the  treebanks v2.8, as well as NER models for a few widely-spoken languages. You can find instructions for downloading and using these models .Batching To Maximize Pipeline SpeedTo maximize speed performance, it is essential to run the pipeline on batches of documents. Running a for loop on one sentence at a time will be very slow. The best approach at this time is to concatenate documents together, with each document separated by a blank line (i.e., two line breaks ).  The tokenizer will recognize blank lines as sentence breaks. We are actively working on improving multi-document processing.Training your own neural pipelinesAll neural modules in this library can be trained with your own data. The tokenizer, the multi-word token (MWT) expander, the POS/morphological features tagger, the lemmatizer and the dependency parser require  formatted data, while the NER model requires the BIOES format. Currently, we do not support model training via the  interface. Therefore, to train your own models, you need to clone this git repository and run training from the source.For detailed step-by-step guidance on how to train and evaluate your own models, please visit our .LICENSEStanza is released under the Apache License, Version 2.0. See the  file for more details."
https://github.com/Project-MONAI/MONAI,AI Toolkit for Healthcare Imaging,"Medical Open Network for AIMONAI is a -based,  framework for deep learning in healthcare imaging, part of .Its ambitions are:FeaturesInstallationTo install , you can simply run:pip install monai
Please refer to  for other installation options.Getting Started and  are available on Colab.Examples and notebook tutorials are located at .Technical documentation is available at .CitationIf you have used MONAI in your research, please cite us! The citation can be exported from: https://arxiv.org/abs/2211.02701.Model Zoo is a place for researchers and data scientists to share the latest and great models from the community.Utilizing  makes it easy to  building workflows with MONAI.ContributingFor guidance on making a contribution to MONAI, see the .CommunityJoin the conversation on Twitter/X  or join our .Ask and answer questions over on .Links"
https://github.com/graphql-python/graphene,GraphQL framework for Python,"     We are looking for contributors! Please check the current issues to see how you can help ❤️Introduction is an opinionated Python library for building GraphQL schemas/types fast and easily.IntegrationsGraphene has multiple integrations with different frameworks:| integration       | Package                                                                                 || ----------------- | --------------------------------------------------------------------------------------- || SQLAlchemy        |            || Mongo             |                      || Apollo Federation |            || Django            |                    |Also, Graphene is fully compatible with the GraphQL spec, working seamlessly with all GraphQL clients, such as ,  and .InstallationTo install , just run this command in your shellpip install ""graphene>=3.1""
ExamplesHere is one example for you to get started:import graphene

class Query(graphene.ObjectType):
    hello = graphene.String(description='A typical hello world')

    def resolve_hello(self, info):
        return 'World'

schema = graphene.Schema(query=Query)
Then Querying  is as simple as:query = '''
    query SayHello {
      hello
    }
'''
result = schema.execute(query)
If you want to learn even more, you can also check the following :DocumentationDocumentation and links to additional resources are available athttps://docs.graphene-python.org/en/latest/ContributingAfter cloning this repo, create a  and ensure dependencies are installed by running:virtualenv venv
source venv/bin/activate
pip install -e "".[test]""
Well-written tests and maintaining good test coverage is important to this project. While developing, run new and existing tests with:pytest graphene/relay/tests/test_node.py # Single file
pytest graphene/relay # All tests in directory
Add the  flag if you have introduced breakpoints into the code for debugging.Add the  (""verbose"") flag to get more detailed test output. For even more detailed output, use .Check out the  for more options and test running controls.Regularly ensure your  hooks are up to date and enabled:pre-commit install
You can also run the benchmarks with:pytest graphene --benchmark-only
Graphene supports several versions of Python. To make sure that changes do not break compatibility with any of those versions, we use  to create virtualenvs for each Python version and run tests with that version. To run against all Python versions defined in the  config file, just run:tox
If you wish to run against a specific version defined in the  file:tox -e py39
Tox can only use whatever versions of Python are installed on your system. When you create a pull request, GitHub Actions pipelines will also be running the same tests and report the results, so there is no need for potential contributors to try to install every single version of Python on their own system ahead of time. We appreciate opening issues and pull requests to make graphene even more stable & useful!Building DocumentationThe documentation is generated using the excellent  and a custom theme.An HTML version of the documentation is produced by running:make docs
"
https://github.com/brython-dev/brython,Brython (Browser Python) is an implementation of Python 3 running in the browser,"brythonBrython (Browser Python) is an implementation of Python 3 running in thebrowser, with an interface to the DOM elements and events.Here is a simple example of an HTML page running Python:<html>

    <head>
        <script type=""text/javascript"" src=""/path/to/brython.js""></script>
    </head>

    <body>

        <script type=""text/python"">
        from browser import document, alert

        def echo(event):
            alert(document[""zone""].value)

        document[""mybutton""].bind(""click"", echo)
        </script>

        <input id=""zone""><button id=""mybutton"">click !</button>

    </body>

</html>
To use Brython, all there is to do is:Main featuresBrython supports the syntax of ,including comprehensions, generators, metaclasses, imports, etc.and many modules of the CPython distribution.Since version 3.8.0, Brython implements the Python version of the same major /minor version number.It includes libraries to interact with DOM elements and events,and with existing Javascript libraries such as jQuery, D3, Highcharts, Raphael etc.It supports the latest specs of HTML5/CSS3, and can use CSS Frameworks likeBootstrap3, LESS, SASS etc.Getting startedZero install !The most simple way to get started, without anything to install, is to use thedistribution available online through .You can choose the latest stable release :<script type=""text/javascript""
    src=""https://cdn.jsdelivr.net/npm/brython@3.12.0/brython.min.js"">
</script>
The previous code will allow you to use raw python code, but if you importmodules from the standard library you have to load a single javascript filewith the :<script type=""text/javascript""
    src=""https://cdn.jsdelivr.net/npm/brython@3.12.0/brython_stdlib.js"">
</script>
jsDelivr supports version ranges, so if you want the latest of the3.12.x versions:<script type=""text/javascript""
    src=""https://cdn.jsdelivr.net/npm/brython@3.11/brython.min.js"">
</script>
<script type=""text/javascript""
    src=""https://cdn.jsdelivr.net/npm/brython@3.11/brython_stdlib.js"">
</script>
or the latest of the 3.x.y versions:<script type=""text/javascript""
    src=""https://cdn.jsdelivr.net/npm/brython@3/brython.min.js"">
</script>
<script type=""text/javascript""
    src=""https://cdn.jsdelivr.net/npm/brython@3/brython_stdlib.js"">
</script>
If you want to use the latest development version, you can load these scriptsinstead:<script src=""https://raw.githack.com/brython-dev/brython/master/www/src/brython.js""
        crossorigin=""anonymous"">
</script>
<script src=""https://raw.githack.com/brython-dev/brython/master/www/src/brython_stdlib.js""
        crossorigin=""anonymous"">
</script>
Local installTo install Brython locally, if you have a CPython distribution with  :pip install brython
then create a new directory and runbrython-cli install
or by loading the latest version of the Brython zip file from the.In both cases, the distribution includes brython.js (the core Brython engine)and brython_stdlib.js (a bundle of all the files in the standard distribution).It also includes the page demo.html that shows a few examples of how youcan interact with a web page using Python as the scripting language : createnew elements, access and modify existing elements, create graphics, animations,send Ajax requests, etc.Test Brython onlineIf you want to test Brython online you can visit the following:Gallery of examplesThere is a where you can see simple and advanced examples using vanilla Brython orinteracting with Javascript libraries.DocumentationYou can start by reading the official .Full documentation is available on the .You can read the docs in  and.Curious about  ?A explains how to build Android applications with Brython.Community (questions, feedback, issues, new features, ...)You can subscribe and post to the.If you find a bug/issue or do you want to see a new feature in Brython, please,.If you want to contribute to Brython, please read the .Thank you"
https://github.com/tensortrade-org/tensortrade,"An open source reinforcement learning framework for training, evaluating, and deploying robust trading agents.","﻿# TensorTrade is still in Beta, meaning it should be used very cautiously if used in production, as it may contain bugs.TensorTrade is an open source Python framework for building, training, evaluating, and deploying robust trading algorithms using reinforcement learning. The framework focuses on being highly composable and extensible, to allow the system to scale from simple trading strategies on a single CPU, to complex investment strategies run on a distribution of HPC machines.Under the hood, the framework uses many of the APIs from existing machine learning libraries to maintain high quality data pipelines and learning models. One of the main goals of TensorTrade is to enable fast experimentation with algorithmic trading strategies, by leveraging the existing tools and pipelines provided by , , , , and .Every piece of the framework is split up into re-usable components, allowing you to take advantage of the general use components built by the community, while keeping your proprietary features private. The aim is to simplify the process of testing and deploying robust trading agents using deep reinforcement learning, to allow you and I to focus on creating profitable strategies.The goal of this framework is to enable fast experimentation, while maintaining production-quality data pipelines.Read .Guiding principlesInspired by Getting StartedYou can get started testing on Google Colab or your local machine, by viewing our InstallationTensorTrade requires Python >= 3.7 for all functionality to work as expected.You can install TensorTrade both as a pre-packaged solution by running the default setup command.pip install tensortrade
You can then alternatively install TensorTrade directly from the master code repository, pulling directly from the latest commits. This will give you the latest features\fixes, but it is highly untested code, so proceed at your own risk.pip install git+https://github.com/tensortrade-org/tensortrade.git
Alternatively you can clone\download the repository in your local environment an manually install the requirements, either the ""base"" ones, or the ones that also include requirements to run the examples in the documentation.pip install -r requirements.txt
pip install -r examples/requirements.txt
DockerTo run the commands below, ensure Docker is installed. Visit https://docs.docker.com/install/ for more information.Run Jupyter NotebooksTo run a jupyter notebook in your browser, execute the following command and visit the  link printed to the command line.make run-notebook
Build DocumentationTo build the HTML documentation, execute the following command.make run-docs
Run Test SuiteTo run the test suite, execute the following command.make run-tests
SupportYou can ask questions and join the development discussion:You can also post bug reports and feature requests in . Make sure to read  first.ContributorsContributions are encouraged and welcomed. This project is meant to grow as the community around it grows. Let me know on Discord in the #suggestions channel if there is anything that you would like to see in the future, or if there is anything you feel is missing.Working on your first Pull Request? You can learn how from this free series "
https://github.com/facebookresearch/detectron2,"Detectron2 is a platform for object detection, segmentation and other visual recognition tasks.","Detectron2 is Facebook AI Research's next generation librarythat provides state-of-the-art detection and segmentation algorithms.It is the successor ofand .It supports a number of computer vision research projects and production applications in Facebook.Learn More about Detectron2Explain Like I’m 5: Detectron2            |  Using Machine Learning with Detectron2:-------------------------:|:-------------------------:  |  What's NewSee our to see more demos and learn about detectron2.InstallationSee .Getting StartedSee ,and the to learn about basic usage.Learn more at our .And see  for some projects that are built on top of detectron2.Model Zoo and BaselinesWe provide a large set of baseline results and trained models available for download in the .LicenseDetectron2 is released under the .Citing Detectron2If you use Detectron2 in your research or wish to refer to the baseline results published in the , please use the following BibTeX entry.@misc{wu2019detectron2,
  author =       {Yuxin Wu and Alexander Kirillov and Francisco Massa and
                  Wan-Yen Lo and Ross Girshick},
  title =        {Detectron2},
  howpublished = {\url{https://github.com/facebookresearch/detectron2}},
  year =         {2019}
}
"
https://github.com/hudson-and-thames/mlfinlab,"MlFinLab helps portfolio managers and traders who want to leverage the power of machine learning by providing reproducible, interpretable, and easy to use tools. ","Welcome to Machine Learning Financial Laboratory!What is MlFinLab?MlFinlab python library is a perfect toolbox that every financial machine learning researcher needs. It covers every step of the ML strategy creation, starting from data structures generation and finishing with backtest statistics.We pride ourselves in the robustness of our codebase - every line of code existing in the modules is extensively tested anddocumented.Documentation, Example Notebooks and Lecture VideosFor every technique present in the library we not only provide extensive documentation, with both theoretical explanationsand detailed descriptions of available functions, but also supplement the modules with ever-growing array of lecture videos and slideson the implemented methods.We want you to be able to use the tools right away. To achieve that, every module comes with a number of example notebookswhich include detailed examples of the usage of the algorithms. Our goal is to show you the whole pipeline, starting fromimporting the libraries and ending with strategy performance metrics so you can get the added value from the get-go.Included modules:Licensing optionsThis project is licensed under an all rights reserved .CommunityWith the purchase of the library, our clients get access to the Hudson & Thames Slack community, where our engineers and other quantsare always ready to answer your questions.Alternatively, you can email us at: research@hudsonthames.org.Who is Hudson & Thames?Hudson and Thames Quantitative Research is a company with the goal of bridging the gap between the advanced research developed inquantitative finance and its practical application. We have created three premium python libraries so you can effortlessly access thelatest techniques and focus on what matters most: creating your own winning strategy.What was only possible with the help of huge R&D teams is now at your disposal, anywhere, anytime."
https://github.com/sfyc23/EverydayWechat,微信助手：1.每日定时给好友（女友）发送定制消息。2.机器人自动回复好友。3.群助手功能（例如：查询垃圾分类、天气、日历、电影实时票房、快递物流、PM2.5等）,"                                    是基于 Python3 与  的微信小工具。可以定时给朋友或者群聊发送每日天气、提醒、每日一句，也可以智能自动回复好友信息。还有群助手功能，让你在好友群中装 X。操作简单，小白用户也可快速上手。  本项目依赖于网页版微信进行开发，如登录不了，则无法使用此项目；且无任何解决办法。本项目依赖于网页版微信进行开发，如登录不了，则无法使用此项目；且无任何解决办法。本项目依赖于网页版微信进行开发，如登录不了，则无法使用此项目；且无任何解决办法。网页版微信地址：。  在北京地区求一份 Python & Android 的工作岗位。加微信：。最近搞了一个类似，但功能不一样的项目：。欢迎大家 star。禁止将本工具用于商业用途，如产生法律纠纷与本人无关。           功能说明相关数据来源天气信息：每日一句：人工智能机器人星座运势万年历票房数据：垃圾分类查询：空气质量PM2.5查询：项目配置目前项目所有的配置都是在 [<marko.inline.RawText object at 0x000001592FD99EC8>] 文件中。配置文件请严格遵循 yaml 语法格式，yaml 学习地址:配置自动回复机器人。1. 开启自动回复2.选择渠道机器人渠道（1: 图灵机器人，2: 一个AI ,3 : 青云客，4 腾讯智能闲聊，5:天行机器人，6：海知智能，7：思知机器人)
bot_channel: 7
3. 指定自动回复的好友名单有两种模式 (1) 不使用自动回复所有好友的情况下，即：is_auto_reply_all：False 时。这时设置可回复的白名单成员，如下：  is_auto_reply_all：False
# 指定自动回复的好友名单。
auto_reply_white_list:
  - '好友1'
  - '好友2'
(2) 开启自动回复所有好友的情况下，即：is_auto_reply_all：True 时。选择不自动回复黑名单成员：如下  is_auto_reply_all：True
auto_reply_black_list:
    - '好友1'
    - '好友2'
4. 配置相关器人除了青云客之外，其他的机器人都需要去对应的官网，注册并获取相应的 key。需要哪个就配置哪个。I.图灵机器人turing_conf:
  apiKey: '你所获取apikey'
II.天行机器人txapi_conf:
  app_key: '个人中心中的key'
  reply_name: '宝宝' # 回复的人的名字(可空)（也可在个人中心->机器人管理 修改）
  bot_name: '老公' # 机器人的名字（可空）
III.智能闲聊（腾讯）qqnlpchat_conf:
    app_id: '你申请的api_id'
    app_key: '你申请的app_key'
IV.配置「一个AI」打开图灵机器人官网： 进行注册。创建应用，得到「API密钥」中的 「客户端访问令牌」将填入到 _config.yaml 文件中的：  yigeai_conf:
  client_token: '客户访问令牌'
V.配置「思知机器人」打开思知官网： 进行注册。创建机器人，得到 appid。将填入到 _config.yaml 文件中的：  ownthink_conf:
    app_key: '你申请的api_id'
关于自动回复，目前可以公开的情报：  配置定时提醒1.开启并设置提醒时间alarm_info:
  is_alarm: True
2.填写需要发送的好友信息填写好友信息，例如：alarm_timed:
  - ""9:00""
  - ""12:30""
  - ""22:00""
wechat_name:
  - '文件传输助手'
  - '诗风'
group_name:
  - 'EverydayWechat 交流群'
is_tomorrow: False
city_name: '桂林'
dictum_channel : 3
start_date: '2017-10-10'
start_date_msg: '爱你的第{}天'
calendar: True
horescope: ""处女座""
sweet_words: '你脚下的蚂蚁'

相关参数说明：| 名称 | 示例       | 必填 | 说明 || -------- | -------------- | ---------- |---------- || wechat_name | '老婆' | 选填 | 好友名：可填多人。好友微信昵称或者备注名（不能输入微信号）|| alarm_timed | '9：30' | 必填 | 定时时间，可填多个 || alarm_jitter | 300 | 可空 | 定时时间的前后300秒内随机发送 || group_name | '交流群' | 选填 | 群聊名称，可填多个。必须要把需要的群聊保存到通讯录。|| is_tomorrow | True | 可空 | 是否发送明日信息（如天气，星座，万年历）。|| city_name | '成都' | 可空 | 城市名：朋友所在城市，用于发送天气。 || air_quality_city | '成都' | 可空 | 空气质量 PM25 的城市。 || dictum_channel | 2 | 可空 | 格言渠道（见下表）|| start_date | '2017-10-10' | 可空 | 相识日期：计算到当天的天数 。 || start_date_msg | '爱你的第{}天' | 可空 | 相识日期文案 || sweet_words | '来自你俊美的老公' | 可空 | 甜密的后缀。（钢铁直男的直描）|| horescope | '处女座' | 可空 | 星座名或好友生日。用于发送星座运势 || calendar | True | 可空 | 万年历信息 |wechat_name，group_name 至少要有一个。  格言渠道 ： 1 : ONE●一个，2 : 词霸（每日双语），3: 土味情话， 4 : 一言，5：笑话，6: 民国情书，7: 彩虹屁。    当然，你也可以根据自己的需求，设置另一套不同的方案。具体参考代码。  一例提醒：  2019-06-29 星期六 农历五月廿七 
【宜】嫁娶,祭祀,沐浴,扫舍,修饰垣墙 
【忌】行丧,安葬 
桂林天气预报 
【今日天气】阵雨
【今日温度】低温 26.0℃,高温 33.0℃ 
【今日风速】南风<3级
【出行提示】阵雨来袭，出门记得带伞 
【桂林PM2.5】142 轻度污染
处女座今日运势 
【幸运颜色】2
【幸运数字】薄荷绿
【综合运势】今天的你有机会重逢旧同学、旧朋友，对方会为你带来一些小惊喜，可能是某个不错的商机，也可能是某个消息。工作/学习上，今天的你目标性很强，能把当初奋斗的初心捡回来，重新出发。感情方面，有伴者今天要提防烂桃花的挑拨离间，多给对方一些信任。
你知道五氧化二磷被氧化前是什么样子嘛，什么样子？五二磷。 
宝贝这是我们在一起的第628天 
你脚下的蚂蚁
配置群助手功能直接放表格说明吧。| 名称 | 示例       | 必填 | 说明 || -------- | -------------- | ---------- |---------- || is_open | True | 必填 | 是否开启群助手功能 || is_all | True | 必填 | 是否对所有群开启。当开启时，只有黑名单的名单才不受影响（慎重开启！） || group_name_white_list | 「群名」 | 选填 | 白名单用户。当 is_all：Fase。只处理这个群里的消息|| group_name_black_list | 「群名」 | 选填 | 黑名单用户。当 is_all ：True 。这个群里的用户不受影响。|| is_at | True | 必填 | 艾特标记。只有当别人艾特自己时，才会处理消息（慎重关闭！）。 || is_auto_reply | True | 必填 | # 开启群自动回复（慎重开启！）|| is_weather | True| 必填 | 是否开启天气查询。 || is_calendar | True | 必填 | 是否开启万历年查询 || is_rubbish | True | 必填 | 是否开启垃圾查询 || is_moviebox | True | 必填 | 是否开启电脑票房查询 || is_express | True | 必填 | 是否开启快递信息查询 || is_air_quality | True | 必填 | 是否开启空气质量查询 |配置数据库（可不配置）首先得安装 mongodb 数据库安装。安装方式，请自行谷歌。官方的安装教程也有：  （1）将 is_open_db 设置成 「True」。（2）设置 host 与 port。tips: 没有特殊要求，或者对数据库不熟悉者不需要设置。安装首先，把 Python3 安装好，并配置好环境，个人建议新手安装 Anaconda，具体安装教程，可自行谷歌搜索~  直接下载此项目或 clone 项目到本地。  使用 pip 安装依赖:pip3 install -r requirements.txt
# 或者是使用 pip
# pip install -r requirements.txt
运行在本地 cmd 中跳转项目目录下，运行:  python run.py
第一次运行会跳出二维码，扫码登录。如输出日志中打印成：『登录成功』，则表示运行成功。登录成功后一段时间内再运行，微信会保持登录状态，不需要再扫码。如果需要切换用户，则在 _config.yaml 文件中，修改 is_forced_switch 的属性为 True。  示例截图：提  & 加群提问的建议。Credits 致谢本项目受以下项目启发，参考了其中一部分思路，向这些开发者表示感谢。  微信交流群因为人数已超 100 人，请加 wx: sfyc1314 机器人为好友，验证信息写填写：「github」！！！，机器人会自动通过。通过后回复：「加群」，会自动拉你入群。加群助手也已开源，地址：，欢迎大家 star。机器人二维码： 捐助如果您认为这个项目有帮助，不妨为它捐助一点钱？不管钱有多少，您的捐助将会激励作者持续开发新功能！🎉感谢您的支持！捐助方法如下：LICENSE"
https://github.com/CorentinJ/Real-Time-Voice-Cloning,Clone a voice in 5 seconds to generate arbitrary speech in real-time,"Real-Time Voice CloningThis repository is an implementation of  (SV2TTS) with a vocoder that works in real-time. This was my .SV2TTS is a deep learning framework in three stages. In the first stage, one creates a digital representation of a voice from a few seconds of audio. In the second and third stages, this representation is used as reference to generate speech given arbitrary text.Video demonstration (click the picture):Papers implemented| URL | Designation | Title | Implementation source || --- | ----------- | ----- | --------------------- || | SV2TTS | Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis | This repo || | WaveRNN (vocoder) | Efficient Neural Audio Synthesis |  || | Tacotron (synthesizer) | Tacotron: Towards End-to-End Speech Synthesis | | | GE2E (encoder)| Generalized End-To-End Loss for Speaker Verification | This repo |Heads upLike everything else in Deep Learning, this repo is quickly getting old. Many other open-source repositories or SaaS apps (often paying) will give you a better audio quality than this repository will. If you care about the fidelity of the voice you're cloning, and its expressivity, here are some personal recommendations of alternative voice cloning solutions:Setup1. Install Requirements2. (Optional) Download Pretrained ModelsPretrained models are now downloaded automatically. If this doesn't work for you, you can manually download them .3. (Optional) Test ConfigurationBefore you download any dataset, you can begin by testing your configuration with:If all tests pass, you're good to go.4. (Optional) Download DatasetsFor playing with the toolbox alone, I only recommend downloading . Extract the contents as  where  is a directory of your choosing. Other datasets are supported in the toolbox, see . You're free not to download any dataset, but then you will need your own data as audio files or you will have to record it with the toolbox.5. Launch the ToolboxYou can then try the toolbox:or  depending on whether you downloaded any datasets. If you are running an X-server or if you have the error , see ."
https://github.com/mesonbuild/meson,The Meson Build System,"StatusDependenciesLatest Meson version supporting previous Python versions:Installing from sourceMeson is available on , soit can be installed with .  The exact command totype to install with  can vary between systems, be sure to usethe Python 3 version of .If you wish you can install it locally with the standard Python command:python3 -m pip install meson
For builds using Ninja, Ninja can be downloaded directly from Ninjaor via python3 -m pip install ninja
More on Installing Meson build can be found at the.Creating a standalone scriptMeson can be run as a . To generate theexecutable run the following command:./packaging/create_zipapp.py --outfile meson.pyz --interpreter '/usr/bin/env python3' <source checkout>
RunningMeson requires that you have a source directory and a build directoryand that these two are different. In your source root must exist afile called . To generate the build system run thiscommand:Depending on how you obtained Meson the command might also be called instead of plain . In the rest of this document weare going to use the latter form.You can omit either of the two directories, and Meson will substitutethe current directory and autodetect what you mean. This allows you todo things like this:cd <source root>
meson setup builddir
To compile, cd into your build directory and type . To run unittests, type .More on running Meson build system commands can be found at theor by typing .ContributingWe love code contributions. See the  on the website fordetails.IRCThe channel to use is  either via Matrix () or .Further infoMore information about the Meson build system can be found at the.Meson is a registered trademark of [<marko.inline.RawText object at 0x000001592FDA4EC8>]."
https://github.com/idealo/imagededup,😎 Finding duplicate images made easy!,"Image Deduplicator (imagededup)imagededup is a python package that simplifies the task of finding exact and near duplicates in an image collection.This package provides functionality to make use of hashing algorithms that are particularly good at finding exactduplicates as well as convolutional neural networks which are also adept at finding near duplicates. An evaluationframework is also provided to judge the quality of deduplication for a given dataset.Following details the functionality provided by the package:Detailed documentation for the package can be found at: imagededup is compatible with Python 3.8+ and runs on Linux, MacOS X and Windows.It is distributed under the Apache 2.0 license.📖 Contents⚙️ InstallationThere are two ways to install imagededup:pip install imagededup
git clone https://github.com/idealo/imagededup.git
cd imagededup
pip install ""cython>=0.29""
python setup.py install
🚀 Quick StartIn order to find duplicates in an image directory using perceptual hashing, following workflow can be used:from imagededup.methods import PHash
phasher = PHash()
encodings = phasher.encode_images(image_dir='path/to/image/directory')
duplicates = phasher.find_duplicates(encoding_map=encodings)
from imagededup.utils import plot_duplicates
plot_duplicates(image_dir='path/to/image/directory',
                duplicate_map=duplicates,
                filename='ukbench00120.jpg')
The output looks as below:The complete code for the workflow is:from imagededup.methods import PHash
phasher = PHash()

# Generate encodings for all images in an image directory
encodings = phasher.encode_images(image_dir='path/to/image/directory')

# Find duplicates using the generated encodings
duplicates = phasher.find_duplicates(encoding_map=encodings)

# plot duplicates obtained for a given file using the duplicates dictionary
from imagededup.utils import plot_duplicates
plot_duplicates(image_dir='path/to/image/directory',
                duplicate_map=duplicates,
                filename='ukbench00120.jpg')
It is also possible to use your own custom models for finding duplicates using the CNN method.For examples, refer  part of therepository.For more detailed usage of the package functionality, refer: ⏳ BenchmarksUpdate: Provided benchmarks are only valid upto . The next releases have significant changes to all methods, so the current benchmarks may not hold.Detailed benchmarks on speed and classification metrics for different methods have been provided in the .Generally speaking, following conclusions can be made:🤝 ContributeWe welcome all kinds of contributions.See the  guide for more details.📝 CitationPlease cite Imagededup in your publications if this is useful for your research. Here is an example BibTeX entry:@misc{idealods2019imagededup,
  title={Imagededup},
  author={Tanuj Jain and Christopher Lennan and Zubin John and Dat Tran},
  year={2019},
  howpublished={\url{https://github.com/idealo/imagededup}},
}
🏗 Maintainers© CopyrightSee  for details."
https://github.com/Kinto/kinto,A generic JSON document store with sharing and synchronisation capabilities.,Kinto|coc| |gitter| |readthedocs| |pypi| |ci| |master-coverage|.. |coc| image:: https://img.shields.io/badge/%E2%9D%A4-code%20of%20conduct-blue.svg:target: https://github.com/Kinto/kinto/blob/master/CODE_OF_CONDUCT.md:alt: Code of conduct.. |gitter| image:: https://badges.gitter.im/Kinto/kinto.svg:target: https://gitter.im/Kinto/kinto.. |ci| image:: https://github.com/Kinto/kinto/actions/workflows/test.yml/badge.svg:target: https://github.com/Kinto/kinto/actions.. |readthedocs| image:: https://readthedocs.org/projects/kinto/badge/?version=latest:target: https://kinto.readthedocs.io/en/latest/:alt: Documentation Status.. |master-coverage| image::https://coveralls.io/repos/Kinto/kinto/badge.svg?branch=master:alt: Coverage:target: https://coveralls.io/r/Kinto/kinto.. |pypi| image:: https://img.shields.io/pypi/v/kinto.svg:target: https://pypi.python.org/pypi/kintoKinto is a minimalist JSON storage service with synchronisation and sharing abilities.Requirements
https://github.com/encode/httpx,A next generation HTTP client for Python. 🦋,"HTTPX is a fully featured HTTP client library for Python 3. It includes an integrated, has support for both HTTP/1.1 and HTTP/2, and provides both sync.Install HTTPX using pip:$ pip install httpx
Now, let's get started:>>> import httpx
>>> r = httpx.get('https://www.example.org/')
>>> r
<Response [200 OK]>
>>> r.status_code
200
>>> r.headers['content-type']
'text/html; charset=UTF-8'
>>> r.text
'<!doctype html>\n<html>\n<head>\n<title>Example Domain</title>...'
Or, using the command-line client.$ pip install 'httpx[cli]'  # The command line client is an optional dependency.
Which now allows us to use HTTPX directly from the command-line...Sending a request...FeaturesHTTPX builds on the well-established usability of , and gives you:Plus all the standard features of ...InstallationInstall with pip:$ pip install httpx
Or, to include the optional HTTP/2 support, use:$ pip install httpx[http2]
HTTPX requires Python 3.8+.DocumentationProject documentation is available at .For a run-through of all the basics, head over to the .For more advanced topics, see the  section, the  section, or the  section.The  provides a comprehensive API reference.To find out about tools that integrate with HTTPX, see .ContributeIf you want to contribute with HTTPX check out the  to learn how to start.DependenciesThe HTTPX project relies on these excellent libraries:As well as these optional installs:A huge amount of credit is due to  for the API layout thatmuch of this work follows, as well as to  for plenty of designinspiration around the lower-level networking details."
https://github.com/awslabs/aws-shell,An integrated shell for working with the AWS CLI.,"aws-shell - The interactive productivity booster for the AWS CLI.. image:: https://aws-developer-blog-media.s3-us-west-2.amazonaws.com/cli/Super-Charge-Your-AWS-Command-Line-Experience-with-aws-shell/aws-shell-final.gifInstallationThe aws-shell requires python and _ to install.You can install the aws-shell using _::$ pip install aws-shell
If you are not installing into a virtualenv you can run::$ sudo pip install aws-shell
Mac OS X (10.11 El Capitan) users: There is a known issue with Apple andits included python package dependencies (more info athttps://github.com/pypa/pip/issues/3165).We are investigating ways to fix this issue but in the meantime,to install the aws-shell, you can run:Once you've installed the aws-shell, you can now run::$ aws-shell
To exit the shell, press .Upgrading the aws-shellIf you want to upgrade to the latest version of the aws-shell,you can run::$ pip install --upgrade aws-shell
You can also use this upgrade command whenever a new version of the AWS CLI isreleased that includes new services and API updates.  You will then beable to use these new services and API updates in the aws-shell.Supported Python VersionsThe aws-shell works on the same python versions supported by the AWS CLI:ConfigurationThe aws-shell uses the same configuration settings as the AWS CLI.If you've never used the AWS CLI before, the easiest way to getstarted is to run the  command::$ aws-shell
aws> configure
AWS Access Key ID [None]: your-access-key-id
AWS Secret Access Key [None]: your-secret-access-key
Default region name [None]: region-to-use (e.g us-west-2, us-west-1, etc).
Default output format [None]:
aws>
For more information about configure settings, see the_.Basic UsageThe aws-shell accepts the same commands as the AWS CLI, except you don'tneed to provide the  prefix.  For example, here are a few commandsyou can try::$ aws-shell
aws> ec2 describe-regions
{
    ""Regions"": [
        {
            ""Endpoint"": ""ec2.eu-west-1.amazonaws.com"",
            ""RegionName"": ""eu-west-1""
        },
        ...
aws> s3 ls
2015-12-07 15:03:34 bucket1
2015-12-07 15:03:34 bucket2
aws> dynamodb list-tables --output text
TABLENAMES     First
TABLENAMES     Second
TABLENAMES     Third
ProfilesThe aws-shell supports AWS CLI profiles.  You have two options to useprofiles.  First, you can provide a profile when you start the aws-shell::$ aws-shell --profile prod
aws>
When you do this all the server side completion as well as CLI commandsyou run will automatically use the  profile.You can also change the current profile while you're in the aws-shell::$ aws-shell
aws> .profile demo
Current shell profile changed to: demo
You can also check what profile you've configured in the aws-shell using::aws> .profile
Current shell profile: demo
After changing your profile using the  dot command, allserver side completion as well as CLI commands will automatically usethe new profile you've configured.FeaturesAuto Completion of Commands and OptionsThe aws-shell provides auto completion of commands andoptions as you type... image:: https://cloud.githubusercontent.com/assets/368057/11824078/784a613e-a32c-11e5-8ac5-f1d1873cc643.pngShorthand Auto CompletionThe aws-shell can also fill in an example of theshorthand syntax used for various AWS CLI options:.. image:: https://cloud.githubusercontent.com/assets/368057/11823453/e95d85da-a328-11e5-8b8d-67566eccf9e3.pngServer Side Auto CompletionThe aws-shell also leverages _, the AWS SDK for Python, to auto completeserver side resources such as Amazon EC2 instance Ids, Amazon Dynamodb tablenames, AWS IAM user names, Amazon S3 bucket names, etc.This feature is under active development.  The list of supported resourcescontinues to grow... image:: https://cloud.githubusercontent.com/assets/368057/11824022/3648b4fc-a32c-11e5-8e18-92f028eb1cee.pngFuzzy SearchingEvery auto completion value supports fuzzy searching.  This enables you tospecify the commands, options, and values you want to run with even lesstyping.  You can try typing:.. image:: https://cloud.githubusercontent.com/assets/368057/11823996/18e69d16-a32c-11e5-80a2-defbaa6a8a80.pngInline DocumentationThe aws-shell will automatically pull up documentation as you type commands.It will show inline documentation for CLI options.  There is also a separatedocumentation panel that will show documentation for the current command oroption you are typing. Pressing F9 will toggle focus to the documentation panelallowing you to navigate it using your selected keybindings... image:: https://cloud.githubusercontent.com/assets/368057/11823320/36ae9b04-a328-11e5-9661-81abfc0afe5a.pngFish-Style Auto SuggestionsThe aws-shell supports Fish-style auto-suggestions. Use the right arrow key tocomplete a suggestion... image:: https://cloud.githubusercontent.com/assets/368057/11822961/4bceff94-a326-11e5-87fa-c664e1e82be4.pngCommand HistoryThe aws-shell records the commands you run and writes them to.  You can use the up and down arrow keys to scrollthrough your history... image:: https://cloud.githubusercontent.com/assets/368057/11823211/b5851e9a-a327-11e5-877f-687dc1f90e27.pngToolbar OptionsThe aws-shell has a bottom toolbar that provides several options:As you toggle options in the toolbar, your preferences are persistedto the  file so that the next time you runthe aws-shell, your preferences will be restored... image:: https://cloud.githubusercontent.com/assets/368057/11823907/8c3f1e60-a32b-11e5-9f99-fe504ea0a5dc.pngDot CommandsThe aws-shell provides additional commands specific to the aws-shell.The commands are available by adding the  prefix before a command.Exiting the ShellYou can run the ``.exit`` or ``.quit`` commands to exit the shell.

Creating Shell Scripts with .edit
There are times when you may want to take a sequence of commandsyou've run in the aws-shell and combine them into a shell script.In addition to the command history that's persisted to thehistory file, the aws-shell also keeps track of all the commandsyou've run since you first started your aws-shell session.You can run the  command to open all these commands inan editor.  The aws-shell will use the  environmentvariable before defaulting to  on Windows and on other platforms.::aws> ec2 describe-instances
aws> dynamodb list-tables
aws> .edit
Changing Profiles with .profile
You can change the current AWS CLI profile used by the aws-shell
by using the ``.profile`` dot command.  If you run the ``.profile``
command with no arguments, the currently configured shell profile
will be printed.

::

    aws> .profile demo
    Current shell profile changed to: demo
    aws> .profile
    Current shell profile: demo


.cd
~~~

You can change the current working directory of the aws-shell by using
the ``.cd`` command::

    aws> !pwd
    /usr
    aws> .cd /tmp
    aws> !pwd
    /tmp


Executing Shell Commands
------------------------

The aws-shell integrates with other commands in several ways.
First, you can pipe AWS CLI commands to other processes as well
as redirect output to a file::

    aws> dynamodb list-tables --output text | head -n 1
    TABLENAMES     First
    aws> dynamodb list-tables --output text > /tmp/foo.txt

Second, if you want to run a shell command rather than an AWS CLI
command, you can add the ``!`` prefix to your command::

    aws> !ls /tmp/
    foo.txt                                    bar.txt

Developer Preview Status
========================

The aws-shell is currently in developer preview.
We welcome feedback, feature requests, and bug reports.
There may be backwards incompatible changes made in order
to respond to customer feedback as we continue to iterate
on the aws-shell.


More Information
================

Below are miscellaneous links for more information:

* `AWS CLI Reference Docs`_
* `AWS CLI User Guide`_
* `AWS CLI Blog`_
* `AWS CLI Github Repo`_

.. _pip: http://www.pip-installer.org/en/latest/
.. _AWS CLI Getting Started Guide: http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html
.. _boto3: https://github.com/boto/boto3
.. _AWS CLI Reference Docs: http://docs.aws.amazon.com/cli/latest/reference/
.. _AWS CLI User Guide: http://docs.aws.amazon.com/cli/latest/userguide/
.. _AWS CLI Blog: https://blogs.aws.amazon.com/cli/
.. _AWS CLI Github Repo: https://github.com/aws/aws-cli
"
https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10,How to train a TensorFlow Object Detection Classifier for multiple object detection on Windows,"How To Train an Object Detection Classifier for Multiple Objects Using TensorFlow (GPU) on Windows 10Brief SummaryLast updated: 6/22/2019 with TensorFlow v1.13.1This repository is a tutorial for how to use TensorFlow's Object Detection API to train an object detection classifier for multiple objects on Windows 10, 8, or 7. (It will also work on Linux-based OSes with some minor changes.) It was originally written using TensorFlow version 1.5, but will also work for newer versions of TensorFlow.Translated versions of this guide are listed below. If you would like to contribute a translation in another language, please feel free! You can add it as a pull request and I will merge it when I get the chance.I also made a YouTube video that walks through this tutorial. Any discrepancies between the video and this written tutorial are due to updates required for using newer versions of TensorFlow. If there are differences between this written tutorial and the video, follow the written tutorial!This readme describes every step required to get going with your own object detection classifier: The repository provides all the files needed to train a ""Pinochle Deck"" playing card detector that can accurately detect nines, tens, jacks, queens, kings, and aces. The tutorial describes how to replace these files with your own files to train a detection classifier for whatever your heart desires. It also has Python scripts to test your classifier out on an image, video, or webcam feed.IntroductionThe purpose of this tutorial is to explain how to train your own convolutional neural network object detection classifier for multiple objects, starting from scratch. At the end of this tutorial, you will have a program that can identify and draw boxes around specific objects in pictures, videos, or in a webcam feed.There are several good tutorials available for how to use TensorFlow’s Object Detection API to train a classifier for a single object. However, these usually assume you are using a Linux operating system. If you’re like me, you might be a little hesitant to install Linux on your high-powered gaming PC that has the sweet graphics card you’re using to train a classifier. The Object Detection API seems to have been developed on a Linux-based OS. To set up TensorFlow to train a model on Windows, there are several workarounds that need to be used in place of commands that would work fine on Linux. Also, this tutorial provides instructions for training a classifier that can detect multiple objects, not just one.The tutorial is written for Windows 10, and it will also work for Windows 7 and 8. The general procedure can also be used for Linux operating systems, but file paths and package installation commands will need to change accordingly. I used TensorFlow-GPU v1.5 while writing the initial version of this tutorial, but it will likely work for future versions of TensorFlow.TensorFlow-GPU allows your PC to use the video card to provide extra processing power while training, so it will be used for this tutorial. In my experience, using TensorFlow-GPU instead of regular TensorFlow reduces training time by a factor of about 8 (3 hours to train instead of 24 hours). The CPU-only version of TensorFlow can also be used for this tutorial, but it will take longer. If you use CPU-only TensorFlow, you do not need to install CUDA and cuDNN in Step 1. Steps1. Install Anaconda, CUDA, and cuDNNAnaconda is a software toolkit that creates virtual Python environments so you can install and use Python libraries without worrying about creating version conflicts with existing installations. Anaconda works well on Windows, and enables you to use many Python libraries that normally would only work on a Linux system. It provides a simple method for installing TensorFlow (which we'll do in Step 2d). It also automatically installs the CUDA and cuDNN versions you need for using TensorFlow on a GPU.Download Anaconda for Windows from  (you have to scroll down a ways to get to the download links). Once it's downloaded, execute the installer file and work through the installation steps.If you are using a version of TensorFlow older than TF v1.13, make sure you use the CUDA and cuDNN versions that are compatible with the TensorFlow version you are using.  is a table showing which version of TensorFlow requires which versions of CUDA and cuDNN. Anaconda will automatically install the correct version of CUDA and cuDNN for the version of TensorFlow you are using, so you shouldn't have to worry about this.2. Set up TensorFlow Directory and Anaconda Virtual EnvironmentThe TensorFlow Object Detection API requires using the specific directory structure provided in its GitHub repository. It also requires several additional Python packages, specific additions to the PATH and PYTHONPATH variables, and a few extra setup commands to get everything set up to run or train an object detection model. This portion of the tutorial goes over the full set up required. It is fairly meticulous, but follow the instructions closely, because improper setup can cause unwieldy errors down the road.2a. Download TensorFlow Object Detection API repository from GitHubCreate a folder directly in C: and name it “tensorflow1”. This working directory will contain the full TensorFlow object detection framework, as well as your training images, training data, trained classifier, configuration files, and everything else needed for the object detection classifier.Download the full TensorFlow object detection repository located at https://github.com/tensorflow/models by clicking the “Clone or Download” button and downloading the zip file. Open the downloaded zip file and extract the “models-master” folder directly into the C:\tensorflow1 directory you just created. Rename “models-master” to just “models”.Note: The TensorFlow models repository's code (which contains the object detection API) is continuously updated by the developers. Sometimes they make changes that break functionality with old versions of TensorFlow. It is always best to use the latest version of TensorFlow and download the latest models repository. If you are not using the latest version, clone or download the commit for the version you are using as listed in the table below.If you are using an older version of TensorFlow, here is a table showing which GitHub commit of the repository you should use. I generated this by going to the release branches for the models repository and getting the commit before the last commit for the branch. (They remove the research folder as the last commit before they create the official version release.)| TensorFlow version | GitHub Models Repository Commit ||--------------------|---------------------------------||TF v1.7             |https://github.com/tensorflow/models/tree/adfd5a3aca41638aa9fb297c5095f33d64446d8f ||TF v1.8             |https://github.com/tensorflow/models/tree/abd504235f3c2eed891571d62f0a424e54a2dabc ||TF v1.9             |https://github.com/tensorflow/models/tree/d530ac540b0103caa194b4824af353f1b073553b ||TF v1.10            |https://github.com/tensorflow/models/tree/b07b494e3514553633b132178b4c448f994d59df ||TF v1.11            |https://github.com/tensorflow/models/tree/23b5b4227dfa1b23d7c21f0dfaf0951b16671f43 ||TF v1.12            |https://github.com/tensorflow/models/tree/r1.12.0 ||TF v1.13            |https://github.com/tensorflow/models/tree/r1.13.0 ||Latest version      |https://github.com/tensorflow/models |This tutorial was originally done using TensorFlow v1.5 and this  of the TensorFlow Object Detection API. If portions of this tutorial do not work, it may be necessary to install TensorFlow v1.5 and use this exact commit rather than the most up-to-date version.2b. Download the Faster-RCNN-Inception-V2-COCO model from TensorFlow's modelTensorFlow provides several object detection models (pre-trained classifiers with specific neural network architectures) in its . Some models (such as the SSD-MobileNet model) have an architecture that allows for faster detection but with less accuracy, while some models (such as the Faster-RCNN model) give slower detection but with more accuracy. I initially started with the SSD-MobileNet-V1 model, but it didn’t do a very good job identifying the cards in my images. I re-trained my detector on the Faster-RCNN-Inception-V2 model, and the detection worked considerably better, but with a noticeably slower speed.You can choose which model to train your objection detection classifier on. If you are planning on using the object detector on a device with low computational power (such as a smart phone or Raspberry Pi), use the SDD-MobileNet model. If you will be running your detector on a decently powered laptop or desktop PC, use one of the RCNN models. This tutorial will use the Faster-RCNN-Inception-V2 model.  Open the downloaded faster_rcnn_inception_v2_coco_2018_01_28.tar.gz file with a file archiver such as WinZip or 7-Zip and extract the faster_rcnn_inception_v2_coco_2018_01_28 folder to the C:\tensorflow1\models\research\object_detection folder. (Note: The model date and version will likely change in the future, but it should still work with this tutorial.)2c. Download this tutorial's repository from GitHubDownload the full repository located on this page (scroll to the top and click Clone or Download) and extract all the contents directly into the C:\tensorflow1\models\research\object_detection directory. (You can overwrite the existing ""README.md"" file.) This establishes a specific directory structure that will be used for the rest of the tutorial. At this point, here is what your \object_detection folder should look like:This repository contains the images, annotation data, .csv files, and TFRecords needed to train a ""Pinochle Deck"" playing card detector. You can use these images and data to practice making your own Pinochle Card Detector. It also contains Python scripts that are used to generate the training data. It has scripts to test out the object detection classifier on images, videos, or a webcam feed. You can ignore the \doc folder and its files; they are just there to hold the images used for this readme.If you want to practice training your own ""Pinochle Deck"" card detector, you can leave all the files as they are. You can follow along with this tutorial to see how each of the files were generated, and then run the training. You will still need to generate the TFRecord files (train.record and test.record) as described in Step 4. You can also download the frozen inference graph for my trained Pinochle Deck card detector  and extract the contents to \object_detection\inference_graph. This inference graph will work ""out of the box"". You can test it after all the setup instructions in Step 2a - 2f have been completed by running the Object_detection_image.py (or video or webcam) script.If you want to train your own object detector, delete the following files (do not delete the folders):Now, you are ready to start from scratch in training your own object detector. This tutorial will assume that all the files listed above were deleted, and will go on to explain how to generate the files for your own training dataset.2d. Set up new Anaconda virtual environmentNext, we'll work on setting up a virtual environment in Anaconda for tensorflow-gpu. From the Start menu in Windows, search for the Anaconda Prompt utility, right click on it, and click “Run as Administrator”. If Windows asks you if you would like to allow it to make changes to your computer, click Yes.In the command terminal that pops up, create a new virtual environment called “tensorflow1” by issuing the following command:C:\> conda create -n tensorflow1 pip python=3.5
Then, activate the environment and update pip by issuing:C:\> activate tensorflow1

(tensorflow1) C:\>python -m pip install --upgrade pip
Install tensorflow-gpu in this environment by issuing:(tensorflow1) C:\> pip install --ignore-installed --upgrade tensorflow-gpu
Since we're using Anaconda, installing tensorflow-gpu will also automatically download and install the correct versions of CUDA and cuDNN.(Note: You can also use the CPU-only version of TensorFow, but it will run much slower. If you want to use the CPU-only version, just use ""tensorflow"" instead of ""tensorflow-gpu"" in the previous command.)Install the other necessary packages by issuing the following commands:(tensorflow1) C:\> conda install -c anaconda protobuf
(tensorflow1) C:\> pip install pillow
(tensorflow1) C:\> pip install lxml
(tensorflow1) C:\> pip install Cython
(tensorflow1) C:\> pip install contextlib2
(tensorflow1) C:\> pip install jupyter
(tensorflow1) C:\> pip install matplotlib
(tensorflow1) C:\> pip install pandas
(tensorflow1) C:\> pip install opencv-python
(Note: The ‘pandas’ and ‘opencv-python’ packages are not needed by TensorFlow, but they are used in the Python scripts to generate TFRecords and to work with images, videos, and webcam feeds.)2e. Configure PYTHONPATH environment variableA PYTHONPATH variable must be created that points to the \models, \models\research, and \models\research\slim directories. Do this by issuing the following commands (from any directory):(tensorflow1) C:\> set PYTHONPATH=C:\tensorflow1\models;C:\tensorflow1\models\research;C:\tensorflow1\models\research\slim
(Note: Every time the ""tensorflow1"" virtual environment is exited, the PYTHONPATH variable is reset and needs to be set up again. You can use ""echo %PYTHONPATH% to see if it has been set or not.)2f. Compile Protobufs and run setup.pyNext, compile the Protobuf files, which are used by TensorFlow to configure model and training parameters. Unfortunately, the short protoc compilation command posted on TensorFlow’s Object Detection API  does not work on Windows. Every  .proto file in the \object_detection\protos directory must be called out individually by the command.In the Anaconda Command Prompt, change directories to the \models\research directory:(tensorflow1) C:\> cd C:\tensorflow1\models\research
Then copy and paste the following command into the command line and press Enter:protoc --python_out=. .\object_detection\protos\anchor_generator.proto .\object_detection\protos\argmax_matcher.proto .\object_detection\protos\bipartite_matcher.proto .\object_detection\protos\box_coder.proto .\object_detection\protos\box_predictor.proto .\object_detection\protos\eval.proto .\object_detection\protos\faster_rcnn.proto .\object_detection\protos\faster_rcnn_box_coder.proto .\object_detection\protos\grid_anchor_generator.proto .\object_detection\protos\hyperparams.proto .\object_detection\protos\image_resizer.proto .\object_detection\protos\input_reader.proto .\object_detection\protos\losses.proto .\object_detection\protos\matcher.proto .\object_detection\protos\mean_stddev_box_coder.proto .\object_detection\protos\model.proto .\object_detection\protos\optimizer.proto .\object_detection\protos\pipeline.proto .\object_detection\protos\post_processing.proto .\object_detection\protos\preprocessor.proto .\object_detection\protos\region_similarity_calculator.proto .\object_detection\protos\square_box_coder.proto .\object_detection\protos\ssd.proto .\object_detection\protos\ssd_anchor_generator.proto .\object_detection\protos\string_int_label_map.proto .\object_detection\protos\train.proto .\object_detection\protos\keypoint_box_coder.proto .\object_detection\protos\multiscale_anchor_generator.proto .\object_detection\protos\graph_rewriter.proto .\object_detection\protos\calibration.proto .\object_detection\protos\flexible_grid_anchor_generator.proto
This creates a name_pb2.py file from every name.proto file in the \object_detection\protos folder.(Note: TensorFlow occassionally adds new .proto files to the \protos folder. If you get an error saying ImportError: cannot import name 'something_something_pb2' , you may need to update the protoc command to include the new .proto files.)Finally, run the following commands from the C:\tensorflow1\models\research directory:(tensorflow1) C:\tensorflow1\models\research> python setup.py build
(tensorflow1) C:\tensorflow1\models\research> python setup.py install
2g. Test TensorFlow setup to verify it worksThe TensorFlow Object Detection API is now all set up to use pre-trained models for object detection, or to train a new one. You can test it out and verify your installation is working by launching the object_detection_tutorial.ipynb script with Jupyter. From the \object_detection directory, issue this command:(tensorflow1) C:\tensorflow1\models\research\object_detection> jupyter notebook object_detection_tutorial.ipynb
This opens the script in your default web browser and allows you to step through the code one section at a time. You can step through each section by clicking the “Run” button in the upper toolbar. The section is done running when the “In [ * ]” text next to the section populates with a number (e.g. “In [1]”). (Note: part of the script downloads the ssd_mobilenet_v1 model from GitHub, which is about 74MB. This means it will take some time to complete the section, so be patient.)Once you have stepped all the way through the script, you should see two labeled images at the bottom section the page. If you see this, then everything is working properly! If not, the bottom section will report any errors encountered. See the  for a list of errors I encountered while setting this up.Note: If you run the full Jupyter Notebook without getting any errors, but the labeled pictures still don't appear, try this: go in to object_detection/utils/visualization_utils.py and comment out the import statements around lines 29 and 30 that include matplotlib. Then, try re-running the Jupyter notebook.3. Gather and Label PicturesNow that the TensorFlow Object Detection API is all set up and ready to go, we need to provide the images it will use to train a new detection classifier.3a. Gather PicturesTensorFlow needs hundreds of images of an object to train a good detection classifier. To train a robust classifier, the training images should have random objects in the image along with the desired objects, and should have a variety of backgrounds and lighting conditions. There should be some images where the desired object is partially obscured, overlapped with something else, or only halfway in the picture. For my Pinochle Card Detection classifier, I have six different objects I want to detect (the card ranks nine, ten, jack, queen, king, and ace – I am not trying to detect suit, just rank). I used my iPhone to take about 40 pictures of each card on its own, with various other non-desired objects in the pictures. Then, I took about another 100 pictures with multiple cards in the picture. I know I want to be able to detect the cards when they’re overlapping, so I made sure to have the cards be overlapped in many images.You can use your phone to take pictures of the objects or download images of the objects from Google Image Search. I recommend having at least 200 pictures overall. I used 311 pictures to train my card detector.Make sure the images aren’t too large. They should be less than 200KB each, and their resolution shouldn’t be more than 720x1280. The larger the images are, the longer it will take to train the classifier. You can use the resizer.py script in this repository to reduce the size of the images.After you have all the pictures you need, move 20% of them to the \object_detection\images\test directory, and 80% of them to the \object_detection\images\train directory. Make sure there are a variety of pictures in both the \test and \train directories.3b. Label PicturesHere comes the fun part! With all the pictures gathered, it’s time to label the desired objects in every picture. LabelImg is a great tool for labeling images, and its GitHub page has very clear instructions on how to install and use it.Download and install LabelImg, point it to your \images\train directory, and then draw a box around each object in each image. Repeat the process for all the images in the \images\test directory. This will take a while! LabelImg saves a .xml file containing the label data for each image. These .xml files will be used to generate TFRecords, which are one of the inputs to the TensorFlow trainer. Once you have labeled and saved each image, there will be one .xml file for each image in the \test and \train directories.4. Generate Training DataWith the images labeled, it’s time to generate the TFRecords that serve as input data to the TensorFlow training model. This tutorial uses the xml_to_csv.py and generate_tfrecord.py scripts from , with some slight modifications to work with our directory structure.First, the image .xml data will be used to create .csv files containing all the data for the train and test images. From the \object_detection folder, issue the following command in the Anaconda command prompt:(tensorflow1) C:\tensorflow1\models\research\object_detection> python xml_to_csv.py
This creates a train_labels.csv and test_labels.csv file in the \object_detection\images folder. Next, open the generate_tfrecord.py file in a text editor. Replace the label map starting at line 31 with your own label map, where each object is assigned an ID number. This same number assignment will be used when configuring the labelmap.pbtxt file in Step 5b. For example, say you are training a classifier to detect basketballs, shirts, and shoes. You will replace the following code in generate_tfrecord.py:# TO-DO replace this with label map
def class_text_to_int(row_label):
    if row_label == 'nine':
        return 1
    elif row_label == 'ten':
        return 2
    elif row_label == 'jack':
        return 3
    elif row_label == 'queen':
        return 4
    elif row_label == 'king':
        return 5
    elif row_label == 'ace':
        return 6
    else:
        None
With this:# TO-DO replace this with label map
def class_text_to_int(row_label):
    if row_label == 'basketball':
        return 1
    elif row_label == 'shirt':
        return 2
    elif row_label == 'shoe':
        return 3
    else:
        None
Then, generate the TFRecord files by issuing these commands from the \object_detection folder:python generate_tfrecord.py --csv_input=images\train_labels.csv --image_dir=images\train --output_path=train.record
python generate_tfrecord.py --csv_input=images\test_labels.csv --image_dir=images\test --output_path=test.record
These generate a train.record and a test.record file in \object_detection. These will be used to train the new object detection classifier.5. Create Label Map and Configure TrainingThe last thing to do before training is to create a label map and edit the training configuration file.5a. Label mapThe label map tells the trainer what each object is by defining a mapping of class names to class ID numbers. Use a text editor to create a new file and save it as labelmap.pbtxt in the C:\tensorflow1\models\research\object_detection\training folder. (Make sure the file type is .pbtxt, not .txt !) In the text editor, copy or type in the label map in the format below (the example below is the label map for my Pinochle Deck Card Detector):item {
  id: 1
  name: 'nine'
}

item {
  id: 2
  name: 'ten'
}

item {
  id: 3
  name: 'jack'
}

item {
  id: 4
  name: 'queen'
}

item {
  id: 5
  name: 'king'
}

item {
  id: 6
  name: 'ace'
}
The label map ID numbers should be the same as what is defined in the generate_tfrecord.py file. For the basketball, shirt, and shoe detector example mentioned in Step 4, the labelmap.pbtxt file will look like:item {
  id: 1
  name: 'basketball'
}

item {
  id: 2
  name: 'shirt'
}

item {
  id: 3
  name: 'shoe'
}
5b. Configure trainingFinally, the object detection training pipeline must be configured. It defines which model and what parameters will be used for training. This is the last step before running training!Navigate to C:\tensorflow1\models\research\object_detection\samples\configs and copy the faster_rcnn_inception_v2_pets.config file into the \object_detection\training directory. Then, open the file with a text editor. There are several changes to make to the .config file, mainly changing the number of classes and examples, and adding the file paths to the training data.Make the following changes to the faster_rcnn_inception_v2_pets.config file. Note: The paths must be entered with single forward slashes (NOT backslashes), or TensorFlow will give a file path error when trying to train the model! Also, the paths must be in double quotation marks ( "" ), not single quotation marks ( ' ).Save the file after the changes have been made. That’s it! The training job is all configured and ready to go!6. Run the TrainingUPDATE 9/26/18:As of version 1.9, TensorFlow has deprecated the ""train.py"" file and replaced it with ""model_main.py"" file. I haven't been able to get model_main.py to work correctly yet (I run in to errors related to pycocotools). Fortunately, the train.py file is still available in the /object_detection/legacy folder. Simply move train.py from /object_detection/legacy into the /object_detection folder and then continue following the steps below.Here we go! From the \object_detection directory, issue the following command to begin training:python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/faster_rcnn_inception_v2_pets.config
If everything has been set up correctly, TensorFlow will initialize the training. The initialization can take up to 30 seconds before the actual training begins. When training begins, it will look like this:Each step of training reports the loss. It will start high and get lower and lower as training progresses. For my training on the Faster-RCNN-Inception-V2 model, it started at about 3.0 and quickly dropped below 0.8. I recommend allowing your model to train until the loss consistently drops below 0.05, which will take about 40,000 steps, or about 2 hours (depending on how powerful your CPU and GPU are). Note: The loss numbers will be different if a different model is used. MobileNet-SSD starts with a loss of about 20, and should be trained until the loss is consistently under 2.You can view the progress of the training job by using TensorBoard. To do this, open a new instance of Anaconda Prompt, activate the tensorflow1 virtual environment, change to the C:\tensorflow1\models\research\object_detection directory, and issue the following command:(tensorflow1) C:\tensorflow1\models\research\object_detection>tensorboard --logdir=training
This will create a webpage on your local machine at YourPCName:6006, which can be viewed through a web browser. The TensorBoard page provides information and graphs that show how the training is progressing. One important graph is the Loss graph, which shows the overall loss of the classifier over time.The training routine periodically saves checkpoints about every five minutes. You can terminate the training by pressing Ctrl+C while in the command prompt window. I typically wait until just after a checkpoint has been saved to terminate the training. You can terminate training and start it later, and it will restart from the last saved checkpoint. The checkpoint at the highest number of steps will be used to generate the frozen inference graph.7. Export Inference GraphNow that training is complete, the last step is to generate the frozen inference graph (.pb file). From the \object_detection folder, issue the following command, where “XXXX” in “model.ckpt-XXXX” should be replaced with the highest-numbered .ckpt file in the training folder:python export_inference_graph.py --input_type image_tensor --pipeline_config_path training/faster_rcnn_inception_v2_pets.config --trained_checkpoint_prefix training/model.ckpt-XXXX --output_directory inference_graph
This creates a frozen_inference_graph.pb file in the \object_detection\inference_graph folder. The .pb file contains the object detection classifier.8. Use Your Newly Trained Object Detection Classifier!The object detection classifier is all ready to go! I’ve written Python scripts to test it out on an image, video, or webcam feed.Before running the Python scripts, you need to modify the NUM_CLASSES variable in the script to equal the number of classes you want to detect. (For my Pinochle Card Detector, there are six cards I want to detect, so NUM_CLASSES = 6.)To test your object detector, move a picture of the object or objects into the \object_detection folder, and change the IMAGE_NAME variable in the Object_detection_image.py to match the file name of the picture. Alternatively, you can use a video of the objects (using Object_detection_video.py), or just plug in a USB webcam and point it at the objects (using Object_detection_webcam.py).To run any of the scripts, type “idle” in the Anaconda Command Prompt (with the “tensorflow1” virtual environment activated) and press ENTER. This will open IDLE, and from there, you can open any of the scripts and run them.If everything is working properly, the object detector will initialize for about 10 seconds and then display a window showing any objects it’s detected in the image!If you encounter errors, please check out the Appendix: it has a list of errors that I ran in to while setting up my object detection classifier. You can also trying Googling the error. There is usually useful information on Stack Exchange or in TensorFlow’s Issues on GitHub.Appendix: Common ErrorsIt appears that the TensorFlow Object Detection API was developed on a Linux-based operating system, and most of the directions given by the documentation are for a Linux OS. Trying to get a Linux-developed software library to work on Windows can be challenging. There are many little snags that I ran in to while trying to set up tensorflow-gpu to train an object detection classifier on Windows 10. This Appendix is a list of errors I ran in to, and their resolutions.1. ModuleNotFoundError: No module named 'deployment' or No module named 'nets'This error occurs when you try to run object_detection_tutorial.ipynb or train.py and you don’t have the PATH and PYTHONPATH environment variables set up correctly. Exit the virtual environment by closing and re-opening the Anaconda Prompt window. Then, issue “activate tensorflow1” to re-enter the environment, and then issue the commands given in Step 2e. You can use “echo %PATH%” and “echo %PYTHONPATH%” to check the environment variables and make sure they are set up correctly.Also, make sure you have run these commands from the \models\research directory:setup.py build
setup.py install
2. ImportError: cannot import name 'preprocessor_pb2'ImportError: cannot import name 'string_int_label_map_pb2'(or similar errors with other pb2 files)This occurs when the protobuf files (in this case, preprocessor.proto) have not been compiled. Re-run the protoc command given in Step 2f. Check the \object_detection\protos folder to make sure there is a name_pb2.py file for every name.proto file.3. object_detection/protos/.proto: No such file or directoryThis occurs when you try to run the“protoc object_detection/protos/*.proto --python_out=.”
command given on the TensorFlow Object Detection API installation page. Sorry, it doesn’t work on Windows! Copy and paste the full command given in Step 2f instead. There’s probably a more graceful way to do it, but I don’t know what it is.4. Unsuccessful TensorSliceReader constructor: Failed to get ""file path"" … The filename, directory name, or volume label syntax is incorrect.This error occurs when the filepaths in the training configuration file (faster_rcnn_inception_v2_pets.config or similar) have not been entered with backslashes instead of forward slashes. Open the .config file and make sure all file paths are given in the following format:“C:/path/to/model.file”
5. ValueError: Tried to convert 't' to a tensor and failed. Error: Argument must be a dense tensor: range(0, 3) - got shape [3], but wanted [].The issue is with models/research/object_detection/utils/learning_schedules.py Currently it israte_index = tf.reduce_max(tf.where(tf.greater_equal(global_step, boundaries),
                                      range(num_boundaries),
                                      [0] * num_boundaries))
Wrap list() around the range() like this:rate_index = tf.reduce_max(tf.where(tf.greater_equal(global_step, boundaries),
                                     list(range(num_boundaries)),
                                      [0] * num_boundaries))
6. ImportError: DLL load failed: The specified procedure could not be found.   (or other DLL-related errors)This error occurs because the CUDA and cuDNN versions you have installed are not compatible with the version of TensorFlow you are using. The easiest way to resolve this error is to use Anaconda's cudatoolkit package rather than manually installing CUDA and cuDNN. If you ran into these errors, try creating a new Anaconda virtual environment:conda create -n tensorflow2 pip python=3.5
Then, once inside the environment, install TensorFlow using CONDA rather than PIP:conda install tensorflow-gpu
Then restart this guide from Step 2 (but you can skip the part where you install TensorFlow in Step 2d).7. In Step 2g, the Jupyter Notebook runs all the way through with no errors, but no pictures are displayed at the end.If you run the full Jupyter Notebook without getting any errors, but the labeled pictures still don't appear, try this: go in to object_detection/utils/visualization_utils.py and comment out the import statements around lines 29 and 30 that include matplotlib. Then, try re-running the Jupyter notebook. (The visualization_utils.py script changes quite a bit, so it might not be exactly line 29 and 30.)"
https://github.com/mahyarnajibi/SNIPER,SNIPER / AutoFocus is an efficient multi-scale object detection training / inference algorithm,"SNIPER / AutoFocus: Efficient Multi-Scale Training / InferenceSNIPER is an efficient multi-scale training approach for instance-level recognition tasks like object detection and instance-level segmentation.Instead of processing all pixels in an image pyramid, SNIPER selectively processes context regions around the ground-truth objects (a.k.a chips).This significantly speeds up multi-scale training as it operates on low-resolution chips.Due to its memory-efficient design, SNIPER can benefit from Batch Normalization during training and it makes larger batch-sizes possible for instance-level recognition tasks on a single GPU. Hence, we do not need to synchronize batch-normalization statistics across GPUs and we can train object detectors similar to the way we do image classification!AutoFocus, on the other hand, is an efficient multi-scale inference algorithm for deep-learning based object detectors. Instead of processing an entire image pyramid, AutoFocus adopts a coarse to fine approach and only processes regions that are likely to contain small objects at finer scales. This is achieved by predicting category agnostic segmentation maps for small objects at coarser scales, called FocusPixels. FocusPixels can be predicted with high recall, and in many cases, they only cover a small fraction of the entire image. To make efficient use of FocusPixels, an algorithm is proposed which generates compact rectangular FocusChips which enclose FocusPixels. The detector is while processing finer scales. is initially described in the following paper published at NeurIPS 2018: is initially described in the following paper published at ICCV 2019:FeaturesResultsCOCO datasetHere are the COCO results for SNIPER trained using this repository. The models are trained on the trainval set (using only the bounding box annotations) and evaluated on the test-dev set.|                                 | network architecture | pre-trained dataset | test dataset | mAP  | mAP@0.5 | mAP@0.75| mAP@S | mAP@M | mAP@L ||:---------------------------------:|:---------------:|:---------------:|:------:|:---------:|:---------:|:-------:|:-------:|:-------:|:-------:|| SNIPER | ResNet-101 | ImageNet | test-dev15 | 46.5 | 67.5    |   52.2  | 30.0  | 49.4  | 58.4 || SNIPER |ResNet-101  | OpenImagesV4 | test-dev15| 47.8 |  68.2   | 53.6   | 31.5  | 50.4  | 59.8  || SNIPER | MobileNetV2 | ImageNet | test-dev15| 34.3 |  54.4   | 37.9   | 18.5  | 36.9  | 46.4  ||                   |                        |                     |                      |      |        |         |        |       |      || AutoFocus | ResNet-101 | OpenImagesV4 | val-2017| 47.5 |  67.7   | 53.2   | 33.3  | 51.2  | 60.8  |You can download the OpenImages pre-trained model by running . The SNIPER detectors trained on both COCO (ResNet-101 and MobileNetV2) and PASCAL VOC datasets and the AutoFocus model trained on the COCO dataset (ResNet-101) can be downloaded by running . LicenseSNIPER is released under Apache license. See LICENSE for details.Citing@article{najibi2019autofocus,
  title={{AutoFocus}: Efficient Multi-Scale Inference},
  author={Najibi, Mahyar and Singh, Bharat and Davis, Larry S},
  journal={ICCV},
  year={2019}
}
@article{sniper2018,
  title={{SNIPER}: Efficient Multi-Scale Training},
  author={Singh, Bharat and Najibi, Mahyar and Davis, Larry S},
  journal={NeurIPS},
  year={2018}
}
@article{analysissnip2017,
  title={An analysis of scale invariance in object detection-snip},
  author={Singh, Bharat and Davis, Larry S},
  journal={CVPR},
  year={2018}
}
Contents Installationgit clone --recursive https://github.com/mahyarnajibi/SNIPER.git
You need to install CUDA, CuDNN, OpenCV, and OpenBLAS. These libraries are set to be used by default in the provided  file in the  repository. You can use the  command to build the MXNet library:cd SNIPER-mxnet
make -j [NUM_OF_PROCESS] USE_CUDA_PATH=[PATH_TO_THE_CUDA_FOLDER]
If you plan to train models on multiple GPUs, it is optional but recommended to install NCCL and build MXNet with the NCCL support as instructed below:make -j [NUM_OF_PROCESS] USE_CUDA_PATH=[PATH_TO_THE_CUDA_FOLDER] USE_NCCL=1 
In this case, you may also need to set the  variable in the above command to point to your NCCL installation path.If you need more information on how to compile MXNet please see .bash scripts/compile.sh
pip install -r requirements.txt
 Running the demoFor running the demo, you need to download the provided SNIPER models. The following script downloads SNIPER models and extracts them into the default location:bash download_sniper_autofocus_detectors.sh
After downloading the model, the following command would run the SNIPER detector trained on the COCO dataset with the default configs on the provided sample image:python demo.py
If everything goes well, the sample detections would be saved as .You can also run the detector on an arbitrary image by providing its path to the script:python demo.py --im_path [PATH to the image]
However, if you plan to run the detector on multiple images, please consider using the provided multi-process and multi-batch  module. You can also test the provided SNIPER model based on the  architecture trained on the COCO dataset by passing the provided config file as follows:python demo.py --cfg configs/faster/sniper_mobilenetv2_e2e.yml
Training a model with SNIPER / AutoFocusFor training SNIPER/AutoFocus, you first need to download the pre-trained models and configure the datasets as described below.Downloading pre-trained modelsRunning the following script downloads and extracts the pre-trained models into the default path ():bash download_pretrained_models.sh
Configuring the datasetCOCO dataset:Please follow the  to download the dataset. After downloading the dataset you should have the following directory structure:data
  |--coco
      |--annotations
      |--images
PASCAL VOC dataset:Please download the training, validation, and test subsets from the [official Pascal VOC dataset website (http://host.robots.ox.ac.uk/pascal/VOC/). After downloading the dataset you should have the following directory structure:data
  |--VOCdevkit
      |--VOC2007
      |--VOC2012
Training the SNIPER detectorYou can train the SNIPER detector with or without negative chip mining as described below.Training with Negative Chip Mining:Negative chip mining results in a relative improvement in AP (please refer to the  for the details). To determine the candidate hard negative regions, SNIPER uses proposals extracted from a proposal network trained for a short training schedule. For COCO and Pascal VOC datasets, we provide the pre-computed proposals. The following commands download the pre-computed proposals, extracts them into the default path (), and trains the SNIPER detector with the default parameters on the COCO dataset:bash download_sniper_neg_props.sh
python main_train.py
For training on Pascal VOC with the provided pre-computed proposals, you can run .However, it is also possible to extract the required proposals using this repository (e.g. if you plan to train SNIPER on a new dataset). We provided an all-in-one script that performs all the required steps for training SNIPER with Negative Chip Mining. Running the following script trains a proposal network for a short cycle (i.e. 2 epochs), extract the proposals, and train the SNIPER detector with Negative Chip Mining:bash train_neg_props_and_sniper.sh --cfg [PATH_TO_CFG_FILE]
Training without Negative Chip Mining:You can disable the negative chip mining by setting the  to . This is useful if you plan to try SNIPER on a new dataset or want to shorten the training cycle. In this case, the training can be started by calling the following command:python main_train.py --set TRAIN.USE_NEG_CHIPS False
In any case, the default training settings can be overwritten by passing a configuration file (see the  folder for example configuration files).The path to the configuration file can be passed as an argument to the above script using the  flag.It is also possible to set individual configuration key-values by passing  as the last argument to the modulefollowed by the desired key-values (i.e. ).Please note that the default config files have the same settings used to train the released models.If you are using a GPU with less amount of memory, please consider reducing the training batch size(by setting  in the config file or passing  as the last argument to the module).Also, multi-processing is used to process the data. For smaller amounts of memory, you may need to reduce the number ofprocesses and number of threads according to your system (by setting  and  respectively).Training the SNIPER detector with AutoFocusFor training SNIPER with the AutoFocus FocusPixel prediction branch, you can pass the AutoFocus config files (e.g.  and ) to the  script. The default AutoFocus training hyper-parameters (for defining positive, negative, or don't care FocusPixels) can be modified through the config files. Please refer to the  for more details.Evaluting SNIPER / AutoFocus modelsEvaluating the provided SNIPER modelsThe repository provides a set of pre-trained SNIPER models which can be downloaded by running the following script:bash download_sniper_detector.sh
This script downloads the model weights and extracts them into the expected directory.To evaluate these models on COCO test-dev with the default configuration, you can run the following script:python main_test.py
For performing inference with AutoFocus, you can run the  script by passing the AutoFocus config file:python main_test.py --cfg sniper_res101_e2e_mask_autofocus.py
It is possible to modify the AutoFocus default hyper-parameters through the config file to control the speed-accuracy tradeoff. Please see the  for more details.If inference is performed on the COCO test-dev set, a  file containing the detections on the  is produced which can be zipped and uploaded to the COCO evaluation server.The default SNIPER settings can be also overwritten by passing the path to a configuration file with the  flag(See the  folder for examples). It is also possible to set individual configuration key-values by passing  as the last argument to the module followed by the desired key-values (i.e. ).As an example, for evaluating the provided PASCAL VOC pre-trained model on the VOC 2007 test-set you can pass the provided PASCAL config file to the script:python main_test.py --cfg configs/faster/sniper_res101_e2e_pascal_voc.yml
Please note that the evaluation is performed in a multi-image per batch and parallel model forward setting. In case of lower GPU memory, please consider reducing the batch size for different scales (by setting ) or reducing the number of parallel jobs (by setting  in the config file).Evaluating a model trained with this repositoryFor evaluating a model trained with this repository, you can run the following script by passing the same configuration file used during the training.The test settings can be set by updating the  section of the configuration file (See the  folder for examples).python main_test.py --cfg [PATH TO THE CONFIG FILE USED FOR TRAINING]
Branches in this repo (SSH Face Detector, R-FCN-3K, Soft Sampling)R-FCN-3KThis repo also contains the  detector. Please switch to the  branch for specific instructions.OpenImagesV4 with Soft SamplingThis repo also contains modules to train on the .Please switch to the  branch for specific instructions. The detector on OpenImagesV4 was trained with .SSH Face DetectorThe  face detector would be added to this repository soon. In the meanwhile, you can use the code available at the original ."
https://github.com/zihangdai/xlnet,XLNet: Generalized Autoregressive Pretraining for Language Understanding,"IntroductionXLNet is a new unsupervised language representation learning method based on a novel generalized permutation language modeling objective. Additionally, XLNet employs  as the backbone model, exhibiting excellent performance for language tasks involving long context. Overall, XLNet achieves state-of-the-art (SOTA) results on various downstream language tasks including question answering, natural language inference, sentiment analysis, and document ranking.For a detailed description of technical details and experimental results, please refer to our paper:​        ​        Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le ​        (*: equal contribution) ​        Preprint 2019Release NotesResultsAs of June 19, 2019, XLNet outperforms BERT on 20 tasks and achieves state-of-the-art results on 18 tasks. Below are some comparison between XLNet-Large and BERT-Large, which have similar model sizes:Results on Reading ComprehensionModel |  | SQuAD1.1 EM | SQuAD2.0 EM--- | --- | --- | ---BERT-Large | 72.0 | 84.1 | 78.98XLNet-Base | | | 80.18XLNet-Large | 81.75 | 88.95 | 86.12We use SQuAD dev results in the table to exclude other factors such as using additional training data or other data augmentation techniques. See  for test numbers.Results on Text ClassificationModel | IMDB | Yelp-2 | Yelp-5 | DBpedia | Amazon-2 | Amazon-5--- | --- | --- | --- | --- | --- | ---BERT-Large | 4.51 | 1.89 | 29.32 | 0.64 | 2.63 | 34.17XLNet-Large | 3.79 | 1.55 | 27.80 | 0.62 | 2.40 | 32.26The above numbers are error rates.Results on GLUEModel | MNLI | QNLI | QQP | RTE | SST-2 | MRPC | CoLA | STS-B--- | --- | --- | --- | --- | --- | --- | --- | ---BERT-Large | 86.6 | 92.3 | 91.3 | 70.4 | 93.2 | 88.0 | 60.6 | 90.0XLNet-Base | 86.8 | 91.7 | 91.4 | 74.0 | 94.7 | 88.2 | 60.2 | 89.5XLNet-Large | 89.8 | 93.9 | 91.8 | 83.8 | 95.6 | 89.2 | 63.6 | 91.8We use single-task dev results in the table to exclude other factors such as multi-task learning or using ensembles.Pre-trained modelsReleased ModelsAs of July 16, 2019, the following models have been made available:We only release cased models for now because on the tasks we consider, we found: (1) for the base setting, cased and uncased models have similar performance; (2) for the large setting, cased models are a bit better in some tasks.Each .zip file contains three items:Future Release PlanWe also plan to continuously release more pretrained models under different settings, including:Subscribing to XLNet on Google GroupsTo receive notifications about updates, announcements and new releases, we recommend subscribing to the XLNet on .Fine-tuning with XLNetAs of June 19, 2019, this code base has been tested with TensorFlow 1.13.1 under Python2.Memory Issue during FinetuningGiven the memory issue mentioned above, using the default finetuning scripts ( and ), we benchmarked the maximum batch size on a single 16GB GPU with TensorFlow 1.13.1:| System        | Seq Length | Max Batch Size || ------------- | ---------- | -------------- ||   | 64         | 120            || ...           | 128        | 56             || ...           | 256        | 24             || ...           | 512        | 8              ||  | 64         | 16             || ...           | 128        | 8              || ...           | 256        | 2              || ...           | 512        | 1              |In most cases, it is possible to reduce the batch size  or the maximum sequence length  to fit in given hardware. The decrease in performance depends on the task and the available resources.Text Classification/RegressionThe code used to perform classification/regression finetuning is in . It also contains examples for standard one-document classification, one-document regression, and document pair classification. Here, we provide two concrete examples of how  can be used.From here on, we assume XLNet-Large and XLNet-base has been downloaded to  and  respectively.(1) STS-B: sentence pair relevance regression (with GPUs)Notes:(2) IMDB: movie review sentiment classification (with TPU V3-8)Notes:SQuAD2.0The code for the SQuAD dataset is included in .To run the code:(1) Download the SQuAD2.0 dataset into  by:mkdir -p ${SQUAD_DIR} && cd ${SQUAD_DIR}
wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json
wget https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json
(2) Perform data preprocessing using the script .(3) Perform training and evaluation.For the best performance, XLNet-Large uses sequence length 512 and batch size 48 for training.Alternatively, one can use XLNet-Base with GPUs (e.g. three V100). One set of reasonable hyper-parameters can be found in the script .RACE reading comprehensionThe code for the reading comprehension task  is included in .To run the code:(1) Download the RACE dataset from the  and unpack the raw data to .(2) Perform training and evaluation:Using Google Colab of using Google Colab with GPUs has been provided. Note that since the hardware is constrained in the example, the results are worse than the best we can get. It mainly serves as an example and should be modified accordingly to maximize performance.Custom Usage of XLNetXLNet AbstractionFor finetuning, it is likely that you will be able to modify existing files such as ,  and  for your task at hand. However, we also provide an abstraction of XLNet to enable more flexible usage. Below is an example:import xlnet

# some code omitted here...
# initialize FLAGS
# initialize instances of tf.Tensor, including input_ids, seg_ids, and input_mask

# XLNetConfig contains hyperparameters that are specific to a model checkpoint.
xlnet_config = xlnet.XLNetConfig(json_path=FLAGS.model_config_path)

# RunConfig contains hyperparameters that could be different between pretraining and finetuning.
run_config = xlnet.create_run_config(is_training=True, is_finetune=True, FLAGS=FLAGS)

# Construct an XLNet model
xlnet_model = xlnet.XLNetModel(
    xlnet_config=xlnet_config,
    run_config=run_config,
    input_ids=input_ids,
    seg_ids=seg_ids,
    input_mask=input_mask)

# Get a summary of the sequence using the last hidden state
summary = xlnet_model.get_pooled_out(summary_type=""last"")

# Get a sequence output
seq_out = xlnet_model.get_sequence_output()

# build your applications based on `summary` or `seq_out`
TokenizationBelow is an example of doing tokenization in XLNet:import sentencepiece as spm
from prepro_utils import preprocess_text, encode_ids

# some code omitted here...
# initialize FLAGS

text = ""An input text string.""

sp_model = spm.SentencePieceProcessor()
sp_model.Load(FLAGS.spiece_model_file)
text = preprocess_text(text, lower=FLAGS.uncased)
ids = encode_ids(sp_model, text)
where  is the SentencePiece model file in the same zip as the pretrained model,  is a bool indicating whether to do uncasing.Pretraining with XLNetRefer to  for pretraining on TPUs and  for pretraining on GPUs. First we need to preprocess the text data into tfrecords.python data_utils.py \
	--bsz_per_host=32 \
	--num_core_per_host=16 \
	--seq_len=512 \
	--reuse_len=256 \
	--input_glob=*.txt \
	--save_dir=${SAVE_DIR} \
	--num_passes=20 \
	--bi_data=True \
	--sp_path=spiece.model \
	--mask_alpha=6 \
	--mask_beta=1 \
	--num_predict=85
where  defines all input text files,  is the output directory for tfrecords, and  is a  model. Here is our script to train the Sentence Piece modelspm_train \
	--input=$INPUT \
	--model_prefix=sp10m.cased.v3 \
	--vocab_size=32000 \
	--character_coverage=0.99995 \
	--model_type=unigram \
	--control_symbols=<cls>,<sep>,<pad>,<mask>,<eod> \
	--user_defined_symbols=<eop>,.,(,),"",-,–,£,€ \
	--shuffle_input_sentence \
	--input_sentence_size=10000000
Special symbols are used, including  and . We use  and  to denote End of Paragraph and End of Document respectively.The input text files to  must use the following format:For example, the text input file could be:This is the first sentence.
This is the second sentence and also the end of the paragraph.<eop>
Another paragraph.

Another document starts here.
After preprocessing, we are ready to pretrain an XLNet. Below are the hyperparameters used for pretraining XLNet-Large:python train.py
  --record_info_dir=$DATA/tfrecords \
  --train_batch_size=2048 \
  --seq_len=512 \
  --reuse_len=256 \
  --mem_len=384 \
  --perm_size=256 \
  --n_layer=24 \
  --d_model=1024 \
  --d_embed=1024 \
  --n_head=16 \
  --d_head=64 \
  --d_inner=4096 \
  --untie_r=True \
  --mask_alpha=6 \
  --mask_beta=1 \
  --num_predict=85
where we only list the most important flags and the other flags could be adjusted based on specific use cases."
https://github.com/tensorpack/tensorpack,"A Neural Net Training Interface on TensorFlow, with focus on speed + flexibility","Tensorpack is a neural network training interface based on graph-mode TensorFlow.Features:It's Yet Another TF high-level API, with the following highlights:See  to know more about these features.Examples:We refuse toy examples.Instead of showing tiny CNNs trained on MNIST/Cifar10,we provide training scripts that reproduce well-known papers.We refuse low-quality implementations.Unlike most open source repos which only implement papers, faithfully reproduce papers,demonstrating its flexibility for actual research.Vision:Reinforcement Learning:Speech / NLP:Install:Dependencies:pip install --upgrade git+https://github.com/tensorpack/tensorpack.git
# or add `--user` to install to user's local directories
Please note that tensorpack is not yet stable.If you use tensorpack in your code, remember to mark the exact version of tensorpack you use as your dependencies.Citing Tensorpack:If you use Tensorpack in your research or wish to refer to the examples, please cite with:@misc{wu2016tensorpack,
  title={Tensorpack},
  author={Wu, Yuxin and others},
  howpublished={\url{https://github.com/tensorpack/}},
  year={2016}
}
"
https://github.com/matplotlib/cheatsheets,Official Matplotlib cheat sheets,"Cheatsheets for Matplotlib usersCheatsheetsCheatsheet  | |:------------------------------------------------------------------------------:|:----------------------------------------------------------:                       | HandoutsBeginner handout  | Intermediate handout  | Tips handout :-----------------------------------------------------------------------------------------:|:--------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------:                               |                                    | For contributors to the cheatsheetsHow to compileOn Linux, with  installed, the fonts can be set up with the following command:make -C fonts
The fonts can be made discoverable by  (through ) by creating the following in  (see ):<?xml version=""1.0""?>
<!DOCTYPE fontconfig SYSTEM ""fonts.dtd"">
<fontconfig>
<dir>/path/to/cheatsheets/fonts/</dir>
...
</fontconfig>
$ cd scripts
$ for script in *.py; do python $script; done
$ cd ..
$ xelatex cheatsheets.tex
$ xelatex cheatsheets.tex
"
https://github.com/Lightning-AI/lightning,"Deep learning framework to train, deploy, and ship AI products Lightning fast.","The Deep Learning framework to train, deploy, and ship AI products Lightning fast.NEW- Lightning 2.0 is featuring a clean and stable API!!Install LightningSimple installation from PyPIpip install lightning
Install with optional dependenciespip install lightning['extra']
Condaconda install lightning -c conda-forge
Install stable versionInstall future release from the sourcepip install https://github.com/Lightning-AI/lightning/archive/refs/heads/release/stable.zip -U
Install bleeding-edgeInstall nightly from the source (no guarantees)pip install https://github.com/Lightning-AI/lightning/archive/refs/heads/master.zip -U
or from testing PyPIpip install -iU https://test.pypi.org/simple/ pytorch-lightning
Lightning has 3 core packages...Lightning gives you granular control over how much abstraction you want to add over PyTorch.PyTorch Lightning: Train and Deploy PyTorch at ScalePyTorch Lightning is just organized PyTorch - Lightning disentangles PyTorch code to decouple the science from the engineering.Hello simple model# main.py
# ! pip install torchvision
import torch, torch.nn as nn, torch.utils.data as data, torchvision as tv, torch.nn.functional as F
import lightning as L

# --------------------------------
# Step 1: Define a LightningModule
# --------------------------------
# A LightningModule (nn.Module subclass) defines a full *system*
# (ie: an LLM, diffusion model, autoencoder, or simple image classifier).


class LitAutoEncoder(L.LightningModule):
    def __init__(self):
        super().__init__()
        self.encoder = nn.Sequential(nn.Linear(28 * 28, 128), nn.ReLU(), nn.Linear(128, 3))
        self.decoder = nn.Sequential(nn.Linear(3, 128), nn.ReLU(), nn.Linear(128, 28 * 28))

    def forward(self, x):
        # in lightning, forward defines the prediction/inference actions
        embedding = self.encoder(x)
        return embedding

    def training_step(self, batch, batch_idx):
        # training_step defines the train loop. It is independent of forward
        x, y = batch
        x = x.view(x.size(0), -1)
        z = self.encoder(x)
        x_hat = self.decoder(z)
        loss = F.mse_loss(x_hat, x)
        self.log(""train_loss"", loss)
        return loss

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)
        return optimizer


# -------------------
# Step 2: Define data
# -------------------
dataset = tv.datasets.MNIST(""."", download=True, transform=tv.transforms.ToTensor())
train, val = data.random_split(dataset, [55000, 5000])

# -------------------
# Step 3: Train
# -------------------
autoencoder = LitAutoEncoder()
trainer = L.Trainer()
trainer.fit(autoencoder, data.DataLoader(train), data.DataLoader(val))
Run the model on your terminalpip install torchvision
python main.py
Advanced featuresLightning has over  designed for professional AI research at scale.Here are some examples:# 8 GPUs
# no code changes needed
trainer = Trainer(accelerator=""gpu"", devices=8)

# 256 GPUs
trainer = Trainer(accelerator=""gpu"", devices=8, num_nodes=32)
# no code changes needed
trainer = Trainer(accelerator=""tpu"", devices=8)
# no code changes needed
trainer = Trainer(precision=16)
from lightning import loggers

# tensorboard
trainer = Trainer(logger=TensorBoardLogger(""logs/""))

# weights and biases
trainer = Trainer(logger=loggers.WandbLogger())

# comet
trainer = Trainer(logger=loggers.CometLogger())

# mlflow
trainer = Trainer(logger=loggers.MLFlowLogger())

# neptune
trainer = Trainer(logger=loggers.NeptuneLogger())

# ... and dozens more
es = EarlyStopping(monitor=""val_loss"")
trainer = Trainer(callbacks=[es])
checkpointing = ModelCheckpoint(monitor=""val_loss"")
trainer = Trainer(callbacks=[checkpointing])
# torchscript
autoencoder = LitAutoEncoder()
torch.jit.save(autoencoder.to_torchscript(), ""model.pt"")
# onnx
with tempfile.NamedTemporaryFile(suffix="".onnx"", delete=False) as tmpfile:
    autoencoder = LitAutoEncoder()
    input_sample = torch.randn((1, 64))
    autoencoder.to_onnx(tmpfile.name, input_sample, export_params=True)
    os.path.isfile(tmpfile.name)
Advantages over unstructured PyTorchLightning Fabric: Expert control.Run on any device at any scale with expert-level control over PyTorch training loop and scaling strategy. You can even write your own Trainer.Fabric is designed for the most complex models like foundation model scaling, LLMs, diffusion, transformers, reinforcement learning, active learning. Of any size.+ import lightning as L
  import torch; import torchvision as tv

 dataset = tv.datasets.CIFAR10(""data"", download=True,
                               train=True,
                               transform=tv.transforms.ToTensor())

+ fabric = L.Fabric()
+ fabric.launch()

  model = tv.models.resnet18()
  optimizer = torch.optim.SGD(model.parameters(), lr=0.001)
- device = ""cuda"" if torch.cuda.is_available() else ""cpu""
- model.to(device)
+ model, optimizer = fabric.setup(model, optimizer)

  dataloader = torch.utils.data.DataLoader(dataset, batch_size=8)
+ dataloader = fabric.setup_dataloaders(dataloader)

  model.train()
  num_epochs = 10
  for epoch in range(num_epochs):
      for batch in dataloader:
          inputs, labels = batch
-         inputs, labels = inputs.to(device), labels.to(device)
          optimizer.zero_grad()
          outputs = model(inputs)
          loss = torch.nn.functional.cross_entropy(outputs, labels)
-         loss.backward()
+         fabric.backward(loss)
          optimizer.step()
          print(loss.data)
import lightning as L
import torch; import torchvision as tv

dataset = tv.datasets.CIFAR10(""data"", download=True,
                              train=True,
                              transform=tv.transforms.ToTensor())

fabric = L.Fabric()
fabric.launch()

model = tv.models.resnet18()
optimizer = torch.optim.SGD(model.parameters(), lr=0.001)
model, optimizer = fabric.setup(model, optimizer)

dataloader = torch.utils.data.DataLoader(dataset, batch_size=8)
dataloader = fabric.setup_dataloaders(dataloader)

model.train()
num_epochs = 10
for epoch in range(num_epochs):
    for batch in dataloader:
        inputs, labels = batch
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = torch.nn.functional.cross_entropy(outputs, labels)
        fabric.backward(loss)
        optimizer.step()
        print(loss.data)
Key features# Use your available hardware
# no code changes needed
fabric = Fabric()

# Run on GPUs (CUDA or MPS)
fabric = Fabric(accelerator=""gpu"")

# 8 GPUs
fabric = Fabric(accelerator=""gpu"", devices=8)

# 256 GPUs, multi-node
fabric = Fabric(accelerator=""gpu"", devices=8, num_nodes=32)

# Run on TPUs
fabric = Fabric(accelerator=""tpu"")
# Use state-of-the-art distributed training techniques
fabric = Fabric(strategy=""ddp"")
fabric = Fabric(strategy=""deepspeed"")
fabric = Fabric(strategy=""fsdp"")

# Switch the precision
fabric = Fabric(precision=""16-mixed"")
fabric = Fabric(precision=""64"")
  # no more of this!
- model.to(device)
- batch.to(device)
import lightning as L


class MyCustomTrainer:
    def __init__(self, accelerator=""auto"", strategy=""auto"", devices=""auto"", precision=""32-true""):
        self.fabric = L.Fabric(accelerator=accelerator, strategy=strategy, devices=devices, precision=precision)

    def fit(self, model, optimizer, dataloader, max_epochs):
        self.fabric.launch()

        model, optimizer = self.fabric.setup(model, optimizer)
        dataloader = self.fabric.setup_dataloaders(dataloader)
        model.train()

        for epoch in range(max_epochs):
            for batch in dataloader:
                input, target = batch
                optimizer.zero_grad()
                output = model(input)
                loss = loss_fn(output, target)
                self.fabric.backward(loss)
                optimizer.step()
You can find a more extensive example in our Lightning Apps: Build AI products and ML workflowsLightning Apps remove the cloud infrastructure boilerplate so you can focus on solving the research or business problems. Lightning Apps can run on the Lightning Cloud, your own cluster or a private cloud.Hello Lightning app world# app.py
import lightning as L


class TrainComponent(L.LightningWork):
    def run(self, x):
        print(f""train a model on {x}"")


class AnalyzeComponent(L.LightningWork):
    def run(self, x):
        print(f""analyze model on {x}"")


class WorkflowOrchestrator(L.LightningFlow):
    def __init__(self) -> None:
        super().__init__()
        self.train = TrainComponent(cloud_compute=L.CloudCompute(""cpu""))
        self.analyze = AnalyzeComponent(cloud_compute=L.CloudCompute(""gpu""))

    def run(self):
        self.train.run(""CPU machine 1"")
        self.analyze.run(""GPU machine 2"")


app = L.LightningApp(WorkflowOrchestrator())
Run on the cloud or locally# run on the cloud
lightning run app app.py --setup --cloud

# run locally
lightning run app app.py
ExamplesSelf-supervised LearningConvolutional ArchitecturesReinforcement LearningGANsClassic MLContinuous IntegrationLightning is rigorously tested across multiple CPUs, GPUs and TPUs and against major Python and PyTorch versions.Codecov is > 90%+ but build delays may show less|       System / PyTorch ver.        |                                                                                                              1.12                                                                                                               | 1.13                                                                                                                                                                                                                            | 2.0                                                                                                                                                                                                                             |                                                                                                               2.1                                                                                                               || :--------------------------------: | :-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:||        Linux py3.9 GPUs        | |  |  |  ||        Linux py3.9 TPUs        |                          |                                                                                                                                                                                                                                 |       |      ||  Linux (multiple Python versions)  |                                  |                                  |                                  |                                  ||   OSX (multiple Python versions)   |                                  |                                  |                                  |                                  || Windows (multiple Python versions) |                                  |                                  |                                  |                                  |CommunityThe lightning community is maintained byWant to help us build Lightning and reduce boilerplate for thousands of researchers? Lightning is also part of the  which requires projects to have solid testing, documentation and support.Asking for helpIf you have any questions please:"
https://github.com/openai/spinningup,An educational resource to help anyone learn deep reinforcement learning.,"Status: Maintenance (expect bug fixes and minor updates)Welcome to Spinning Up in Deep RL!This is an educational resource produced by OpenAI that makes it easier to learn about deep reinforcement learning (deep RL).For the unfamiliar:  (RL) is a machine learning approach for teaching agents how to solve tasks by trial and error. Deep RL refers to the combination of RL with .This module contains a variety of helpful resources, including:Get started at !Citing Spinning UpIf you reference or use Spinning Up in your research, please cite:@article{SpinningUp2018,
    author = {Achiam, Joshua},
    title = {{Spinning Up in Deep Reinforcement Learning}},
    year = {2018}
}
"
https://github.com/pytorch/tutorials,PyTorch tutorials.,"PyTorch TutorialsAll the tutorials are now presented as sphinx style documentation at:Asking a questionIf you have a question about a tutorial, post in https://dev-discuss.pytorch.org/ rather than creating an issue in this repo. Your question will be answered much faster on the dev-discuss forum.Submitting an issueYou can submit the following types of issues:ContributingWe use sphinx-gallery's  to create the tutorials. Syntax is very simple. In essence, you write a slightly well formatted Python file and it shows up as an HTML page. In addition, a Jupyter notebook is autogenerated and available to run in Google Colab.Here is how you can create a new tutorial (for a detailed description, see ):If you are starting off with a Jupyter notebook, you can use  to convert the notebook to Python file. After conversion and addition to the project, please make sure that section headings and other things are in logical order.Building locallyThe tutorial build is very large and requires a GPU. If your machine does not have a GPU device, you can preview your HTML build without actually downloading the data and running the tutorial code: Building a single tutorialYou can build a single tutorial by using the  environment variable. For example to run only , run:GALLERY_PATTERN=""neural_style_transfer_tutorial.py"" make html
orGALLERY_PATTERN=""neural_style_transfer_tutorial.py"" sphinx-build . _build
The  variable respects regular expressions.About contributing to PyTorch Documentation and Tutorials"
https://github.com/canonical/microk8s,"MicroK8s is a small, fast, single-package Kubernetes for datacenters and the edge.","The smallest, fastest KubernetesSingle-package fully conformant lightweight Kubernetes that works on . Perfect for:Why MicroK8s?Drop us a line at  if you aredoing something fun with MicroK8s!QuickstartInstall MicroK8s with:snap install microk8s --classic
MicroK8s includes a  command:sudo microk8s kubectl get nodes
sudo microk8s kubectl get services
To use MicroK8s with your existing kubectl:sudo microk8s kubectl config view --raw > $HOME/.kube/config
User access without sudoThe microk8s user group is created during the snap installation. Users in that groupare granted access to  commands. To add a user to that group:sudo usermod -a -G microk8s <username>
Kubernetes add-onsMicroK8s installs a barebones upstream Kubernetes. Additional services like dns and the Kubernetes dashboard can be enabled using the  command.sudo microk8s enable dns
sudo microk8s enable dashboard
Use  to see a list of enabled and available addons. You can find the addon manifests and/or scripts under , with  pointing by default to .DocumentationThe  are maintained in theKubernetes upstream Discourse.Take a look at the  if you want tocontribute to MicroK8s."
https://github.com/FactoryBoy/factory_boy,A test fixtures replacement for Python,"factory_boy.. image:: https://github.com/FactoryBoy/factory_boy/workflows/Test/badge.svg:target: https://github.com/FactoryBoy/factory_boy/actions?query=workflow%3ATest.. image:: https://github.com/FactoryBoy/factory_boy/workflows/Check/badge.svg:target: https://github.com/FactoryBoy/factory_boy/actions?query=workflow%3ACheck.. image:: https://img.shields.io/pypi/v/factory_boy.svg:target: https://factoryboy.readthedocs.io/en/latest/changelog.html:alt: Latest Version.. image:: https://img.shields.io/pypi/pyversions/factory_boy.svg:target: https://pypi.org/project/factory-boy/:alt: Supported Python versions.. image:: https://img.shields.io/pypi/wheel/factory_boy.svg:target: https://pypi.org/project/factory-boy/:alt: Wheel status.. image:: https://img.shields.io/pypi/l/factory_boy.svg:target: https://pypi.org/project/factory-boy/:alt: Licensefactory_boy is a fixtures replacement based on thoughtbot's _.As a fixtures replacement tool, it aims to replace static, hard to maintain fixtureswith easy-to-use factories for complex objects.Instead of building an exhaustive test setup with every possible combination of corner cases, allows you to use objects customized for the current test,while only declaring the test-specific fields:.. code-block:: pythonclass FooTests(unittest.TestCase):

    def test_with_factory_boy(self):
        # We need a 200€, paid order, shipping to australia, for a VIP customer
        order = OrderFactory(
            amount=200,
            status='PAID',
            customer__is_vip=True,
            address__country='AU',
        )
        # Run the tests here

    def test_without_factory_boy(self):
        address = Address(
            street=""42 fubar street"",
            zipcode=""42Z42"",
            city=""Sydney"",
            country=""AU"",
        )
        customer = Customer(
            first_name=""John"",
            last_name=""Doe"",
            phone=""+1234"",
            email=""john.doe@example.org"",
            active=True,
            is_vip=True,
            address=address,
        )
        # etc.
factory_boy is designed to work well with various ORMs (Django, MongoDB, SQLAlchemy),and can easily be extended for other libraries.Its main features include:LinksDownloadPyPI: https://pypi.org/project/factory-boy/.. code-block:: sh$ pip install factory_boy
Source: https://github.com/FactoryBoy/factory_boy/.. code-block:: sh$ git clone git://github.com/FactoryBoy/factory_boy/
$ python setup.py install
Usage.. note:: This section provides a quick summary of factory_boy features.A more detailed listing is available in the full documentation.Defining factories""""""""""""""""""""""""""""""""""""Factories declare a set of attributes used to instantiate a Python object.The class of the object must be defined in the  field of a  attribute:.. code-block:: pythonimport factory
from . import models

class UserFactory(factory.Factory):
    class Meta:
        model = models.User

    first_name = 'John'
    last_name = 'Doe'
    admin = False

# Another, different, factory for the same object
class AdminFactory(factory.Factory):
    class Meta:
        model = models.User

    first_name = 'Admin'
    last_name = 'User'
    admin = True
ORM integration""""""""""""""""""""""""""""""factory_boy integration with Object Relational Mapping (ORM) tools is providedthrough specific  subclasses:More details can be found in the ORM section.Using factories""""""""""""""""""""""""""""""factory_boy supports several different instantiation strategies: build, create, and stub:.. code-block:: python# Returns a User instance that's not saved
user = UserFactory.build()

# Returns a saved User instance.
# UserFactory must subclass an ORM base class, such as DjangoModelFactory.
user = UserFactory.create()

# Returns a stub object (just a bunch of attributes)
obj = UserFactory.stub()
You can use the Factory class as a shortcut for the default instantiation strategy:.. code-block:: python# Same as UserFactory.create()
user = UserFactory()
No matter which strategy is used, it's possible to override the defined attributes by passing keyword arguments:.. code-block:: pycon# Build a User instance and override first_name
>>> user = UserFactory.build(first_name='Joe')
>>> user.first_name
""Joe""
It is also possible to create a bunch of objects in a single call:.. code-block:: pycon>>> users = UserFactory.build_batch(10, first_name=""Joe"")
>>> len(users)
10
>>> [user.first_name for user in users]
[""Joe"", ""Joe"", ""Joe"", ""Joe"", ""Joe"", ""Joe"", ""Joe"", ""Joe"", ""Joe"", ""Joe""]
Realistic, random values""""""""""""""""""""""""""""""""""""""""""""""""Demos look better with random yet realistic values; and those realistic values can also help discover bugs.For this, factory_boy relies on the excellent _ library:.. code-block:: pythonclass RandomUserFactory(factory.Factory):
    class Meta:
        model = models.User

    first_name = factory.Faker('first_name')
    last_name = factory.Faker('last_name')
.. code-block:: pycon>>> RandomUserFactory()
<User: Lucy Murray>
Reproducible random values""""""""""""""""""""""""""""""""""""""""""""""""""""The use of fully randomized data in tests is quickly a problem for reproducing broken builds.To that purpose, factory_boy provides helpers to handle the random seeds it uses, located in the  module:.. code-block:: pythonimport factory.random

def setup_test_environment():
    factory.random.reseed_random('my_awesome_project')
    # Other setup here
Lazy Attributes""""""""""""""""""""""""""""""Most factory attributes can be added using static values that are evaluated when the factory is defined,but some attributes (such as fields whose value is computed from other elements)will need values assigned each time an instance is generated.These ""lazy"" attributes can be added as follows:.. code-block:: pythonclass UserFactory(factory.Factory):
    class Meta:
        model = models.User

    first_name = 'Joe'
    last_name = 'Blow'
    email = factory.LazyAttribute(lambda a: '{}.{}@example.com'.format(a.first_name, a.last_name).lower())
    date_joined = factory.LazyFunction(datetime.now)
.. code-block:: pycon>>> UserFactory().email
""joe.blow@example.com""
.. note::  calls the function with the object being constructed as an argument, when does not send any argument.Sequences""""""""""""""""""Unique values in a specific format (for example, e-mail addresses) can be generated using sequences. Sequences are defined by using  or the decorator :.. code-block:: pythonclass UserFactory(factory.Factory):
    class Meta:
        model = models.User

    email = factory.Sequence(lambda n: 'person{}@example.com'.format(n))

>>> UserFactory().email
'person0@example.com'
>>> UserFactory().email
'person1@example.com'
Associations""""""""""""""""""""""""Some objects have a complex field, that should itself be defined from a dedicated factories.This is handled by the  helper:.. code-block:: pythonclass PostFactory(factory.Factory):
    class Meta:
        model = models.Post

    author = factory.SubFactory(UserFactory)
The associated object's strategy will be used:.. code-block:: python# Builds and saves a User and a Post
>>> post = PostFactory()
>>> post.id is None  # Post has been 'saved'
False
>>> post.author.id is None  # post.author has been saved
False

# Builds but does not save a User, and then builds but does not save a Post
>>> post = PostFactory.build()
>>> post.id is None
True
>>> post.author.id is None
True
Support Policy supports active Python versions as well as PyPy3.Debugging factory_boyDebugging factory_boy can be rather complex due to the long chains of calls.Detailed logging is available through the  logger.A helper, , is available to ease debugging:.. code-block:: pythonwith factory.debug():
    obj = TestModel2Factory()


import logging
logger = logging.getLogger('factory')
logger.addHandler(logging.StreamHandler())
logger.setLevel(logging.DEBUG)
This will yield messages similar to those (artificial indentation):.. code-block:: iniBaseFactory: Preparing tests.test_using.TestModel2Factory(extra={})
  LazyStub: Computing values for tests.test_using.TestModel2Factory(two=<OrderedDeclarationWrapper for <factory.declarations.SubFactory object at 0x1e15610>>)
    SubFactory: Instantiating tests.test_using.TestModelFactory(__containers=(<LazyStub for tests.test_using.TestModel2Factory>,), one=4), create=True
    BaseFactory: Preparing tests.test_using.TestModelFactory(extra={'__containers': (<LazyStub for tests.test_using.TestModel2Factory>,), 'one': 4})
      LazyStub: Computing values for tests.test_using.TestModelFactory(one=4)
      LazyStub: Computed values, got tests.test_using.TestModelFactory(one=4)
    BaseFactory: Generating tests.test_using.TestModelFactory(one=4)
  LazyStub: Computed values, got tests.test_using.TestModel2Factory(two=<tests.test_using.TestModel object at 0x1e15410>)
BaseFactory: Generating tests.test_using.TestModel2Factory(two=<tests.test_using.TestModel object at 0x1e15410>)
Contributingfactory_boy is distributed under the MIT License.Issues should be opened through ; whenever possible, a pull request should be included..Development dependencies can be installed in a _ with:.. code-block:: sh$ pip install --editable '.[dev]'
All pull requests should pass the test suite, which can be launched simply with:.. code-block:: sh$ make testall
In order to test coverage, please use:.. code-block:: sh$ make coverage
To test with a specific framework version, you may use a  target:.. code-block:: sh# list all tox environments
$ tox --listenvs

# run tests inside a specific environment (django/mongoengine/SQLAlchemy are not installed)
$ tox -e py310

# run tests inside a specific environment (django)
$ tox -e py310-djangomain
$ tox -e py310-djangomain-postgres

# run tests inside a specific environment (alchemy)
$ tox -e py310-alchemy
$ tox -e py310-alchemy-postgres

# run tests inside a specific environment (mongoengine)
$ tox -e py310-mongo
PackagingFor users interesting in packaging FactoryBoy into downstream distribution channels(e.g. , , ), the following tips might be helpful:Dependencies""""""""""""""""""""""""The package's run-time dependencies are listed in .The dependencies useful for building and testing the library are covered by the and  extras.Moreover, all development / testing tasks are driven through .Building""""""""""""""""In order to run the build steps (currently only for docs), run:.. code-block:: shpython setup.py egg_info
make doc
Testing""""""""""""""When testing for the active Python environment, run the following:.. code-block:: shmake test
.. note::You must make sure that the ``factory`` module is importable, as it is imported from
the testing code.
"
https://github.com/jazzband/django-debug-toolbar,A configurable set of panels that display various debug information about the current request/response.,"=====================================Django Debug Toolbar |latest-version||jazzband| |build-status| |coverage| |docs| |python-support| |django-support|.. |latest-version| image:: https://img.shields.io/pypi/v/django-debug-toolbar.svg:target: https://pypi.org/project/django-debug-toolbar/:alt: Latest version on PyPI.. |jazzband| image:: https://jazzband.co/static/img/badge.svg:target: https://jazzband.co/:alt: Jazzband.. |build-status| image:: https://github.com/jazzband/django-debug-toolbar/workflows/Test/badge.svg:target: https://github.com/jazzband/django-debug-toolbar/actions:alt: Build Status.. |coverage| image:: https://img.shields.io/badge/Coverage-94%25-green:target: https://github.com/jazzband/django-debug-toolbar/actions/workflows/test.yml?query=branch%3Amain:alt: Test coverage status.. |docs| image:: https://img.shields.io/readthedocs/django-debug-toolbar/latest.svg:target: https://readthedocs.org/projects/django-debug-toolbar/:alt: Documentation status.. |python-support| image:: https://img.shields.io/pypi/pyversions/django-debug-toolbar:target: https://pypi.org/project/django-debug-toolbar/:alt: Supported Python versions.. |django-support| image:: https://img.shields.io/pypi/djversions/django-debug-toolbar:target: https://pypi.org/project/django-debug-toolbar/:alt: Supported Django versionsThe Django Debug Toolbar is a configurable set of panels that display variousdebug information about the current request/response and when clicked, displaymore details about the panel's content.Here's a screenshot of the toolbar in action:.. image:: https://raw.github.com/jazzband/django-debug-toolbar/main/example/django-debug-toolbar.png:alt: Django Debug Toolbar screenshotIn addition to the built-in panels, a number of third-party panels arecontributed by the community.The current stable version of the Debug Toolbar is 4.1.0. It works onDjango ≥ 3.2.4.The Debug Toolbar does not currently support _.Documentation, including installation and configuration instructions, isavailable at https://django-debug-toolbar.readthedocs.io/.The Django Debug Toolbar is released under the BSD license, like Djangoitself. If you like it, please consider contributing!The Django Debug Toolbar was originally created by Rob Hudson in August 2008 and was further developed by many contributors_... _contributors: https://github.com/jazzband/django-debug-toolbar/graphs/contributors"
https://github.com/RocketMap/RocketMap,🌏 Live visualization of all the pokemon in your area... and more!,"RocketMap  Live visualization of all the Pokémon (with option to show gyms, raids and PokéStops) in your area. This is a proof of concept that we can load all the Pokémon visible nearby given a location. Currently runs on a Flask server displaying Google Maps with markers on it.Features:InformationInstallationFor instructions on how to setup and run the tool, please refer to the project .DeploymentPlease note, deployments are not supported officially. You are using these deployment links at your own risk. ContributionsPlease submit all pull requests to  branch.Building off , 's API,  and . Current version relies primarily on the pgoapi and Google Maps JS API.Discord and front-end use  by  and  by  from . License: CC 3.0 BY can be found ."
https://github.com/easy-tensorflow/easy-tensorflow,Simple and comprehensive tutorials in TensorFlow,"Easy-TensorFlow.. image:: https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat:target: https://github.com/easy-tensorflow/easy-tensorflow/pulls.. image:: https://img.shields.io/website-up-down-green-red/http/shields.io.svg:target: http://www.easy-tensorflow.com/.. image:: https://img.shields.io/badge/Made%20with-Python-1f425f.svg:target: https://www.python.org/.. image:: https://img.shields.io/pypi/l/ansicolortags.svg:target: https://pypi.python.org/pypi/ansicolortags/.. image:: https://img.shields.io/github/contributors/Naereen/StrapDown.js.svg:target: https://github.com/easy-tensorflow/easy-tensorflow/graphs/contributors.. image:: https://img.shields.io/github/issues/Naereen/StrapDown.js.svg:target: https://github.com/easy-tensorflow/easy-tensorflow/issuesThe goal of this repository is to provide comprehensive tutorials for TensorFlow while maintaining the simplicity of the code.Each tutorial includes a detailed explanation (written in .ipynb) format, as well as the source code (in .py format).#################Table of Contents#################.. contents:::local::depth: 3============MotivationThere is a necessity to address the motivations for this project. TensorFlow is one of the deep learning frameworks available with the largest community.This repository is dedicated to suggesting a simple path to learn TensorFlow.Why use TensorFlow?
.. image:: _img/why_tensorflow.png:height: 100px:width: 200 px:scale: 50 %:target: https://github.com/easy-tensorflow/easy-tensorflow/blob/master/_img/why_tensorflow.png:align: rightWe can summarize the pros as below:- It’s developed and maintained by Google. As such, a continued support and development is ensured
- Very large and active community
- Low-level and high-level interfaces to network training
- Tensorboard is the powerful visualization suite which is developed to track both the network topology and performance, making debugging even simpler.
- Written in Python (even though some parts crucial for performance is implemented in C++) which is a very attractive language to read and develop in
- Multiple GPUs support. So you can freely run the code on different machines without having to stop or restart the program
- Faster model compilation than Theano-based options
- Faster compile times than Theano
- Is about more than deep learning. TensorFlow actually has tools to support reinforcement learning and other algorithms.
In addition to the aforementioned points, the large community of TensorFlow enrich the developers with the answer to almost all thequestions one may encounter. Furthermore, since most of the developers are using TensorFlow for code development, having a hands-on on TensorFlow is a necessity these days.Why this repository?
In most of the available projects on the web, one of the below issues exists:In this project, we tried to connect parts from easy to advanced with detailed tutorials while keeping the code implementationas simple as possible.=================================================TensorFlow Installation and Setup the EnvironmentThe aim here is to explain how to install TensorFlow library ""step by step"" and ondifferent operating systems. TensorFlow is a python library. Similar to many others, we triedinstalling many side packages and libraries and experienced lots of problems and errors.In order to install TensorFlow please refer to the following link:.. _TensorFlow Installation: http://easy-tensorflow.com/tf-tutorials/basics/install====================TensorFlow Tutorials.. image:: _img/tensorflow.png:height: 100px:width: 200 px:scale: 50 %:target: https://github.com/easy-tensorflow/easy-tensorflow/blob/master/_img/tensorflow.png:align: rightThe tutorials in this repository are partitioned into relevant categories.==========================.. +----+---------------------+----------------------------------------------------------------------------------------+----------------------------------------------+.. | #  |       topic         |   Source Code                                                                          |                                              |.. +====+=====================+========================================================================================+==============================================+.. | 1  | Start-up            | _  / _                        |  _ |.. +----+---------------------+----------------------------------------------------------------------------------------+----------------------------------------------+.. _Installation: https://github.com/easy-tensorflow/easy-tensorflow/tree/master/0_Setup_TensorFlow.. _Basics: https://github.com/easy-tensorflow/easy-tensorflow/tree/master/1_TensorFlow_Basics.. _Logistic_Regression: https://github.com/easy-tensorflow/easy-tensorflow/tree/master/2_Linear_Classifier.. _Feed_Forward_Neural_Network: https://github.com/easy-tensorflow/easy-tensorflow/tree/master/3_Neural_Network.. _Tensorboard: https://github.com/easy-tensorflow/easy-tensorflow/tree/master/4_Tensorboard.. _AutoEncoder: https://github.com/easy-tensorflow/easy-tensorflow/tree/master/5_AutoEncoder.. _Convolutional_Neural_Network: https://github.com/easy-tensorflow/easy-tensorflow/tree/master/6_Convolutional_Neural_Network+----+-----------------------------+----------------------------------------------------------------------------------------+| #  |       topic                 |                                                                                        |+====+=============================+========================================================================================+| 0  | Installation                | _                                                                |+----+-----------------------------+----------------------------------------------------------------------------------------+| 1  | Basics                      | _                                                                      |+----+-----------------------------+----------------------------------------------------------------------------------------+| 2  | Logistic_Regression         | _                                                         |+----+-----------------------------+----------------------------------------------------------------------------------------+| 3  | Feed_Forward_Neural_Network | _                                                 |+----+-----------------------------+----------------------------------------------------------------------------------------+| 4  | Tensorboard                 | _                                                                 |+----+-----------------------------+----------------------------------------------------------------------------------------+| 5  | AutoEncoder                 | _                                                                 |+----+-----------------------------+----------------------------------------------------------------------------------------+| 6  | Convolutional_Neural_Network| _                                                |+----+-----------------------------+----------------------------------------------------------------------------------------+=====================Some Useful Tutorials=============ContributingWhen contributing to this repository, please first discuss the change you wish to make via issue,email, or any other method with the owners of this repository before making a change. For typos, please.Please note we have a code of conduct, please follow it in all your interactions with the project.Pull Request Process
Please consider the following criterions in order to help us in a better way:Final Note
We are looking forward to your kind feedback. Please help us to improve this open source project and make our work better.For contribution, please create a pull request and we will investigate it promptly. Once again, we appreciateyour kind feedback and elaborate code inspections."
https://github.com/cornellius-gp/gpytorch,A highly efficient implementation of Gaussian Processes in PyTorch,"GPyTorchGPyTorch is a Gaussian process library implemented using PyTorch. GPyTorch is designed for creating scalable, flexible, and modular Gaussian process models with ease.Internally, GPyTorch differs from many existing approaches to GP inference by performing most inference operations using numerical linear algebra techniques like preconditioned conjugate gradients.Implementing a scalable GP method is as simple as providing a matrix multiplication routine with the kernel matrix and its derivative via our  interface,or by composing many of our already existing .This allows not only for easy implementation of popular scalable GP techniques,but often also for significantly improved utilization of GPU computing compared to solvers based on the Cholesky decomposition.GPyTorch provides (1) significant GPU acceleration (through MVM based inference);(2) state-of-the-art implementations of the latest algorithmic advances for scalability and flexibility (, , , ,  , ...);(3) easy integration with deep learning frameworks.Examples, Tutorials, and DocumentationSee our  on how to construct all sorts of models in GPyTorch.InstallationRequirements:Install GPyTorch using pip or conda:pip install gpytorch
conda install gpytorch -c gpytorch
(To use packages globally but install GPyTorch as a user-only package, use  above.)Latest (Unstable) VersionTo upgrade to the latest (unstable) version, runpip install --upgrade git+https://github.com/cornellius-gp/linear_operator.git
pip install --upgrade git+https://github.com/cornellius-gp/gpytorch.git
Development versionIf you are contributing a pull request, it is best to perform a manual installation:git clone https://github.com/cornellius-gp/gpytorch.git
cd gpytorch
pip install -e .[dev,docs,examples,keops,pyro,test]  # keops and pyro are optional
ArchLinux PackageNote: Experimental AUR package. For most users, we recommend installation by conda or pip.GPyTorch is also available on the  (AUR).You can install it with an , like , as follows:yay -S python-gpytorch
To discuss any issues related to this AUR package refer to the comments section of.Citing UsIf you use GPyTorch, please cite the following papers:@inproceedings{gardner2018gpytorch,
  title={GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration},
  author={Gardner, Jacob R and Pleiss, Geoff and Bindel, David and Weinberger, Kilian Q and Wilson, Andrew Gordon},
  booktitle={Advances in Neural Information Processing Systems},
  year={2018}
}
ContributingSee the contributing guidelines for information on submitting issues and pull requests.The TeamGPyTorch is primarily maintained by:We would like to thank our other contributors including (but not limited to)Eytan Bakshy,Wesley Maddox,Ke Alexander Wang,Ruihan Wu,Sait Cakmak,David Eriksson,Sam Daulton,Martin Jankowiak,Sam Stanton,Zitong Zhou,David Arbour,Karthik Rajkumar,Bram Wallace,Jared Frank,and many more!AcknowledgementsDevelopment of GPyTorch is supported by funding fromthe ,the ,,the ,and the .LicenseGPyTorch is ."
https://github.com/espressif/esptool,Espressif SoC serial bootloader utility,"esptool.pyA Python-based, open-source, platform-independent utility to communicate with the ROM bootloader in Espressif chips. DocumentationVisit the  or run .ContributeIf you're interested in contributing to esptool.py, please check the .Aboutesptool.py was initially created by Fredrik Ahlberg (@), and later maintained by Angus Gratton (@). It is now supported by Espressif Systems. It has also received improvements from many members of the community.LicenseThis document and the attached source code are released as Free Software under GNU General Public License Version 2 or later. See the accompanying  for a copy."
https://github.com/readthedocs/readthedocs.org,The source code that powers readthedocs.org,"Welcome to Read the Docs|build-status| |docs| |coverage|Purpose_ hosts documentation for the open source community. It supportsSphinx_ docs written with reStructuredText_, and can pull from your Subversion_,Bazaar_, Git_, and Mercurial_ repositories.Then we build documentation and host it for you.Think of it as Continuous Documentation... _Read the docs: https://readthedocs.org/.. _Sphinx: http://www.sphinx-doc.org/.. _reStructuredText: http://www.sphinx-doc.org/en/master/usage/restructuredtext/basics.html.. _Subversion: http://subversion.tigris.org/.. _Bazaar: http://bazaar.canonical.com/.. _Git: http://git-scm.com/.. _Mercurial: https://www.mercurial-scm.org/Documentation for RTDYou will find complete documentation for setting up your project at _... _the Read the Docs site: https://docs.readthedocs.io/Get in touchYou can find information about getting in touch with Read the Docs at our _.ContributingYou can find information about contributing to Read the Docs at our _.Quickstart for GitHub hosted projectsBy the end of this quickstart, you will have a new project automatically updatedwhen you push to GitHub.#. Create an account on _.  You will get an email verifying youremail address which you should accept within 7 days.#. Log in and click on ""Import a Project"".#. Click ""Connect to GitHub"" in order to connect your account's repositories to GitHub.#. When prompted on GitHub, give access to your account.#. Click ""Import a Repository"" and select any desired repository.#. Change any information if desired and click ""Next"".#. All done.  Commit away and your project will auto-update... |build-status| image:: https://circleci.com/gh/readthedocs/readthedocs.org.svg?style=svg:alt: build status:target: https://circleci.com/gh/readthedocs/readthedocs.org.. |docs| image:: https://readthedocs.org/projects/docs/badge/?version=latest:alt: Documentation Status:scale: 100%:target: https://docs.readthedocs.io/en/latest/?badge=latest.. |coverage| image:: https://codecov.io/gh/readthedocs/readthedocs.org/branch/main/graph/badge.svg:alt: Test coverage:scale: 100%:target: https://codecov.io/gh/readthedocs/readthedocs.orgLicense_ © 2010 Read the Docs, Inc. & contributors.. _MIT: LICENSE"
https://github.com/hankcs/pyhanlp,中文分词,"pyhanlp: Python interfaces for HanLP1.x    的Python接口，支持自动下载与升级，兼容Python<=3.8。内部算法经过工业界和学术界考验，配套书籍已经出版，欢迎查阅或点击在线运行。基于深度学习的已于2020年初发布，次世代最先进的多语种NLP技术，与1.x相辅相成，平行发展。安装懒人请点击；小白可直接使用；工程师请先安装，然后执行：conda install -c conda-forge openjdk python=3.8 jpype1=0.7.0 -y
pip install pyhanlp
使用命令来验证安装，如因网络等原因自动安装失败，可参考或。命令行中文分词使用命令进入交互分词模式，输入一个句子并回车，会输出分词结果：$ hanlp segment
商品和服务
商品/n 和/cc 服务/vn
当下雨天地面积水分外严重
当/p 下雨天/n 地面/n 积水/n 分外/d 严重/a
龚学平等领导说,邓颖超生前杜绝超生
龚学平/nr 等/udeng 领导/n 说/v ,/w 邓颖超/nr 生前/t 杜绝/v 超生/vi
还可以重定向输入输出到文件等：$ hanlp segment <<< '欢迎新老师生前来就餐'               
欢迎/v 新/a 老/a 师生/n 前来/vi 就餐/vi
依存句法分析命令为，同样支持交互模式和重定向：$ hanlp parse <<< '徐先生还具体帮助他确定了把画雄鹰、松鼠和麻雀作为主攻目标。'         
1	徐先生	徐先生	nh	nr	_	4	主谓关系	_	_
2	还	还	d	d	_	4	状中结构	_	_
3	具体	具体	a	a	_	4	状中结构	_	_
4	帮助	帮助	v	v	_	0	核心关系	_	_
5	他	他	r	rr	_	4	兼语	_	_
6	确定	确定	v	v	_	4	动宾关系	_	_
7	了	了	u	ule	_	6	右附加关系	_	_
8	把	把	p	pba	_	15	状中结构	_	_
9	画	画	v	v	_	8	介宾关系	_	_
10	雄鹰	雄鹰	n	n	_	9	动宾关系	_	_
11	、	、	wp	w	_	12	标点符号	_	_
12	松鼠	松鼠	n	n	_	10	并列关系	_	_
13	和	和	c	cc	_	14	左附加关系	_	_
14	麻雀	麻雀	n	n	_	10	并列关系	_	_
15	作为	作为	p	p	_	6	动宾关系	_	_
16	主攻	主攻	v	vn	_	17	定中关系	_	_
17	目标	目标	n	n	_	15	动宾关系	_	_
18	。	。	wp	w	_	4	标点符号	_	_
服务器通过来启动内置的http服务器，默认本地访问地址为：http://localhost:8765 ；也可以访问官网演示页面：http://hanlp.hankcs.com/ 。升级通过命令来将升级到最新版。该命令会获取并自动下载安装。欢迎通过查看最新帮助手册。API通过工具类调用常用接口：from pyhanlp import *

print(HanLP.segment('你好，欢迎在Python中调用HanLP的API'))
for term in HanLP.segment('下雨天地面积水'):
    print('{}\t{}'.format(term.word, term.nature)) # 获取单词与词性
testCases = [
    ""商品和服务"",
    ""结婚的和尚未结婚的确实在干扰分词啊"",
    ""买水果然后来世博园最后去世博会"",
    ""中国的首都是北京"",
    ""欢迎新老师生前来就餐"",
    ""工信处女干事每月经过下属科室都要亲口交代24口交换机等技术性器件的安装工作"",
    ""随着页游兴起到现在的页游繁盛，依赖于存档进行逻辑判断的设计减少了，但这块也不能完全忽略掉。""]
for sentence in testCases: print(HanLP.segment(sentence))
# 关键词提取
document = ""水利部水资源司司长陈明忠9月29日在国务院新闻办举行的新闻发布会上透露，"" \
           ""根据刚刚完成了水资源管理制度的考核，有部分省接近了红线的指标，"" \
           ""有部分省超过红线的指标。对一些超过红线的地方，陈明忠表示，对一些取用水项目进行区域的限批，"" \
           ""严格地进行水资源论证和取水许可的批准。""
print(HanLP.extractKeyword(document, 2))
# 自动摘要
print(HanLP.extractSummary(document, 3))
# 依存句法分析
print(HanLP.parseDependency(""徐先生还具体帮助他确定了把画雄鹰、松鼠和麻雀作为主攻目标。""))
更多功能更多功能，包括但不限于：请阅读和以了解更多。调用更底层的API需要参考Java语法用JClass引入更深的类路径。以感知机词法分析器为例，这个类位于包名下，所以先用得到类，然后就可以调用了：PerceptronLexicalAnalyzer = JClass('com.hankcs.hanlp.model.perceptron.PerceptronLexicalAnalyzer')
analyzer = PerceptronLexicalAnalyzer()
print(analyzer.analyze(""上海华安工业（集团）公司董事长谭旭光和秘书胡花蕊来到美国纽约现代艺术博物馆参观""))
输出：[上海/ns 华安/nz 工业/n （/w 集团/n ）/w 公司/n]/nt 董事长/n 谭旭光/nr 和/c 秘书/n 胡花蕊/nr 来到/v [美国/ns 纽约/ns 现代/t 艺术/n 博物馆/n]/ns 参观/v
如果你需要多线程安全性，可使用；如果你需要延迟加载，可使用。如果你经常使用某个类，欢迎将其写入中并提交pull request，谢谢！与其他项目共享data具备高度可自定义的特点，所有模型和词典都可以自由替换。如果你希望与别的项目共享同一套data，只需将该项目的配置文件拷贝到pyhanlp的安装目录下即可。本机安装目录可以通过获取。同时，还可以通过临时加载另一个配置文件：hanlp segment --config path/to/another/hanlp.properties
测试git clone https://github.com/hankcs/pyhanlp.git
cd pyhanlp
pip install -e .
python tests/test_hanlp.py
反馈任何bug，请前往。提问请上反馈，谢谢。自然语言处理是一门博大精深的学科，掌握理论才能发挥出工具的全部性能。新手可考虑这本入门书：一本配套HanLP的NLP入门书，基础理论与生产代码并重，Python与Java双实现。从基本概念出发，逐步介绍中文分词、词性标注、命名实体识别、信息抽取、文本聚类、文本分类、句法分析这几个热门问题的算法原理与工程实现。书中通过对多种算法的讲解，比较了它们的优缺点和适用场景，同时详细演示生产级成熟代码，助你真正将自然语言处理应用在生产环境中。由南方科技大学数学系创系主任夏志宏、微软亚洲研究院副院长周明、字节跳动人工智能实验室总监李航、华为诺亚方舟实验室语音语义首席科学家刘群、小米人工智能实验室主任兼NLP首席科学家王斌、中国科学院自动化研究所研究员宗成庆、清华大学副教授刘知远、北京理工大学副教授张华平和52nlp作序推荐。感谢各位前辈老师，希望这个项目和这本书能成为大家工程和学习上的“蝴蝶效应”，帮助大家在NLP之路上蜕变成蝶。授权协议Apache License 2.0"
https://github.com/arrow-py/arrow,🏹 Better dates & times for Python,"Arrow: Better dates & times for Python.. start-inclusion-marker-do-not-remove.. image:: https://github.com/arrow-py/arrow/workflows/tests/badge.svg?branch=master:alt: Build Status:target: https://github.com/arrow-py/arrow/actions?query=workflow%3Atests+branch%3Amaster.. image:: https://codecov.io/gh/arrow-py/arrow/branch/master/graph/badge.svg:alt: Coverage:target: https://codecov.io/gh/arrow-py/arrow.. image:: https://img.shields.io/pypi/v/arrow.svg:alt: PyPI Version:target: https://pypi.python.org/pypi/arrow.. image:: https://img.shields.io/pypi/pyversions/arrow.svg:alt: Supported Python Versions:target: https://pypi.python.org/pypi/arrow.. image:: https://img.shields.io/pypi/l/arrow.svg:alt: License:target: https://pypi.python.org/pypi/arrow.. image:: https://img.shields.io/badge/code%20style-black-000000.svg:alt: Code Style: Black:target: https://github.com/psf/blackArrow is a Python library that offers a sensible and human-friendly approach to creating, manipulating, formatting and converting dates, times and timestamps. It implements and updates the datetime type, plugging gaps in functionality and providing an intelligent module API that supports many common creation scenarios. Simply put, it helps you work with dates and times with fewer imports and a lot less code.Arrow is named after the _ and is heavily inspired by _ and _.Why use Arrow over built-in modules?Python's standard library and some other low-level modules have near-complete date, time and timezone functionality, but don't work very well from a usability perspective:FeaturesQuick StartInstallation
To install Arrow, use `pip <https://pip.pypa.io/en/stable/quickstart/>`_ or `pipenv <https://docs.pipenv.org>`_:

.. code-block:: console

    $ pip install -U arrow

Example Usage
.. code-block:: python>>> import arrow
>>> arrow.get('2013-05-11T21:23:58.970460+07:00')
<Arrow [2013-05-11T21:23:58.970460+07:00]>

>>> utc = arrow.utcnow()
>>> utc
<Arrow [2013-05-11T21:23:58.970460+00:00]>

>>> utc = utc.shift(hours=-1)
>>> utc
<Arrow [2013-05-11T20:23:58.970460+00:00]>

>>> local = utc.to('US/Pacific')
>>> local
<Arrow [2013-05-11T13:23:58.970460-07:00]>

>>> local.timestamp()
1368303838.970460

>>> local.format()
'2013-05-11 13:23:58 -07:00'

>>> local.format('YYYY-MM-DD HH:mm:ss ZZ')
'2013-05-11 13:23:58 -07:00'

>>> local.humanize()
'an hour ago'

>>> local.humanize(locale='ko-kr')
'한시간 전'
.. end-inclusion-marker-do-not-removeDocumentationFor full documentation, please visit _.ContributingContributions are welcome for both code and localizations (adding and updating locales). Begin by gaining familiarity with the Arrow library and its features. Then, jump into contributing:#. Find an issue or feature to tackle on the . Issues marked with the  may be a great place to start!#. Fork _ on GitHub and begin making changes in a branch.#. Add a few tests to ensure that the bug was fixed or the feature works as expected.#. Run the entire test suite and linting checks by running one of the following commands:  (if you have _ installed) OR  (if you do not have Python 3.9 installed, replace  with the latest Python version on your system).#. Submit a pull request and await feedback 😃.If you have any questions along the way, feel free to ask them _.Support Arrow_ is an online funding platform that provides tools to raise money and share your finances with full transparency. It is the platform of choice for individuals and companies to make one-time or recurring donations directly to the project. If you are interested in making a financial contribution, please visit the _."
https://github.com/liuhuanyong/QASystemOnMedicalKG, A tutorial and implement of disease centered Medical knowledge graph and qa system based on it。知识图谱构建，自动问答，基于kg的自动问答。以疾病为中心的一定规模医药领域知识图谱，并以该知识图谱完成自动问答与分析服务。,"QABasedOnMedicaKnowledgeGraphself-implement of disease centered Medical graph from zero to full and sever as question answering base. 从无到有搭建一个以疾病为中心的一定规模医药领域知识图谱，并以该知识图谱完成自动问答与分析服务。项目介绍知识图谱是目前自然语言处理的一个热门方向，关于较全面的参考资料，可以查看我的ccks2018参会总结(https://github.com/liuhuanyong/CCKS2018Summary )。与知识图谱相关的另一种形态，即事理图谱，本人在这方面也尝试性地积累了一些工作，可参考：(https://github.com/liuhuanyong/ComplexEventExtraction )关于知识图谱概念性的介绍就不在此赘述。目前知识图谱在各个领域全面开花，如教育、医疗、司法、金融等。本项目立足医药领域，以垂直型医药网站为数据来源，以疾病为核心，构建起一个包含7类规模为4.4万的知识实体，11类规模约30万实体关系的知识图谱。本项目将包括以下两部分的内容：项目最终效果话不多少，直接上图。以下两图是实际问答运行过程中的截图：项目运行方式1、配置要求：要求配置neo4j数据库及相应的python依赖包。neo4j数据库用户名密码记住，并修改相应文件。2、知识图谱数据导入：python build_medicalgraph.py，导入的数据较多，估计需要几个小时。3、启动问答：python chat_graph.py以下介绍详细方案一、医疗知识图谱构建1.1 业务驱动的知识图谱构建框架1.2 脚本目录prepare_data/datasoider.py：网络资讯采集脚本prepare_data/datasoider.py：网络资讯采集脚本prepare_data/max_cut.py：基于词典的最大向前/向后切分脚本build_medicalgraph.py：知识图谱入库脚本    　　1.3 医药领域知识图谱规模1.3.1 neo4j图数据库存储规模1.3.2 知识图谱实体类型| 实体类型 | 中文含义 | 实体数量 |举例 || :--- | :---: | :---: | :--- || Check | 诊断检查项目 | 3,353| 支气管造影;关节镜检查|| Department | 医疗科目 | 54 |  整形美容科;烧伤科|| Disease | 疾病 | 8,807 |  血栓闭塞性脉管炎;胸降主动脉动脉瘤|| Drug | 药品 | 3,828 |  京万红痔疮膏;布林佐胺滴眼液|| Food | 食物 | 4,870 |  番茄冲菜牛肉丸汤;竹笋炖羊肉|| Producer | 在售药品 | 17,201 |  通药制药青霉素V钾片;青阳醋酸地塞米松片|| Symptom | 疾病症状 | 5,998 |  乳腺组织肥厚;脑实质深部出血|| Total | 总计 | 44,111 | 约4.4万实体量级|1.3.3 知识图谱实体关系类型| 实体关系类型 | 中文含义 | 关系数量 | 举例|| :--- | :---: | :---: | :--- || belongs_to | 属于 | 8,844| <妇科,属于,妇产科>|| common_drug | 疾病常用药品 | 14,649 | <阳强,常用,甲磺酸酚妥拉明分散片>|| do_eat |疾病宜吃食物 | 22,238| <胸椎骨折,宜吃,黑鱼>|| drugs_of |  药品在售药品 | 17,315| <青霉素V钾片,在售,通药制药青霉素V钾片>|| need_check | 疾病所需检查 | 39,422| <单侧肺气肿,所需检查,支气管造影>|| no_eat | 疾病忌吃食物 | 22,247| <唇病,忌吃,杏仁>|| recommand_drug | 疾病推荐药品 | 59,467 | <混合痔,推荐用药,京万红痔疮膏>|| recommand_eat | 疾病推荐食谱 | 40,221 | <鞘膜积液,推荐食谱,番茄冲菜牛肉丸汤>|| has_symptom | 疾病症状 | 5,998 |  <早期乳腺癌,疾病症状,乳腺组织肥厚>|| acompany_with | 疾病并发疾病 | 12,029 | <下肢交通静脉瓣膜关闭不全,并发疾病,血栓闭塞性脉管炎>|| Total | 总计 | 294,149 | 约30万关系量级|1.3.4 知识图谱属性类型| 属性类型 | 中文含义 | 举例 || :--- | :---: | :---: || name | 疾病名称 | 喘息样支气管炎 || desc | 疾病简介 | 又称哮喘性支气管炎... || cause | 疾病病因 | 常见的有合胞病毒等...|| prevent | 预防措施 | 注意家族与患儿自身过敏史... || cure_lasttime | 治疗周期 | 6-12个月 || cure_way | 治疗方式 | ""药物治疗"",""支持性治疗"" || cured_prob | 治愈概率 | 95% || easy_get | 疾病易感人群 | 无特定的人群 |二、基于医疗知识图谱的自动问答2.1 技术架构2.2 脚本结构question_classifier.py：问句类型分类脚本question_parser.py：问句解析脚本chatbot_graph.py：问答程序脚本  2.3　支持问答类型| 问句类型 | 中文含义 | 问句举例 || :--- | :---: | :---: || disease_symptom | 疾病症状| 乳腺癌的症状有哪些？ || symptom_disease | 已知症状找可能疾病 | 最近老流鼻涕怎么办？ || disease_cause | 疾病病因 | 为什么有的人会失眠？|| disease_acompany | 疾病的并发症 | 失眠有哪些并发症？ || disease_not_food | 疾病需要忌口的食物 | 失眠的人不要吃啥？ || disease_do_food | 疾病建议吃什么食物 | 耳鸣了吃点啥？ || food_not_disease | 什么病最好不要吃某事物 | 哪些人最好不好吃蜂蜜？ || food_do_disease | 食物对什么病有好处| 鹅肉有什么好处？ || disease_drug | 啥病要吃啥药 | 肝病要吃啥药？ || drug_disease | 药品能治啥病 | 板蓝根颗粒能治啥病？ || disease_check | 疾病需要做什么检查 | 脑膜炎怎么才能查出来？|| check_disease |　检查能查什么病 | 全血细胞计数能查出啥来？ || disease_prevent | 预防措施| 怎样才能预防肾虚？ || disease_lasttime | 治疗周期 | 感冒要多久才能好？ || disease_cureway | 治疗方式 | 高血压要怎么治？ || disease_cureprob | 治愈概率 | 白血病能治好吗？ || disease_easyget | 疾病易感人群 | 什么人容易得高血压？ || disease_desc | 疾病描述 | 糖尿病 |问答结果展示    用户:乳腺癌的症状有哪些？
    小勇: 乳腺癌的症状包括：乳腺癌的远处转移；胸痛；乳头溢液；乳头破碎；肝肿大；泌乳障碍；乳头内陷；乳房肿块；剧痛
    ******************************************************************************************
    用户:最近老流鼻涕怎么办？
    小勇: 症状流鼻涕可能染上的疾病有：枯草热；副流行性感冒；急性上呼吸道感染；硫化氢中毒；小儿衣原体肺炎；风寒感冒；慢性额窦炎；鼻源性头痛；人禽流行性感冒；小儿流行性喘憋性肺炎；病毒性感冒；慢性鼻炎；风热犯肺；感冒；顿呛；小儿急性上呼吸道感染；嗜酸细胞增多性非变态反应性鼻炎；干酪性鼻窦炎；下呼吸道感染；麻疹
    ******************************************************************************************
    用户:为什么有的人会失眠？
    小勇: 失眠可能的成因有：躯体疾病和服用药物可以影响睡眠，如消化不良，头痛，背痛，关节炎，心脏病，糖尿病，哮喘，鼻窦炎，溃疡病，或服用某些影响中枢神经的药物。
    由于生活方式引起睡眠问题也很常见，如饮用咖啡或茶叶，晚间饮酒，睡前进食或晚饭较晚造成满腹食物尚未消化，大量吸烟，睡前剧烈的体力活动，睡前过度的精神活动，夜班工作，白天小睡，上床时间不规律，起床时间不规律。
    可能的原因有压力很大，过度忧虑，紧张或焦虑，悲伤或抑郁，生气，容易出现睡眠问题。
    吵闹的睡眠环境，睡眠环境过于明亮，污染，过度拥挤。
    ******************************************************************************************
    用户:失眠有哪些并发症？
    小勇: 失眠的症状包括：心肾不交；神经性耳鸣；咽鼓管异常开放症；偏执狂；十二指肠胃反流及胆汁反流性胃炎；腋臭；黧黑斑；巨细胞动脉炎；Stargardt病；抑郁症；腔隙性脑梗死；甲状腺功能亢进伴发的精神障碍；紧张性头痛；胃下垂；心血虚；迷路震荡；口腔结核性溃疡；痰饮；游走性结节性脂膜炎；小儿脑震荡
    ******************************************************************************************
    用户:失眠的人不要吃啥？
    小勇: 失眠忌食的食物包括有：油条；河蚌；猪油（板油）；淡菜(鲜)
    ******************************************************************************************
    用户:耳鸣了吃点啥？
    小勇: 耳鸣宜食的食物包括有：南瓜子仁;鸡翅;芝麻;腰果
    推荐食谱包括有：紫菜芙蓉汤;羊肉汤面;油豆腐油菜;紫菜鸡蛋莲草汤;乌药羊肉汤;可乐鸡翅;栗子鸡翅;冬菇油菜心
    ******************************************************************************************
    用户:哪些人最好不好吃蜂蜜？
    小勇: 患有散发性脑炎伴发的精神障碍；情感性心境障碍；蝎螫伤；四肢淋巴水肿；农药中毒所致的精神障碍；肝错构瘤；细菌性肺炎；急性高原病；小儿颅后窝室管膜瘤；柯萨奇病毒疹；眼眶静脉性血管瘤；乙脑伴发的精神障碍；晚期产后出血；吸入性肺炎；腓总神经损伤；铍及其化合物引起的皮肤病；猝死型冠心病；彼得异常；过敏性急性小管间质性肾炎；小儿腹胀的人最好不要吃蜂蜜
    ******************************************************************************************
    用户:鹅肉有什么好处？
    小勇: 患有子宫内膜厚；呼吸疾病；肛肠病；闭经；丧偶后适应性障碍；宫颈外翻；巨球蛋白血症；急性颌下腺炎；锥体外系损害；腺样体炎；咳嗽；错构瘤；牙科病；子宫内膜炎；闭锁综合征；结膜炎；恶性淋巴瘤；足外翻；神经炎；病理性近视的人建议多试试鹅肉
    ******************************************************************************************
    用户:肝病要吃啥药？
    小勇: 肝病宜食的食物包括有：鹅肉;鸡肉;鸡肝;鸡腿
    推荐食谱包括有：小米红糖粥;小米蛋奶粥;扁豆小米粥;黄豆小米粥;人参小米粥;小米粉粥;鲜菇小米粥;芝麻小米粥
    肝病通常的使用的药品包括：恩替卡韦分散片；维生素C片；二十五味松石丸；拉米夫定胶囊；阿德福韦酯片
    ******************************************************************************************
    用户:板蓝根颗粒能治啥病？
    小勇: 板蓝根颗粒主治的疾病有流行性腮腺炎；喉痹；喉炎；咽部异感症；急性单纯性咽炎；腮腺隙感染；过敏性咽炎；咽囊炎；急性鼻咽炎；喉水肿；慢性化脓性腮腺炎；慢性咽炎；急性喉炎；咽异感症；鼻咽炎；锁喉痈；小儿咽喉炎；喉返神经损伤；化脓性腮腺炎；喉血管瘤,可以试试
    ******************************************************************************************
    用户:脑膜炎怎么才能查出来？
    小勇: 脑膜炎通常可以通过以下方式检查出来：脑脊液钠；尿常规；Fisher手指试验；颈项强直；脑脊液细菌培养；尿谷氨酰胺；脑脊液钾；脑脊液天门冬氨酸氨基转移酶；脑脊液病原体检查；硝酸盐还原试验
    ******************************************************************************************
    用户:怎样才能预防肾虚？
    小勇: 肾虚可能的成因有：1、多因房劳过度，或少年频繁手淫。2、思虑忧郁，损伤心脾，则病及阳明冲脉。3、恐惧伤肾，恐则伤肾。4、肝主筋，阴器为宗筋之汇，若情志不遂，忧思郁怒，肝失疏泄条达，则宗筋所聚无能。5、湿热下注，宗筋弛纵。
    肾虚是肾脏精气阴阳不足所产生的诸如精神疲乏、头晕耳鸣、健忘脱发、腰脊酸痛、遗精阳痿、男子不育、女子不孕、更年期综合征等多种病证的一个综合概念。关于肾虚形成的原因，可归结为两个方面，一为先天禀赋不足，二为后天因素引起。
    从引起肾虚的先天因素来看，首先是先天禀赋薄弱。《灵枢.寿天刚柔》篇说：“人之生也，有刚有柔，有弱有强。”由于父母体弱多病，精血亏虚时怀孕;或酒后房事怀孕;或年过五十精气力量大减之时怀孕;或男女双方年龄不够，身体发育不完全结婚，也就是早婚时怀孕，或生育过多，精血过度耗损;或妊娠期中失于调养，胎气不足等等都可导致肾的精气亏虚成为肾虚证形成的重要原因;其次，如果肾藏精功能失常就会导致性功能异常，生殖功能下降，影响生殖能力，便会引起下一代形体虚衰，或先天畸形、痴呆、缺陷、男子出现精少不育、早泄，女子出现闭经不孕、小产、习惯性流产等等。
    肾虚的预防措施包括：肾虚日常预防
    在预防方面，因起病与恣情纵欲有关的，应清心寡欲，戒除手淫;如与全身衰弱、营养不良或身心过劳有关的，应适当增加营养或注意劳逸结合，节制性欲。
    1、性生活要适度，不勉强，不放纵。
    2、饮食方面：无力疲乏时多吃含铁、蛋白质的食物，如木耳、大枣、乌鸡等;消化不良者多喝酸奶，吃山楂;平日护肾要多吃韭菜、海参、人参、乌鸡、家鸽等。
    3、经常进行腰部活动，这些运动可以健运命门，补肾纳气。还可多做一些刺激脚心的按摩，中医认为，脚心的涌泉穴是浊气下降的地方，经常按摩涌泉穴，可益精补肾、强身健体、防止早衰，并能舒肝明目，清喉定心，促进睡眠，增进食欲。
    4、充足的睡眠也是恢复精气神的重要保障，工作再紧张，家里的烦心事再多，到了该睡觉的时候也要按时休息。
    健康教育
    1、过度苦寒、冰凉的食物易伤肾，如芦荟、苦瓜、雪糕、鹅肉、啤酒进食过多都伤肾，应该多食黑色素含量高和温补性中药如黑米黑豆等。
    2、男性接触过多的洗涤剂也伤肾，家庭应少用洗涤剂清洗餐具及蔬果，以免洗涤剂残留物被过多摄入。
    3、适当运动可延缓衰老，但强度不宜太大，应选能力所及的运动项目，以促进血液循环，可改善血淤、气损等情况。散步、慢跑、快步走，或在鹅卵石上赤足适当行走，都会促进血液循环，对肾虚有辅助治疗作用。
    4、保持良好的作息习惯，尽量避免熬夜。
    5、积极参加户外运动，放松心情。
    6、不要给自己太大的压力，学会合理减压。
    ******************************************************************************************
    用户:感冒要多久才能好？
    小勇: 感冒治疗可能持续的周期为：7-14天
    ******************************************************************************************
    用户:高血压要怎么治？
    小勇: 高血压可以尝试如下治疗：药物治疗;手术治疗;支持性治疗
    ******************************************************************************************
    用户:白血病能治好吗？
    小勇: 白血病治愈的概率为（仅供参考）：50%-70%
    ******************************************************************************************
    用户:什么人容易得高血压？
    小勇: 高血压的易感人群包括：有高血压家族史，不良的生活习惯，缺乏运动的人群
    ******************************************************************************************
    用户:糖尿病
    小勇: 糖尿病,熟悉一下：糖尿病是一种比较常见的内分泌代谢性疾病。该病发病原因主要是由于胰岛素分泌不足，以及胰升高血糖素不适当地分泌过多所引起。多见于40岁以上喜食甜食而肥胖的病人，城市多于农村，常有家族史，故与遗传有关。少数病人与病毒感染和自身免疫反应有关。主要表现为烦渴、多饮、多尿、多食、乏力、消瘦等症状。生命的常见病，伴发高血压、冠心病、高脂血症等，严重时危及生命。
    中医学认为，肝主疏泄，关系人体接收机的升降与调畅，肝气郁滞则气机升降输布紊乱，肝失疏泄则血糖等精微物质不能随清阳之气输布于周身而郁滞于血中，出现高血糖或精微物质的输布紊乱，反见血糖升高，进一步导致血脂、蛋白等其它精微物质紊乱，引起其他合并症，治疗以疏肝调气为主，顺肝条达之性以恢复其生理功能，肝气条达，气机调畅，精微得以输布，糖被利用而血糖自然下降。
    另外，因糖尿病的发生和饮食有关，饮食控制的好坏直接影响着治疗的效果。再就是配合运动，注意调摄情志，再适当的配合中药治疗会取得良好的治疗效果。 
    ******************************************************************************************
    用户:全血细胞计数能查出啥来
    小勇: 通常可以通过全血细胞计数检查出来的疾病有成人类风湿性关节炎性巩膜炎；外阴-阴道-牙龈综合征；电击伤；老年收缩期高血压；小儿肝硬化；异常血红蛋白病；痴呆综合征；高血压病伴发的精神障碍；睾丸淋巴瘤；叶酸缺乏所致贫血；眼球内炎；不稳定血红蛋白病；类癌综合征；老年痴呆；急性淋巴管炎；宫颈妊娠；蚕食性角膜溃疡；低增生性急性白血病；交感性眼炎；原发性免疫缺陷病
总结１、本项目完成了从无到有，以垂直网站为数据来源，构建起以疾病为中心的医疗知识图谱，实体规模4.4万，实体关系规模30万。并基于此，搭建起了一个可以回答18类问题的自动问答小系统,总共耗时3天。其中，数据采集与整理1天，知识图谱构建与入库0.5天，问答系统组件1.5天。总的来说，还是比较快速。2、本项目以业务驱动，构建医疗知识图谱，知识schema设计基于所采集的结构化数据生成(对网页结构化数据进行xpath解析)。3、本项目以neo4j作为存储，并基于传统规则的方式完成了知识问答，并最终以cypher查询语句作为问答搜索sql，支持了问答服务。4、本项目可以快速部署，数据已经放在data/medical.json当中，本项目的数据，如侵犯相关单位权益，请联系我删除。本数据请勿商用，以免引起不必要的纠纷。在本项目中的部署上，可以遵循项目运行步骤，完成数据库搭建，并提供搜索服务。5、本项目还有不足：关于疾病的起因、预防等，实际返回的是一大段文字，这里其实可以引入事件抽取的概念，进一步将原因结构化表示出来。这个可以后面进行尝试。    If any question about the project or me ,see https://liuhuanyong.github.io/如有自然语言处理、知识图谱、事理图谱、社会计算、语言资源建设等问题或合作，可联系我：1、我的github项目介绍：https://liuhuanyong.github.io2、我的csdn博客：https://blog.csdn.net/lhy20143、about me:刘焕勇，中国科学院软件研究所，lhy_in_blcu@126.com.4、我的技术公众号:老刘说NLP,扫码一键关注："
https://github.com/lyst/lightfm,"A Python implementation of LightFM, a hybrid recommendation algorithm.","LightFM| Build status | ||---|---|| Linux ||| OSX (OpenMP disabled)||| Windows (OpenMP disabled) || LightFM is a Python implementation of a number of popular recommendation algorithms for both implicit and explicit feedback, including efficient implementation of BPR and WARP ranking losses. It's easy to use, fast (via multithreaded model estimation), and produces high quality results.It also makes it possible to incorporate both item and user metadata into the traditional matrix factorization algorithms. It represents each user and item as the sum of the latent representations of their features, thus allowing recommendations to generalise to new items (via item features) and to new users (via user features).For more details, see the .Need help? Contact me via , , or .InstallationInstall from :pip install lightfm
or Conda:conda install -c conda-forge lightfm
QuickstartFitting an implicit feedback model on the MovieLens 100k dataset is very easy:from lightfm import LightFM
from lightfm.datasets import fetch_movielens
from lightfm.evaluation import precision_at_k

# Load the MovieLens 100k dataset. Only five
# star ratings are treated as positive.
data = fetch_movielens(min_rating=5.0)

# Instantiate and train the model
model = LightFM(loss='warp')
model.fit(data['train'], epochs=30, num_threads=2)

# Evaluate the trained model
test_precision = precision_at_k(model, data['test'], k=5).mean()
Articles and tutorials on using LightFMHow to citePlease cite LightFM if it helps your research. You can use the following BibTeX entry:@inproceedings{DBLP:conf/recsys/Kula15,
  author    = {Maciej Kula},
  editor    = {Toine Bogers and
               Marijn Koolen},
  title     = {Metadata Embeddings for User and Item Cold-start Recommendations},
  booktitle = {Proceedings of the 2nd Workshop on New Trends on Content-Based Recommender
               Systems co-located with 9th {ACM} Conference on Recommender Systems
               (RecSys 2015), Vienna, Austria, September 16-20, 2015.},
  series    = {{CEUR} Workshop Proceedings},
  volume    = {1448},
  pages     = {14--21},
  publisher = {CEUR-WS.org},
  year      = {2015},
  url       = {http://ceur-ws.org/Vol-1448/paper4.pdf},
}
DevelopmentPull requests are welcome. To install for development:When making changes to the  extension files, you'll need to run  in order to produce the extension  files before running ."
https://github.com/ungoogled-software/ungoogled-chromium,"Google Chromium, sans integration with Google","ungoogled-chromiumA lightweight approach to removing Google web service dependencyHelp is welcome! See the  document for more information.ObjectivesIn descending order of significance (i.e. most important objective first):In scenarios where the objectives conflict, the objective of higher significance should take precedence.Content OverviewMotivation and PhilosophyWithout signing in to a Google Account, Chromium does pretty well in terms of security and privacy. However, Chromium still has some dependency on Google web services and binaries. In addition, Google designed Chromium to be easy and intuitive for users, which means they compromise on transparency and control of internal operations.ungoogled-chromium addresses these issues in the following ways:These features are implemented as configuration flags, patches, and custom scripts. For more details, consult the .Feature OverviewThis section overviews the features of ungoogled-chromium. For more detailed information, it is best to consult the source code.Contents of this section:Key FeaturesThese are the core features introduced by ungoogled-chromium.Enhancing FeaturesThese are the non-essential features introduced by ungoogled-chromium.Borrowed FeaturesIn addition to the features introduced by ungoogled-chromium, ungoogled-chromium selectively borrows many features from the following projects (in approximate order of significance):Supported Platforms and Distributions.Other platforms are discussed and tracked in this repository's Issue Tracker. Learn more about using the Issue Tracker under the section .DownloadsAutomated or maintained buildsungoogled-chromium is available in the following software repositories:If your GNU/Linux distribution is not listed, there are distro-independent builds available via the following package managers:Third-party binariesIf your operating system is not listed above, you can also try to NOTE: These binaries are provided by anyone who are willing to build and submit them. Because these binaries are not necessarily These binaries are known as contributor binaries.Source CodeThis repository only contains the common code for all platforms; it does not contain all the configuration and scripts necessary to build ungoogled-chromium. Most users will want to use platform-specific repos, where all the remaining configuration and scripts are provided for specific platforms:.If you wish to include ungoogled-chromium code in your own build process, consider using . These tags follow the format  whereAdditionally, most platform-specific repos extend their tag scheme upon this one.Building the source code: MirrorsList of mirrors:FAQBuilding InstructionsDesign DocumentationContributing, Reporting, ContactingCreditsRelated ProjectsList of known projects that fork or use changes from ungoogled-chromium:LicenseBSD-3-clause. See "
https://github.com/crypto101/book,"Crypto 101, the introductory book on cryptography.","======================Crypto 101: the book.. image:: https://github.com/crypto101/book/actions/workflows/ci.yml/badge.svg?branch=master:target: https://github.com/crypto101/book/actions/workflows/ci.yml?branch=masterThis is the source repository for , the introductory book... _: https://www.crypto101.io/.. _lvh: https://twitter.com/lvhLicenseSee the _ file.TranslationsFor now, crypto101 is only available in english, but _.BuildingRun  in the root directory of the repository to convert thesource files into rendered versions of all supported formats.DependenciesDue to the high number of dependencies, using docker is highly recommended:.. code-block:: shdocker build -t crypto101 docker/docker run --rm -it -v ""$(realpath .)"":/repo -u ""$(id -u)"" crypto101 ./make-lang YOUR_LANGUAGE_CODE html latexpdf epub must a valid _,like , ,  or .You can find the install procedure for the dependencies for _ and _ intheir dedicated dockerfiles."
https://github.com/omab/django-social-auth,Django social authentication made simple,"NOTE: THIS LIBRARY IS DEPRECATED IN FAVOR OF .  AND SHOULD BE CONSIDEREDDjango Social AuthDjango Social Auth is an easy way to setup social authentication/authorizationmechanism for Django projects.Crafted using base code from django-twitter-oauth_ and django-openid-auth_,it implements a common interface to define new authentication providers fromthird parties.You can view this app's documentation on _ too... contents:: Table of ContentsFeaturesThis application provides user registration and login using social sitecredentials. Some features are:DemoThere's a demo at http://social.matiasaguirre.net/.Note: It lacks some backends' support at the moment.ContactJoin the _ and bring any questions or suggestionsthat would improve this application.Also join the IRC channel  on Freenode server.DocumentationExtensive documentation at _.DependenciesDependencies that must be met to use the application:InstallationFrom pypi_::$ pip install django-social-auth
or::$ easy_install django-social-auth
or clone from github_::$ git clone git://github.com/omab/django-social-auth.git
and add social_auth to PYTHONPATH::$ export PYTHONPATH=$PYTHONPATH:$(pwd)/django-social-auth/
or::$ cd django-social-auth
$ sudo python setup.py install
SupportIf you're having problems with using the project, use the support forum at CodersClan... image:: http://www.codersclan.net/graphics/getSupport_github4.png:width: 100px:height: 100px:scale: 10:target: http://codersclan.net/forum/index.php?repo_id=7Copyrights and Licence is protected by BSD licence.Some bits were derived from others' work and copyrighted by:.. _django-twitter-oauth: https://github.com/henriklied/django-twitter-oauth.. _django-openid-auth: https://launchpad.net/django-openid-auth.. _Read the Docs: http://django-social-auth.readthedocs.org/.. _Google OpenID: https://developers.google.com/accounts/docs/OpenID.. _Google OAuth: https://developers.google.com/accounts/docs/OAuth.. _Google OAuth2: https://developers.google.com/accounts/docs/OAuth2.. _Yahoo OpenID: http://openid.yahoo.com/.. _OpenId: http://openid.net/.. _myOpenID: https://www.myopenid.com/.. _Twitter OAuth: http://dev.twitter.com/pages/oauth_faq.. _Facebook OAuth: http://developers.facebook.com/docs/authentication/.. _DISQUS OAuth: http://disqus.com/api/docs/auth/.. _LiveJournal OpenID: http://www.livejournal.com/support/faqbrowse.bml?faqid=283.. _Orkut OAuth:  http://code.google.com/apis/orkut/docs/rest/developers_guide_protocol.html#Authenticating.. _Linkedin OAuth: https://www.linkedin.com/secure/developer.. _Foursquare OAuth2: https://developer.foursquare.com/docs/oauth.html.. _GitHub OAuth: http://developer.github.com/v3/oauth/.. _Dropbox OAuth: https://www.dropbox.com/developers_beta/reference/api.. _Flickr OAuth: http://www.flickr.com/services/api/.. Vkontakte OAuth: http://vk.com/developers.php?oid=-1&p=%D0%90%D0%B2%D1%82%D0%BE%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D1%8F%D1%81%D0%B0%D0%B9%D1%82%D0%BE%D0%B2.. _MSN Live Connect OAuth2: http://msdn.microsoft.com/en-us/library/live/hh243647.aspx.. _Skyrock OAuth: http://www.skyrock.com/developer/.. _Yahoo OAuth: http://developer.yahoo.com/oauth/guide/oauth-auth-flow.html.. _Evernote OAuth: http://dev.evernote.com/documentation/cloud/chapters/Authentication.php.. _Mail.ru OAuth: http://api.mail.ru/docs/guides/oauth/.. _Odnoklassniki OAuth: http://dev.odnoklassniki.ru/wiki/display/ok/The+OAuth+2.0+Protocol.. _Mixcloud OAuth2: http://www.mixcloud.com/developers/documentation/#authorization.. _BitBucket OAuth: https://confluence.atlassian.com/display/BITBUCKET/OAuth+on+Bitbucket.. _Douban OAuth: http://www.douban.com/service/apidoc/auth.. _Fitbit OAuth: https://wiki.fitbit.com/display/API/OAuth+Authentication+in+the+Fitbit+API.. _Instagram OAuth2: http://instagram.com/developer/authentication/.. _Twilio: https://www.twilio.com/user/account/connect/apps.. _Trello OAuth: https://trello.com/docs/gettingstarted/index.html#getting-an-application-key.. _Weibo OAuth2: http://open.weibo.com/wiki/Oauth2.. _Yandex OpenId: http://openid.yandex.ru/.. _Shopify OAuth2: http://api.shopify.com/authentication.html.. _StockTwits OAuth2: http://stocktwits.com/developers/docs/authentication.. _auth.User: http://code.djangoproject.com/browser/django/trunk/django/contrib/auth/models.py#L186.. _python-openid: http://pypi.python.org/pypi/python-openid/.. _python-oauth2: https://github.com/simplegeo/python-oauth2.. _OAuth: http://oauth.net/.. _pypi: http://pypi.python.org/pypi/django-social-auth/.. _github: https://github.com/omab/django-social-auth.. _django-social-auth discussion list: https://groups.google.com/forum/?fromgroups#!forum/django-social-auth.. _Stackoverflow OAuth2: http://api.stackexchange.com/.. _Fedora OpenID: https://fedoraproject.org/wiki/OpenID.. _Exacttarget HubExchange: http://code.exacttarget.com/.. _Appsfuel OAuth2: http://docs.appsfuel.com/api_reference#api_reference.. _python-social-auth: https://github.com/omab/python-social-auth"
https://github.com/mitmproxy/mitmproxy,An interactive TLS-capable intercepting HTTP proxy for penetration testers and software developers.,"mitmproxy is an interactive, SSL/TLS-capable intercepting proxy with a consoleinterface for HTTP/1, HTTP/2, and WebSockets. is the command-line version of mitmproxy. Think tcpdump for HTTP. is a web-based interface for mitmproxy.InstallationThe installation instructions are .If you want to install from source, see .Documentation & HelpGeneral information, tutorials, and precompiled binaries can be found on the mitmproxy website.The documentation for mitmproxy is available on our website:If you have questions on how to use mitmproxy, pleaseuse GitHub Discussions!ContributingAs an open source project, mitmproxy welcomes contributions of all forms.Also, please feel free to join our developer Slack! However, please note that the primary purpose of our Slack is direct communication between maintainers and contributors. If you have questions where the answer might be valuable to others, please use "
https://github.com/pallets/flask,The Python micro framework for building web applications.,"FlaskFlask is a lightweight _ web application framework. It is designedto make getting started quick and easy, with the ability to scale up tocomplex applications. It began as a simple wrapper around _and _ and has become one of the most popular Python webapplication frameworks.Flask offers suggestions, but doesn't enforce any dependencies orproject layout. It is up to the developer to choose the tools andlibraries they want to use. There are many extensions provided by thecommunity that make adding new functionality easy... _WSGI: https://wsgi.readthedocs.io/.. _Werkzeug: https://werkzeug.palletsprojects.com/.. _Jinja: https://jinja.palletsprojects.com/InstallingInstall and update using _:.. code-block:: text$ pip install -U Flask
.. _pip: https://pip.pypa.io/en/stable/getting-started/A Simple Example.. code-block:: python# save this as app.py
from flask import Flask

app = Flask(__name__)

@app.route(""/"")
def hello():
    return ""Hello, World!""
.. code-block:: text$ flask run
  * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
ContributingFor guidance on setting up a development environment and how to make acontribution to Flask, see the _... _contributing guidelines: https://github.com/pallets/flask/blob/main/CONTRIBUTING.rstDonateThe Pallets organization develops and supports Flask and the librariesit uses. In order to grow the community of contributors and users, andallow the maintainers to devote more time to the projects, _... _please donate today: https://palletsprojects.com/donateLinks"
https://github.com/boto/boto3,AWS SDK for Python,"===============================Boto3 - The AWS SDK for Python|Version| |Python| |License|Boto3 is the Amazon Web Services (AWS) Software Development Kit (SDK) forPython, which allows Python developers to write software that makes useof services like Amazon S3 and Amazon EC2. You can find the latest, mostup to date, documentation at our _, including a list ofservices that are supported.Boto3 is maintained and published by _.Boto (pronounced boh-toh) was named after the fresh water dolphin native to the Amazon river. The name was chosen by the author of the original Boto library, Mitch Garnaat, as a reference to the company.NoticesOn 2023-12-13, support for Python 3.7 will end for Boto3. This follows thePython Software Foundation __for the runtime which occurred on 2023-06-27.For more information, see this __... _boto: https://docs.pythonboto.org/.. _: https://boto3.amazonaws.com/v1/documentation/api/latest/index.html.. _: https://aws.amazon.com/what-is-aws/.. |Python| image:: https://img.shields.io/pypi/pyversions/boto3.svg?style=flat:target: https://pypi.python.org/pypi/boto3/:alt: Python Versions.. |Version| image:: http://img.shields.io/pypi/v/boto3.svg?style=flat:target: https://pypi.python.org/pypi/boto3/:alt: Package Version.. |License| image:: http://img.shields.io/pypi/l/boto3.svg?style=flat:target: https://github.com/boto/boto3/blob/develop/LICENSE:alt: LicenseGetting StartedAssuming that you have a supported version of Python installed, you can firstset up your environment with:.. code-block:: sh$ python -m venv .venv
...
$ . .venv/bin/activate
Then, you can install boto3 from PyPI with:.. code-block:: sh$ python -m pip install boto3
or install from source with:.. code-block:: sh$ git clone https://github.com/boto/boto3.git
$ cd boto3
$ python -m pip install -r requirements.txt
$ python -m pip install -e .
Using Boto3After installing boto3

Next, set up credentials (in e.g. ``~/.aws/credentials``):

.. code-block:: ini

    [default]
    aws_access_key_id = YOUR_KEY
    aws_secret_access_key = YOUR_SECRET

Then, set up a default region (in e.g. ``~/.aws/config``):

.. code-block:: ini

   [default]
   region=us-east-1

Other credential configuration methods can be found `here <https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html>`__

Then, from a Python interpreter:

.. code-block:: python

    >>> import boto3
    >>> s3 = boto3.resource('s3')
    >>> for bucket in s3.buckets.all():
            print(bucket.name)

Running Tests
~~~~~~~~~~~~~
You can run tests in all supported Python versions using ``tox``. By default,
it will run all of the unit and functional tests, but you can also specify your own
``pytest`` options. Note that this requires that you have all supported
versions of Python installed, otherwise you must pass ``-e`` or run the
``pytest`` command directly:

.. code-block:: sh

    $ tox
    $ tox -- unit/test_session.py
    $ tox -e py26,py33 -- integration/

You can also run individual tests with your default Python version:

.. code-block:: sh

    $ pytest tests/unit


Getting Help
------------

We use GitHub issues for tracking bugs and feature requests and have limited
bandwidth to address them. Please use these community resources for getting
help:

* Ask a question on `Stack Overflow <https://stackoverflow.com/>`__ and tag it with `boto3 <https://stackoverflow.com/questions/tagged/boto3>`__
* Open a support ticket with `AWS Support <https://console.aws.amazon.com/support/home#/>`__
* If it turns out that you may have found a bug, please `open an issue <https://github.com/boto/boto3/issues/new>`__


Contributing
------------

We value feedback and contributions from our community. Whether it's a bug report, new feature, correction, or additional documentation, we welcome your issues and pull requests. Please read through this `CONTRIBUTING <https://github.com/boto/boto3/blob/develop/CONTRIBUTING.rst>`__ document before submitting any issues or pull requests to ensure we have all the necessary information to effectively respond to your contribution.


Maintenance and Support for SDK Major Versions
----------------------------------------------

Boto3 was made generally available on 06/22/2015 and is currently in the full support phase of the availability life cycle.

For information about maintenance and support for SDK major versions and their underlying dependencies, see the following in the AWS SDKs and Tools Shared Configuration and Credentials Reference Guide:

* `AWS SDKs and Tools Maintenance Policy <https://docs.aws.amazon.com/sdkref/latest/guide/maint-policy.html>`__
* `AWS SDKs and Tools Version Support Matrix <https://docs.aws.amazon.com/sdkref/latest/guide/version-support-matrix.html>`__


More Resources
--------------

* `NOTICE <https://github.com/boto/boto3/blob/develop/NOTICE>`__
* `Changelog <https://github.com/boto/boto3/blob/develop/CHANGELOG.rst>`__
* `License <https://github.com/boto/boto3/blob/develop/LICENSE>`__
"
https://github.com/getmoto/moto,A library that allows you to easily mock out tests based on AWS infrastructure.,"Moto - Mock AWS ServicesInstall$ pip install 'moto[ec2,s3,all]'
In a nutshellMoto is a library that allows your tests to easily mock out AWS Services.Imagine you have the following python code that you want to test:import boto3


class MyModel:
    def __init__(self, name, value):
        self.name = name
        self.value = value

    def save(self):
        s3 = boto3.client(""s3"", region_name=""us-east-1"")
        s3.put_object(Bucket=""mybucket"", Key=self.name, Body=self.value)
Take a minute to think how you would have tested that in the past.Now see how you could test it with Moto:import boto3
from moto import mock_s3
from mymodule import MyModel


@mock_s3
def test_my_model_save():
    conn = boto3.resource(""s3"", region_name=""us-east-1"")
    # We need to create the bucket since this is all in Moto's 'virtual' AWS account
    conn.create_bucket(Bucket=""mybucket"")
    model_instance = MyModel(""steve"", ""is awesome"")
    model_instance.save()
    body = conn.Object(""mybucket"", ""steve"").get()[""Body""].read().decode(""utf-8"")
    assert body == ""is awesome""
With the decorator wrapping the test, all the calls to s3 are automatically mocked out. The mock keeps track of the state of the buckets and keys.For a full list of which services and features are covered, please see our .DocumentationThe full documentation can be found here:Financial ContributionsSupport this project and its continued development, by sponsoring us!Click the -button at the top of the page for more information.Our finances are managed by OpenCollective, which means you have full visibility into all our contributions and expenses:https://opencollective.com/motoSecurity contact informationTo report a security vulnerability, please use the.Tidelift will coordinate the fix and disclosure."
https://github.com/3b1b/manim,Animation engine for explanatory math videos,"Manim is an engine for precise programmatic animations, designed for creating explanatory math videos.Note, there are two versions of manim.  This repository began as a personal project by the author of  for the purpose of animating those videos, with video-specific code available .  In 2020 a group of developers forked it into what is now the , with a goal of being more stable, better tested, quicker to respond to community contributions, and all around friendlier to get started with. See  for more details.InstallationManim runs on Python 3.7 or higher.System requirements are ,  and  (optional, if you want to use LaTeX).For Linux,  along with its development headers are required. See instruction .Directly# Install manimgl
pip install manimgl

# Try it out
manimgl
For more options, take a look at the  sections further below.If you want to hack on manimlib itself, clone this repository and in that directory execute:# Install manimgl
pip install -e .

# Try it out
manimgl example_scenes.py OpeningManimExample
# or
manim-render example_scenes.py OpeningManimExample
Directly (Windows)Mac OSXAnaconda InstallUsing manimTry running the following:manimgl example_scenes.py OpeningManimExample
This should pop up a window playing a simple scene.Some useful flags include:Take a look at custom_config.yml for further configuration.  To add your customization, you can either edit this file, or add another file by the same name ""custom_config.yml"" to whatever directory you are running manim from.  For example  for 3blue1brown videos.  There you can specify where videos should be output to, where manim should look for image files and sounds you want to read in, and other defaults regarding style and video quality.Look through the  to get a sense of how it is used, and feel free to look through the code behind  for a much larger set of example. Note, however, that developments are often made to the library without considering backwards compatibility with those old videos. To run an old project with a guarantee that it will work, you will have to go back to the commit which completed that project.DocumentationDocumentation is in progress at . And there is also a Chinese version maintained by :  (in Chinese). wrote and collected some useful extra classes and some codes of videos in .ContributingIs always welcome.  As mentioned above, the  has the most active ecosystem for contributions, with testing and continuous integration, but pull requests are welcome here too.  Please explain the motivation for a given change and examples of its effect.LicenseThis project falls under the MIT license."
https://github.com/yutiansut/QUANTAXIS,QUANTAXIS 支持任务调度 分布式部署的 股票/期货/期权  数据/回测/模拟/交易/可视化/多账户 纯本地量化解决方案,"QUANTAXIS 2.0.0[点击右上角Star和Watch来跟踪项目进展! 点击Fork来创建属于你的QUANTAXIS!]更多文档在Quantitative Financial FrameWork本项目分为几个大块:本版本为不兼容升级的 2.0 quantaxis, 涉及一些改变数据部分微服务部分账户部分实盘模拟盘部分多语言部分社区/项目捐赠githubQUANTAXIS 是一个开放的项目, 在开源的3年中有大量的小伙伴加入了我, 并提交了相关的代码, 感谢以下的同学们许多问题 可以在 中找到, 你可以提出新的issue捐赠写代码不易...请作者喝杯咖啡呗?(PS: 支付的时候 请带上你的名字/昵称呀 会维护一个赞助列表~ )QQ群欢迎加群讨论: 563280067  DISCORD 社区  https://discord.gg/mkk5RgNQUANTAXIS 开发群: 773602202 (如果想要贡献代码 请加这个群 需要备注你的GITHUB ID)QUANTAXIS 期货实盘多账户的本地部署群 (请勿浪费群资源 没有本地多账户部署的请勿加): 945822690公共号欢迎关注公众号: QAPRO公共号免费提供了下单推送接口, 关注公共号回复trade即可使用论坛 QACLUBQUANTAXIS 内测版论坛 http://www.yutiansut.com:3000凡通过论坛进行提问的 均有最高的回复优先级"
https://github.com/dmlc/dgl,"Python package built to ease deep learning on graph, on top of existing DL frameworks."," |  | Documentation ( | ) |  |  | DGL is an easy-to-use, high performance and scalable Python package for deep learning on graphs. DGL is framework agnostic, meaning if a deep graph model is a component of an end-to-end application, the rest of the logics can be implemented in any major frameworks, such as PyTorch, Apache MXNet or TensorFlow.Highlighted FeaturesA GPU-ready graph libraryDGL provides a powerful graph object that can reside on either CPU or GPU. It bundles structural data as well as features for better control. We provide a variety of functions for computing with graph objects including efficient and customizable message passing primitives for Graph Neural Networks.A versatile tool for GNN researchers and practitionersThe field of graph deep learning is still rapidly evolving and many research ideas emerge by standing on the shoulders of giants. To ease the process,  is a command-line interface to get started with training, using and studying state-of-the-art GNNs.DGL collects a rich set of  of popular GNN models of a wide range of topics. Researchers can  for related models to innovate new ideas from or use them as baselines for experiments. Moreover, DGL provides many state-of-the-art  for users to build new model architectures. DGL is one of the preferred platforms for many standard graph deep learning benchmarks including  and .Easy to learn and useDGL provides plenty of learning materials for all kinds of users from ML researchers to domain experts. The  is a 120-minute tour of the basics of graph machine learning. The  explains in more details the concepts of graphs as well as the training methodology. All of them include code snippets in DGL that are runnable and ready to be plugged into one’s own pipeline.Scalable and efficientIt is convenient to train models using DGL on large-scale graphs across multiple GPUs or multiple machines. DGL extensively optimizes the whole stack to reduce the overhead in communication, memory consumption and synchronization. As a result, DGL can easily scale to billion-sized graphs. Get started with the  and  for distributed training. See the  for the comparison with other tools.Get StartedUsers can install DGL from . You can also download GPU enabled DGL docker  (backended by PyTorch) from NVIDIA NGC for both x86 and ARM based linux systems. Advanced users can follow the  to install from source.For absolute beginners, start with . It covers the basic concepts of common graph machine learning tasks and a step-by-step on building Graph Neural Networks (GNNs) to solve them.For acquainted users who wish to learn more,All the learning materials are available at our . If you are new to deep learning in general,check out the open source book .CommunityGet connectedWe provide multiple channels to connect you to the community of the DGL developers, users, and the general GNN academic researchers:Take the survey  and leave any feedback to make DGL better fit for your needs. Thanks!DGL-powered projectsAwesome Papers Using DGLContributingPlease let us know if you encounter a bug or have any suggestions by .We welcome all contributions from bug fixes to new features and extensions.We expect all contributions discussed in the issue tracker and going through PRs.  Please refer to our .CiteIf you use DGL in a scientific publication, we would appreciate citations to the following paper:@article{wang2019dgl,
    title={Deep Graph Library: A Graph-Centric, Highly-Performant Package for Graph Neural Networks},
    author={Minjie Wang and Da Zheng and Zihao Ye and Quan Gan and Mufei Li and Xiang Song and Jinjing Zhou and Chao Ma and Lingfan Yu and Yu Gai and Tianjun Xiao and Tong He and George Karypis and Jinyang Li and Zheng Zhang},
    year={2019},
    journal={arXiv preprint arXiv:1909.01315}
}
The TeamDGL is developed and maintained by .LicenseDGL uses Apache License 2.0."
https://github.com/mininet/mininet,Emulator for rapid prototyping of Software Defined Networks,"Mininet: Rapid Prototyping for Software Defined NetworksThe best way to emulate almost any network on your laptop!Mininet 2.3.1b4What is Mininet?Mininet emulates a complete network of hosts, links, and switcheson a single machine.  To create a sample two-host, one-switch network,just run:Mininet is useful for interactive development, testing, and demos,especially those using OpenFlow and SDN.  OpenFlow-based networkcontrollers prototyped in Mininet can usually be transferred tohardware with minimal changes for full line-rate execution.How does it work?Mininet creates virtual networks using process-based virtualizationand network namespaces - features that are available in recent Linuxkernels.  In Mininet, hosts are emulated as  processes running ina network namespace, so any code that would normally run on a Linuxserver (like a web server or client program) should run just finewithin a Mininet ""Host"".  The Mininet ""Host"" will have its own privatenetwork interface and can only see its own processes.  Switches inMininet are software-based switches like Open vSwitch or the OpenFlowreference switch.  Links are virtual ethernet pairs, which live in theLinux kernel and connect our emulated switches to emulated hosts(processes).FeaturesMininet includes:Python 3 SupportOther Enhancements and InformationInstallationSee  for installation instructions and details.DocumentationIn addition to the API documentation (), much usefulinformation, including a Mininet walkthrough and an introductionto the Python API, is available on the.There is also a wiki which you are encouraged to read and tocontribute to, particularly the Frequently Asked Questions(FAQ) at http://faq.mininet.org.SupportMininet is community-supported. We encourage you to join theMininet mailing list,  at:Join UsThanks again to all of the Mininet contributors and users!Mininet is an open source project and is currently hostedat . You are encouraged to download,examine, and modify the code, and to submit bug reports, bug fixes,feature requests, new features, and other issues and pull requests.Thanks to everyone who has contributed code to the Mininet project(see CONTRIBUTORS for more info!) It is because of everyone'shard work that Mininet continues to grow and improve.Enjoy MininetHave fun! We look forward to seeing what you will do with Mininetto change the networking world.Bob Lantz,on behalf of the Mininet Contributors"
https://github.com/iperov/DeepFaceLab,DeepFaceLab is the leading software for creating deepfakes.,"﻿DeepFaceLabhttps://arxiv.org/abs/2005.05535the leading software for creating deepfakesMore than 95% of deepfake videos are created with DeepFaceLab.DeepFaceLab is used by such popular youtube channels as| | | |---|---|---|| | | | |---|---|---|---|| | ||---|---|| | | ||---|---|---|| | | ||---|---|---|| | ||---|---|| | ||---|---|What can I do using DeepFaceLab?Replace the faceDe-age the face https://www.youtube.com/watch?v=Ddx5B-84eboReplace the head https://www.youtube.com/watch?v=xr5FHd0AdlQ https://www.youtube.com/watch?v=RTjgkhMugVw https://www.youtube.com/watch?v=R9f7WD0gKPoManipulate politicians lips(voice replacement is not included!)(also requires a skill in video editors such as Adobe After Effects or Davinci Resolve) https://www.youtube.com/watch?v=IvY-Abd2FfM https://www.youtube.com/watch?v=ERQlaJ_czHUDeepfake native resolution progressUnfortunately, there is no ""make everything ok"" button in DeepFaceLab. You should spend time studying the workflow and growing your skills. A skill in programs such as AfterEffects or Davinci Resolve is also desirable.Mini tutorialReleasesLinksGuides and tutorialsSupplementary materialCommunication groupsRelated worksHow I can help the project?Sponsor deepfake research and DeepFaceLab development.Collect facesetsYou can collect faceset of any celebrity that can be used in DeepFaceLab and share it in the communityStar this repoRegister github account and push ""Star"" button.Meme zone#deepfacelab #deepfakes #faceswap #face-swap #deep-learning #deeplearning #deep-neural-networks #deepface #deep-face-swap #fakeapp #fake-app #neural-networks #neural-nets #tensorflow #cuda #nvidia"
https://github.com/tiangolo/typer,"Typer, build great CLIs. Easy to code. Based on Python type hints.","Documentation: https://typer.tiangolo.comSource Code: https://github.com/tiangolo/typerTyper is a library for building CLI applications that users will love using and developers will love creating. Based on Python 3.6+ type hints.The key features are:FastAPI of CLIsTyper is FastAPI's little sibling.And it's intended to be the FastAPI of CLIs.RequirementsPython 3.6+Typer stands on the shoulders of a giant. Its only internal dependency is Click.Installation$ pip install ""typer[all]""
---> 100%
Successfully installed typer
Note: that will include Rich. Rich is the recommended library to display information on the terminal, it is optional, but when installed, it's deeply integrated into Typer to display beautiful output.ExampleThe absolute minimumimport typer


def main(name: str):
    print(f""Hello {name}"")


if __name__ == ""__main__"":
    typer.run(main)
Run itRun your application:// Run your application
$ python main.py

// You get a nice error, you are missing NAME
Usage: main.py [OPTIONS] NAME
Try 'main.py --help' for help.
╭─ Error ───────────────────────────────────────────╮
│ Missing argument 'NAME'.                          │
╰───────────────────────────────────────────────────╯


// You get a --help for free
$ python main.py --help

Usage: main.py [OPTIONS] NAME

╭─ Arguments ───────────────────────────────────────╮
│ *    name      TEXT  [default: None] [required]   |
╰───────────────────────────────────────────────────╯
╭─ Options ─────────────────────────────────────────╮
│ --help          Show this message and exit.       │
╰───────────────────────────────────────────────────╯

// Now pass the NAME argument
$ python main.py Camila

Hello Camila

// It works! 🎉
Note: auto-completion works when you create a Python package and run it with  or when you use Typer CLI.Example upgradeThis was the simplest example possible.Now let's see one a bit more complex.An example with two subcommandsModify the file .Create a  app, and create two subcommands with their parameters.import typer

app = typer.Typer()


@app.command()
def hello(name: str):
    print(f""Hello {name}"")


@app.command()
def goodbye(name: str, formal: bool = False):
    if formal:
        print(f""Goodbye Ms. {name}. Have a good day."")
    else:
        print(f""Bye {name}!"")


if __name__ == ""__main__"":
    app()
And that will:Run the upgraded exampleCheck the new help:$ python main.py --help

 Usage: main.py [OPTIONS] COMMAND [ARGS]...

╭─ Options ─────────────────────────────────────────╮
│ --install-completion          Install completion  │
│                               for the current     │
│                               shell.              │
│ --show-completion             Show completion for │
│                               the current shell,  │
│                               to copy it or       │
│                               customize the       │
│                               installation.       │
│ --help                        Show this message   │
│                               and exit.           │
╰───────────────────────────────────────────────────╯
╭─ Commands ────────────────────────────────────────╮
│ goodbye                                           │
│ hello                                             │
╰───────────────────────────────────────────────────╯

// When you create a package you get ✨ auto-completion ✨ for free, installed with --install-completion

// You have 2 subcommands (the 2 functions): goodbye and hello
Now check the help for the  command:$ python main.py hello --help

 Usage: main.py hello [OPTIONS] NAME

╭─ Arguments ───────────────────────────────────────╮
│ *    name      TEXT  [default: None] [required]   │
╰───────────────────────────────────────────────────╯
╭─ Options ─────────────────────────────────────────╮
│ --help          Show this message and exit.       │
╰───────────────────────────────────────────────────╯
And now check the help for the  command:$ python main.py goodbye --help

 Usage: main.py goodbye [OPTIONS] NAME

╭─ Arguments ───────────────────────────────────────╮
│ *    name      TEXT  [default: None] [required]   │
╰───────────────────────────────────────────────────╯
╭─ Options ─────────────────────────────────────────╮
│ --formal    --no-formal      [default: no-formal] │
│ --help                       Show this message    │
│                              and exit.            │
╰───────────────────────────────────────────────────╯

// Automatic --formal and --no-formal for the bool option 🎉
Now you can try out the new command line application:// Use it with the hello command

$ python main.py hello Camila

Hello Camila

// And with the goodbye command

$ python main.py goodbye Camila

Bye Camila!

// And with --formal

$ python main.py goodbye --formal Camila

Goodbye Ms. Camila. Have a good day.
RecapIn summary, you declare once the types of parameters (CLI arguments and CLI options) as function parameters.You do that with standard modern Python types.You don't have to learn a new syntax, the methods or classes of a specific library, etc.Just standard Python 3.6+.For example, for an :total: int
or for a  flag:force: bool
And similarly for files, paths, enums (choices), etc. And there are tools to create groups of subcommands, add metadata, extra validation, etc.You get: great editor support, including completion and type checks everywhere.Your users get: automatic --help, auto-completion in their terminal (Bash, Zsh, Fish, PowerShell) when they install your package or when using Typer CLI.For a more complete example including more features, see the Tutorial - User Guide.Optional DependenciesTyper uses Click internally. That's the only dependency.But you can also install extras:You can install  with  and  with .LicenseThis project is licensed under the terms of the MIT license."
https://github.com/bigchaindb/bigchaindb,Meet BigchainDB. The blockchain database.,"BigchainDB ServerBigchainDB is the blockchain database. This repository is for BigchainDB Server.The BasicsRun and Test BigchainDB Server from the  BranchRunning and testing the latest version of BigchainDB Server is easy. Make sure you have a recent version of  installed. When you are ready, fire up a terminal and run:git clone https://github.com/bigchaindb/bigchaindb.git
cd bigchaindb
make run
BigchainDB should be reachable now on .There are also other commands you can execute:To view all commands available, run .Links for EveryoneLinks for DevelopersLegal"
https://github.com/NVIDIA/apex,A PyTorch Extension:  Tools for easy mixed precision and distributed training in Pytorch,"IntroductionThis repository holds NVIDIA-maintained utilities to streamline mixed precision and distributed training in Pytorch.Some of the code here will be included in upstream Pytorch eventually.The intent of Apex is to make up-to-date utilities available to users as quickly as possible.Full API Documentation:  and  SlidesContents1. Amp:  Automatic Mixed PrecisionDeprecated. Use  is a tool to enable mixed precision training by changing only 3 lines of your script.Users can easily experiment with different pure and mixed precision training modes by supplyingdifferent flags to .(The flag  has been renamed to ). (for users of the deprecated ""Amp"" and ""FP16_Optimizer"" APIs)2. Distributed Trainingapex.parallel.DistributedDataParallel is a module wrapper, similar to.  It enables convenient multiprocess distributed training,optimized for NVIDIA's NCCL communication library.The shows use of  along with .Synchronized Batch NormalizationDeprecated. Use  extends  tosupport synchronized BN.It allreduces stats across processes during multiprocess (DistributedDataParallel) training.Synchronous BN has been used in cases where only a smalllocal minibatch can fit on each GPU.Allreduced stats increase the effective batch size for the BN layer to theglobal batch size across all processes (which, technically, is the correctformulation).Synchronous BN has been observed to improve converged accuracy in some of our research models.CheckpointingTo properly save and load your  training, we introduce the , which contains all  and their corresponding unskipped steps,as well as  to restore these attributes.In order to get bitwise accuracy, we recommend the following workflow:# Initialization
opt_level = 'O1'
model, optimizer = amp.initialize(model, optimizer, opt_level=opt_level)

# Train your model
...
with amp.scale_loss(loss, optimizer) as scaled_loss:
    scaled_loss.backward()
...

# Save checkpoint
checkpoint = {
    'model': model.state_dict(),
    'optimizer': optimizer.state_dict(),
    'amp': amp.state_dict()
}
torch.save(checkpoint, 'amp_checkpoint.pt')
...

# Restore
model = ...
optimizer = ...
checkpoint = torch.load('amp_checkpoint.pt')

model, optimizer = amp.initialize(model, optimizer, opt_level=opt_level)
model.load_state_dict(checkpoint['model'])
optimizer.load_state_dict(checkpoint['optimizer'])
amp.load_state_dict(checkpoint['amp'])

# Continue training
...
Note that we recommend restoring the model using the same . Also note that we recommend calling the  methods after .InstallationEach  module requires one or more install options other than  and .Note that contrib modules do not necessarily support stable PyTorch releases.ContainersNVIDIA PyTorch Containers are available on NGC: https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch.The containers come with all the custom extensions available at the moment. See  for details such as:From SourceTo install Apex from source, we recommend using the nightly Pytorch obtainable from https://github.com/pytorch/pytorch.The latest stable release obtainable from https://pytorch.org should also work.We recommend installing  to make compilation faster.LinuxFor performance and full functionality, we recommend installing Apex withCUDA and C++ extensions viagit clone https://github.com/NVIDIA/apex
cd apex
# if pip >= 23.1 (ref: https://pip.pypa.io/en/stable/news/#v23-1) which supports multiple `--config-settings` with the same key... 
pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --config-settings ""--build-option=--cpp_ext"" --config-settings ""--build-option=--cuda_ext"" ./
# otherwise
pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --global-option=""--cpp_ext"" --global-option=""--cuda_ext"" ./
APEX also supports a Python-only build viapip install -v --disable-pip-version-check --no-build-isolation --no-cache-dir ./
A Python-only build omits:[Experimental] Windows may work if you were able to build Pytorch from sourceon your system. A Python-only build via  is more likely to work.If you installed Pytorch in a Conda environment, make sure to install Apex in that same environment.Custom C++/CUDA Extensions and Install OptionsIf a requirement of a module is not met, then it will not be built.|  Module Name  |  Install Option  |  Misc  ||---------------|------------------|--------||       |       | ||        |      | ||       |      | ||    |    |  ||     |      | ||    |    | ||    |    | ||    |    | ||    |    | Requires CUDA>=11 ||    |    |   ||          |           |   ||     |      |    ||    |    |    ||    |    |    ||    |    |    ||    |    |    ||    |    |  . different from  ||      |          |    ||    |    |    ||    |    |    ||     |    |    ||    |    | Requires cuDNN>=8.5,  ||    |    |    ||    |    | Requires NCCL >= 2.10,   ||    |    |  Requires  and ,  ||    |    | Requires cuDNN>=8.4,  |"
https://github.com/skorokithakis/catt,"Cast All The Things allows you to send videos from many, many online sources to your Chromecast.","Cast All The ThingsCast All The Things allows you to send videos from many, many onlinesources (YouTube, Vimeo, and a few hundred others) to your Chromecast.It also allows you to cast local files or render websites.InstallationYou can install Cast All The Things with pipx:pipx install catt
Or with pip, but that's not as good:pip3 install catt
 is only compatible with Python 3. If you need a Python2-compatible version, please install , the last py2-compatiblerelease.UsageTo use Cast All The Things, just specify a URL:catt cast ""https://www.youtube.com/watch?v=dQw4w9WgXcQ""
 supports any service that yt-dlp supports, which includes mostonline video hosting services. can also cast local files (if they're in a format the Chromecastsupports natively):catt cast ./myvideo.mp4
You can also control your Chromecast through  commands, forexample with . Try running  to see the fulllist of commands.If you have subtitles and the name is similar to the name of the localfile,  will add them automatically. You can, of course, specifyany other subtitle if you want. Although Chromecast only supportsWEBVTT, TTML and Line 21 subtitles,  conveniently converts SRTs toWEBVTT for you on the fly. Here is how to use it:catt cast -s ./mysubtitle.srt /myvideo.mp4
 can also tell your Chromecast to display any website:catt cast_site https://en.wikipedia.org/wiki/Rickrolling
Please note that the Chromecast has a slow CPU but a reasonably recentversion of Google Chrome. The display resolution is 1280x720.If you want to pass yt-dlp options to catt through the [-y]{.title-ref}command-line flag, you need to use yt-dlp's ,rather than its command-line name.If you notice that catt stops working with video sites (YouTube, Vimeo,etc), just upgrade yt-dlp with [pip install -U yt-dlp]{.title-ref} andthat will probably fix it. This is because sites keep changing andyt-dlp is updated very regularly to keep them all working.You can also run  in Docker, if you prefer:docker run --net=host --rm -it python:3.7 /bin/bash -c ""pip install catt; catt cast 'https://www.youtube.com/watch?v=dQw4w9WgXcQ'""
Configuration fileCATT can utilize a config-file stored at ( on Windows,  on macOS).The format is as following:[options]
device = chromecast_one

[aliases]
one = chromecast_one
two = chromecast_two
In the  section,  denotes the default device thatwill be selected, when you have not selected a device via the cli.You can write your choice of default device to  by doing:catt -d <name_of_chromecast> set_default
In the  section, you can specify aliases for the names ofyour chromecasts. You can then select a device just by doing:catt -d <alias> <command>
You can write an alias name for a device to  by doing:catt -d <name_of_chromecast> set_alias <alias>
FirewallFor the casting of local files to work you need to allow in the port range 45000-47000 over tcp.ContributingIf you want to contribute a feature to , please open an issue (orcomment on an existing one) first, to make sure it's something that themaintainers are interested in. Afterwards, just clone the repository andhack away!To run  in development, you can use the following command:python -m catt.cli --help
Before committing, please make sure you install  and installits hooks:pip install pre-commit
pre-commit install
That's all, now you can commit and the hooks will run. Black (which isused to format the code) requires Python 3.6 to run, but please make theeffort, as our CI will yell at you if the code is not formatted, andnobody wants that.Thanks!InfoFeaturesThanksCatt would not be possible without these great projects:"
https://github.com/python-attrs/attrs,Python Classes Without Boilerplate,"attrs is the Python package that will bring back the joy of writing classes by relieving you from the drudgery of implementing object protocols (aka ). for Mars missions since 2020!Its main goal is to help you to write concise and correct software without slowing down your code.Sponsorsattrs would not be possible without our .Especially those generously supporting us at the The Organization tier and higher:Exampleattrs gives you a class decorator and a way to declaratively define the attributes on that class:>>> from attrs import asdict, define, make_class, Factory

>>> @define
... class SomeClass:
...     a_number: int = 42
...     list_of_numbers: list[int] = Factory(list)
...
...     def hard_math(self, another_number):
...         return self.a_number + sum(self.list_of_numbers) * another_number


>>> sc = SomeClass(1, [1, 2, 3])
>>> sc
SomeClass(a_number=1, list_of_numbers=[1, 2, 3])

>>> sc.hard_math(3)
19
>>> sc == SomeClass(1, [1, 2, 3])
True
>>> sc != SomeClass(2, [3, 2, 1])
True

>>> asdict(sc)
{'a_number': 1, 'list_of_numbers': [1, 2, 3]}

>>> SomeClass()
SomeClass(a_number=42, list_of_numbers=[])

>>> C = make_class(""C"", [""a"", ""b""])
>>> C(""foo"", ""bar"")
C(a='foo', b='bar')
After declaring your attributes, attrs gives you:without writing dull boilerplate code again and again and without runtime performance penalties.Hate type annotations!?No problem!Types are entirely optional with attrs.Simply assign  to the attributes instead of annotating them with types.This example uses attrs's modern APIs that have been introduced in version 20.1.0, and the attrs package import name that has been added in version 21.3.0.The classic APIs (, , plus their serious-business aliases) and the  package import name will remain indefinitely.Please check out  for a more in-depth explanation.Data ClassesOn the tin, attrs might remind you of  (and indeed,   of attrs).In practice it does a lot more and is more flexible.For instance it allows you to define , allows more ways to , and allows for stepping through the generated methods using a debugger.For more details, please refer to our .Project Informationattrs for EnterpriseAvailable as part of the Tidelift Subscription.The maintainers of attrs and thousands of other packages are working with Tidelift to deliver commercial support and maintenance for the open source packages you use to build your applications.Save time, reduce risk, and improve code health, while paying the maintainers of the exact packages you use."
https://github.com/spipm/Depix,Recovers passwords from pixelized screenshots,"DepixDepix is a tool for recovering passwords from pixelized screenshots.This implementation works on pixelized images that were created with a linear box filter.In  I cover background information on pixelization and similar research.ExampleInstallationpip install git+https://github.com/beurtschipper/Depix
depix \
    -p /path/to/your/input/image.png \
    -s images/searchimages/debruinseq_notepad_Windows10_closeAndSpaced.png \
    -o /path/to/your/output.png
Example usagedepix \
    -p images/testimages/testimage3_pixels.png \
    -s images/searchimages/debruinseq_notepad_Windows10_closeAndSpaced.png
Result: depix \
    -p images/testimages/sublime_screenshot_pixels_gimp.png \
    -s images/searchimages/debruin_sublime_Linux_small.png \
    --backgroundcolor 40,41,35 \
    --averagetype linear
Result: genpixed -i /path/to/image.png -o pixed_output.png
AboutMaking a Search ImageAlgorithmThe algorithm uses the fact that the linear box filter processes every block separately. For every block it pixelizes all blocks in the search image to check for direct matches.For most pixelized images Depix manages to find single-match results. It assumes these are correct. The matches of surrounding multi-match blocks are then compared to be geometrically at the same distance as in the pixelized image. Matches are also treated as correct. This process is repeated a couple of times.After correct blocks have no more geometrical matches, it will output all correct blocks directly. For multi-match blocks, it outputs the average of all matches.The algorithm uses the fact that the linear box filter processes every block separately. For every block it pixelizes all blocks in the search image to check for direct matches.Known limitationsFuture developmentCreate more averaging filters that work like some popular editors do.After creating this program, someone pointed me to a  from 2016 where a group of researchers managed to create a similar tool. Their tool has better precision and works across many different fonts.While their original source code is not public, an open-source implementation exists at .Edit 16 Feb '22:  created the tool UnRedacter (, ) to crack a  that was created as a response to Depix!Still, anyone who is passionate about this type of depixelization is encouraged to implement their own HMM-based version and share it."
https://github.com/ijl/orjson,"Fast, correct Python JSON library supporting dataclasses, datetimes, and numpy","orjsonorjson is a fast, correct JSON library for Python. It as the fastest Pythonlibrary for JSON and is more correct than the standard json library or otherthird-party libraries. It serializes,,, and instances natively.Its features and drawbacks compared to other Python JSON libraries:orjson supports CPython 3.8, 3.9, 3.10, 3.11, and 3.12. It distributesamd64/x86_64, aarch64/armv8, arm7, POWER/ppc64le, and s390x wheels for Linux,amd64 and aarch64 wheels for macOS, and amd64 and i686/x86 wheels for Windows.orjson does not and will not support PyPy. orjson does not and will notsupport PEP 554 subinterpreters. Releases follow semantic versioning andserializing a new object type without an opt-in flag is considered abreaking change.orjson is licensed under both the Apache 2.0 and MIT licenses. Therepository and issue tracker is, and patches may besubmitted there. There is aavailable in the repository.UsageInstallTo install a wheel from PyPI:pip install --upgrade ""pip>=20.3"" # manylinux_x_y, universal2 wheel support
pip install --upgrade orjson
To build a wheel, see .QuickstartThis is an example of serializing, with options specified, and deserializing:>>> import orjson, datetime, numpy
>>> data = {
    ""type"": ""job"",
    ""created_at"": datetime.datetime(1970, 1, 1),
    ""status"": ""🆗"",
    ""payload"": numpy.array([[1, 2], [3, 4]]),
}
>>> orjson.dumps(data, option=orjson.OPT_NAIVE_UTC | orjson.OPT_SERIALIZE_NUMPY)
b'{""type"":""job"",""created_at"":""1970-01-01T00:00:00+00:00"",""status"":""\xf0\x9f\x86\x97"",""payload"":[[1,2],[3,4]]}'
>>> orjson.loads(_)
{'type': 'job', 'created_at': '1970-01-01T00:00:00+00:00', 'status': '🆗', 'payload': [[1, 2], [3, 4]]}
Migratingorjson version 3 serializes more types than version 2. Subclasses of ,, , and  are now serialized. This is faster and more similarto the standard library. It can be disabled with. instancesare now serialized by default and cannot be customized in a function unless  isspecified.  instances are serialized by default.For any type that is now serialized,implementations in a  function and options enabling them can beremoved but do not need to be. There was no change in deserialization.To migrate from the standard library, the largest difference is that returns  and  returns a . Users with objects using non- keys should specify.  is replaced by.  is replaced by and other levels of indentation are notsupported.Serializedef dumps(
    __obj: Any,
    default: Optional[Callable[[Any], Any]] = ...,
    option: Optional[int] = ...,
) -> bytes: ...
 serializes Python objects to JSON.It natively serializes, , , , , , , ,, , ,, , , , and instances. It supports arbitrary types through . Itserializes subclasses of , , , ,, and . It does not serialize subclassesof  to avoid serializing  objects as arrays. To avoidserializing subclasses, specify the option .The output is a  object containing UTF-8.The global interpreter lock (GIL) is held for the duration of the call.It raises  on an unsupported type. This exception messagedescribes the invalid object with the error message. To fix this, specify.It raises  on a  that contains invalid UTF-8.It raises  on an integer that exceeds 64 bits by default or,with , 53 bits.It raises  if a  has a key of a type other than ,unless  is specified.It raises  if the output of  recurses to handling by more than 254 levels deep.It raises  on circular references.It raises   if a  on a datetime object isunsupported. is a subclass of . This is for compatibilitywith the standard library.If the failure was caused by an exception in  then chains the original exception as .defaultTo serialize a subclass or arbitrary types, specify  as acallable that returns a supported type.  may be a function,lambda, or callable class instance. To specify that a type was nothandled by , raise an exception such as .>>> import orjson, decimal
>>>
def default(obj):
    if isinstance(obj, decimal.Decimal):
        return str(obj)
    raise TypeError

>>> orjson.dumps(decimal.Decimal(""0.0842389659712649442845""))
JSONEncodeError: Type is not JSON serializable: decimal.Decimal
>>> orjson.dumps(decimal.Decimal(""0.0842389659712649442845""), default=default)
b'""0.0842389659712649442845""'
>>> orjson.dumps({1, 2}, default=default)
orjson.JSONEncodeError: Type is not JSON serializable: set
The  callable may return an object that itselfmust be handled by  up to 254 times before an exceptionis raised.It is important that  raise an exception if a type cannot be handled.Python otherwise implicitly returns , which appears to the callerlike a legitimate value and is serialized:>>> import orjson, json, rapidjson
>>>
def default(obj):
    if isinstance(obj, decimal.Decimal):
        return str(obj)

>>> orjson.dumps({""set"":{1, 2}}, default=default)
b'{""set"":null}'
>>> json.dumps({""set"":{1, 2}}, default=default)
'{""set"":null}'
>>> rapidjson.dumps({""set"":{1, 2}}, default=default)
'{""set"":null}'
optionTo modify how data is serialized, specify . Each  is an integerconstant in . To specify multiple options, mask them together, e.g.,.OPT_APPEND_NEWLINEAppend  to the output. This is a convenience and optimization for thepattern of .  objects are immutable and thispattern copies the original contents.>>> import orjson
>>> orjson.dumps([])
b""[]""
>>> orjson.dumps([], option=orjson.OPT_APPEND_NEWLINE)
b""[]\n""
OPT_INDENT_2Pretty-print output with an indent of two spaces. This is equivalent to in the standard library. Pretty printing is slower and the outputlarger. orjson is the fastest compared library at pretty printing and hasmuch less of a slowdown to pretty print than the standard library does. Thisoption is compatible with all other options.>>> import orjson
>>> orjson.dumps({""a"": ""b"", ""c"": {""d"": True}, ""e"": [1, 2]})
b'{""a"":""b"",""c"":{""d"":true},""e"":[1,2]}'
>>> orjson.dumps(
    {""a"": ""b"", ""c"": {""d"": True}, ""e"": [1, 2]},
    option=orjson.OPT_INDENT_2
)
b'{\n  ""a"": ""b"",\n  ""c"": {\n    ""d"": true\n  },\n  ""e"": [\n    1,\n    2\n  ]\n}'
If displayed, the indentation and linebreaks appear like this:{
  ""a"": ""b"",
  ""c"": {
    ""d"": true
  },
  ""e"": [
    1,
    2
  ]
}
This measures serializing the github.json fixture as compact (52KiB) orpretty (64KiB):| Library    |   compact (ms) |   pretty (ms) |   vs. orjson ||------------|----------------|---------------|--------------|| orjson     |           0.03 |          0.04 |          1   || ujson      |           0.18 |          0.19 |          4.6 || rapidjson  |           0.1  |          0.12 |          2.9 || simplejson |           0.25 |          0.89 |         21.4 || json       |           0.18 |          0.71 |         17   |This measures serializing the citm_catalog.json fixture, more of a worstcase due to the amount of nesting and newlines, as compact (489KiB) orpretty (1.1MiB):| Library    |   compact (ms) |   pretty (ms) |   vs. orjson ||------------|----------------|---------------|--------------|| orjson     |           0.59 |          0.71 |          1   || ujson      |           2.9  |          3.59 |          5   || rapidjson  |           1.81 |          2.8  |          3.9 || simplejson |          10.43 |         42.13 |         59.1 || json       |           4.16 |         33.42 |         46.9 |This can be reproduced using the  script.OPT_NAIVE_UTCSerialize  objects without a  as UTC. Thishas no effect on  objects that have  set.>>> import orjson, datetime
>>> orjson.dumps(
        datetime.datetime(1970, 1, 1, 0, 0, 0),
    )
b'""1970-01-01T00:00:00""'
>>> orjson.dumps(
        datetime.datetime(1970, 1, 1, 0, 0, 0),
        option=orjson.OPT_NAIVE_UTC,
    )
b'""1970-01-01T00:00:00+00:00""'
OPT_NON_STR_KEYSSerialize  keys of type other than . This allows  keysto be one of , , , , , ,, , , and . For comparison,the standard library serializes , , ,  or  bydefault. orjson benchmarks as being faster at serializing non- keysthan other libraries. This option is slower for  keys than the default.>>> import orjson, datetime, uuid
>>> orjson.dumps(
        {uuid.UUID(""7202d115-7ff3-4c81-a7c1-2a1f067b1ece""): [1, 2, 3]},
        option=orjson.OPT_NON_STR_KEYS,
    )
b'{""7202d115-7ff3-4c81-a7c1-2a1f067b1ece"":[1,2,3]}'
>>> orjson.dumps(
        {datetime.datetime(1970, 1, 1, 0, 0, 0): [1, 2, 3]},
        option=orjson.OPT_NON_STR_KEYS | orjson.OPT_NAIVE_UTC,
    )
b'{""1970-01-01T00:00:00+00:00"":[1,2,3]}'
These types are generally serialized how they would be asvalues, e.g.,  is still an RFC 3339 string and respectsoptions affecting it. The exception is that  serialization does notrespect .This option has the risk of creating duplicate keys. This is because non-objects may serialize to the same  as an existing key, e.g.,. The last key to be inserted to the  will beserialized last and a JSON deserializer will presumably take the lastoccurrence of a key (in the above, ). The first value will be lost.This option is compatible with . If sorting is used,note the sort is unstable and will be unpredictable for duplicate keys.>>> import orjson, datetime
>>> orjson.dumps(
    {""other"": 1, datetime.date(1970, 1, 5): 2, datetime.date(1970, 1, 3): 3},
    option=orjson.OPT_NON_STR_KEYS | orjson.OPT_SORT_KEYS
)
b'{""1970-01-03"":3,""1970-01-05"":2,""other"":1}'
This measures serializing 589KiB of JSON comprising a  of 100 in which each  has both 365 randomly-sorted  keys representing epochtimestamps as well as one  key and the value for each key is asingle integer. In ""str keys"", the keys were converted to  beforeserialization, and orjson still specifes (which is always somewhat slower).| Library    |   str keys (ms) | int keys (ms)   | int keys sorted (ms)   ||------------|-----------------|-----------------|------------------------|| orjson     |            1.53 | 2.16            | 4.29                   || ujson      |            3.07 | 5.65            |                        || rapidjson  |            4.29 |                 |                        || simplejson |           11.24 | 14.50           | 21.86                  || json       |            7.17 | 8.49            |                        |ujson is blank for sorting because it segfaults. json is blank because itraises  on attempting to sort before converting all keys to .rapidjson is blank because it does not support non- keys. This canbe reproduced using the  script.OPT_OMIT_MICROSECONDSDo not serialize the  field on  and instances.>>> import orjson, datetime
>>> orjson.dumps(
        datetime.datetime(1970, 1, 1, 0, 0, 0, 1),
    )
b'""1970-01-01T00:00:00.000001""'
>>> orjson.dumps(
        datetime.datetime(1970, 1, 1, 0, 0, 0, 1),
        option=orjson.OPT_OMIT_MICROSECONDS,
    )
b'""1970-01-01T00:00:00""'
OPT_PASSTHROUGH_DATACLASSPassthrough  instances to . This allowscustomizing their output but is much slower.>>> import orjson, dataclasses
>>>
@dataclasses.dataclass
class User:
    id: str
    name: str
    password: str

def default(obj):
    if isinstance(obj, User):
        return {""id"": obj.id, ""name"": obj.name}
    raise TypeError

>>> orjson.dumps(User(""3b1"", ""asd"", ""zxc""))
b'{""id"":""3b1"",""name"":""asd"",""password"":""zxc""}'
>>> orjson.dumps(User(""3b1"", ""asd"", ""zxc""), option=orjson.OPT_PASSTHROUGH_DATACLASS)
TypeError: Type is not JSON serializable: User
>>> orjson.dumps(
        User(""3b1"", ""asd"", ""zxc""),
        option=orjson.OPT_PASSTHROUGH_DATACLASS,
        default=default,
    )
b'{""id"":""3b1"",""name"":""asd""}'
OPT_PASSTHROUGH_DATETIMEPassthrough , , and  instancesto . This allows serializing datetimes to a custom format, e.g.,HTTP dates:>>> import orjson, datetime
>>>
def default(obj):
    if isinstance(obj, datetime.datetime):
        return obj.strftime(""%a, %d %b %Y %H:%M:%S GMT"")
    raise TypeError

>>> orjson.dumps({""created_at"": datetime.datetime(1970, 1, 1)})
b'{""created_at"":""1970-01-01T00:00:00""}'
>>> orjson.dumps({""created_at"": datetime.datetime(1970, 1, 1)}, option=orjson.OPT_PASSTHROUGH_DATETIME)
TypeError: Type is not JSON serializable: datetime.datetime
>>> orjson.dumps(
        {""created_at"": datetime.datetime(1970, 1, 1)},
        option=orjson.OPT_PASSTHROUGH_DATETIME,
        default=default,
    )
b'{""created_at"":""Thu, 01 Jan 1970 00:00:00 GMT""}'
This does not affect datetimes in  keys if using OPT_NON_STR_KEYS.OPT_PASSTHROUGH_SUBCLASSPassthrough subclasses of builtin types to .>>> import orjson
>>>
class Secret(str):
    pass

def default(obj):
    if isinstance(obj, Secret):
        return ""******""
    raise TypeError

>>> orjson.dumps(Secret(""zxc""))
b'""zxc""'
>>> orjson.dumps(Secret(""zxc""), option=orjson.OPT_PASSTHROUGH_SUBCLASS)
TypeError: Type is not JSON serializable: Secret
>>> orjson.dumps(Secret(""zxc""), option=orjson.OPT_PASSTHROUGH_SUBCLASS, default=default)
b'""******""'
This does not affect serializing subclasses as  keys if usingOPT_NON_STR_KEYS.OPT_SERIALIZE_DATACLASSThis is deprecated and has no effect in version 3. In version 2 this wasrequired to serialize   instances. For more, see.OPT_SERIALIZE_NUMPYSerialize  instances. For more, see.OPT_SERIALIZE_UUIDThis is deprecated and has no effect in version 3. In version 2 this wasrequired to serialize  instances. For more, see.OPT_SORT_KEYSSerialize  keys in sorted order. The default is to serialize in anunspecified order. This is equivalent to  in the standardlibrary.This can be used to ensure the order is deterministic for hashing or tests.It has a substantial performance penalty and is not recommended in general.>>> import orjson
>>> orjson.dumps({""b"": 1, ""c"": 2, ""a"": 3})
b'{""b"":1,""c"":2,""a"":3}'
>>> orjson.dumps({""b"": 1, ""c"": 2, ""a"": 3}, option=orjson.OPT_SORT_KEYS)
b'{""a"":3,""b"":1,""c"":2}'
This measures serializing the twitter.json fixture unsorted and sorted:| Library    |   unsorted (ms) |   sorted (ms) |   vs. orjson ||------------|-----------------|---------------|--------------|| orjson     |            0.32 |          0.54 |          1   || ujson      |            1.6  |          2.07 |          3.8 || rapidjson  |            1.12 |          1.65 |          3.1 || simplejson |            2.25 |          3.13 |          5.8 || json       |            1.78 |          2.32 |          4.3 |The benchmark can be reproduced using the  script.The sorting is not collation/locale-aware:>>> import orjson
>>> orjson.dumps({""a"": 1, ""ä"": 2, ""A"": 3}, option=orjson.OPT_SORT_KEYS)
b'{""A"":3,""a"":1,""\xc3\xa4"":2}'
This is the same sorting behavior as the standard library, rapidjson,simplejson, and ujson. also serialize as maps but this has no effect on them.OPT_STRICT_INTEGEREnforce 53-bit limit on integers. The limit is otherwise 64 bits, the same asthe Python standard library. For more, see .OPT_UTC_ZSerialize a UTC timezone on  instances as  insteadof .>>> import orjson, datetime, zoneinfo
>>> orjson.dumps(
        datetime.datetime(1970, 1, 1, 0, 0, 0, tzinfo=zoneinfo.ZoneInfo(""UTC"")),
    )
b'""1970-01-01T00:00:00+00:00""'
>>> orjson.dumps(
        datetime.datetime(1970, 1, 1, 0, 0, 0, tzinfo=zoneinfo.ZoneInfo(""UTC"")),
        option=orjson.OPT_UTC_Z
    )
b'""1970-01-01T00:00:00Z""'
Fragment includes already-serialized JSON in a document. This is anefficient way to include JSON blobs from a cache, JSONB field, or separatelyserialized object without first deserializing to Python objects via .>>> import orjson
>>> orjson.dumps({""key"": ""zxc"", ""data"": orjson.Fragment(b'{""a"": ""b"", ""c"": 1}')})
b'{""key"":""zxc"",""data"":{""a"": ""b"", ""c"": 1}}'
It does no reformatting:  will not affect acompact blob nor will a pretty-printed JSON blob be rewritten as compact.The input must be  or  and given as a positional argument.This raises  if a  is given and the input isnot valid UTF-8. It otherwise does no validation and it is possible towrite invalid JSON. This does not escape characters. The implementation istested to not crash if given invalid strings or invalid JSON.This is similar to  in rapidjson.Deserializedef loads(__obj: Union[bytes, bytearray, memoryview, str]) -> Any: ...
 deserializes JSON to Python objects. It deserializes to ,, , , , , and  objects., , , and  input are accepted. If the inputexists as a , , or  object, it is recommended topass these directly rather than creating an unnecessary  object. That is, instead of . Thishas lower memory usage and lower latency.The input must be valid UTF-8.orjson maintains a cache of map keys for the duration of the process. Thiscauses a net reduction in memory usage by avoiding duplicate strings. Thekeys must be at most 64 bytes to be cached and 1024 entries are stored.The global interpreter lock (GIL) is held for the duration of the call.It raises  if given an invalid type or invalidJSON. This includes if the input contains , , or ,which the standard library allows, but is not valid JSON. is a subclass of  and .This is for compatibility with the standard library.Typesdataclassorjson serializes instances of  natively. It serializesinstances 40-50x as fast as other libraries and avoids a severe slowdown seenin other libraries compared to serializing .It is supported to pass all variants of dataclasses, including dataclassesusing , frozen dataclasses, those with optional or defaultattributes, and subclasses. There is a performance benefit to notusing .| Library    | dict (ms)   | dataclass (ms)   | vs. orjson   ||------------|-------------|------------------|--------------|| orjson     | 1.40        | 1.60             | 1            || ujson      |             |                  |              || rapidjson  | 3.64        | 68.48            | 42           || simplejson | 14.21       | 92.18            | 57           || json       | 13.28       | 94.90            | 59           |This measures serializing 555KiB of JSON, orjson natively and other librariesusing  to serialize the output of . This can bereproduced using the  script.Dataclasses are serialized as maps, with every attribute serialized and inthe order given on class definition:>>> import dataclasses, orjson, typing

@dataclasses.dataclass
class Member:
    id: int
    active: bool = dataclasses.field(default=False)

@dataclasses.dataclass
class Object:
    id: int
    name: str
    members: typing.List[Member]

>>> orjson.dumps(Object(1, ""a"", [Member(1, True), Member(2)]))
b'{""id"":1,""name"":""a"",""members"":[{""id"":1,""active"":true},{""id"":2,""active"":false}]}'
datetimeorjson serializes  objects to format,e.g., ""1970-01-01T00:00:00+00:00"". This is a subset of ISO 8601 and iscompatible with  in the standard library.>>> import orjson, datetime, zoneinfo
>>> orjson.dumps(
    datetime.datetime(2018, 12, 1, 2, 3, 4, 9, tzinfo=zoneinfo.ZoneInfo(""Australia/Adelaide""))
)
b'""2018-12-01T02:03:04.000009+10:30""'
>>> orjson.dumps(
    datetime.datetime(2100, 9, 1, 21, 55, 2).replace(tzinfo=zoneinfo.ZoneInfo(""UTC""))
)
b'""2100-09-01T21:55:02+00:00""'
>>> orjson.dumps(
    datetime.datetime(2100, 9, 1, 21, 55, 2)
)
b'""2100-09-01T21:55:02""'
 supports instances with a  that is ,, a timezone instance from the python3.9+ module, or a timezone instance from the third-party , , or/ libraries.It is fastest to use the standard library's  for timezones. objects must not have a .>>> import orjson, datetime
>>> orjson.dumps(datetime.time(12, 0, 15, 290))
b'""12:00:15.000290""'
 objects will always serialize.>>> import orjson, datetime
>>> orjson.dumps(datetime.date(1900, 1, 2))
b'""1900-01-02""'
Errors with  result in  being raised.To disable serialization of  objects specify the option.To use ""Z"" suffix instead of ""+00:00"" to indicate UTC (""Zulu"") time, use the option.To assume datetimes without timezone are UTC, use the option .enumorjson serializes enums natively. Options apply to their values.>>> import enum, datetime, orjson
>>>
class DatetimeEnum(enum.Enum):
    EPOCH = datetime.datetime(1970, 1, 1, 0, 0, 0)
>>> orjson.dumps(DatetimeEnum.EPOCH)
b'""1970-01-01T00:00:00""'
>>> orjson.dumps(DatetimeEnum.EPOCH, option=orjson.OPT_NAIVE_UTC)
b'""1970-01-01T00:00:00+00:00""'
Enums with members that are not supported types can be serialized using:>>> import enum, orjson
>>>
class Custom:
    def __init__(self, val):
        self.val = val

def default(obj):
    if isinstance(obj, Custom):
        return obj.val
    raise TypeError

class CustomEnum(enum.Enum):
    ONE = Custom(1)

>>> orjson.dumps(CustomEnum.ONE, default=default)
b'1'
floatorjson serializes and deserializes double precision floats with no loss ofprecision and consistent rounding. serializes Nan, Infinity, and -Infinity, which are notcompliant JSON, as :>>> import orjson, ujson, rapidjson, json
>>> orjson.dumps([float(""NaN""), float(""Infinity""), float(""-Infinity"")])
b'[null,null,null]'
>>> ujson.dumps([float(""NaN""), float(""Infinity""), float(""-Infinity"")])
OverflowError: Invalid Inf value when encoding double
>>> rapidjson.dumps([float(""NaN""), float(""Infinity""), float(""-Infinity"")])
'[NaN,Infinity,-Infinity]'
>>> json.dumps([float(""NaN""), float(""Infinity""), float(""-Infinity"")])
'[NaN, Infinity, -Infinity]'
intorjson serializes and deserializes 64-bit integers by default. The rangesupported is a signed 64-bit integer's minimum (-9223372036854775807) toan unsigned 64-bit integer's maximum (18446744073709551615). Thisis widely compatible, but there are implementationsthat only support 53-bits for integers, e.g.,web browsers. For those implementations,  can be configured toraise a  on values exceeding the 53-bit range.>>> import orjson
>>> orjson.dumps(9007199254740992)
b'9007199254740992'
>>> orjson.dumps(9007199254740992, option=orjson.OPT_STRICT_INTEGER)
JSONEncodeError: Integer exceeds 53-bit range
>>> orjson.dumps(-9007199254740992, option=orjson.OPT_STRICT_INTEGER)
JSONEncodeError: Integer exceeds 53-bit range
numpyorjson natively serializes  and individual, ,, , , ,, , , ,, , , and instances.orjson is faster than all compared libraries at serializingnumpy instances. Serializing numpy data requires specifying.>>> import orjson, numpy
>>> orjson.dumps(
        numpy.array([[1, 2, 3], [4, 5, 6]]),
        option=orjson.OPT_SERIALIZE_NUMPY,
)
b'[[1,2,3],[4,5,6]]'
The array must be a contiguous C array () and one of thesupported datatypes.Note a difference between serializing  using or :  convertsto a  before serializing and orjson's native path does not. Thiscan result in different rounding. instances are serialized as RFC 3339 strings anddatetime options affect them.>>> import orjson, numpy
>>> orjson.dumps(
        numpy.datetime64(""2021-01-01T00:00:00.172""),
        option=orjson.OPT_SERIALIZE_NUMPY,
)
b'""2021-01-01T00:00:00.172000""'
>>> orjson.dumps(
        numpy.datetime64(""2021-01-01T00:00:00.172""),
        option=(
            orjson.OPT_SERIALIZE_NUMPY |
            orjson.OPT_NAIVE_UTC |
            orjson.OPT_OMIT_MICROSECONDS
        ),
)
b'""2021-01-01T00:00:00+00:00""'
If an array is not a contiguous C array, contains an unsupported datatype,or contains a  using an unsupported representation(e.g., picoseconds), orjson falls through to . In , can be specified. If an array is malformed, whichis not expected,  is raised.This measures serializing 92MiB of JSON from an  withdimensions of  and  values:| Library    | Latency (ms)   | RSS diff (MiB)   | vs. orjson   ||------------|----------------|------------------|--------------|| orjson     | 194            | 99               | 1.0          || ujson      |                |                  |              || rapidjson  | 3,048          | 309              | 15.7         || simplejson | 3,023          | 297              | 15.6         || json       | 3,133          | 297              | 16.1         |This measures serializing 100MiB of JSON from an  withdimensions of  and  values:| Library    | Latency (ms)   | RSS diff (MiB)   | vs. orjson   ||------------|----------------|------------------|--------------|| orjson     | 178            | 115              | 1.0          || ujson      |                |                  |              || rapidjson  | 1,512          | 551              | 8.5          || simplejson | 1,606          | 504              | 9.0          || json       | 1,506          | 503              | 8.4          |This measures serializing 105MiB of JSON from an  withdimensions of  and  values:| Library    | Latency (ms)   | RSS diff (MiB)   | vs. orjson   ||------------|----------------|------------------|--------------|| orjson     | 157            | 120              | 1.0          || ujson      |                |                  |              || rapidjson  | 710            | 327              | 4.5          || simplejson | 931            | 398              | 5.9          || json       | 996            | 400              | 6.3          |In these benchmarks, orjson serializes natively, ujson is blank because itdoes not support a  parameter, and the other libraries serialize via . The RSS column measures peak memoryusage during serialization. This can be reproduced using the  script.orjson does not have an installation or compilation dependency on numpy. Theimplementation is independent, reading  using.strorjson is strict about UTF-8 conformance. This is stricter than the standardlibrary's json module, which will serialize and deserialize UTF-16 surrogates,e.g., ""\ud800"", that are invalid UTF-8.If  is given a  that does not contain valid UTF-8, is raised. If  receives invalid UTF-8, is raised.orjson and rapidjson are the only compared JSON libraries to consistentlyerror on bad input.>>> import orjson, ujson, rapidjson, json
>>> orjson.dumps('\ud800')
JSONEncodeError: str is not valid UTF-8: surrogates not allowed
>>> ujson.dumps('\ud800')
UnicodeEncodeError: 'utf-8' codec ...
>>> rapidjson.dumps('\ud800')
UnicodeEncodeError: 'utf-8' codec ...
>>> json.dumps('\ud800')
'""\\ud800""'
>>> orjson.loads('""\\ud800""')
JSONDecodeError: unexpected end of hex escape at line 1 column 8: line 1 column 1 (char 0)
>>> ujson.loads('""\\ud800""')
''
>>> rapidjson.loads('""\\ud800""')
ValueError: Parse error at offset 1: The surrogate pair in string is invalid.
>>> json.loads('""\\ud800""')
'\ud800'
To make a best effort at deserializing bad input, first decode  usingthe  or  argument for :>>> import orjson
>>> orjson.loads(b'""\xed\xa0\x80""')
JSONDecodeError: str is not valid UTF-8: surrogates not allowed
>>> orjson.loads(b'""\xed\xa0\x80""'.decode(""utf-8"", ""replace""))
'���'
uuidorjson serializes  instances to format, e.g.,""f81d4fae-7dec-11d0-a765-00a0c91e6bf6"".>>> import orjson, uuid
>>> orjson.dumps(uuid.UUID('f81d4fae-7dec-11d0-a765-00a0c91e6bf6'))
b'""f81d4fae-7dec-11d0-a765-00a0c91e6bf6""'
>>> orjson.dumps(uuid.uuid5(uuid.NAMESPACE_DNS, ""python.org""))
b'""886313e1-3b8a-5372-9b90-0c9aee199e5d""'
TestingThe library has comprehensive tests. There are tests against fixtures in the andrepositories. It is tested to not crash against the.It is tested to not leak memory. It is tested to not crashagainst and not accept invalid UTF-8. There are integration testsexercising the library's use in web servers (gunicorn using multiprocess/forkedworkers) and whenmultithreaded. It also uses some tests from the ultrajson library.orjson is the most correct of the compared libraries. This graph shows how eachlibrary handles a combined 342 JSON fixtures from the and tests:| Library    |   Invalid JSON documents not rejected |   Valid JSON documents not deserialized ||------------|---------------------------------------|-----------------------------------------|| orjson     |                                     0 |                                       0 || ujson      |                                    38 |                                       0 || rapidjson  |                                     6 |                                       0 || simplejson |                                    13 |                                       0 || json       |                                    17 |                                       0 |This shows that all libraries deserialize valid JSON but only orjsoncorrectly rejects the given invalid JSON fixtures. Errors are largely due toaccepting invalid strings and numbers.The graph above can be reproduced using the  script.PerformanceSerialization and deserialization performance of orjson is better thanultrajson, rapidjson, simplejson, or json. The benchmarks are done onfixtures of real data:Latencytwitter.json serialization| Library    |   Median latency (milliseconds) |   Operations per second |   Relative (latency) ||------------|---------------------------------|-------------------------|----------------------|| orjson     |                            0.33 |                  3069.4 |                 1    || ujson      |                            1.68 |                   592.8 |                 5.15 || rapidjson  |                            1.12 |                   891   |                 3.45 || simplejson |                            2.29 |                   436.2 |                 7.03 || json       |                            1.8  |                   556.6 |                 5.52 |twitter.json deserialization| Library    |   Median latency (milliseconds) |   Operations per second |   Relative (latency) ||------------|---------------------------------|-------------------------|----------------------|| orjson     |                            0.81 |                  1237.6 |                 1    || ujson      |                            1.87 |                   533.9 |                 2.32 || rapidjson  |                            2.97 |                   335.8 |                 3.67 || simplejson |                            2.15 |                   463.8 |                 2.66 || json       |                            2.45 |                   408.2 |                 3.03 |github.json serialization| Library    |   Median latency (milliseconds) |   Operations per second |   Relative (latency) ||------------|---------------------------------|-------------------------|----------------------|| orjson     |                            0.03 |                 28817.3 |                 1    || ujson      |                            0.18 |                  5478.2 |                 5.26 || rapidjson  |                            0.1  |                  9686.4 |                 2.98 || simplejson |                            0.26 |                  3901.3 |                 7.39 || json       |                            0.18 |                  5437   |                 5.27 |github.json deserialization| Library    |   Median latency (milliseconds) |   Operations per second |   Relative (latency) ||------------|---------------------------------|-------------------------|----------------------|| orjson     |                            0.07 |                 15270   |                 1    || ujson      |                            0.19 |                  5374.8 |                 2.84 || rapidjson  |                            0.17 |                  5854.9 |                 2.59 || simplejson |                            0.15 |                  6707.4 |                 2.27 || json       |                            0.16 |                  6397.3 |                 2.39 |citm_catalog.json serialization| Library    |   Median latency (milliseconds) |   Operations per second |   Relative (latency) ||------------|---------------------------------|-------------------------|----------------------|| orjson     |                            0.58 |                  1722.5 |                 1    || ujson      |                            2.89 |                   345.6 |                 4.99 || rapidjson  |                            1.83 |                   546.4 |                 3.15 || simplejson |                           10.39 |                    95.9 |                17.89 || json       |                            3.93 |                   254.6 |                 6.77 |citm_catalog.json deserialization| Library    |   Median latency (milliseconds) |   Operations per second |   Relative (latency) ||------------|---------------------------------|-------------------------|----------------------|| orjson     |                            1.76 |                   569.2 |                 1    || ujson      |                            3.5  |                   284.3 |                 1.99 || rapidjson  |                            5.77 |                   173.2 |                 3.28 || simplejson |                            5.13 |                   194.7 |                 2.92 || json       |                            4.99 |                   200.5 |                 2.84 |canada.json serialization| Library    |   Median latency (milliseconds) |   Operations per second |   Relative (latency) ||------------|---------------------------------|-------------------------|----------------------|| orjson     |                            3.62 |                   276.3 |                 1    || ujson      |                           14.16 |                    70.6 |                 3.91 || rapidjson  |                           33.64 |                    29.7 |                 9.29 || simplejson |                           57.46 |                    17.4 |                15.88 || json       |                           35.7  |                    28   |                 9.86 |canada.json deserialization| Library    |   Median latency (milliseconds) |   Operations per second |   Relative (latency) ||------------|---------------------------------|-------------------------|----------------------|| orjson     |                            3.89 |                   256.6 |                 1    || ujson      |                            8.73 |                   114.3 |                 2.24 || rapidjson  |                           23.33 |                    42.8 |                 5.99 || simplejson |                           23.99 |                    41.7 |                 6.16 || json       |                           21.1  |                    47.4 |                 5.42 |Memoryorjson as of 3.7.0 has higher baseline memory usage than other librariesdue to a persistent buffer used for parsing. Incremental memory usage whendeserializing is similar to the standard library and other third-partylibraries.This measures, in the first column, RSS after importing a library and readingthe fixture, and in the second column, increases in RSS after repeatedlycalling  on the fixture.twitter.json| Library    |   import, read() RSS (MiB) |   loads() increase in RSS (MiB) ||------------|----------------------------|---------------------------------|| orjson     |                       21.8 |                             2.8 || ujson      |                       14.3 |                             4.8 || rapidjson  |                       14.9 |                             4.6 || simplejson |                       13.4 |                             2.4 || json       |                       13.1 |                             2.3 |github.json| Library    |   import, read() RSS (MiB) |   loads() increase in RSS (MiB) ||------------|----------------------------|---------------------------------|| orjson     |                       21.2 |                             0.5 || ujson      |                       13.6 |                             0.6 || rapidjson  |                       14.1 |                             0.5 || simplejson |                       12.5 |                             0.3 || json       |                       12.4 |                             0.3 |citm_catalog.json| Library    |   import, read() RSS (MiB) |   loads() increase in RSS (MiB) ||------------|----------------------------|---------------------------------|| orjson     |                       23   |                            10.6 || ujson      |                       15.2 |                            11.2 || rapidjson  |                       15.8 |                            29.7 || simplejson |                       14.4 |                            24.7 || json       |                       13.9 |                            24.7 |canada.json| Library    |   import, read() RSS (MiB) |   loads() increase in RSS (MiB) ||------------|----------------------------|---------------------------------|| orjson     |                       23.2 |                            21.3 || ujson      |                       15.6 |                            19.2 || rapidjson  |                       16.3 |                            23.4 || simplejson |                       15   |                            21.1 || json       |                       14.3 |                            20.9 |ReproducingThe above was measured using Python 3.10.5 on Linux (amd64) withorjson 3.7.9, ujson 5.4.0, python-rapidson 1.8, and simplejson 3.17.6.The latency results can be reproduced using the  and scripts. The memory results can be reproduced using the  script.QuestionsWhy can't I install it from PyPI?Probably  needs to be upgraded to version 20.3 or later to supportthe latest manylinux_x_y or universal2 wheel formats.""Cargo, the Rust package manager, is not installed or is not on PATH.""This happens when there are no binary wheels (like manylinux) for yourplatform on PyPI. You can install  through or a package manager and then it will compile.Will it deserialize to dataclasses, UUIDs, decimals, etc or support object_hook?No. This requires a schema specifying what types are expected and how tohandle errors etc. This is addressed by data validation libraries alevel above this.Will it serialize to ?No.  is the correct type for a serialized blob.PackagingTo package orjson requires at least  1.65and the  build tool. The recommendedbuild command is:maturin build --release --strip
It benefits from also having a C build environment to compile a fasterdeserialization backend. See this project's  builds for anexample using clang and LTO.The project's own CI tests against  and stable 1.65. Itis prudent to pin the nightly version because that channel can introducebreaking changes.orjson is tested for amd64, aarch64, arm7, ppc64le, and s390x on Linux. Itis tested for amd64 on macOS and cross-compiles for aarch64. For Windowsit is tested on amd64 and i686.There are no runtime dependencies other than libc.The source distribution on PyPI contains all dependencies' source and can bebuilt without network access. The file can be downloaded from.orjson's tests are included in the source distribution on PyPI. Therequirements to run the tests are specified in . Thetests should be run as part of the build. It can be run with.Licenseorjson was written by ijl <>, copyright 2018 - 2023, licensedunder both the Apache 2 and MIT licenses."
https://github.com/apache/airflow,"Apache Airflow - A platform to programmatically author, schedule, and monitor workflows","Apache Airflow (or simply Airflow) is a platform to programmatically author, schedule, and monitor workflows.When workflows are defined as code, they become more maintainable, versionable, testable, and collaborative.Use Airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The Airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.Table of contentsProject FocusAirflow works best with workflows that are mostly static and slowly changing. When the DAG structure is similar from one run to the next, it clarifies the unit of work and continuity. Other similar projects include ,  and .Airflow is commonly used to process data, but has the opinion that tasks should ideally be idempotent (i.e., results of the task will be the same, and will not create duplicated data in a destination system), and should not pass large quantities of data from one task to the next (though tasks can pass metadata using Airflow's ). For high-volume, data-intensive tasks, a best practice is to delegate to external services specializing in that type of work.Airflow is not a streaming solution, but it is often used to process real-time data, pulling data off streams in batches.PrinciplesRequirementsApache Airflow is tested with:|             | Main version (dev)           | Stable version (2.7.1) ||-------------|------------------------------|------------------------|| Python      | 3.8, 3.9, 3.10, 3.11         | 3.8, 3.9, 3.10, 3.11   || Platform    | AMD64/ARM64()              | AMD64/ARM64()        || Kubernetes  | 1.24, 1.25, 1.26, 1.27, 1.28 | 1.24, 1.25, 1.26, 1.27 || PostgreSQL  | 11, 12, 13, 14, 15, 16       | 11, 12, 13, 14, 15     || MySQL       | 8.0, 8.1                     | 5.7, 8.0               || SQLite      | 3.15.0+                      | 3.15.0+                || MSSQL       | 2017(), 2019()       | 2017(), 2019() | Experimental Discontinued soon, not recommended for the new installationNote: MySQL 5.x versions are unable to or have limitations withrunning multiple schedulers -- please see the .MariaDB is not tested/recommended.Note: SQLite is used in Airflow tests. Do not use it in production. We recommendusing the latest stable version of SQLite for local development.Note: Airflow currently can be run on POSIX-compliant Operating Systems. For development, it is regularlytested on fairly modern Linux Distros and recent versions of macOS.On Windows you can run it via WSL2 (Windows Subsystem for Linux 2) or via Linux Containers.The work to add Windows support is tracked via , butit is not a high priority. You should only use Linux-based distros as ""Production"" execution environmentas this is the only environment that is supported. The only distro that is used in our CI tests and thatis used in the  is.Getting startedVisit the official Airflow website documentation (latest stable release) for help with,, or walkingthrough a more complete .For more information on Airflow Improvement Proposals (AIPs), visitthe .Documentation for dependent projects like provider packages, Docker image, Helm Chart, you'll find it in .Installing from PyPIWe publish Apache Airflow as  package in PyPI. Installing it however might be sometimes trickybecause Airflow is a bit of both a library and application. Libraries usually keep their dependencies open, andapplications usually pin them, but we should do neither and both simultaneously. We decided to keepour dependencies as open as possible (in ) so users can install different versions of librariesif needed. This means that  will not work from time to time or willproduce unusable Airflow installation.To have repeatable installation, however, we keep a set of ""known-to-be-working"" constraintfiles in the orphan  and  branches. We keep those ""known-to-be-working""constraints files separately per major/minor Python version.You can use them as constraint files when installing Airflow from PyPI. Note that you have to specifycorrect Airflow tag/version/branch and Python versions in the URL.While it is possible to install Airflow with tools like  or, they do not share the same workflow as - especially when it comes to constraint vs. requirements management.Installing via  or  is not currently supported.There are known issues with  that might lead to circular dependencies when using it to installAirflow. Please switch to  if you encounter such problems.  community works on fixingthe problem in _ so it might be thatnewer versions of  will handle it.If you wish to install Airflow using those tools, you should use the constraint files and convertthem to the appropriate format and workflow that your tool requires.pip install 'apache-airflow==2.7.1' \
 --constraint ""https://raw.githubusercontent.com/apache/airflow/constraints-2.7.1/constraints-3.8.txt""
pip install 'apache-airflow[postgres,google]==2.7.1' \
 --constraint ""https://raw.githubusercontent.com/apache/airflow/constraints-2.7.1/constraints-3.8.txt""
For information on installing provider packages, check.Official source codeApache Airflow is an  (ASF) project,and our official source code releases:Following the ASF rules, the source packages released must be sufficient for a user to build and test therelease provided they have access to the appropriate platform and tools.Convenience packagesThere are other ways of installing and using Airflow. Those are ""convenience"" methods - they arenot ""official releases"" as stated by the , but they can be used by the userswho do not want to build the software themselves.Those are - in the order of most common ways people install Airflow:All those artifacts are not official releases, but they are prepared using officially released sources.Some of those artifacts are ""development"" or ""pre-release"" ones, and they are clearly marked as suchfollowing the ASF Policy.User InterfaceSemantic versioningAs of Airflow 2.0.0, we support a strict  approach for all packages released.There are few specific rules that we agreed to that define details of versioning of the differentpackages:Version Life CycleApache Airflow version life cycle:| Version   | Current Patch/Minor   | State     | First Release   | Limited Support   | EOL/Terminated   ||-----------|-----------------------|-----------|-----------------|-------------------|------------------|| 2         | 2.7.2                 | Supported | Dec 17, 2020    | TBD               | TBD              || 1.10      | 1.10.15               | EOL       | Aug 27, 2018    | Dec 17, 2020      | June 17, 2021    || 1.9       | 1.9.0                 | EOL       | Jan 03, 2018    | Aug 27, 2018      | Aug 27, 2018     || 1.8       | 1.8.2                 | EOL       | Mar 19, 2017    | Jan 03, 2018      | Jan 03, 2018     || 1.7       | 1.7.1.2               | EOL       | Mar 28, 2016    | Mar 19, 2017      | Mar 19, 2017     |Limited support versions will be supported with security and critical bug fix only.EOL versions will not get any fixes nor support.We always recommend that all users run the latest available minor release for whatever major version is in use.We highly recommend upgrading to the latest Airflow major release at the earliest convenient time and before the EOL date.Support for Python and Kubernetes versionsAs of Airflow 2.0, we agreed to certain rules we follow for Python and Kubernetes support.They are based on the official release schedule of Python and Kubernetes, nicely summarized in the and.Base OS support for reference Airflow imagesThe Airflow Community provides conveniently packaged container images that are published wheneverwe publish an Apache Airflow release. Those images contain:The version of the base OS image is the stable version of Debian. Airflow supports using all currently activestable versions - as soon as all Airflow dependencies support building, and we set up the CI pipeline forbuilding and testing the OS version. Approximately 6 months before the end-of-life of a previous stableversion of the OS, Airflow switches the images released to use the latest supported version of the OS.For example since  end-of-life was August 2022, Airflow switched the images in  branchto use  in February/March 2022. The version was used in the next MINOR release afterthe switch happened. In case of the Bullseye switch - 2.3.0 version used .The images released  in the previous MINOR version continue to use the version that all other releasesfor the MINOR version used.Support for  image was dropped in August 2022 completely and everyone is expected tostop building their images using .Users will continue to be able to build their images using stable Debian releases until the end of life andbuilding and verifying of the images happens in our CI but no unit tests were executed using this image inthe  branch.Approach to dependencies of AirflowAirflow has a lot of dependencies - direct and transitive, also Airflow is both - library and application,therefore our policies to dependencies has to include both - stability of installation of application,but also ability to install newer version of dependencies for those users who develop DAGs. We developedthe approach where  are used to make sure airflow can be installed in a repeatable way, whilewe do not limit our users to upgrade most of the dependencies. As a result we decided not to upper-boundversion of Airflow dependencies by default, unless we have good reasons to believe upper-bounding them isneeded because of importance of the dependency as well as risk it involves to upgrade specific dependency.We also upper-bound the dependencies that we know cause problems.The constraint mechanism of ours takes care about finding and upgrading all the non-upper bound dependenciesautomatically (providing that all the tests pass). Our  build failures will indicate in case thereare versions of dependencies that break our tests - indicating that we should either upper-bind them orthat we should fix our code/tests to account for the upstream changes from those dependencies.Whenever we upper-bound such a dependency, we should always comment why we are doing it - i.e. we should havea good reason why dependency is upper-bound. And we should also mention what is the condition to remove thebinding.Approach for dependencies for Airflow CoreThose  and  dependencies are maintained in .There are few dependencies that we decided are important enough to upper-bound them by default, as they areknown to follow predictable versioning scheme, and we know that new versions of those are very likely tobring breaking changes. We commit to regularly review and attempt to upgrade to the newer versions ofthe dependencies as they are released, but this is manual process.The important dependencies are:Approach for dependencies in Airflow Providers and extrasThe main part of the Airflow is the Airflow Core, but the power of Airflow also comes from a number ofproviders that extend the core functionality and are released separately, even if we keep them (for now)in the same monorepo for convenience. You can read more about the providers in the. We alsohave set of policies implemented for maintaining and releasing community-managed providers as wellas the approach for community vs. 3rd party providers in the  document.Those  and  dependencies are maintained in  of each provider.By default, we should not upper-bound dependencies for providers, however each provider's maintainermight decide to add additional limits (and justify them with comment).ContributingWant to help build Apache Airflow? Check out our .Official Docker (container) images for Apache Airflow are described in .Who uses Apache Airflow?We know about around 500 organizations that are using Apache Airflow (but there are likely many more).If you use Airflow - feel free to make a PR to add your organisation to the list.Who maintains Apache Airflow?Airflow is the work of the ,but the are responsible for reviewing and merging PRs as well as steering conversations around new feature requests.If you would like to become a maintainer, please review the Apache Airflow.What goes into the next release?Often you will see an issue that is assigned to specific milestone with Airflow version, or PR that gets mergedto the main branch and you might wonder which release the merged PR will be released in or which release theissue will be fixed in. The answer to it is as usual - it depends. The answer is different for PRs and Issues.To add a bit of context, ee are following the  versioning scheme as described in. Moredetails are explained in detail in this README in  chapter, butin short, we have  versions of Airflow, where  version is incremented when thereare breaking changes,  version is incremented when there are new features added, and  versionis incremented when there are only bug-fixes and doc-only changes.Generally we release  versions of Airflow from a branch that is named after the MINOR version. For example releases are released from  branch,  releases are released from branch, etc.Most of the time in our release cycle, when the branch for next  branch is not yet created, allPRs merged to  (unless they get reverted), will find their way to the next  release. For exampleif the last release is  or  and  branch is not created yet, the next  releaseis  and all PRs merged to main will be released in . There is a brief period of time when wecut a new  release branch and prepare alpha, beta, RC candidates for the  versionwhere PRs merged to main will only be released in the following  release.However, some PRs (bug-fixes and doc-only changes) when merged, can be cherry-picked to current  branchand released in the next  release - for example when the last released version from branch is . Some of the PRs from main can be marked as  milestone by committers and attempt by therelease manager to cherry-pick them is made. If successful, they will be released in . The finaldecision about cherry-picking is made by the release manager.Marking issues with a milestone is a bit different. Maintainers do not mark issues with a milestone usually,normally they are only marked in PRs. If PR linked to the issue (and ""fixing it"") gets merged and releasedin a specific version following the process described above, the issue will be automatically closed, nomilestone will be set for the issue, you need to check the PR that fixed the issue to see which versionit was released in. However sometimes maintainers mark issues with specific milestone, which means that theissue is important to become a candidate to take a look when the release is being prepared. Since this is anOpen-Source project, where basically all contributors volunteer their time, there is no guarantee that specificissue will be fixed in specific version. We do not want to hold the release because some issue is not fixed,so in such case release manager will reassign such unfixed issues to the next milestone in case they are notfixed in time for the current release. Therefore, the milestone for issue is more of an intent that it should belooked at, than promise it will be fixed in the version.Can I use the Apache Airflow logo in my presentation?Yes! Be sure to abide by the Apache Foundation  and the Apache Airflow . The most up-to-date logos are found in  and on the Apache Software Foundation .Airflow merchandiseIf you would love to have Apache Airflow stickers, t-shirt, etc. then check out.LinksSponsorsThe CI infrastructure for Apache Airflow has been sponsored by:"
https://github.com/NVlabs/stylegan2,StyleGAN2 - Official TensorFlow Implementation,"StyleGAN2 &mdash; Official TensorFlow ImplementationAnalyzing and Improving the Image Quality of StyleGANTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo AilaPaper: http://arxiv.org/abs/1912.04958Video: https://youtu.be/c-NJtV9Jvp0Abstract: The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent vectors to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably detect if an image is generated by a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.For business inquiries, please visit our website and submit the form: &#9733;&#9733;&#9733; NEW: | Additional material | &nbsp;| :--- | :----------|  | Main Google Drive folder| &boxvr;&nbsp;  | High-quality version of the paper| &boxvr;&nbsp;  | High-quality version of the video| &boxvr;&nbsp;  | Example images produced using our method| &boxv;&nbsp; &boxvr;&nbsp;   | Hand-picked images showcasing our results| &boxv;&nbsp; &boxur;&nbsp;   | Random images with and without truncation| &boxvr;&nbsp;  | Individual clips of the video as high-quality MP4| &boxur;&nbsp;  | Pre-trained networks| &ensp;&ensp; &boxvr;&nbsp;  stylegan2-ffhq-config-f.pkl | StyleGAN2 for FFHQ dataset at 1024&times;1024| &ensp;&ensp; &boxvr;&nbsp;  stylegan2-car-config-f.pkl | StyleGAN2 for LSUN Car dataset at 512&times;384| &ensp;&ensp; &boxvr;&nbsp;  stylegan2-cat-config-f.pkl | StyleGAN2 for LSUN Cat dataset at 256&times;256| &ensp;&ensp; &boxvr;&nbsp;  stylegan2-church-config-f.pkl | StyleGAN2 for LSUN Church dataset at 256&times;256| &ensp;&ensp; &boxvr;&nbsp;  stylegan2-horse-config-f.pkl | StyleGAN2 for LSUN Horse dataset at 256&times;256| &ensp;&ensp; &boxur;&nbsp;&#x22ef;  | Other training configurations used in the paperRequirementsStyleGAN2 relies on custom TensorFlow ops that are compiled on the fly using . To test that your NVCC installation is working correctly, run:nvcc test_nvcc.cu -o test_nvcc -run
| CPU says hello.
| GPU says hello.
On Windows, the compilation requires Microsoft Visual Studio to be in . We recommend installing  and adding into  using .Using pre-trained networksPre-trained networks are stored as  files on the . Below, you can either reference them directly using the syntax , or download them manually and reference by filename.# Generate uncurated ffhq images (matches paper Figure 12)
python run_generator.py generate-images --network=gdrive:networks/stylegan2-ffhq-config-f.pkl \
  --seeds=6600-6625 --truncation-psi=0.5

# Generate curated ffhq images (matches paper Figure 11)
python run_generator.py generate-images --network=gdrive:networks/stylegan2-ffhq-config-f.pkl \
  --seeds=66,230,389,1518 --truncation-psi=1.0

# Generate uncurated car images
python run_generator.py generate-images --network=gdrive:networks/stylegan2-car-config-f.pkl \
  --seeds=6000-6025 --truncation-psi=0.5

# Example of style mixing (matches the corresponding video clip)
python run_generator.py style-mixing-example --network=gdrive:networks/stylegan2-ffhq-config-f.pkl \
  --row-seeds=85,100,75,458,1500 --col-seeds=55,821,1789,293 --truncation-psi=1.0
The results are placed in . You can change the location with . For example, .You can import the networks in your own Python code using . For this to work, you need to include the  source directory in  and create a default TensorFlow session by calling . See  and  for examples.Preparing datasetsDatasets are stored as multi-resolution TFRecords, similar to the . Each dataset consists of multiple  files stored under a common directory, e.g., . In the following sections, the datasets are referenced using a combination of  and  arguments, e.g., .FFHQ. To download the  dataset as multi-resolution TFRecords, run:pushd ~
git clone https://github.com/NVlabs/ffhq-dataset.git
cd ffhq-dataset
python download_ffhq.py --tfrecords
popd
python dataset_tool.py display ~/ffhq-dataset/tfrecords/ffhq
LSUN. Download the desired LSUN categories in LMDB format from the . To convert the data to multi-resolution TFRecords, run:python dataset_tool.py create_lsun_wide ~/datasets/car ~/lsun/car_lmdb --width=512 --height=384
python dataset_tool.py create_lsun ~/datasets/cat ~/lsun/cat_lmdb --resolution=256
python dataset_tool.py create_lsun ~/datasets/church ~/lsun/church_outdoor_train_lmdb --resolution=256
python dataset_tool.py create_lsun ~/datasets/horse ~/lsun/horse_lmdb --resolution=256
Custom. Create custom datasets by placing all training images under a single directory. The images must be square-shaped and they must all have the same power-of-two dimensions. To convert the images to multi-resolution TFRecords, run:python dataset_tool.py create_from_images ~/datasets/my-custom-dataset ~/my-custom-images
python dataset_tool.py display ~/datasets/my-custom-dataset
Projecting images to latent spaceTo find the matching latent vectors for a set of images, run:# Project generated images
python run_projector.py project-generated-images --network=gdrive:networks/stylegan2-car-config-f.pkl \
  --seeds=0,1,5

# Project real images
python run_projector.py project-real-images --network=gdrive:networks/stylegan2-car-config-f.pkl \
  --dataset=car --data-dir=~/datasets
Training networksTo reproduce the training runs for config F in Tables 1 and 3, run:python run_training.py --num-gpus=8 --data-dir=~/datasets --config=config-f \
  --dataset=ffhq --mirror-augment=true
python run_training.py --num-gpus=8 --data-dir=~/datasets --config=config-f \
  --dataset=car --total-kimg=57000
python run_training.py --num-gpus=8 --data-dir=~/datasets --config=config-f \
  --dataset=cat --total-kimg=88000
python run_training.py --num-gpus=8 --data-dir=~/datasets --config=config-f \
  --dataset=church --total-kimg 88000 --gamma=100
python run_training.py --num-gpus=8 --data-dir=~/datasets --config=config-f \
  --dataset=horse --total-kimg 100000 --gamma=100
For other configurations, see .We have verified that the results match the paper when training with 1, 2, 4, or 8 GPUs. Note that training FFHQ at 1024&times;1024 resolution requires GPU(s) with at least 16 GB of memory. The following table lists typical training times using NVIDIA DGX-1 with 8 Tesla V100 GPUs:| Configuration | Resolution      | Total kimg | 1 GPU   | 2 GPUs  | 4 GPUs  | 8 GPUs | GPU mem || :------------ | :-------------: | :--------: | :-----: | :-----: | :-----: | :----: | :-----: ||     | 1024&times;1024 | 25000      | 69d 23h | 36d 4h  | 18d 14h | 9d 18h | 13.3 GB ||     | 1024&times;1024 | 10000      | 27d 23h | 14d 11h | 7d 10h  | 3d 22h | 13.3 GB ||     | 1024&times;1024 | 25000      | 35d 11h | 18d 15h | 9d 15h  | 5d 6h  | 8.6 GB  ||     | 1024&times;1024 | 10000      | 14d 4h  | 7d 11h  | 3d 20h  | 2d 3h  | 8.6 GB  ||     | 256&times;256   | 25000      | 32d 13h | 16d 23h | 8d 21h  | 4d 18h | 6.4 GB  ||     | 256&times;256   | 10000      | 13d 0h  | 6d 19h  | 3d 13h  | 1d 22h | 6.4 GB  |Training curves for FFHQ config F (StyleGAN2) compared to original StyleGAN using 8 GPUs:After training, the resulting networks can be used the same way as the official pre-trained networks:# Generate 1000 random images without truncation
python run_generator.py generate-images --seeds=0-999 --truncation-psi=1.0 \
  --network=results/00006-stylegan2-ffhq-8gpu-config-f/networks-final.pkl
Evaluation metricsTo reproduce the numbers for config F in Tables 1 and 3, run:python run_metrics.py --data-dir=~/datasets --network=gdrive:networks/stylegan2-ffhq-config-f.pkl \
  --metrics=fid50k,ppl_wend --dataset=ffhq --mirror-augment=true
python run_metrics.py --data-dir=~/datasets --network=gdrive:networks/stylegan2-car-config-f.pkl \
  --metrics=fid50k,ppl2_wend --dataset=car
python run_metrics.py --data-dir=~/datasets --network=gdrive:networks/stylegan2-cat-config-f.pkl \
  --metrics=fid50k,ppl2_wend --dataset=cat
python run_metrics.py --data-dir=~/datasets --network=gdrive:networks/stylegan2-church-config-f.pkl \
  --metrics=fid50k,ppl2_wend --dataset=church
python run_metrics.py --data-dir=~/datasets --network=gdrive:networks/stylegan2-horse-config-f.pkl \
  --metrics=fid50k,ppl2_wend --dataset=horse
For other configurations, see the .Note that the metrics are evaluated using a different random seed each time, so the results will vary between runs. In the paper, we reported the average result of running each metric 10 times. The following table lists the available metrics along with their expected runtimes and random variation:| Metric      | FFHQ config F  | 1 GPU  | 2 GPUs  | 4 GPUs | Description || :---------- | :------------: | :----: | :-----: | :----: | :---------- ||     | 2.84 &pm; 0.03 | 22 min | 14 min  | 10 min | |      | 5.13 &pm; 0.02 | 23 min | 14 min  | 8 min  | |  | 348.0 &pm; 3.8 | 41 min | 22 min  | 14 min |  in Z, full paths|  | 126.9 &pm; 0.2 | 42 min | 22 min  | 13 min |  in W, full paths|   | 348.6 &pm; 3.0 | 41 min | 22 min  | 14 min |  in Z, path endpoints|   | 129.4 &pm; 0.8 | 40 min | 23 min  | 13 min |  in W, path endpoints|  | 145.0 &pm; 0.5 | 41 min | 23 min  | 14 min |  without center crop|         | 154.2 / 4.27   | 10 hrs | 6 hrs   | 4 hrs  | |     | 0.689 / 0.492  | 26 min | 17 min  | 12 min | Note that some of the metrics cache dataset-specific data on the disk, and they will take somewhat longer when run for the first time.LicenseCopyright &copy; 2019, NVIDIA Corporation. All rights reserved.This work is made available under the Nvidia Source Code License-NC. To view a copy of this license, visit https://nvlabs.github.io/stylegan2/license.htmlCitation@inproceedings{Karras2019stylegan2,
  title     = {Analyzing and Improving the Image Quality of {StyleGAN}},
  author    = {Tero Karras and Samuli Laine and Miika Aittala and Janne Hellsten and Jaakko Lehtinen and Timo Aila},
  booktitle = {Proc. CVPR},
  year      = {2020}
}
AcknowledgementsWe thank Ming-Yu Liu for an early review, Timo Viitanen for his help with code release, and Tero Kuosmanen for compute infrastructure."
https://github.com/yandex/gixy,Nginx configuration static analyzer,"GIXYOverviewGixy is a tool to analyze Nginx configuration.The main goal of Gixy is to prevent security misconfiguration and automate flaw detection.Currently supported Python versions are 2.7, 3.5, 3.6 and 3.7.Disclaimer: Gixy is well tested only on GNU/Linux, other OSs may have some issues.What it can doRight now Gixy can find:You can find things that Gixy is learning to detect at InstallationGixy is distributed on . The best way to install it is with pip:pip install gixy
Run Gixy and check results:gixy
UsageBy default Gixy will try to analyze Nginx configuration placed in .But you can always specify needed path:$ gixy /etc/nginx/nginx.conf

==================== Results ===================

Problem: [http_splitting] Possible HTTP-Splitting vulnerability.
Description: Using variables that can contain ""\n"" may lead to http injection.
Additional info: https://github.com/yandex/gixy/blob/master/docs/ru/plugins/httpsplitting.md
Reason: At least variable ""$action"" can contain ""\n""
Pseudo config:
include /etc/nginx/sites/default.conf;

	server {

		location ~ /v1/((?<action>[^.]*)\.json)?$ {
			add_header X-Action $action;
		}
	}


==================== Summary ===================
Total issues:
    Unspecified: 0
    Low: 0
    Medium: 0
    High: 1
Or skip some tests:$ gixy --skips http_splitting /etc/nginx/nginx.conf

==================== Results ===================
No issues found.

==================== Summary ===================
Total issues:
    Unspecified: 0
    Low: 0
    Medium: 0
    High: 0
Or something else, you can find all other  arguments with the help command: Docker usageGixy is available as a Docker image . Touse it, mount the configuration that you want to analyse as a volume and provide the path to theconfiguration file when running the Gixy image.$ docker run --rm -v `pwd`/nginx.conf:/etc/nginx/conf/nginx.conf yandex/gixy /etc/nginx/conf/nginx.conf
If you have an image that already contains your nginx configuration, you can share the configurationwith the Gixy container as a volume.$  docker run --rm --name nginx -d -v /etc/nginx
nginx:alpinef68f2833e986ae69c0a5375f9980dc7a70684a6c233a9535c2a837189f14e905

$  docker run --rm --volumes-from nginx yandex/gixy /etc/nginx/nginx.conf

==================== Results ===================
No issues found.

==================== Summary ===================
Total issues:
    Unspecified: 0
    Low: 0
    Medium: 0
    High: 0

ContributingContributions to Gixy are always welcome! You can help us in different ways:Code guidelines:"
https://github.com/OWASP/owasp-mastg,The Mobile Application Security Testing Guide (MASTG) is a comprehensive manual for mobile app security testing and reverse engineering. It describes the technical processes for verifying the controls listed in the OWASP Mobile Application Security Verification Standard (MASVS).,"OWASP Mobile Application Security Testing Guide (MASTG)This is the official GitHub Repository of the OWASP Mobile Application Security Testing Guide (MASTG). The MASTG is a comprehensive manual for mobile app security testing and reverse engineering. It describes technical processes for verifying the controls listed in the .Trusted by ...The OWASP MASVS and MASTG are trusted by the following platform providers and standardization, governmental and educational institutions. .🥇 MAS AdvocatesMAS Advocates are industry adopters of the OWASP MASVS and MASTG who have invested a significant and consistent amount of resources to push the project forward by providing consistent high-impact contributions and continuously spreading the word. .Connect with UsOther FormatsAbout Hybrid AppsPlease note that the MASTG focuses primarily on native apps. These are apps built with Java or Kotlin using the Android SDK for Android or built with Swift or Objective-C using the Apple SDKs for iOS. Apps using frameworks such as Nativescript, React-native, Xamarin, Cordova, etc. are not within the main focus of the MASTG. However, some essential controls, such as certificate pinning, have been explained already for some of these platforms. For now, you can take a look and contribute to the work-in-progress being made in the discussions  and ."
https://github.com/dask/dask,Parallel computing with task scheduling,Dask|Build Status| |Coverage| |Doc Status| |Discourse| |Version Status| |NumFOCUS|Dask is a flexible parallel computing library for analytics.  Seedocumentation_ for more information.LICENSENew BSD. See __... _documentation: https://dask.org.. |Build Status| image:: https://github.com/dask/dask/actions/workflows/tests.yml/badge.svg:target: https://github.com/dask/dask/actions/workflows/tests.yml.. |Coverage| image:: https://codecov.io/gh/dask/dask/branch/main/graph/badge.svg:target: https://codecov.io/gh/dask/dask/branch/main:alt: Coverage status.. |Doc Status| image:: https://readthedocs.org/projects/dask/badge/?version=latest:target: https://dask.org:alt: Documentation Status.. |Discourse| image:: https://img.shields.io/discourse/users?logo=discourse&server=https%3A%2F%2Fdask.discourse.group:alt: Discuss Dask-related things and ask for help:target: https://dask.discourse.group.. |Version Status| image:: https://img.shields.io/pypi/v/dask.svg:target: https://pypi.python.org/pypi/dask/.. |NumFOCUS| image:: https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A:target: https://www.numfocus.org/
https://github.com/python/typeshed,"Collection of library stubs for Python, with static types","typeshedAboutTypeshed contains external type annotations for the Python standard libraryand Python builtins, as well as third party packages as contributed bypeople external to those projects.This data can e.g. be used for static analysis, type checking or type inference.For information on how to use , read below.  Information forcontributors can be found in .  Please readFurther documentation on stub files, typeshed, and Python's typing system ingeneral, can also be found at https://typing.readthedocs.io/en/latest/.Typeshed fully supports Python versions 3.8 and up. Support for Python 3.7is limited: see https://github.com/python/typeshed/issues/10113for details.UsingIf you're just using a type checker (,,, PyCharm, ...), as opposed todeveloping it, you don't need to interact with the typeshed repo atall: a copy of standard library part of typeshed is bundled with type checkers.And type stubs for third party packages and modules you are using canbe installed from PyPI. For example, if you are using  and ,you can install the type stubs using$ pip install types-six types-requests
These PyPI packages follow and are automatically released (multiple times a day, when needed) by.Type checkers should be able to use these stub packages when installed. For moredetails, see the documentation for your type checker.Package versioning for third-party stubsVersion numbers of third-party stub packages consist of at least four parts.All parts of the stub version, except for the last part, correspond to theversion of the runtime package being stubbed. For example, if the package has version , this guarantees that the  packagecontains stubs targeted against  and tested against the latestversion of  matching that specifier. In this example, the final elementof the version (7) indicates that this is the eighth revision of the stubs for. If an update to the stubs were pushed (but the stubs were stillaiming to provide annotations for ), then the version of would increment to .At typeshed, we try to keep breaking changes to a minimum. However, due to thenature of stubs, any version bump can introduce changes that might make yourcode fail to type check.There are several strategies available for specifying the version of a stubspackage you're using, each with its own tradeoffs:You can also switch between the different strategies as needed. For example,you could default to strategy (1), but fall back to strategy (2) whena problem arises that can't easily be fixed.The  packagetypeshed includes a package  as part of the standard library.This package and its submodules contains utility types, but is notavailable at runtime. For more information about how to use this package,.DiscussionIf you've run into behavior in the type checker that suggests the typestubs for a given library are incorrect or incomplete,we want to hear from you!Our main forum for discussion is the project's .  This is the rightplace to start a discussion of any of the above or most any othertopic concerning the project.If you have general questions about typing with Python, or you needa review of your type annotations or stubs outside of typeshed, head over to.For less formal discussion, try the typing chat room on.  Some typeshed maintainersare almost always present; feel free to find us there and we're happyto chat.  Substantive technical discussion will be directed to theissue tracker."
https://github.com/dagster-io/dagster,"An orchestration platform for the development, production, and observation of data assets.","Dagster is a cloud-native data pipeline orchestrator for the whole development lifecycle, with integrated lineage and observability, a declarative programming model, and best-in-class testability.It is designed for developing and maintaining data assets, such as tables, data sets, machine learning models, and reports.With Dagster, you declare—as Python functions—the data assets that you want to build. Dagster then helps you run your functions at the right time and keep your assets up-to-date.Here is an example of a graph of three assets defined in Python:from dagster import asset
from pandas import DataFrame, read_html, get_dummies
from sklearn.linear_model import LinearRegression

@asset
def country_populations() -> DataFrame:
    df = read_html(""https://tinyurl.com/mry64ebh"")[0]
    df.columns = [""country"", ""continent"", ""rg"", ""pop2018"", ""pop2019"", ""change""]
    df[""change""] = df[""change""].str.rstrip(""%"").str.replace(""−"", ""-"").astype(""float"")
    return df

@asset
def continent_change_model(country_populations: DataFrame) -> LinearRegression:
    data = country_populations.dropna(subset=[""change""])
    return LinearRegression().fit(get_dummies(data[[""continent""]]), data[""change""])

@asset
def continent_stats(country_populations: DataFrame, continent_change_model: LinearRegression) -> DataFrame:
    result = country_populations.groupby(""continent"").sum()
    result[""pop_change_factor""] = continent_change_model.coef_
    return result
The graph loaded into Dagster's web UI:Dagster is built to be used at every stage of the data development lifecycle - local development, unit tests, integration tests, staging environments, all the way up to production.Quick Start:If you're new to Dagster, we recommend reading about its  or learning with the hands-on .Dagster is available on PyPI and officially supports Python 3.8+.pip install dagster dagster-webserver
This installs two packages:Running on Using a Mac with an M1 or M2 chip? Check the .DocumentationYou can find the full Dagster documentation , including the .Key Features:Dagster as a productivity platformIdentify the key assets you need to create using a declarative approach, or you can focus on running basic tasks. Embrace CI/CD best practices from the get-go: build reusable components, spot data quality issues, and flag bugs early.Dagster as a robust orchestration enginePut your pipelines into production with a robust multi-tenant, multi-tool engine that scales technically and organizationally.Dagster as a unified control planeMaintain control over your data as the complexity scales. Centralize your metadata in one tool with built-in observability, diagnostics, cataloging, and lineage. Spot any issues and identify performance improvement opportunities.Master the Modern Data Stack with integrationsDagster provides a growing library of integrations for today’s most popular data tools. Integrate with the tools you already use, and deploy to your infrastructure.CommunityConnect with thousands of other data practitioners building with Dagster. Share knowledge, get help,and contribute to the open-source project. To see featured material and upcoming events, check outour  page.Join our community here:ContributingFor details on contributing or running the project for development, check out our .LicenseDagster is ."
https://github.com/pytransitions/transitions,"A lightweight, object-oriented finite state machine implementation in Python with many extensions"," transitionsA lightweight, object-oriented state machine implementation in Python with many extensions. Compatible with Python 2.7+ and 3.0+.Installationpip install transitions
... or clone the repo from GitHub and then:python setup.py install
Table of ContentsQuickstartThey say  100 pages of API documentation, a million directives, or a thousand words.Well, ""they"" probably lie... but here's an example anyway:from transitions import Machine
import random

class NarcolepticSuperhero(object):

    # Define some states. Most of the time, narcoleptic superheroes are just like
    # everyone else. Except for...
    states = ['asleep', 'hanging out', 'hungry', 'sweaty', 'saving the world']

    def __init__(self, name):

        # No anonymous superheroes on my watch! Every narcoleptic superhero gets
        # a name. Any name at all. SleepyMan. SlumberGirl. You get the idea.
        self.name = name

        # What have we accomplished today?
        self.kittens_rescued = 0

        # Initialize the state machine
        self.machine = Machine(model=self, states=NarcolepticSuperhero.states, initial='asleep')

        # Add some transitions. We could also define these using a static list of
        # dictionaries, as we did with states above, and then pass the list to
        # the Machine initializer as the transitions= argument.

        # At some point, every superhero must rise and shine.
        self.machine.add_transition(trigger='wake_up', source='asleep', dest='hanging out')

        # Superheroes need to keep in shape.
        self.machine.add_transition('work_out', 'hanging out', 'hungry')

        # Those calories won't replenish themselves!
        self.machine.add_transition('eat', 'hungry', 'hanging out')

        # Superheroes are always on call. ALWAYS. But they're not always
        # dressed in work-appropriate clothing.
        self.machine.add_transition('distress_call', '*', 'saving the world',
                         before='change_into_super_secret_costume')

        # When they get off work, they're all sweaty and disgusting. But before
        # they do anything else, they have to meticulously log their latest
        # escapades. Because the legal department says so.
        self.machine.add_transition('complete_mission', 'saving the world', 'sweaty',
                         after='update_journal')

        # Sweat is a disorder that can be remedied with water.
        # Unless you've had a particularly long day, in which case... bed time!
        self.machine.add_transition('clean_up', 'sweaty', 'asleep', conditions=['is_exhausted'])
        self.machine.add_transition('clean_up', 'sweaty', 'hanging out')

        # Our NarcolepticSuperhero can fall asleep at pretty much any time.
        self.machine.add_transition('nap', '*', 'asleep')

    def update_journal(self):
        """""" Dear Diary, today I saved Mr. Whiskers. Again. """"""
        self.kittens_rescued += 1

    @property
    def is_exhausted(self):
        """""" Basically a coin toss. """"""
        return random.random() < 0.5

    def change_into_super_secret_costume(self):
        print(""Beauty, eh?"")
There, now you've baked a state machine into . Let's take him/her/it out for a spin...>>> batman = NarcolepticSuperhero(""Batman"")
>>> batman.state
'asleep'

>>> batman.wake_up()
>>> batman.state
'hanging out'

>>> batman.nap()
>>> batman.state
'asleep'

>>> batman.clean_up()
MachineError: ""Can't trigger event clean_up from state asleep!""

>>> batman.wake_up()
>>> batman.work_out()
>>> batman.state
'hungry'

# Batman still hasn't done anything useful...
>>> batman.kittens_rescued
0

# We now take you live to the scene of a horrific kitten entreement...
>>> batman.distress_call()
'Beauty, eh?'
>>> batman.state
'saving the world'

# Back to the crib.
>>> batman.complete_mission()
>>> batman.state
'sweaty'

>>> batman.clean_up()
>>> batman.state
'asleep'   # Too tired to shower!

# Another productive day, Alfred.
>>> batman.kittens_rescued
1
While we cannot read the mind of the actual batman, we surely can visualize the current state of our .Have a look at the  extensions if you want to know how.The non-quickstartBasic initializationGetting a state machine up and running is pretty simple. Let's say you have the object  (an instance of class ), and you want to manage its states:class Matter(object):
    pass

lump = Matter()
You can initialize a (minimal) working state machine bound to  like this:from transitions import Machine
machine = Machine(model=lump, states=['solid', 'liquid', 'gas', 'plasma'], initial='solid')

# Lump now has state!
lump.state
>>> 'solid'
I say ""minimal"", because while this state machine is technically operational, it doesn't actually do anything. It starts in the  state, but won't ever move into another state, because no transitions are defined... yet!Let's try again.# The states
states=['solid', 'liquid', 'gas', 'plasma']

# And some transitions between states. We're lazy, so we'll leave out
# the inverse phase transitions (freezing, condensation, etc.).
transitions = [
    { 'trigger': 'melt', 'source': 'solid', 'dest': 'liquid' },
    { 'trigger': 'evaporate', 'source': 'liquid', 'dest': 'gas' },
    { 'trigger': 'sublimate', 'source': 'solid', 'dest': 'gas' },
    { 'trigger': 'ionize', 'source': 'gas', 'dest': 'plasma' }
]

# Initialize
machine = Machine(lump, states=states, transitions=transitions, initial='liquid')

# Now lump maintains state...
lump.state
>>> 'liquid'

# And that state can change...
lump.evaporate()
lump.state
>>> 'gas'
lump.trigger('ionize')
lump.state
>>> 'plasma'
Notice the shiny new methods attached to the  instance (, , etc.). Each method triggers the corresponding transition. You don't have to explicitly define these methods anywhere; the name of each transition is bound to the model passed to the  initializer (in this case, ).To be more precise, your model should not already contain methods with the same name as event triggers since  will only attach convenience methods to your model if the spot is not already taken.If you want to modify that behaviour, have a look at the .Furthermore, there is a method called  now attached to your model (if it hasn't been there before).This method lets you execute transitions by name in case dynamic triggering is required.StatesThe soul of any good state machine (and of many bad ones, no doubt) is a set of states. Above, we defined the valid model states by passing a list of strings to the  initializer. But internally, states are actually represented as  objects.You can initialize and modify States in a number of ways. Specifically, you can:The following snippets illustrate several ways to achieve the same goal:# import Machine and State class
from transitions import Machine, State

# Create a list of 3 states to pass to the Machine
# initializer. We can mix types; in this case, we
# pass one State, one string, and one dict.
states = [
    State(name='solid'),
    'liquid',
    { 'name': 'gas'}
    ]
machine = Machine(lump, states)

# This alternative example illustrates more explicit
# addition of states and state callbacks, but the net
# result is identical to the above.
machine = Machine(lump)
solid = State('solid')
liquid = State('liquid')
gas = State('gas')
machine.add_states([solid, liquid, gas])
States are initialized once when added to the machine and will persist until they are removed from it. In other words: if you alter the attributes of a state object, this change will NOT be reset the next time you enter that state. Have a look at how to  in case you require some other behaviour.CallbacksA  can also be associated with a list of  and  callbacks, which are called whenever the state machine enters or leaves that state. You can specify callbacks during initialization by passing them to a  object constructor, in a state property dictionary, or add them later.For convenience, whenever a new  is added to a , the methods  and  are dynamically created on the Machine (not on the model!), which allow you to dynamically add new enter and exit callbacks later if you need them.# Our old Matter class, now with  a couple of new methods we
# can trigger when entering or exit states.
class Matter(object):
    def say_hello(self): print(""hello, new state!"")
    def say_goodbye(self): print(""goodbye, old state!"")

lump = Matter()

# Same states as above, but now we give StateA an exit callback
states = [
    State(name='solid', on_exit=['say_goodbye']),
    'liquid',
    { 'name': 'gas', 'on_exit': ['say_goodbye']}
    ]

machine = Machine(lump, states=states)
machine.add_transition('sublimate', 'solid', 'gas')

# Callbacks can also be added after initialization using
# the dynamically added on_enter_ and on_exit_ methods.
# Note that the initial call to add the callback is made
# on the Machine and not on the model.
machine.on_enter_gas('say_hello')

# Test out the callbacks...
machine.set_state('solid')
lump.sublimate()
>>> 'goodbye, old state!'
>>> 'hello, new state!'
Note that  callback will not fire when a Machine is first initialized. For example if you have an  callback defined, and initialize the  with ,  will not be fired until the next time you enter state . (If you need to make sure  fires at initialization, you can simply create a dummy initial state and then explicitly call  inside the  method.)In addition to passing in callbacks when initializing a , or adding them dynamically, it's also possible to define callbacks in the model class itself, which may increase code clarity. For example:class Matter(object):
    def say_hello(self): print(""hello, new state!"")
    def say_goodbye(self): print(""goodbye, old state!"")
    def on_enter_A(self): print(""We've just entered state A!"")

lump = Matter()
machine = Machine(lump, states=['A', 'B', 'C'])
Now, any time  transitions to state , the  method defined in the  class will fire.Checking stateYou can always check the current state of the model by either:And if you want to retrieve the actual  object for the current state, you can do that through the  instance's  method.lump.state
>>> 'solid'
lump.is_gas()
>>> False
lump.is_solid()
>>> True
machine.get_state(lump.state).name
>>> 'solid'
If you'd like you can choose your own state attribute name by passing the  argument while initializing the . This will also change the name of  to  though. Similarly, auto transitions will be named  instead of . This is done to allow multiple machines to work on the same model with individual state attribute names.lump = Matter()
machine = Machine(lump, states=['solid', 'liquid', 'gas'],  model_attribute='matter_state', initial='solid')
lump.matter_state
>>> 'solid'
# with a custom 'model_attribute', states can also be checked like this:
lump.is_matter_state_solid()
>>> True
lump.to_matter_state_gas()
>>> True
EnumerationsSo far we have seen how we can give state names and use these names to work with our state machine.If you favour stricter typing and more IDE code completion (or you just can't type 'sesquipedalophobia' any longer because the word scares you) using  might be what you are looking for:import enum  # Python 2.7 users need to have 'enum34' installed
from transitions import Machine

class States(enum.Enum):
    ERROR = 0
    RED = 1
    YELLOW = 2
    GREEN = 3

transitions = [['proceed', States.RED, States.YELLOW],
               ['proceed', States.YELLOW, States.GREEN],
               ['error', '*', States.ERROR]]

m = Machine(states=States, transitions=transitions, initial=States.RED)
assert m.is_RED()
assert m.state is States.RED
state = m.get_state(States.RED)  # get transitions.State object
print(state.name)  # >>> RED
m.proceed()
m.proceed()
assert m.is_GREEN()
m.error()
assert m.state is States.ERROR
You can mix enums and strings if you like (e.g. ) but note that internally,  will still handle states by name ().Thus, it is not possible to have the states  and  at the same time.TransitionsSome of the above examples already illustrate the use of transitions in passing, but here we'll explore them in more detail.As with states, each transition is represented internally as its own object – an instance of class . The quickest way to initialize a set of transitions is to pass a dictionary, or list of dictionaries, to the  initializer. We already saw this above:transitions = [
    { 'trigger': 'melt', 'source': 'solid', 'dest': 'liquid' },
    { 'trigger': 'evaporate', 'source': 'liquid', 'dest': 'gas' },
    { 'trigger': 'sublimate', 'source': 'solid', 'dest': 'gas' },
    { 'trigger': 'ionize', 'source': 'gas', 'dest': 'plasma' }
]
machine = Machine(model=Matter(), states=states, transitions=transitions)
Defining transitions in dictionaries has the benefit of clarity, but can be cumbersome. If you're after brevity, you might choose to define transitions using lists. Just make sure that the elements in each list are in the same order as the positional arguments in the  initialization (i.e., , , , etc.).The following list-of-lists is functionally equivalent to the list-of-dictionaries above:transitions = [
    ['melt', 'solid', 'liquid'],
    ['evaporate', 'liquid', 'gas'],
    ['sublimate', 'solid', 'gas'],
    ['ionize', 'gas', 'plasma']
]
Alternatively, you can add transitions to a  after initialization:machine = Machine(model=lump, states=states, initial='solid')
machine.add_transition('melt', source='solid', dest='liquid')
The  argument defines the name of the new triggering method that gets attached to the base model. When this method is called, it will try to execute the transition:>>> lump.melt()
>>> lump.state
'liquid'
By default, calling an invalid trigger will raise an exception:>>> lump.to_gas()
>>> # This won't work because only objects in a solid state can melt
>>> lump.melt()
transitions.core.MachineError: ""Can't trigger event melt from state gas!""
This behavior is generally desirable, since it helps alert you to problems in your code. But in some cases, you might want to silently ignore invalid triggers. You can do this by setting  (either on a state-by-state basis, or globally for all states):>>> # Globally suppress invalid trigger exceptions
>>> m = Machine(lump, states, initial='solid', ignore_invalid_triggers=True)
>>> # ...or suppress for only one group of states
>>> states = ['new_state1', 'new_state2']
>>> m.add_states(states, ignore_invalid_triggers=True)
>>> # ...or even just for a single state. Here, exceptions will only be suppressed when the current state is A.
>>> states = [State('A', ignore_invalid_triggers=True), 'B', 'C']
>>> m = Machine(lump, states)
>>> # ...this can be inverted as well if just one state should raise an exception
>>> # since the machine's global value is not applied to a previously initialized state.
>>> states = ['A', 'B', State('C')] # the default value for 'ignore_invalid_triggers' is False
>>> m = Machine(lump, states, ignore_invalid_triggers=True)
If you need to know which transitions are valid from a certain state, you can use :m.get_triggers('solid')
>>> ['melt', 'sublimate']
m.get_triggers('liquid')
>>> ['evaporate']
m.get_triggers('plasma')
>>> []
# you can also query several states at once
m.get_triggers('solid', 'liquid', 'gas', 'plasma')
>>> ['melt', 'evaporate', 'sublimate', 'ionize']
If you have followed this documentation from the beginning, you will notice that  actually returns more triggers than the explicitly defined ones shown above, such as  and so on.These are called  and will be introduced in the next section.Automatic transitions for all statesIn addition to any transitions added explicitly, a  method is created automatically whenever a state is added to a  instance. This method transitions to the target state no matter which state the machine is currently in:lump.to_liquid()
lump.state
>>> 'liquid'
lump.to_solid()
lump.state
>>> 'solid'
If you desire, you can disable this behavior by setting  in the  initializer.Transitioning from multiple statesA given trigger can be attached to multiple transitions, some of which can potentially begin or end in the same state. For example:machine.add_transition('transmogrify', ['solid', 'liquid', 'gas'], 'plasma')
machine.add_transition('transmogrify', 'plasma', 'solid')
# This next transition will never execute
machine.add_transition('transmogrify', 'plasma', 'gas')
In this case, calling  will set the model's state to  if it's currently , and set it to  otherwise. (Note that only the first matching transition will execute; thus, the transition defined in the last line above won't do anything.)You can also make a trigger cause a transition from all states to a particular destination by using the  wildcard:machine.add_transition('to_liquid', '*', 'liquid')
Note that wildcard transitions will only apply to states that exist at the time of the add_transition() call. Calling a wildcard-based transition when the model is in a state added after the transition was defined will elicit an invalid transition message, and will not transition to the target state.Reflexive transitions from multiple statesA reflexive trigger (trigger that has the same state as source and destination) can easily be added specifying  as destination.This is handy if the same reflexive trigger should be added to multiple states.For example:machine.add_transition('touch', ['liquid', 'gas', 'plasma'], '=', after='change_shape')
This will add reflexive transitions for all three states with  as trigger and with  executed after each trigger.Internal transitionsIn contrast to reflexive transitions, internal transitions will never actually leave the state.This means that transition-related callbacks such as  or  will be processed while state-related callbacks  or  will not.To define a transition to be internal, set the destination to .machine.add_transition('internal', ['liquid', 'gas'], None, after='change_shape')
 Ordered transitionsA common desire is for state transitions to follow a strict linear sequence. For instance, given states , you might want valid transitions for  → ,  → , and  →  (but no other pairs).To facilitate this behavior, Transitions provides an  method in the  class:states = ['A', 'B', 'C']
 # See the ""alternative initialization"" section for an explanation of the 1st argument to init
machine = Machine(states=states, initial='A')
machine.add_ordered_transitions()
machine.next_state()
print(machine.state)
>>> 'B'
# We can also define a different order of transitions
machine = Machine(states=states, initial='A')
machine.add_ordered_transitions(['A', 'C', 'B'])
machine.next_state()
print(machine.state)
>>> 'C'
# Conditions can be passed to 'add_ordered_transitions' as well
# If one condition is passed, it will be used for all transitions
machine = Machine(states=states, initial='A')
machine.add_ordered_transitions(conditions='check')
# If a list is passed, it must contain exactly as many elements as the
# machine contains states (A->B, ..., X->A)
machine = Machine(states=states, initial='A')
machine.add_ordered_transitions(conditions=['check_A2B', ..., 'check_X2A'])
# Conditions are always applied starting from the initial state
machine = Machine(states=states, initial='B')
machine.add_ordered_transitions(conditions=['check_B2C', ..., 'check_A2B'])
# With `loop=False`, the transition from the last state to the first state will be omitted (e.g. C->A)
# When you also pass conditions, you need to pass one condition less (len(states)-1)
machine = Machine(states=states, initial='A')
machine.add_ordered_transitions(loop=False)
machine.next_state()
machine.next_state()
machine.next_state() # transitions.core.MachineError: ""Can't trigger event next_state from state C!""
Queued transitionsThe default behaviour in Transitions is to process events instantly. This means events within an  method will be processed before callbacks bound to  are called.def go_to_C():
    global machine
    machine.to_C()

def after_advance():
    print(""I am in state B now!"")

def entering_C():
    print(""I am in state C now!"")

states = ['A', 'B', 'C']
machine = Machine(states=states, initial='A')

# we want a message when state transition to B has been completed
machine.add_transition('advance', 'A', 'B', after=after_advance)

# call transition from state B to state C
machine.on_enter_B(go_to_C)

# we also want a message when entering state C
machine.on_enter_C(entering_C)
machine.advance()
>>> 'I am in state C now!'
>>> 'I am in state B now!' # what?
The execution order of this example isprepare -> before -> on_enter_B -> on_enter_C -> after.
If queued processing is enabled, a transition will be finished before the next transition is triggered:machine = Machine(states=states, queued=True, initial='A')
...
machine.advance()
>>> 'I am in state B now!'
>>> 'I am in state C now!' # That's better!
This results inprepare -> before -> on_enter_B -> queue(to_C) -> after  -> on_enter_C.
Important note: when processing events in a queue, the trigger call will always return , since there is no way to determine at queuing time whether a transition involving queued calls will ultimately complete successfully. This is true even when only a single event is processed.machine.add_transition('jump', 'A', 'C', conditions='will_fail')
...
# queued=False
machine.jump()
>>> False
# queued=True
machine.jump()
>>> True
When a model is removed from the machine,  will also remove all related events from the queue.class Model:
    def on_enter_B(self):
        self.to_C()  # add event to queue ...
        self.machine.remove_model(self)  # aaaand it's gone
Conditional transitionsSometimes you only want a particular transition to execute if a specific condition occurs. You can do this by passing a method, or list of methods, in the  argument:# Our Matter class, now with a bunch of methods that return booleans.
class Matter(object):
    def is_flammable(self): return False
    def is_really_hot(self): return True

machine.add_transition('heat', 'solid', 'gas', conditions='is_flammable')
machine.add_transition('heat', 'solid', 'liquid', conditions=['is_really_hot'])
In the above example, calling  when the model is in state  will transition to state  if  returns . Otherwise, it will transition to state  if  returns .For convenience, there's also an  argument that behaves exactly like conditions, but inverted:machine.add_transition('heat', 'solid', 'gas', unless=['is_flammable', 'is_really_hot'])
In this case, the model would transition from solid to gas whenever  fires, provided that both  and  return .Note that condition-checking methods will passively receive optional arguments and/or data objects passed to triggering methods. For instance, the following call:lump.heat(temp=74)
# equivalent to lump.trigger('heat', temp=74)
... would pass the  optional kwarg to the  check (possibly wrapped in an  instance). For more on this, see the  section below.Check transitionsIf you want to check whether a transition is possible before you execute it ('look before you leap'), you can use  convenience functions that have been attached to your model:# check if the current temperature is hot enough to trigger a transition
if lump.may_heat():
    lump.heat()
This will execute all  callbacks and evaluate the conditions assigned to the potential transitions.Transition checks can also be used when a transition's destination is not available (yet):machine.add_transition('elevate', 'solid', 'spiritual')
assert not lump.may_elevate()  # not ready yet :(
CallbacksYou can attach callbacks to transitions as well as states. Every transition has  and  attributes that contain a list of methods to call before and after the transition executes:class Matter(object):
    def make_hissing_noises(self): print(""HISSSSSSSSSSSSSSSS"")
    def disappear(self): print(""where'd all the liquid go?"")

transitions = [
    { 'trigger': 'melt', 'source': 'solid', 'dest': 'liquid', 'before': 'make_hissing_noises'},
    { 'trigger': 'evaporate', 'source': 'liquid', 'dest': 'gas', 'after': 'disappear' }
]

lump = Matter()
machine = Machine(lump, states, transitions=transitions, initial='solid')
lump.melt()
>>> ""HISSSSSSSSSSSSSSSS""
lump.evaporate()
>>> ""where'd all the liquid go?""
There is also a  callback that is executed as soon as a transition starts, before any  are checked or other callbacks are executed.class Matter(object):
    heat = False
    attempts = 0
    def count_attempts(self): self.attempts += 1
    def heat_up(self): self.heat = random.random() < 0.25
    def stats(self): print('It took you %i attempts to melt the lump!' %self.attempts)

    @property
    def is_really_hot(self):
        return self.heat


states=['solid', 'liquid', 'gas', 'plasma']

transitions = [
    { 'trigger': 'melt', 'source': 'solid', 'dest': 'liquid', 'prepare': ['heat_up', 'count_attempts'], 'conditions': 'is_really_hot', 'after': 'stats'},
]

lump = Matter()
machine = Machine(lump, states, transitions=transitions, initial='solid')
lump.melt()
lump.melt()
lump.melt()
lump.melt()
>>> ""It took you 4 attempts to melt the lump!""
Note that  will not be called unless the current state is a valid source for the named transition.Default actions meant to be executed before or after every transition can be passed to  during initialization with and  respectively:class Matter(object):
    def make_hissing_noises(self): print(""HISSSSSSSSSSSSSSSS"")
    def disappear(self): print(""where'd all the liquid go?"")

states=['solid', 'liquid', 'gas', 'plasma']

lump = Matter()
m = Machine(lump, states, before_state_change='make_hissing_noises', after_state_change='disappear')
lump.to_gas()
>>> ""HISSSSSSSSSSSSSSSS""
>>> ""where'd all the liquid go?""
There are also two keywords for callbacks which should be executed independently a) of how many transitions are possible,b) if any transition succeeds and c) even if an error is raised during the execution of some other callback.Callbacks passed to  with  will be executed once before processing possible transitions(and their individual  callbacks) takes place.Callbacks of  will be executed regardless of the success of the processed transitions.Note that if an error occurred it will be attached to  as  and can be retrieved with .from transitions import Machine

class Matter(object):
    def raise_error(self, event): raise ValueError(""Oh no"")
    def prepare(self, event): print(""I am ready!"")
    def finalize(self, event): print(""Result: "", type(event.error), event.error)

states=['solid', 'liquid', 'gas', 'plasma']

lump = Matter()
m = Machine(lump, states, prepare_event='prepare', before_state_change='raise_error',
            finalize_event='finalize', send_event=True)
try:
    lump.to_gas()
except ValueError:
    pass
print(lump.state)

# >>> I am ready!
# >>> Result:  <class 'ValueError'> Oh no
# >>> initial
Sometimes things just don't work out as intended and we need to handle exceptions and clean up the mess to keep things going.We can pass callbacks to  to do this:from transitions import Machine

class Matter(object):
    def raise_error(self, event): raise ValueError(""Oh no"")
    def handle_error(self, event):
        print(""Fixing things ..."")
        del event.error  # it did not happen if we cannot see it ...

states=['solid', 'liquid', 'gas', 'plasma']

lump = Matter()
m = Machine(lump, states, before_state_change='raise_error', on_exception='handle_error', send_event=True)
try:
    lump.to_gas()
except ValueError:
    pass
print(lump.state)

# >>> Fixing things ...
# >>> initial
Callable resolutionAs you have probably already realized, the standard way of passing callables to states, conditions and transitions is by name. When processing callbacks and conditions,  will use their name to retrieve the related callable from the model. If the method cannot be retrieved and it contains dots,  will treat the name as a path to a module function and try to import it. Alternatively, you can pass names of properties or attributes. They will be wrapped into functions but cannot receive event data for obvious reasons. You can also pass callables such as (bound) functions directly. As mentioned earlier, you can also pass lists/tuples of callables names to the callback parameters. Callbacks will be executed in the order they were added.from transitions import Machine
from mod import imported_func

import random


class Model(object):

    def a_callback(self):
        imported_func()

    @property
    def a_property(self):
        """""" Basically a coin toss. """"""
        return random.random() < 0.5

    an_attribute = False


model = Model()
machine = Machine(model=model, states=['A'], initial='A')
machine.add_transition('by_name', 'A', 'A', conditions='a_property', after='a_callback')
machine.add_transition('by_reference', 'A', 'A', unless=['a_property', 'an_attribute'], after=model.a_callback)
machine.add_transition('imported', 'A', 'A', after='mod.imported_func')

model.by_name()
model.by_reference()
model.imported()
The callable resolution is done in .This method can be overridden in case more complex callable resolution strategies are required.Exampleclass CustomMachine(Machine):
    @staticmethod
    def resolve_callable(func, event_data):
        # manipulate arguments here and return func, or super() if no manipulation is done.
        super(CustomMachine, CustomMachine).resolve_callable(func, event_data)
Callback execution orderIn summary, there are currently three ways to trigger events. You can call a model's convenience functions like ,execute triggers by name such as  or dispatch events on multiple models with (see section about multiple models in ).Callbacks on transitions are then executed in the following order:| Callback                        |    Current State     | Comments                                                                                    || ------------------------------- | :------------------: | ------------------------------------------------------------------------------------------- ||        |              | executed once before individual transitions are processed                                 ||           |              | executed as soon as the transition starts                                                   ||        |              | conditions may fail and halt the transition                                               ||            |              | conditions may fail and halt the transition                                               ||  |              | default callbacks declared on model                                                         ||            |              |                                                                                             ||                |              | callbacks declared on the source state                                                      ||                 |                      |                                                                                             ||               |         | callbacks declared on the destination state                                                 ||             |         |                                                                                             ||   |         | default callbacks declared on model                                                         ||         |  | callbacks will be executed when an exception has been raised                                ||       |  | callbacks will be executed even if no transition took place or an exception has been raised |If any callback raises an exception, the processing of callbacks is not continued. This means that when an error occurs before the transition (in  or earlier), it is halted. In case there is a raise after the transition has been conducted (in  or later), the state change persists and no rollback is happening. Callbacks specified in  will always be executed unless the exception is raised by a finalizing callback itself. Note that each callback sequence has to be finished before the next stage is executed. Blocking callbacks will halt the execution order and therefore block the  or  call itself. If you want callbacks to be executed in parallel, you could have a look at the   for asynchronous processing or  for threading.Passing dataSometimes you need to pass the callback functions registered at machine initialization some data that reflects the model's current state.Transitions allows you to do this in two different ways.First (the default), you can pass any positional or keyword arguments directly to the trigger methods (created when you call ):class Matter(object):
    def __init__(self): self.set_environment()
    def set_environment(self, temp=0, pressure=101.325):
        self.temp = temp
        self.pressure = pressure
    def print_temperature(self): print(""Current temperature is %d degrees celsius."" % self.temp)
    def print_pressure(self): print(""Current pressure is %.2f kPa."" % self.pressure)

lump = Matter()
machine = Machine(lump, ['solid', 'liquid'], initial='solid')
machine.add_transition('melt', 'solid', 'liquid', before='set_environment')

lump.melt(45)  # positional arg;
# equivalent to lump.trigger('melt', 45)
lump.print_temperature()
>>> 'Current temperature is 45 degrees celsius.'

machine.set_state('solid')  # reset state so we can melt again
lump.melt(pressure=300.23)  # keyword args also work
lump.print_pressure()
>>> 'Current pressure is 300.23 kPa.'

You can pass any number of arguments you like to the trigger.There is one important limitation to this approach: every callback function triggered by the state transition must be able to handle all of the arguments. This may cause problems if the callbacks each expect somewhat different data.To get around this, Transitions supports an alternate method for sending data. If you set  at  initialization, all arguments to the triggers will be wrapped in an  instance and passed on to every callback. (The  object also maintains internal references to the source state, model, transition, machine, and trigger associated with the event, in case you need to access these for anything.)class Matter(object):

    def __init__(self):
        self.temp = 0
        self.pressure = 101.325

    # Note that the sole argument is now the EventData instance.
    # This object stores positional arguments passed to the trigger method in the
    # .args property, and stores keywords arguments in the .kwargs dictionary.
    def set_environment(self, event):
        self.temp = event.kwargs.get('temp', 0)
        self.pressure = event.kwargs.get('pressure', 101.325)

    def print_pressure(self): print(""Current pressure is %.2f kPa."" % self.pressure)

lump = Matter()
machine = Machine(lump, ['solid', 'liquid'], send_event=True, initial='solid')
machine.add_transition('melt', 'solid', 'liquid', before='set_environment')

lump.melt(temp=45, pressure=1853.68)  # keyword args
lump.print_pressure()
>>> 'Current pressure is 1853.68 kPa.'

Alternative initialization patternsIn all of the examples so far, we've attached a new  instance to a separate model (, an instance of class ). While this separation keeps things tidy (because you don't have to monkey patch a whole bunch of new methods into the  class), it can also get annoying, since it requires you to keep track of which methods are called on the state machine, and which ones are called on the model that the state machine is bound to (e.g.,  vs. ).Fortunately, Transitions is flexible, and supports two other initialization patterns.First, you can create a standalone state machine that doesn't require another model at all. Simply omit the model argument during initialization:machine = Machine(states=states, transitions=transitions, initial='solid')
machine.melt()
machine.state
>>> 'liquid'
If you initialize the machine this way, you can then attach all triggering events (like , , etc.) and all callback functions directly to the  instance.This approach has the benefit of consolidating all of the state machine functionality in one place, but can feel a little bit unnatural if you think state logic should be contained within the model itself rather than in a separate controller.An alternative (potentially better) approach is to have the model inherit from the  class. Transitions is designed to support inheritance seamlessly. (just be sure to override class 's  method!):class Matter(Machine):
    def say_hello(self): print(""hello, new state!"")
    def say_goodbye(self): print(""goodbye, old state!"")

    def __init__(self):
        states = ['solid', 'liquid', 'gas']
        Machine.__init__(self, states=states, initial='solid')
        self.add_transition('melt', 'solid', 'liquid')

lump = Matter()
lump.state
>>> 'solid'
lump.melt()
lump.state
>>> 'liquid'
Here you get to consolidate all state machine functionality into your existing model, which often feels more natural than sticking all of the functionality we want in a separate standalone  instance.A machine can handle multiple models which can be passed as a list like .In cases where you want to add models as well as the machine instance itself, you can pass the class variable placeholder (string)  during initialization like .You can also create a standalone machine, and register models dynamically via  by passing  to the constructor.Furthermore, you can use  to trigger events on all currently added models.Remember to call  if machine is long-lasting and your models are temporary and should be garbage collected:class Matter():
    pass

lump1 = Matter()
lump2 = Matter()

# setting 'model' to None or passing an empty list will initialize the machine without a model
machine = Machine(model=None, states=states, transitions=transitions, initial='solid')

machine.add_model(lump1)
machine.add_model(lump2, initial='liquid')

lump1.state
>>> 'solid'
lump2.state
>>> 'liquid'

# custom events as well as auto transitions can be dispatched to all models
machine.dispatch(""to_plasma"")

lump1.state
>>> 'plasma'
assert lump1.state == lump2.state

machine.remove_model([lump1, lump2])
del lump1  # lump1 is garbage collected
del lump2  # lump2 is garbage collected
If you don't provide an initial state in the state machine constructor,  will create and add a default state called .If you do not want a default initial state, you can pass .However, in this case you need to pass an initial state every time you add a model.machine = Machine(model=None, states=states, transitions=transitions, initial=None)

machine.add_model(Matter())
>>> ""MachineError: No initial state configured for machine, must specify when adding model.""
machine.add_model(Matter(), initial='liquid')
Models with multiple states could attach multiple machines using different  values. As mentioned in , this will add custom  functions:lump = Matter()

matter_machine = Machine(lump, states=['solid', 'liquid', 'gas'], initial='solid')
# add a second machine to the same model but assign a different state attribute
shipment_machine = Machine(lump, states=['delivered', 'shipping'], initial='delivered', model_attribute='shipping_state')

lump.state
>>> 'solid'
lump.is_solid()  # check the default field
>>> True
lump.shipping_state
>>> 'delivered'
lump.is_shipping_state_delivered()  # check the custom field.
>>> True
lump.to_shipping_state_shipping()
>>> True
lump.is_shipping_state_delivered()
>>> False
LoggingTransitions includes very rudimentary logging capabilities. A number of events – namely, state changes, transition triggers, and conditional checks – are logged as INFO-level events using the standard Python  module. This means you can easily configure logging to standard output in a script:# Set up logging; The basic log level will be DEBUG
import logging
logging.basicConfig(level=logging.DEBUG)
# Set transitions' log level to INFO; DEBUG messages will be omitted
logging.getLogger('transitions').setLevel(logging.INFO)

# Business as usual
machine = Machine(states=states, transitions=transitions, initial='solid')
...
(Re-)Storing machine instancesMachines are picklable and can be stored and loaded with . For Python 3.3 and earlier  is required.import dill as pickle # only required for Python 3.3 and earlier

m = Machine(states=['A', 'B', 'C'], initial='A')
m.to_B()
m.state
>>> B

# store the machine
dump = pickle.dumps(m)

# load the Machine instance again
m2 = pickle.loads(dump)

m2.state
>>> B

m2.states.keys()
>>> ['A', 'B', 'C']
 ExtensionsEven though the core of transitions is kept lightweight, there are a variety of MixIns to extend its functionality. Currently supported are:There are two mechanisms to retrieve a state machine instance with the desired features enabled.The first approach makes use of the convenience  with the four parameters , ,  or  set to  if the feature is required:from transitions.extensions import MachineFactory

# create a machine with mixins
diagram_cls = MachineFactory.get_predefined(graph=True)
nested_locked_cls = MachineFactory.get_predefined(nested=True, locked=True)
async_machine_cls = MachineFactory.get_predefined(asyncio=True)

# create instances from these classes
# instances can be used like simple machines
machine1 = diagram_cls(model, state, transitions)
machine2 = nested_locked_cls(model, state, transitions)
This approach targets experimental use since in this case the underlying classes do not have to be known.However, classes can also be directly imported from . The naming scheme is as follows:|                                | Diagrams | Nested | Locked | Asyncio || -----------------------------: | :------: | :----: | :----: | :-----: ||                        Machine |    ✘     |   ✘    |   ✘    |    ✘    ||                   GraphMachine |    ✓     |   ✘    |   ✘    |    ✘    ||            HierarchicalMachine |    ✘     |   ✓    |   ✘    |    ✘    ||                  LockedMachine |    ✘     |   ✘    |   ✓    |    ✘    ||       HierarchicalGraphMachine |    ✓     |   ✓    |   ✘    |    ✘    ||             LockedGraphMachine |    ✓     |   ✘    |   ✓    |    ✘    ||      LockedHierarchicalMachine |    ✘     |   ✓    |   ✓    |    ✘    || LockedHierarchicalGraphMachine |    ✓     |   ✓    |   ✓    |    ✘    ||                   AsyncMachine |    ✘     |   ✘    |   ✘    |    ✓    ||              AsyncGraphMachine |    ✓     |   ✘    |   ✘    |    ✓    ||       HierarchicalAsyncMachine |    ✘     |   ✓    |   ✘    |    ✓    ||  HierarchicalAsyncGraphMachine |    ✓     |   ✓    |   ✘    |    ✓    |To use a feature-rich state machine, one could write:from transitions.extensions import LockedHierarchicalGraphMachine as LHGMachine

machine = LHGMachine(model, states, transitions)
 DiagramsAdditional Keywords:Transitions can generate basic state diagrams displaying all valid transitions between states. To use the graphing functionality, you'll need to have  and/or  installed:To generate graphs with the package , you need to install  manually or via a package manager.sudo apt-get install graphviz graphviz-dev  # Ubuntu and Debian
brew install graphviz  # MacOS
conda install graphviz python-graphviz  # (Ana)conda
Now you can install the actual Python packagespip install graphviz pygraphviz # install graphviz and/or pygraphviz manually...
pip install transitions[diagrams]  # ... or install transitions with 'diagrams' extras which currently depends on pygraphviz
Currently,  will use  when available and fall back to  when  cannot befound. This can be overridden by passing  to the constructor. Note that this default might changein the future and  support may be dropped.With  you can get the current graph or the region of interest (roi) and draw it like this:# import transitions

from transitions.extensions import GraphMachine
m = Model()
# without further arguments pygraphviz will be used
machine = GraphMachine(model=m, ...)
# when you want to use graphviz explicitly
machine = GraphMachine(model=m, use_pygraphviz=False, ...)
# in cases where auto transitions should be visible
machine = GraphMachine(model=m, show_auto_transitions=True, ...)

# draw the whole graph ...
m.get_graph().draw('my_state_diagram.png', prog='dot')
# ... or just the region of interest
# (previous state, active state and all reachable states)
roi = m.get_graph(show_roi=True).draw('my_state_diagram.png', prog='dot')
This produces something like this:Independent of the backend you use, the draw function also accepts a file descriptor or a binary stream as the first argument. If you set this parameter to , the byte stream will be returned:import io

with open('a_graph.png', 'bw') as f:
    # you need to pass the format when you pass objects instead of filenames.
    m.get_graph().draw(f, format=""png"", prog='dot')

# you can pass a (binary) stream too
b = io.BytesIO()
m.get_graph().draw(b, format=""png"", prog='dot')

# or just handle the binary string yourself
result = m.get_graph().draw(None, format=""png"", prog='dot')
assert result == b.getvalue()
References and partials passed as callbacks will be resolved as good as possible:from transitions.extensions import GraphMachine
from functools import partial


class Model:

    def clear_state(self, deep=False, force=False):
        print(""Clearing state ..."")
        return True


model = Model()
machine = GraphMachine(model=model, states=['A', 'B', 'C'],
                       transitions=[
                           {'trigger': 'clear', 'source': 'B', 'dest': 'A', 'conditions': model.clear_state},
                           {'trigger': 'clear', 'source': 'C', 'dest': 'A',
                            'conditions': partial(model.clear_state, False, force=True)},
                       ],
                       initial='A', show_conditions=True)

model.get_graph().draw('my_state_diagram.png', prog='dot')
This should produce something similar to this:If the format of references does not suit your needs, you can override the static method . If you want to skip reference entirely, just let  return .Also, have a look at our  IPython/Jupyter notebooks for a more detailed example about how to use and edit graphs.Hierarchical State Machine (HSM)Transitions includes an extension module which allows nesting states.This allows us to create contexts and to model cases where states are related to certain subtasks in the state machine.To create a nested state, either import  from transitions or use a dictionary with the initialization arguments  and .Optionally,  can be used to define a sub state to transit to, when the nested state is entered.from transitions.extensions import HierarchicalMachine

states = ['standing', 'walking', {'name': 'caffeinated', 'children':['dithering', 'running']}]
transitions = [
  ['walk', 'standing', 'walking'],
  ['stop', 'walking', 'standing'],
  ['drink', '*', 'caffeinated'],
  ['walk', ['caffeinated', 'caffeinated_dithering'], 'caffeinated_running'],
  ['relax', 'caffeinated', 'standing']
]

machine = HierarchicalMachine(states=states, transitions=transitions, initial='standing', ignore_invalid_triggers=True)

machine.walk() # Walking now
machine.stop() # let's stop for a moment
machine.drink() # coffee time
machine.state
>>> 'caffeinated'
machine.walk() # we have to go faster
machine.state
>>> 'caffeinated_running'
machine.stop() # can't stop moving!
machine.state
>>> 'caffeinated_running'
machine.relax() # leave nested state
machine.state # phew, what a ride
>>> 'standing'
# machine.on_enter_caffeinated_running('callback_method')
A configuration making use of  could look like this:# ...
states = ['standing', 'walking', {'name': 'caffeinated', 'initial': 'dithering', 'children': ['dithering', 'running']}]
transitions = [
  ['walk', 'standing', 'walking'],
  ['stop', 'walking', 'standing'],
  # this transition will end in 'caffeinated_dithering'...
  ['drink', '*', 'caffeinated'],
  # ... that is why we do not need do specify 'caffeinated' here anymore
  ['walk', 'caffeinated_dithering', 'caffeinated_running'],
  ['relax', 'caffeinated', 'standing']
]
# ...
The  keyword of the  constructor accepts nested states (e.g. ) and a list of states which is considered to be a parallel state (e.g. ) or the current state of another model () which should be effectively one of the previous mentioned options. Note that when passing a string,  will check the targeted state for  substates and use this as an entry state. This will be done recursively until a substate does not mention an initial state. Parallel states or a state passed as a list will be used 'as is' and no further initial evaluation will be conducted.Note that your previously created state object must be a  or a derived class of it.The standard  class used in simple  instances lacks features required for nesting.from transitions.extensions.nesting import HierarchicalMachine, NestedState
from transitions import  State
m = HierarchicalMachine(states=['A'], initial='initial')
m.add_state('B')  # fine
m.add_state({'name': 'C'})  # also fine
m.add_state(NestedState('D'))  # fine as well
m.add_state(State('E'))  # does not work!
Some things that have to be considered when working with nested states: State names are concatenated with .Currently the separator is set to underscore ('') and therefore behaves similar to the basic machine.This means a substate  from state  will be known by . A substate  of  will be referred to as  and so on.When entering a substate,  will be called for all parent states. The same is true for exiting substates.Third, nested states can overwrite transition behaviour of their parents.If a transition is not known to the current state it will be delegated to its parent.This means that in the standard configuration, state names in HSMs MUST NOT contain underscores.For  it's impossible to tell whether  should add a state named  or add a substate  to the state .In some cases this is not sufficient however.For instance if state names consist of more than one word and you want/need to use underscore to separate them instead of .To deal with this, you can change the character used for separation quite easily.You can even use fancy unicode characters if you use Python 3.Setting the separator to something else than underscore changes some of the behaviour (auto_transition and setting callbacks) though:from transitions.extensions import HierarchicalMachine
from transitions.extensions.nesting import NestedState
NestedState.separator = '↦'
states = ['A', 'B',
  {'name': 'C', 'children':['1', '2',
    {'name': '3', 'children': ['a', 'b', 'c']}
  ]}
]

transitions = [
    ['reset', 'C', 'A'],
    ['reset', 'C↦2', 'C']  # overwriting parent reset
]

# we rely on auto transitions
machine = HierarchicalMachine(states=states, transitions=transitions, initial='A')
machine.to_B()  # exit state A, enter state B
machine.to_C()  # exit B, enter C
machine.to_C.s3.a()  # enter C↦a; enter C↦3↦a;
machine.state
>>> 'C↦3↦a'
assert machine.is_C.s3.a()
machine.to('C↦2')  # not interactive; exit C↦3↦a, exit C↦3, enter C↦2
machine.reset()  # exit C↦2; reset C has been overwritten by C↦3
machine.state
>>> 'C'
machine.reset()  # exit C, enter A
machine.state
>>> 'A'
# s.on_enter('C↦3↦a', 'callback_method')
Instead of  auto transition is called as . If your substate starts with a digit, transitions adds a prefix 's' ('3' becomes 's3') to the auto transition  to comply with the attribute naming scheme of Python.If interactive completion is not required,  can be called directly. Additionally,  is replaced with . State checks can be conducted in a similar fashion. Instead of , the  variant  can be used.To check whether the current state is a substate of a specific state,  supports the keyword :machine.state
>>> 'C.2.a'
machine.is_C() # checks for specific states
>>> False
machine.is_C(allow_substates=True)
>>> True
assert machine.is_C.s2() is False
assert machine.is_C.s2(allow_substates=True)  # FunctionWrapper support allow_substate as well
new in 0.8.0You can use enumerations in HSMs as well but keep in mind that  are compared by value.If you have a value more than once in a state tree those states cannot be distinguished.states = [States.RED, States.YELLOW, {'name': States.GREEN, 'children': ['tick', 'tock']}]
states = ['A', {'name': 'B', 'children': states, 'initial': States.GREEN}, States.GREEN]
machine = HierarchicalMachine(states=states)
machine.to_B()
machine.is_GREEN()  # returns True even though the actual state is B_GREEN
new in 0.8.0 has been rewritten from scratch to support parallel states and better isolation of nested states.This involves some tweaks based on community feedback.To get an idea of processing order and configuration have a look at the following example:from transitions.extensions.nesting import HierarchicalMachine
import logging
states = ['A', 'B', {'name': 'C', 'parallel': [{'name': '1', 'children': ['a', 'b', 'c'], 'initial': 'a',
                                                'transitions': [['go', 'a', 'b']]},
                                               {'name': '2', 'children': ['x', 'y', 'z'], 'initial': 'z'}],
                      'transitions': [['go', '2_z', '2_x']]}]

transitions = [['reset', 'C_1_b', 'B']]
logging.basicConfig(level=logging.INFO)
machine = HierarchicalMachine(states=states, transitions=transitions, initial='A')
machine.to_C()
# INFO:transitions.extensions.nesting:Exited state A
# INFO:transitions.extensions.nesting:Entered state C
# INFO:transitions.extensions.nesting:Entered state C_1
# INFO:transitions.extensions.nesting:Entered state C_2
# INFO:transitions.extensions.nesting:Entered state C_1_a
# INFO:transitions.extensions.nesting:Entered state C_2_z
machine.go()
# INFO:transitions.extensions.nesting:Exited state C_1_a
# INFO:transitions.extensions.nesting:Entered state C_1_b
# INFO:transitions.extensions.nesting:Exited state C_2_z
# INFO:transitions.extensions.nesting:Entered state C_2_x
machine.reset()
# INFO:transitions.extensions.nesting:Exited state C_1_b
# INFO:transitions.extensions.nesting:Exited state C_2_x
# INFO:transitions.extensions.nesting:Exited state C_1
# INFO:transitions.extensions.nesting:Exited state C_2
# INFO:transitions.extensions.nesting:Exited state C
# INFO:transitions.extensions.nesting:Entered state B
When using  instead of ,  will enter all states of the passed list at the same time.Which substate to enter is defined by  which should always point to a direct substate.A novel feature is to define local transitions by passing the  keyword in a state definition.The above defined transition  is only valid in .While you can reference substates as done in  you cannot reference parent states directly in locally defined transitions.When a parent state is exited, its children will also be exited.In addition to the processing order of transitions known from  where transitions are considered in the order they were added,  considers hierarchy as well.Transitions defined in substates will be evaluated first (e.g.  is left before ) and transitions defined with wildcard  will (for now) only add transitions to root states (in this example , , )Starting with 0.8.0 nested states can be added directly and will issue the creation of parent states on-the-fly:m = HierarchicalMachine(states=['A'], initial='A')
m.add_state('B_1_a')
m.to_B_1()
assert m.is_B(allow_substates=True)
Reuse of previously created HSMsBesides semantic order, nested states are very handy if you want to specify state machines for specific tasks and plan to reuse them.Before 0.8.0, a  would not integrate the machine instance itself but the states and transitions by creating copies of them.However, since 0.8.0  instances are just referenced which means changes in one machine's collection of states and events will influence the other machine instance. Models and their state will not be shared though.Note that events and transitions are also copied by reference and will be shared by both instances if you do not use the  keyword.This change was done to be more in line with  which also uses passed  instances by reference.count_states = ['1', '2', '3', 'done']
count_trans = [
    ['increase', '1', '2'],
    ['increase', '2', '3'],
    ['decrease', '3', '2'],
    ['decrease', '2', '1'],
    ['done', '3', 'done'],
    ['reset', '*', '1']
]

counter = HierarchicalMachine(states=count_states, transitions=count_trans, initial='1')

counter.increase() # love my counter
states = ['waiting', 'collecting', {'name': 'counting', 'children': counter}]

transitions = [
    ['collect', '*', 'collecting'],
    ['wait', '*', 'waiting'],
    ['count', 'collecting', 'counting']
]

collector = HierarchicalMachine(states=states, transitions=transitions, initial='waiting')
collector.collect()  # collecting
collector.count()  # let's see what we got; counting_1
collector.increase()  # counting_2
collector.increase()  # counting_3
collector.done()  # collector.state == counting_done
collector.wait()  # collector.state == waiting
If a  is passed with the  keyword, the initial state of this machine will be assigned to the new parent state.In the above example we see that entering  will also enter .If this is undesired behaviour and the machine should rather halt in the parent state, the user can pass  as  like .Sometimes you want such an embedded state collection to 'return' which means after it is done it should exit and transit to one of your super states.To achieve this behaviour you can remap state transitions.In the example above we would like the counter to return if the state  was reached.This is done as follows:states = ['waiting', 'collecting', {'name': 'counting', 'children': counter, 'remap': {'done': 'waiting'}}]

... # same as above

collector.increase() # counting_3
collector.done()
collector.state
>>> 'waiting' # be aware that 'counting_done' will be removed from the state machine
As mentioned above, using  will copy events and transitions since they could not be valid in the original state machine.If a reused state machine does not have a final state, you can of course add the transitions manually.If 'counter' had no 'done' state, we could just add  to achieve the same behaviour.In cases where you want states and transitions to be copied by value rather than reference (for instance, if you want to keep the pre-0.8 behaviour) you can do so by creating a  and assigning deep copies of the machine's events and states to it.from transitions.extensions.nesting import NestedState
from copy import deepcopy

# ... configuring and creating counter

counting_state = NestedState(name=""counting"", initial='1')
counting_state.states = deepcopy(counter.states)
counting_state.events = deepcopy(counter.events)

states = ['waiting', 'collecting', counting_state]
For complex state machines, sharing configurations rather than instantiated machines might be more feasible.Especially since instantiated machines must be derived from .Such configurations can be stored and loaded easily via JSON or YAML (see the ). allows defining substates either with the keyword  or .If both are present, only  will be considered.counter_conf = {
    'name': 'counting',
    'states': ['1', '2', '3', 'done'],
    'transitions': [
        ['increase', '1', '2'],
        ['increase', '2', '3'],
        ['decrease', '3', '2'],
        ['decrease', '2', '1'],
        ['done', '3', 'done'],
        ['reset', '*', '1']
    ],
    'initial': '1'
}

collector_conf = {
    'name': 'collector',
    'states': ['waiting', 'collecting', counter_conf],
    'transitions': [
        ['collect', '*', 'collecting'],
        ['wait', '*', 'waiting'],
        ['count', 'collecting', 'counting']
    ],
    'initial': 'waiting'
}

collector = HierarchicalMachine(**collector_conf)
collector.collect()
collector.count()
collector.increase()
assert collector.is_counting_2()
 Threadsafe(-ish) State MachineIn cases where event dispatching is done in threads, one can use either  or  where function access (!sic) is secured with reentrant locks.This does not save you from corrupting your machine by tinkering with member variables of your model or state machine.from transitions.extensions import LockedMachine
from threading import Thread
import time

states = ['A', 'B', 'C']
machine = LockedMachine(states=states, initial='A')

# let us assume that entering B will take some time
thread = Thread(target=machine.to_B)
thread.start()
time.sleep(0.01) # thread requires some time to start
machine.to_C() # synchronized access; won't execute before thread is done
# accessing attributes directly
thread = Thread(target=machine.to_B)
thread.start()
machine.new_attrib = 42 # not synchronized! will mess with execution order
Any python context manager can be passed in via the  keyword argument:from transitions.extensions import LockedMachine
from threading import RLock

states = ['A', 'B', 'C']

lock1 = RLock()
lock2 = RLock()

machine = LockedMachine(states=states, initial='A', machine_context=[lock1, lock2])
Any contexts via  will be shared between all models registered with the .Per-model contexts can be added as well:lock3 = RLock()

machine.add_model(model, model_context=lock3)
It's important that all user-provided context managers are re-entrant since the state machine will call them multipletimes, even in the context of a single trigger invocation. Using async callbacksIf you are using Python 3.7 or later, you can use  to work with asynchronous callbacks.You can mix synchronous and asynchronous callbacks if you like but this may have undesired side effects.Note that events need to be awaited and the event loop must also be handled by you.from transitions.extensions.asyncio import AsyncMachine
import asyncio
import time


class AsyncModel:

    def prepare_model(self):
        print(""I am synchronous."")
        self.start_time = time.time()

    async def before_change(self):
        print(""I am asynchronous and will block now for 100 milliseconds."")
        await asyncio.sleep(0.1)
        print(""I am done waiting."")

    def sync_before_change(self):
        print(""I am synchronous and will block the event loop (what I probably shouldn't)"")
        time.sleep(0.1)
        print(""I am done waiting synchronously."")

    def after_change(self):
        print(f""I am synchronous again. Execution took {int((time.time() - self.start_time) * 1000)} ms."")


transition = dict(trigger=""start"", source=""Start"", dest=""Done"", prepare=""prepare_model"",
                  before=[""before_change""] * 5 + [""sync_before_change""],
                  after=""after_change"")  # execute before function in asynchronously 5 times
model = AsyncModel()
machine = AsyncMachine(model, states=[""Start"", ""Done""], transitions=[transition], initial='Start')

asyncio.get_event_loop().run_until_complete(model.start())
# >>> I am synchronous.
#     I am asynchronous and will block now for 100 milliseconds.
#     I am asynchronous and will block now for 100 milliseconds.
#     I am asynchronous and will block now for 100 milliseconds.
#     I am asynchronous and will block now for 100 milliseconds.
#     I am asynchronous and will block now for 100 milliseconds.
#     I am synchronous and will block the event loop (what I probably shouldn't)
#     I am done waiting synchronously.
#     I am done waiting.
#     I am done waiting.
#     I am done waiting.
#     I am done waiting.
#     I am done waiting.
#     I am synchronous again. Execution took 101 ms.
assert model.is_Done()
So, why do you need to use Python 3.7 or later you may ask.Async support has been introduced earlier. makes use of  to handle running callbacks when new events arrive before a transitionhas been finished:async def await_never_return():
    await asyncio.sleep(100)
    raise ValueError(""That took too long!"")

async def fix():
    await m2.fix()

m1 = AsyncMachine(states=['A', 'B', 'C'], initial='A', name=""m1"")
m2 = AsyncMachine(states=['A', 'B', 'C'], initial='A', name=""m2"")
m2.add_transition(trigger='go', source='A', dest='B', before=await_never_return)
m2.add_transition(trigger='fix', source='A', dest='C')
m1.add_transition(trigger='go', source='A', dest='B', after='go')
m1.add_transition(trigger='go', source='B', dest='C', after=fix)
asyncio.get_event_loop().run_until_complete(asyncio.gather(m2.go(), m1.go()))

assert m1.state == m2.state
This example actually illustrates two things:First, that 'go' called in m1's transition from  to be  is not cancelled and second, calling  willhalt the transition attempt of m2 from  to  by executing 'fix' from  to .This separation would not be possible without .Note that  and  are NOT treated as ongoing transitions.This means that after  have been evaluated, a transition is executed even though another event already happened.Tasks will only be cancelled when run as a  callback or later. features a model-special queue mode which can be used when  is passed to the constructor.With a model-specific queue, events will only be queued when they belong to the same model.Furthermore, a raised exception will only clear the event queue of the model that raised that exception.For the sake of simplicity, let's assume that every event in  below is not triggered at the same time but slightly delayed:asyncio.gather(model1.event1(), model1.event2(), model2.event1())
# execution order with AsyncMachine(queued=True)
# model1.event1 -> model1.event2 -> model2.event1
# execution order with AsyncMachine(queued='model')
# (model1.event1, model2.event1) -> model1.event2

asyncio.gather(model1.event1(), model1.error(), model1.event3(), model2.event1(), model2.event2(), model2.event3())
# execution order with AsyncMachine(queued=True)
# model1.event1 -> model1.error
# execution order with AsyncMachine(queued='model')
# (model1.event1, model2.event1) -> (model1.error, model2.event2) -> model2.event3
Note that queue modes must not be changed after machine construction.Adding features to statesIf your superheroes need some custom behaviour, you can throw in some extra functionality by decorating machine states:from time import sleep
from transitions import Machine
from transitions.extensions.states import add_state_features, Tags, Timeout


@add_state_features(Tags, Timeout)
class CustomStateMachine(Machine):
    pass


class SocialSuperhero(object):
    def __init__(self):
        self.entourage = 0

    def on_enter_waiting(self):
        self.entourage += 1


states = [{'name': 'preparing', 'tags': ['home', 'busy']},
          {'name': 'waiting', 'timeout': 1, 'on_timeout': 'go'},
          {'name': 'away'}]  # The city needs us!

transitions = [['done', 'preparing', 'waiting'],
               ['join', 'waiting', 'waiting'],  # Entering Waiting again will increase our entourage
               ['go', 'waiting', 'away']]  # Okay, let' move

hero = SocialSuperhero()
machine = CustomStateMachine(model=hero, states=states, transitions=transitions, initial='preparing')
assert hero.state == 'preparing'  # Preparing for the night shift
assert machine.get_state(hero.state).is_busy  # We are at home and busy
hero.done()
assert hero.state == 'waiting'  # Waiting for fellow superheroes to join us
assert hero.entourage == 1  # It's just us so far
sleep(0.7)  # Waiting...
hero.join()  # Weeh, we got company
sleep(0.5)  # Waiting...
hero.join()  # Even more company \o/
sleep(2)  # Waiting...
assert hero.state == 'away'  # Impatient superhero already left the building
assert machine.get_state(hero.state).is_home is False  # Yupp, not at home anymore
assert hero.entourage == 3  # At least he is not alone
Currently, transitions comes equipped with the following state features:You can write your own  extensions and add them the same way. Just note that  expects Mixins. This means your extension should always call the overridden methods ,  and . Your extension may inherit from State but will also work without it.Using  has a drawback which is that decorated machines cannot be pickled (more precisely, the dynamically generated  cannot be pickled).This might be a reason to write a dedicated custom state class instead.Depending on the chosen state machine, your custom state class may need to provide certain state features. For instance,  requires your custom state to be an instance of  ( is not sufficient). To inject your states you can either assign them to your 's class attribute  or override  in case you need some specific procedures done whenever a state is created:from transitions import Machine, State

class MyState(State):
    pass

class CustomMachine(Machine):
    # Use MyState as state class
    state_cls = MyState


class VerboseMachine(Machine):

    # `Machine._create_state` is a class method but we can
    # override it to be an instance method
    def _create_state(self, *args, **kwargs):
        print(""Creating a new state with machine '{0}'"".format(self.name))
        return MyState(*args, **kwargs)
If you want to avoid threads in your  entirely, you can replace the  state feature with  from the  extension:import asyncio
from transitions.extensions.states import add_state_features
from transitions.extensions.asyncio import AsyncTimeout, AsyncMachine

@add_state_features(AsyncTimeout)
class TimeoutMachine(AsyncMachine):
    pass

states = ['A', {'name': 'B', 'timeout': 0.2, 'on_timeout': 'to_C'}, 'C']
m = TimeoutMachine(states=states, initial='A', queued=True)  # see remark below
asyncio.run(asyncio.wait([m.to_B(), asyncio.sleep(0.1)]))
assert m.is_B()  # timeout shouldn't be triggered
asyncio.run(asyncio.wait([m.to_B(), asyncio.sleep(0.3)]))
assert m.is_C()   # now timeout should have been processed
You should consider passing  to the  constructor. This will make sure that events are processed sequentially and avoid asynchronous  that may appear when timeout and event happen in close proximity. Using transitions together with DjangoYou can have a look at the  for some inspiration or checkout .It has been developed by Christian Ledermann and is also hosted on . contains some usage examples.I have a [bug report/issue/question]...First, congratulations! You reached the end of the documentation!If you want to try out  before you install it, you can do that in an interactive Jupyter notebook at mybinder.org.Just click this button 👉 .For bug reports and other issues, please  on GitHub.For usage questions, post on Stack Overflow, making sure to tag your question with the . Do not forget to have a look at the !For any other questions, solicitations, or large unrestricted monetary gifts, email  (initial author) and/or  (current maintainer)."
https://github.com/schollz/howmanypeoplearearound,Count the number of people around you :family_man_man_boy: by monitoring wifi signals :satellite:,"howmanypeoplearearoundCount the number of people around you :family_man_man_boy: by monitoring wifi signals :satellite:.howmanypeoplearearound calculates the number of people in the vicinityusing the approximate number of smartphones as a proxy (since ).A cellphone is determined to be in proximity to the computer based on sniffing WiFi proberequests. Possible uses of howmanypeoplearearound include: monitoring foot traffic in your housewith Raspberry Pis, seeing if your roommates are home, etc.Tested on Linux (Raspbian and Ubuntu) and Mac OS X.It may be illegal to monitor networks for MAC addresses, especially on networks that you do not own. Please check your country's laws (for US ) - .Getting startedFor a video walkthrough on how to install, checkout .DependenciesPython 2.7 or preferably Python 3 must be installed on your machine with the  command also available.  python -V
  pip -V
WiFi adapter that supports monitor modeThere are a number of possible USB WiFi adapters that support monitor mode. Here's a list that are popular:Namely you want to find a USB adapter with one of the following chipsets: Atheros AR9271, Ralink RT3070, Ralink RT3572, or Ralink RT5572.Mac OS X  brew install wireshark
  brew cask install wireshark-chmodbpf
You need to dissociate from any AP before initiating the scanning:sudo /System/Library/PrivateFrameworks/Apple80211.framework/Versions/Current/Resources/airport -z
Linux sudo apt-get install tshark
Then update it so it can be run as non-root:sudo dpkg-reconfigure wireshark-common     (select YES)
sudo usermod -a -G wireshark ${USER:-root}
newgrp wireshark
Installpip install howmanypeoplearearound
RunQuickstartTo run, simply type in$ howmanypeoplearearound
Using wlan1 adapter and scanning for 60 seconds...
[==================================================] 100%        0s left
There are about 3 people around.
You will be prompted for the WiFi adapter to use for scanning. Make sure to usean adapter that supports ""monitor"" mode.Docker alternativeIf Docker is installed locally and you want to take howmanypeoplearearound out for a quick spin, you can try the following:NOTE: This Docker alternative is known to work on Ubuntu but not on Mac OS X.  Feedback on other platforms would be appreciated.OptionsYou can modify the scan time, designate the adapter, or modify the output using some command-line options.$ howmanypeoplearearound --help

Options:
  -a, --adapter TEXT   adapter to use
  -z, --analyze TEXT   analyze file
  -s, --scantime TEXT  time in seconds to scan
  -o, --out TEXT       output cellphone data to file
  -v, --verbose        verbose mode
  --number             just print the number
  -j, --jsonprint      print JSON of cellphone data
  -n, --nearby         only quantify signals that are nearby (rssi > -70)
  --nocorrection       do not apply correction
  --loop               loop forever
  --sort               sort cellphone data by distance (rssi)
Print JSONYou can generate an JSON-formatted output to see what kind of phones are around:$ howmanypeoplearearound -o test.json -a wlan1
[==================================================] 100%         0s left
There are about 4 people around.
$ cat test.json | python3 -m json.tool
[
  {
    ""rssi"": -86.0,
    ""mac"": ""90:e7:c4:xx:xx:xx"",
    ""company"": ""HTC Corporation""
  },
  {
    ""rssi"": -84.0,
    ""mac"": ""80:e6:50:xx:xx:xx"",
    ""company"": ""Apple, Inc.""
  },
  {
    ""rssi"": -49.0,
    ""mac"": ""ac:37:43:xx:xx:xx"",
    ""company"": ""HTC Corporation""
  }
]
A higher rssi means closer (one of these phones is mine, and the other two are my roommates' who were upstairs). Run foreverYou can add  to make this run forever and append new lines an output file, :$ howmanypeoplearearound -o test.json -a wlan1 --loop
VisualizeYou can visualize the output from a looped command via a browser using:$ howmanypeoplearearound --analyze test.json 
Wrote index.html
Open browser to http://localhost:8001
Type Ctl+C to exit
Then just open up  in a browser and you should see plots. The first plot shows the number of people over time. Here you can see that people start arriving at work place around 8-9am (when work starts!).The second plot shows the RSSI values for the mac addresses seen. You can double-click on one of them in particular to highlight that trajectory, as I have done here for my phone (you can see when I leave from and when I arrive to work!):How does it work?howmanypeoplearearound counts up the number of probe requests coming from cellphones in a given amount of time.The probe requests can be ""sniffed"" from a monitor-mode enabled WiFi adapter using . An accurate count doesdepend on everyone having cellphone and also scanning long enough (1 - 10 minutes) to capture the packet whena phone pings the WiFi network (which happens every 1 to 10 minutes unless the phone is off or WiFi is disabled).This is a simplification of another program I wrote,  which uses a similar idea with a cluster of Raspberry Pis to geolocate positions of cellphones within the vicinity.LicenseMIT"
https://github.com/sebastianruder/NLP-progress,"Repository to track the progress in Natural Language Processing (NLP), including the datasets and the current state-of-the-art for the most common NLP tasks.","Tracking Progress in Natural Language ProcessingTable of contentsEnglishVietnameseHindiChineseFor more tasks, datasets and results in Chinese, check out the  website.FrenchRussianSpanishPortugueseKoreanNepaliBengaliPersianTurkishGermanArabicThis document aims to track the progress in Natural Language Processing (NLP) and give an overviewof the state-of-the-art (SOTA) across the most common NLP tasks and their corresponding datasets.It aims to cover both traditional and core NLP tasks such as dependency parsing and part-of-speech taggingas well as more recent ones such as reading comprehension and natural language inference. The main objectiveis to provide the reader with a quick overview of benchmark datasets and the state-of-the-art for theirtask of interest, which serves as a stepping stone for further research. To this end, if there is aplace where results for a task are already published and regularly maintained, such as a public leaderboard,the reader will be pointed there.If you want to find this document again in the future, just go to or  in your browser.ContributingGuidelinesResults &nbsp; Results reported in published papers are preferred; an exception may be made for influential preprints.Datasets &nbsp; Datasets should have been used for evaluation in at least one published paper besidesthe one that introduced the dataset.Code &nbsp; We recommend to add a link to an implementationif available. You can add a  column (see below) to the table if it does not exist.In the  column, indicate an official implementation with .If an unofficial implementation is available, use  (see below).If no implementation is available, you can leave the cell empty.Adding a new resultIf you would like to add a new result, you can just click on the small edit button in the top-rightcorner of the file for the respective task (see below).This allows you to edit the file in Markdown. Simply add a row to the corresponding table in thesame format. Make sure that the table stays sorted (with the best result on top).After you've made your change, make sure that the table still looks ok by clicking on the""Preview changes"" tab at the top of the page. If everything looks good, go to the bottom of the page,where you see the below form. Add a name for your proposed change, an optional description, indicate that you would like to""Create a new branch for this commit and start a pull request"", and click on ""Propose file change"".Adding a new dataset or taskFor adding a new dataset or task, you can also follow the steps above. Alternatively, you can fork the repository.In both cases, follow the steps below:| Model           | Score  |  Paper / Source | Code || ------------- | :-----:| --- | --- ||  |  |  | |Wish listThese are tasks and datasets that are still missing:Exporting into a structured formatYou can extract all the data into a structured, machine-readable JSON format with parsed tasks, descriptions and SOTA tables. The instructions are in .Instructions for building the site locallyInstructions for building the website locally using Jekyll can be found ."
https://github.com/rtqichen/torchdiffeq,Differentiable ODE solvers with full GPU support and O(1)-memory backpropagation.,"PyTorch Implementation of Differentiable ODE SolversThis library provides ordinary differential equation (ODE) solvers implemented in PyTorch. Backpropagation through ODE solutions is supported using the adjoint method for constant memory cost. For usage of ODE solvers in deep learning applications, see reference [1].As the solvers are implemented in PyTorch, algorithms in this repository are fully supported to run on the GPU.InstallationTo install latest stable version:pip install torchdiffeq
To install latest on GitHub:pip install git+https://github.com/rtqichen/torchdiffeq
ExamplesExamples are placed in the  directory.We encourage those who are interested in using this library to take a look at  for understanding how to use  to fit a simple spiral ODE.Basic usageThis library provides one main interface  which contains general-purpose algorithms for solving initial value problems (IVP), with gradients implemented for all main arguments. An initial value problem consists of an ODE and an initial value,dy/dt = f(t, y)    y(t_0) = y_0.
The goal of an ODE solver is to find a continuous trajectory satisfying the ODE that passes through the initial condition.To solve an IVP using the default solver:from torchdiffeq import odeint

odeint(func, y0, t)
where  is any callable implementing the ordinary differential equation ,  is an any-D Tensor representing the initial values, and  is a 1-D Tensor containing the evaluation points. The initial time is taken to be .Backpropagation through  goes through the internals of the solver. Note that this is not numerically stable for all solvers (but should probably be fine with the default  method). Instead, we encourage the use of the adjoint method explained in [1], which will allow solving with as many steps as necessary due to O(1) memory usage.To use the adjoint method:from torchdiffeq import odeint_adjoint as odeint

odeint(func, y0, t)
 simply wraps around , but will use only O(1) memory in exchange for solving an adjoint ODE in the backward call.The biggest gotcha is that  must be a  when using the adjoint method. This is used to collect parameters of the differential equation.Differentiable event handlingWe allow terminating an ODE solution based on an event function. Backpropagation through most solvers is supported. For usage of event handling in deep learning applications, see reference [2].This can be invoked with :from torchdiffeq import odeint_event
odeint_event(func, y0, t0, *, event_fn, reverse_time=False, odeint_interface=odeint, **kwargs)
The solve is terminated at an event time  and state  when an element of  is equal to zero. Multiple outputs from  can be used to specify multiple event functions, of which the first to trigger will terminate the solve.Both the event time and final state are returned from , and can be differentiated. Gradients will be backpropagated through the event function. NOTE: parameters for the event function must be in the state itself to obtain gradients. The numerical precision for the event time is determined by the  argument.See example of simulating and differentiating through a bouncing ball in . See example code for learning a simple event function in .Keyword arguments for odeint(_adjoint)Keyword arguments:List of ODE Solvers:Adaptive-step:Fixed-step:Additionally, all solvers available through SciPy are wrapped for use with .For most problems, good choices are the default , or to use  with  set appropriately small. Adjusting the tolerances (adaptive solvers) or step size (fixed solvers), will allow for trade-offs between speed and accuracy.Frequently Asked QuestionsTake a look at our  for frequently asked questions.Further documentationFor details of the adjoint-specific and solver-specific options, check out the .ReferencesApplications of differentiable ODE solvers and event handling are discussed in these two papers:Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, David Duvenaud. ""Neural Ordinary Differential Equations."" Advances in Neural Information Processing Systems. 2018. @article{chen2018neuralode,
  title={Neural Ordinary Differential Equations},
  author={Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
  journal={Advances in Neural Information Processing Systems},
  year={2018}
}
Ricky T. Q. Chen, Brandon Amos, Maximilian Nickel. ""Learning Neural Event Functions for Ordinary Differential Equations."" International Conference on Learning Representations. 2021. @article{chen2021eventfn,
  title={Learning Neural Event Functions for Ordinary Differential Equations},
  author={Chen, Ricky T. Q. and Amos, Brandon and Nickel, Maximilian},
  journal={International Conference on Learning Representations},
  year={2021}
}
The seminorm option for computing adjoints is discussed inPatrick Kidger, Ricky T. Q. Chen, Terry Lyons. ""'Hey, that’s not an ODE': Faster ODE Adjoints via Seminorms."" International Conference on Machine 2021. @article{kidger2021hey,
  title={""Hey, that's not an ODE"": Faster ODE Adjoints via Seminorms.},
  author={Kidger, Patrick and Chen, Ricky T. Q. and Lyons, Terry J.},
  journal={International Conference on Machine Learning},
  year={2021}
}
If you found this library useful in your research, please consider citing.@misc{torchdiffeq,
	author={Chen, Ricky T. Q.},
	title={torchdiffeq},
	year={2018},
	url={https://github.com/rtqichen/torchdiffeq},
}
"
https://github.com/google-deepmind/pysc2,StarCraft II Learning Environment,"PySC2 - StarCraft II Learning Environment is 'sPython component of the StarCraft II Learning Environment (SC2LE). It exposes's  as a Python RL Environment.This is a collaboration between DeepMind and Blizzard to develop StarCraft IIinto a rich environment for RL research. PySC2 provides an interface for RLagents to interact with StarCraft 2, getting observations and sending actions.We have published an accompanyingand , which outlines ourmotivation for using StarCraft II for DeepRL research, and some initial researchresults using the environment.AboutDisclaimer: This is not an official Google product.If you use the StarCraft II Machine Learning API and/or PySC2 in your research,please cite the You can reach us at .Quick Start GuideGet PySC2PyPIThe easiest way to get PySC2 is to use pip:$ pip install pysc2
That will install the  package along with all the required dependencies. can help manage yourdependencies. You may also need to upgrade pip: for the  install to work. If you're running on an older system you mayneed to install  libraries for the  dependency.Pip will install a few of the  binaries to your bin directory.  canbe used as a shortcut to .From SourceAlternatively you can install latest PySC2 codebase from git master branch:$ pip install --upgrade https://github.com/deepmind/pysc2/archive/master.zip
or from a local clone of the git repo:$ git clone https://github.com/deepmind/pysc2.git
$ pip install --upgrade pysc2/
Get StarCraft IIPySC2 depends on the full StarCraft II game and only works with versions thatinclude the API, which is 3.16.1 and above.LinuxFollow Blizzard's  toget the linux version. By default, PySC2 expects the game to live in. You can override this path by setting the environment variable or creating your own run_config.Windows/MacOSInstall of the game as normal from . Even the will work.If you used the default install location PySC2 should find the latest binary.If you changed the install location, you might need to set the environment variable with the correct location.PySC2 should work on MacOS and Windows systems running Python 3.8+,but has only been thoroughly tested on Linux. We welcome suggestions and patchesfor better compatibility with other systems.Get the mapsPySC2 has many maps pre-configured, but they need to be downloaded into the SC2 directory before they can be played.Download the and the and extract them to your  directory.Run an agentYou can run an agent to test the environment. The UI shows you the actions ofthe agent and is helpful for debugging and visualization purposes.$ python -m pysc2.bin.agent --map Simple64
It runs a random agent by default, but you can specify others if you'd like,including your own.$ python -m pysc2.bin.agent --map CollectMineralShards --agent pysc2.agents.scripted_agent.CollectMineralShards
You can also run two agents against each other.$ python -m pysc2.bin.agent --map Simple64 --agent2 pysc2.agents.random_agent.RandomAgent
To specify the agent's race, the opponent's difficulty, and more, you can passadditional flags. Run with  to see what you can change.Play the game as a humanThere is a human agent interface which is mainly used for debugging, but it canalso be used to play the game. The UI is fairly simple and incomplete, but it'senough to understand the basics of the game. Also, it runs on Linux.$ python -m pysc2.bin.play --map Simple64
In the UI, hit  for a list of the hotkeys. The most basic ones are:  toquit,  to restart,  to save a replay, and / to control thespeed of the game. Otherwise use the mouse for selection and keyboard forcommands listed on the left.The left side is a basic rendering. The right side is the feature layers thatthe agent receives, with some coloring to make it more useful to us. You canenable or disable RGB or feature layer rendering and their resolutions withcommand-line flags.Watch a replayRunning an agent and playing as a human save a replay by default. You can watchthat replay by running:$ python -m pysc2.bin.play --replay <path-to-replay>
This works for any replay as long as the map can be found by the game.The same controls work as for playing the game, so  to exit, /to control the speed, etc.You can save a video of the replay with the  flag.List the maps need to be configured before they're known to theenvironment. You can see the list of known maps by running:$ python -m pysc2.bin.map_list
Run the testsIf you want to submit a pull request, please make sure the tests pass on bothpython 2 and 3.$ python -m pysc2.bin.run_tests
Environment DetailsFor a full description of the specifics of how the environment is configured,the observations and action spaces work read the.Note that an alternative to this environment is now available which providesan enriched action and observation format using the C++ wrappers developedfor AlphaStar. See  for moreinformation.Mini-game mapsThe mini-game map files referenced in the paper are stored under but must be installed in . Make sure to follow the downloadinstructions above.Maps are configured in the Python files in . The configs can setplayer and time limits, whether to use the game outcome or curriculum score, anda handful of other things. For more information about the maps, and how toconfigure your own, read the .ReplaysA replay lets you review what happened during a game. You can see the actionsand observations that each player made as they played.Blizzard is releasing a large number of anonymized 1v1 replays played on theladder. You can find instructions for how to get the on theirsite. You can also review your own replays.Replays can be played back to get the observations and actions made during thatgame. The observations are rendered at the resolution you request, so may differfrom what the human actually saw. Similarly the actions specify a point, whichcould reflect a different pixel on the human's screen, so may not have an exactmatch in our observations, though they should be fairly similar.Replays are version dependent, so a 3.16 replay will fail in a 3.16.1 or 3.17binary.You can visualize the replays with the full game, or with .Alternatively you can run  to process many replaysin parallel."
https://github.com/offu/WeRoBot,WeRoBot 是一个微信公众号开发框架,"====================================WeRoBot.. image:: https://github.com/offu/werobot/workflows/tests/badge.svg:target: https://github.com/offu/werobot/actions.. image:: https://codecov.io/gh/offu/WeRoBot/branch/master/graph/badge.svg:target: https://codecov.io/gh/offu/WeRoBot.. image:: https://img.shields.io/badge/QQ%20Group-283206829-brightgreen.svg?logo=data%3Aimage%2Fsvg%2Bxml%3Bbase64%2CPHN2ZyB3aWR0aD0iMTc5MiIgaGVpZ2h0PSIxNzkyIiB2aWV3Qm94PSIwIDAgMTc5MiAxNzkyIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjxwYXRoIGQ9Ik0yNzAgODA2cS04LTE5LTgtNTIgMC0yMCAxMS00OXQyNC00NXEtMS0yMiA3LjUtNTN0MjIuNS00M3EwLTEzOSA5Mi41LTI4OC41dDIxNy41LTIwOS41cTEzOS02NiAzMjQtNjYgMTMzIDAgMjY2IDU1IDQ5IDIxIDkwIDQ4dDcxIDU2IDU1IDY4IDQyIDc0IDMyLjUgODQuNSAyNS41IDg5LjUgMjIgOThsMSA1cTU1IDgzIDU1IDE1MCAwIDE0LTkgNDB0LTkgMzhxMCAxIDEuNSAzLjV0My41IDUgMiAzLjVxNzcgMTE0IDEyMC41IDIxNC41dDQzLjUgMjA4LjVxMCA0My0xOS41IDEwMHQtNTUuNSA1N3EtOSAwLTE5LjUtNy41dC0xOS0xNy41LTE5LTI2LTE2LTI2LjUtMTMuNS0yNi05LTE3LjVxLTEtMS0zLTFsLTUgNHEtNTkgMTU0LTEzMiAyMjMgMjAgMjAgNjEuNSAzOC41dDY5IDQxLjUgMzUuNSA2NXEtMiA0LTQgMTZ0LTcgMThxLTY0IDk3LTMwMiA5Ny01MyAwLTExMC41LTl0LTk4LTIwLTEwNC41LTMwcS0xNS01LTIzLTctMTQtNC00Ni00LjV0LTQwLTEuNXEtNDEgNDUtMTI3LjUgNjV0LTE2OC41IDIwcS0zNSAwLTY5LTEuNXQtOTMtOS0xMDEtMjAuNS03NC41LTQwLTMyLjUtNjRxMC00MCAxMC01OS41dDQxLTQ4LjVxMTEtMiA0MC41LTEzdDQ5LjUtMTJxNCAwIDE0LTIgMi0yIDItNGwtMi0zcS00OC0xMS0xMDgtMTA1LjV0LTczLTE1Ni41bC01LTNxLTQgMC0xMiAyMC0xOCA0MS01NC41IDc0LjV0LTc3LjUgMzcuNWgtMXEtNCAwLTYtNC41dC01LTUuNXEtMjMtNTQtMjMtMTAwIDAtMjc1IDI1Mi00NjZ6IiBmaWxsPSIjZmZmIi8%2BPC9zdmc%2B:target: https://jq.qq.com/?_wv=1027&k=449sXsVWeRoBot 是一个微信公众号开发框架，采用MIT协议发布。文档在这里： https://werobot.readthedocs.org/zh_CN/latest/安装推荐使用 pip 进行安装 ::pip install werobot
Hello World一个非常简单的 Hello World 微信公众号，会对收到的所有文本消息回复 Hello World ::import werobot

robot = werobot.WeRoBot(token='tokenhere')

@robot.text
def hello_world():
    return 'Hello World!'

robot.run()
CreditsContributorsThank you to all the people who have already contributed.|occontributorimage|.. |occontributorimage| image:: https://opencollective.com/werobot/contributors.svg?width=890&button=false:target: https://opencollective.com/werobot:alt: Repo Contributors"
https://github.com/google/python-fire,Python Fire is a library for automatically generating command line interfaces (CLIs) from absolutely any Python object.,"Python Fire Python Fire is a library for automatically generating command line interfacesInstallationTo install Python Fire with pip, run: To install Python Fire with conda, run: To install Python Fire from source, first clone the repository and then run:Basic UsageYou can call  on any Python object:functions, classes, modules, objects, dictionaries, lists, tuples, etc.They all work!Here's an example of calling Fire on a function.import fire

def hello(name=""World""):
  return ""Hello %s!"" % name

if __name__ == '__main__':
  fire.Fire(hello)
Then, from the command line, you can run:python hello.py  # Hello World!
python hello.py --name=David  # Hello David!
python hello.py --help  # Shows usage information.
Here's an example of calling Fire on a class.import fire

class Calculator(object):
  """"""A simple calculator class.""""""

  def double(self, number):
    return 2 * number

if __name__ == '__main__':
  fire.Fire(Calculator)
Then, from the command line, you can run:python calculator.py double 10  # 20
python calculator.py double --number=15  # 30
To learn how Fire behaves on functions, objects, dicts, lists, etc, and to learnabout Fire's other features, see the .For additional examples, see .Why is it called Fire?When you call , it fires off (executes) your command.Where can I learn more?Please see .Reference| Setup   | Command             | Notes| :------ | :------------------ | :---------| install |   || Creating a CLI | Command                | Notes| :--------------| :--------------------- | :---------| import         |           || Call           |           | Turns the current module into a Fire CLI.| Call           |  | Turns  into a Fire CLI.| Using a CLI                                     | Command                                 | Notes| :---------------------------------------------- | :-------------------------------------- | :----|              |  or  ||       |               | Enters interactive mode.|    |               | Sets the separator to . The default separator is .|  |        | Generates a completion script for the CLI.|            |                     | Gets a Fire trace for the command.|        |                   |Note that these flags are separated from the Fire command by an isolated LicenseLicensed under the License.DisclaimerThis is not an official Google product."
https://github.com/conan-io/conan,Conan - The open-source C and C++ package manager,"ConanDecentralized, open-source (MIT), C/C++ package manager.Conan is a package manager for C and C++ developers:This is the developer/maintainer documentation. For user documentation, go to https://docs.conan.io| develop2            ||-------------------------||   |SetupYou can run Conan from source in Windows, MacOS, and Linux:Contributing to the projectFeedback and contribution are always welcome in this project.Please read our .Also, if you plan to contribute, please add some testing for your changes. You can read the  forsome advice on how to write tests for Conan.Running the testsInstall Python requirements$ python -m pip install -r conans/requirements_server.txt
$ python -m pip install -r conans/requirements_dev.txt
If you are not on Windows and you are not using a Python virtual environment, you will need to run thesecommands using .Before you can run the tests, you need to set a few environment variables first.$ export PYTHONPATH=$PYTHONPATH:$(pwd)
On Windows it would be (while being in the Conan root directory):$ set PYTHONPATH=.
Conan test suite defines and configures some required tools (CMake, Ninja, etc) in the and allows to define a custom .Some specific versions, like cmake>=3.15 are necessary.You can run the tests like this:$ python -m pytest .
A few minutes later it should print :............................................................................................
----------------------------------------------------------------------
Ran 146 tests in 50.993s

OK
To run specific tests, you can specify the test name too, something like:$ python -m pytest conans/test/unittests/client/cmd/export_test.py::ExportTest::test_export_warning -s
The  argument can be useful to see some output that otherwise is captured by pytest.Also, you can run tests against an instance of Artifactory. Those tests should add the attribute.$ python -m pytest . -m artifactory_ready
Some environment variables have to be defined to run them. For example, for anArtifactory instance that is running on the localhost with default user and password configured, thevariables could take the values:$ export CONAN_TEST_WITH_ARTIFACTORY=1
$ export ARTIFACTORY_DEFAULT_URL=http://localhost:8081/artifactory
$ export ARTIFACTORY_DEFAULT_USER=admin
$ export ARTIFACTORY_DEFAULT_PASSWORD=password
 is the base URL for the Artifactory repo, not one for a specificrepository. Running the tests with a real Artifactory instance will create repos on the fly so pleaseuse a separate server for testing purposes.License"
https://github.com/rushter/MLAlgorithms,Minimal and clean examples of machine learning algorithms implementations,"Machine learning algorithmsA collection of minimal and clean implementations of machine learning algorithms.Why?This project is targeting people who want to learn internals of ml algorithms or implement them from scratch.The code is much easier to follow than the optimized libraries and easier to play with.All algorithms are implemented in Python, using numpy, scipy and autograd.  Implemented:Installation        git clone https://github.com/rushter/MLAlgorithms
        cd MLAlgorithms
        pip install scipy numpy
        python setup.py develop
How to run examples without installation        cd MLAlgorithms
        python -m examples.linear_models
How to run examples within Docker        cd MLAlgorithms
        docker build -t mlalgorithms .
        docker run --rm -it mlalgorithms bash
        python -m examples.linear_models
ContributingYour contributions are always welcome!Feel free to improve existing code, documentation or implement new algorithm.Please open an issue to propose your changes if they are big enough.  "
https://github.com/yahoo/open_nsfw,Not Suitable for Work (NSFW) classification using deep neural network Caffe models.,"Open nsfw modelThis repo contains code for running Not Suitable for Work (NSFW) classification deep neural network Caffe models. Please refer our  post which describes this work and experiments in more detail.Not suitable for work classifierDetecting offensive / adult images is an important problem which researchers have tackled for decades. With the evolution of computer vision and deep learning the algorithms have matured and we are now able to classify an image as not suitable for work with greater precision.Defining NSFW material is subjective and the task of identifying these images is non-trivial. Moreover, what may be objectionable in one context can be suitable in another. For this reason, the model we describe below focuses only on one type of NSFW content: pornographic images. The identification of NSFW sketches, cartoons, text, images of graphic violence, or other types of unsuitable content is not addressed with this model.Since images and user generated content dominate the internet today, filtering nudity and other not suitable for work images becomes an important problem. In this repository we opensource a Caffe deep neural network for preliminary filtering of NSFW images. UsageDescription of modelWe trained the model on the dataset with NSFW images as positive and SFW(suitable for work) images as negative. These images were editorially labelled. We cannot release the dataset or other details due to the nature of the data. We use  which is a wonderful framework for distributed learning that brings deep learning to Hadoop and Spark clusters for training models for our experiments. Big thanks to the CaffeOnSpark team!The deep model was first pretrained on ImageNet 1000 class dataset. Then we finetuned the weights on the NSFW dataset.We used the thin resnet 50 1by2 architecture as the pretrained network. The model was generated using  tool and replicates the  paper's 50 layer network (with half number of filters in each layer).  You can find more details on how the model was generated and trained Please note that deeper networks, or networks with more filters can improve accuracy. We train the model using a thin residual network architecture, since it provides good tradeoff in terms of accuracy, and the model is light-weight in terms of runtime (or flops) and memory (or number of parameters).Docker QuickstartThis Docker quickstart guide can be used for evaluating the model quickly with minimal dependency installation.Install Docker EngineBuild a caffe docker image (CPU) docker build -t caffe:cpu https://raw.githubusercontent.com/BVLC/caffe/master/docker/cpu/Dockerfile
Check the caffe installationdocker run caffe:cpu caffe --version
caffe version 1.0.0-rc3
Run the docker image with a volume mapped to your  repository. Your  should be located in this same directory.cd open_nsfw
docker run --volume=$(pwd):/workspace caffe:cpu \
python ./classify_nsfw.py \
--model_def nsfw_model/deploy.prototxt \
--pretrained_model nsfw_model/resnet_50_1by2_nsfw.caffemodel \
test_image.jpg
We will get the NSFW score returned:NSFW score:   0.14057905972
Running the modelTo run this model, please install  and its python extension and make sure pycaffe is available in your PYTHONPATH.We can use the  script to run the NSFW model. For convenience, we have provided the script in this repo as well, and it prints the NSFW score. python ./classify_nsfw.py \
--model_def nsfw_model/deploy.prototxt \
--pretrained_model nsfw_model/resnet_50_1by2_nsfw.caffemodel \
INPUT_IMAGE_PATH 
[<marko.inline.RawText object at 0x000001592FDA4248>]The definition of NSFW is subjective and contextual. This model is a general purpose reference model, which can be used for the preliminary filtering of pornographic images. We do not provide guarantees of accuracy of output, rather we make this available for developers to explore and enhance as an open source project. Results can be improved by  the model for your dataset.LicenseCode licensed under the [BSD 2 clause license] (https://github.com/BVLC/caffe/blob/master/LICENSE). See LICENSE file for terms.ContactThe model was trained by ,  in collaboration with  , ,  and others. Special thanks to Gerry Pesavento for taking the initiative for open-sourcing this model. If you have any queries, please raise an issue and we will get back ASAP."
https://github.com/Jack-Cherish/Machine-Learning,:zap:机器学习实战（Python3）：kNN、决策树、贝叶斯、逻辑回归、SVM、线性回归、树回归,Machine-Learning原创文章每周最少两篇，后续最新文章会在首发，视频首发，大家可以加我进交流群，技术交流或提意见都可以，欢迎Star！文章首发声明第二章：kNN（k-邻域算法）|   文章   |  个人网站  |    CSDN    |    知乎    || :------  | :--------: | :--------: | :--------: || Python3《机器学习实战》学习笔记(一)：k-近邻算法(史诗级干货长文) |  |  |  |代码第三章：Decision Tree（决策树）|   文章   |  个人网站  |    CSDN    |    知乎    || :------  | :--------: | :--------: | :--------: || Python3《机器学习实战》学习笔记(二)：决策树基础篇之让我们从相亲说起 |  |  |  || Python3《机器学习实战》学习笔记(三)：决策树实战篇之为自己配个隐形眼镜 |  |  |  |代码第四章：Navie Bayes（朴素贝叶斯）|   文章   |  个人网站  |    CSDN    |    知乎    || :------  | :--------: | :--------: | :--------: || Python3《机器学习实战》学习笔记（四）：朴素贝叶斯基础篇之言论过滤器 |  |  |  || Python3《机器学习实战》学习笔记（五）：朴素贝叶斯实战篇之新浪新闻分类 |  |  |  |代码第五章：Logistic（Logistic回归）|   文章   |  个人网站  |    CSDN    |    知乎    || :------  | :--------: | :--------: | :--------: || Python3《机器学习实战》学习笔记（六）：Logistic回归基础篇之梯度上升算法 |  |  |  || Python3《机器学习实战》学习笔记（七）：Logistic回归实战篇之预测病马死亡率 |  |  |  |代码第六章：SVM（支持向量机）|   文章   |  个人网站  |    CSDN    |    知乎    || :------  | :--------: | :--------: | :--------: || Python3《机器学习实战》学习笔记（八）：支持向量机原理篇之手撕线性SVM |  |  |  || Python3《机器学习实战》学习笔记（九）：支持向量机实战篇之再撕非线性SVM |  |  |  |代码第七章：AdaBoost|   文章   |  个人网站  |    CSDN    |    知乎    || :------  | :--------: | :--------: | :--------: || Python3《机器学习实战》学习笔记（十）：提升分类器性能利器-AdaBoost |  |  |  |代码第八章：Regression|   文章   |  个人网站  |    CSDN    |    知乎    || :------  | :--------: | :--------: | :--------: || Python3《机器学习实战》学习笔记（十一）：线性回归基础篇之预测鲍鱼年龄 |  | | || Python3《机器学习实战》学习笔记（十二）：线性回归提高篇之乐高玩具套件二手价预测 |  | no | no |代码第九章：Regression Tree|   文章   |  个人网站  |    CSDN    |    知乎    || :------  | :--------: | :--------: | :--------: || Python3《机器学习实战》学习笔记（十三）：树回归基础篇之CART算法与树剪枝 |  | no | no |代码聚类|   文章   |  个人网站  |    公众号    || :------  | :--------: | :--------: || 嘿，来聚个类！|  |  |更多精彩，敬请期待！  
https://github.com/Kyubyong/transformer,A TensorFlow Implementation of the Transformer: Attention Is All You Need,"[UPDATED] A TensorFlow Implementation of When I opened this repository in 2017, there was no official code yet.I tried to implement the paper as I understood, but to no surpriseit had several bugs. I realized them mostly thanks to people who issued here, soI'm very grateful to all of them. Though there is the  as well asseveral other unofficial github repos, I decided to update my own one.This update focuses on:I still stick to IWSLT 2016 de-en. I guess if you'd like to test on a big data suchas WMT, you would rely on the official implementation.After all, it's pleasant to check quickly if your model works.The initial code for TF1.2 is moved to the  folder for the record.RequirementsTrainingbash download.sh
It should be extracted to  folder automatically.python prepro.py
If you want to change the vocabulary size (default:32000), do this.python prepro.py --vocab_size 8000
It should create two folders  and .python train.py
Check  to see which parameters are possible. For example,python train.py --logdir myLog --batch_size 256 --dropout_rate 0.5
wget https://dl.dropbox.com/s/4lom1czy5xfzr4q/log.zip; unzip log.zip; rm log.zip
Training Loss CurveLearning rateBleu score on devsetInference (=test)python test.py --ckpt log/1/iwslt2016_E19L2.64-29146 (OR yourCkptFile OR yourCkptFileDirectory)
Results|tst2013 (dev) | tst2014 (test) ||--|--||28.06|23.88|Notes"
https://github.com/donnemartin/dev-setup,"macOS development environment setup:  Easy-to-understand instructions with automated setup scripts for developer tools like Vim, Sublime Text, Bash, iTerm, Python data analysis, Spark, Hadoop MapReduce, AWS, Heroku, JavaScript web development, Android development, common data stores, and dev-based OS X defaults.","dev-setupMotivationSetting up a new developer machine can be an ad-hoc, manual, and time-consuming process.   aims to simplify the process with easy-to-understand instructions and dotfiles/scripts to automate the setup of the following:But...I Don't Need All These Tools!dev-setupYou're If you're interested in automation,  provides a customizable .  There's really no one-size-fits-all solution for developers so you're encouraged to make tweaks to suit your needs.: This repo builds on the awesome work from  and .For Automation, What About Vagrant, Docker, or Boxen? and  are great tools and are set up by this repo. I've found that Vagrant works well to ensure dev matches up with test and production tiers. I've only started playing around with Docker for side projects and it looks very promising. However, for Mac users, Docker and Vagrant both rely on virtual machines, which have their own considerations/pros/cons. is a cool solution, although some might find it better geared towards ""more mature companies or devops teams"". I've seen some discussions of .This repo takes a more light-weight approach to automation using a combination of Homebrew, Homebrew Cask, and shell scripts to do basic system setup.  It also provides easy-to understand instructions for installation, configuration, and usage for each developer app or tool.Sections SummarySection 1: InstallationScripts tested on OS X 10.10 Yosemite and 10.11 El Capitan.Section 2: General Apps and ToolsSection 3: Python Data AnalysisSection 4: Big Data, AWS, and HerokuSection 5: Data StoresSection 6: JavaScript Web DevelopmentSection 7: Android DevelopmentSection 8: MiscSection 1: InstallationSingle Setup ScriptRunning with GitClone the Repo$ git clone https://github.com/donnemartin/dev-setup.git && cd dev-setup
Run the .dots Script with Command Line ArgumentsSince you probably don't want to install every section, the  script supports command line arguments to run only specified sections.  Simply pass in the  that you want to install.  Below are some examples.For more customization, you can Run all:$ ./.dots all
Run , , , and :$ ./.dots bootstrap osxprep brew osx
Run , , , and , , , and :$ ./.dots bootstrap osxprep brew osx pydata aws datastores
Running without Git$ curl -O https://raw.githubusercontent.com/donnemartin/dev-setup/master/.dots && ./.dots [Add ARGS Here]
ScriptsNotes:I encourage you to read through Section 1 so you have a better idea of what each installation script does.  The following discussions describe in greater detail what is executed when running the  script.bootstrap.sh scriptThe  script will sync the dev-setup repo to your local home directory.  This will include customizations for Vim, bash, curl, git, tab completion, aliases, a number of utility functions, etc.  Section 2 of this repo describes some of the customizations.Running with GitFirst, fork or .  The  script will pull in the latest version and copy the files to your home folder :$ source bootstrap.sh
To update later on, just run that command again.Alternatively, to update while avoiding the confirmation prompt:$ set -- -f; source bootstrap.sh
Running without GitTo sync dev-setup to your local home directory without Git, run the following:$ cd ~; curl -#L https://github.com/donnemartin/dev-setup/tarball/master | tar -xzv --strip-components 1 --exclude={README.md,bootstrap.sh,LICENSE}
To update later on, just run that command again.Optional: Specify PATHIf  exists, it will be sourced along with the other files before any feature testing (such as detecting which version of  is being used takes place.Here’s an example  file that adds  to the :export PATH=""/usr/local/bin:$PATH""
Optional: Add Custom CommandsIf  exists, it will be sourced along with the other files. You can use this to add a few custom commands without the need to fork this entire repository, or to add commands you don’t want to commit to a public repository.My  looks something like this:# Git credentials
GIT_AUTHOR_NAME=""Donne Martin""
GIT_COMMITTER_NAME=""$GIT_AUTHOR_NAME""
git config --global user.name ""$GIT_AUTHOR_NAME""
GIT_AUTHOR_EMAIL=""donne.martin@gmail.com""
GIT_COMMITTER_EMAIL=""$GIT_AUTHOR_EMAIL""
git config --global user.email ""$GIT_AUTHOR_EMAIL""

# Pip should only run if there is a virtualenv currently activated
export PIP_REQUIRE_VIRTUALENV=true

# Install or upgrade a global package
# Usage: gpip install –upgrade pip setuptools virtualenv
gpip(){
   PIP_REQUIRE_VIRTUALENV="""" pip ""$@""
}
You could also use  to override settings, functions, and aliases from the dev-setup repository, although it’s probably better to .osxprep.sh scriptRun the  script:$ ./osxprep.sh
 will first install all updates.  If a restart is required, simply run the script again.  Once all updates are installed,  will then .If you want to go the manual route, you can also install all updates by running ""App Store"", selecting the ""Updates"" icon, then updating both the OS and installed apps.Install Xcode Command Line ToolsAn important dependency before many tools such as Homebrew can work is the Command Line Tools for Xcode. These include compilers like gcc that will allow you to build from source.If you are running OS X 10.9 Mavericks or later, then you can install the Xcode Command Line Tools directly from the command line with:$ xcode-select --install
Note: the  script executes this command.Running the command above will display a dialog where you can either:OS X 10.8 and OlderIf you're running 10.8 or older, you'll need to go to , and sign in with your Apple ID (the same one you use for iTunes and app purchases). Unfortunately, you're greeted by a rather annoying questionnaire. All questions are required, so feel free to answer at random.Once you reach the downloads page, search for ""command line tools"", and download the latest Command Line Tools (OS X Mountain Lion) for Xcode. Open the .dmg file once it's done downloading, and double-click on the .mpkg installer to launch the installation. When it's done, you can unmount the disk in Finder.brew.sh scriptWhen setting up a new Mac, you may want to install , a package manager that simplifies installing and updating applications or libraries.Some of the apps installed by the  script include: Chrome, Firefox, Sublime Text, Atom, Dropbox, Evernote, Skype, Slack, Alfred, VirtualBox, Vagrant, Docker, etc.  For a full listing of installed formulae and apps, refer to the commented Run the  script:$ ./brew.sh
The  script takes awhile to complete, as some formulae need to be installed from source.For your terminal customization to take full effect, quit and re-start the terminalosx.sh scriptWhen setting up a new Mac, you may want to set OS X defaults geared towards developers.  The  script also configures common third-party apps such Sublime Text and Chrome.Note: I strongly encourage you read through the commented   For example, if you are not running an SSD you might want to change some of the settings listed in the SSD section.Run the  script:$ ./osx.sh
For your terminal customization to take full effect, quit and re-start the terminal.pydata.sh scriptTo set up a development environment to work with Python and data analysis without relying on the more heavyweight  distribution, run the  script:$ ./pydata.sh
This will install  and .  It will then set up two virtual environments loaded with the packages you will need to work with data in Python 2 and Python 3.To switch to the Python 2 virtual environment, run the following Virtualenvwrapper command:$ workon py2-data
To switch to the Python 3 virtual environment, run the following Virtualenvwrapper command:$ workon py3-data
Then start working with the installed packages, for example:$ ipython notebook
 describes the installed packages and usage.aws.sh scriptTo set up a development environment to work with Spark, Hadoop MapReduce, and Amazon Web Services, run the  script:$ ./aws.sh
 describes the installed packages and usage.datastores.sh scriptTo set up common data stores, run the  script:$ ./datastores.sh
 describes the installed packages and usage.web.sh scriptTo set up a JavaScript web development environment, Run the  script:$ ./web.sh
 describes the installed packages and usage.android.sh scriptTo set up an Android development environment, run the  script:$ ./android.sh
 describes the installed packages and usage.Section 2: General Apps and ToolsSublime TextWith the terminal, the text editor is a developer's most important tool. Everyone has their preferences, but unless you're a hardcore  user, a lot of people are going to tell you that  is currently the best one out there.InstallationThe  installs Sublime Text.If you prefer to install it separately, go ahead and  it. Open the .dmg file, drag-and-drop in the Applications folder.Note: At this point I'm going to create a shortcut on the OS X Dock for both for Sublime Text. To do so, right-click on the running application and select Options > Keep in Dock.Sublime Text is not free, but I think it has an unlimited ""evaluation period"". Anyhow, we're going to be using it so much that even the seemingly expensive $70 price tag is worth every penny. If you can afford it, I suggest you  this awesome tool.ConfigurationThe  contains Sublime Text configurations.Soda ThemeThe  is a great UI theme for Sublime Text, especially if you use a dark theme and think the side bar sticks out like a sore thumb.Installation with Sublime Package ControlIf you are using Will Bond's excellent , you can easily install Soda Theme via the  menu item. The Soda Theme package is listed as  in the packages list.Installation with GitAlternatively, if you are a git user, you can install the theme and keep up to date by cloning the repo directly into your  directory in the Sublime Text application settings area.You can locate your Sublime Text  directory by using the menu item .While inside the  directory, clone the theme repository using the command below:$ git clone https://github.com/buymeasoda/soda-theme/ ""Theme - Soda""
Activating the Theme on Sublime Text 2Example Sublime Text 2 User Settings{
    ""theme"": ""Soda Light.sublime-theme""
}
Activating the Theme on Sublime Text 3Example Sublime Text 3 User Settings{
    ""theme"": ""Soda Light 3.sublime-theme""
}
Changing Monokai Comment ColorAlthough Monokai is a great color scheme, I find that comments can be difficult to see.  You can follow these  to change the color of the default theme.I set my comments color to .<dict>
    ...
    <dict>
        <key>foreground</key>
        <string>#E6DB74</string>
    </dict>
    ...
</dict>
Atom is a great open-source editor from GitHub that is rapidly gaining contributors and popularity.InstallationThe  installs Atom.If you prefer to install it separately,  it, open the .dmg file, drag-and-drop in the Applications folder.ConfigurationAtom has a great package manager that allows you to easily install both core and community packages.Terminal CustomizationSince we spend so much time in the terminal, we should try to make it a more pleasant and colorful place.ConfigurationThe  and  contain terminal customizations.iTerm2I prefer iTerm2 over the stock Terminal, as it has some additional . Download and install iTerm2 (the newest version, even if it says ""beta release"").In Finder, drag and drop the iTerm Application file into the Applications folder.You can now launch iTerm, through the Launchpad for instance.Let's just quickly change some preferences. In iTerm > Preferences..., in the tab Profiles, create a new one with the ""+"" icon, and rename it to your first name for example. Then, select Other Actions... > Set as Default. Under the section Window, change the size to something better, like Columns: 125 and Rows: 35.  I also like to set General > Working Directory > Reuse previous session's directory.  Finally, I change the way the option key works so that I can quickly jump between words as described .When done, hit the red ""X"" in the upper left (saving is automatic in OS X preference panes). Close the window and open a new one to see the size change.ConfigurationSince we spend so much time in the terminal, we should try to make it a more pleasant and colorful place. What follows might seem like a lot of work, but trust me, it'll make the development experience so much better.Now let's add some color. I'm a big fan of the  color scheme. It is supposed to be scientifically optimal for the eyes. I just find it pretty.At this point you can also change your computer's name, which shows up in this terminal prompt. If you want to do so, go to System Preferences > Sharing. For example, I changed mine from ""Donne's MacBook Pro"" to just ""MacBook Pro"", so it shows up as  in the terminal.Now we have a terminal we can work with!VimAlthough Sublime Text will be our main editor, it is a good idea to learn some very basic usage of . It is a very popular text editor inside the terminal, and is usually pre-installed on any Unix system.For example, when you run a Git commit, it will open Vim to allow you to type the commit message.I suggest you read a tutorial on Vim. Grasping the concept of the two ""modes"" of the editor, Insert (by pressing ) and Normal (by pressing  to exit Insert mode), will be the part that feels most unnatural. After that it's just remembering a few important keys.ConfigurationThe  contains Vim customizations.VirtualBoxVirtualBox creates and manages virtual machines.  It's a solid free solution to its commercial rival VMware.InstallationThe  installs VirtualBoxIf you prefer to install it separately, you can download it  or run:$ brew update
$ brew install caskroom/cask/brew-cask
$ brew cask install --appdir=""/Applications"" virtualbox
VagrantVagrant creates and configures development environments.  You can think of it as a higher-level wrapper around VirtualBox and configuration management tools like Ansible, Chef, Puppet, and Salt.  Vagrant also supports Docker containers and server environments like Amazon EC2.InstallationThe  installs Vagrant.If you prefer to install it separately, you can download it  or run:$ brew update
$ brew install caskroom/cask/brew-cask
$ brew cask install --appdir=""/Applications"" vagrant
DockerDocker automates the deployment of applications inside software containers.  I think the following  explains docker nicely: ""Docker is a tool that can package an application and its dependencies in a virtual container that can run on any Linux server. This helps enable flexibility and portability on where the application can run, whether on premise, public cloud, private cloud, bare metal, etc"".InstallationThe  installs Docker.If you prefer to install it separately, you can download it  or run:$ brew update
$ brew install docker
$ brew install boot2docker
ConfigurationInitialize and start  (only need to do this once):$ boot2docker init
Start the VM:$ boot2docker up
Set the  environment variable and fill in IP and PORT based on the output from the  command:$ export DOCKER_HOST=tcp://IP:PORT
GitWhat's a developer without ?InstallationGit should have been installed when you ran through the  section.ConfigurationTo check your version of Git, run the following command:$ git --version
And  should output .Let's set up some basic configuration. Download the  file to your home directory:$ cd ~
$ curl -O https://raw.githubusercontent.com/donnemartin/dev-setup/master/.gitconfig
It will add some color to the , , and  Git commands, as well as a couple aliases. Feel free to take a look at the contents of the file, and add to it to your liking.Next, we'll define your Git user (should be the same name and email you use for  and ):$ git config --global user.name ""Your Name Here""
$ git config --global user.email ""your_email@youremail.com""
They will get added to your  file.To push code to your GitHub repositories, we're going to use the recommended HTTPS method (versus SSH). So you don't have to type your username and password everytime, let's enable Git password caching as described :$ git config --global credential.helper osxkeychain
Note: On a Mac, it is important to remember to add  (a hidden OS X system file that's put in folders) to your  files. You can take a look at this repository's  file for inspiration.  Also check out GitHub's .HomebrewPackage managers make it so much easier to install and update applications (for Operating Systems) or libraries (for programming languages). The most popular one for OS X is .InstallationThe  installs Homebrew and a number of useful Homebrew formulae and apps.If you prefer to install it separately, run the following command and follow the steps on the screen:$ ruby -e ""$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)""
UsageTo install a package (or Formula in Homebrew vocabulary) simply type:$ brew install <formula>
To update Homebrew's directory of formulae, run:$ brew update
Note: I've seen that command fail sometimes because of a bug. If that ever happens, run the following (when you have Git installed):$ cd /usr/local
$ git fetch origin
$ git reset --hard origin/master
To see if any of your packages need to be updated:$ brew outdated
To update a package:$ brew upgrade <formula>
Homebrew keeps older versions of packages installed, in case you want to roll back. That rarely is necessary, so you can do some cleanup to get rid of those old versions:$ brew cleanup
To see what you have installed (with their version numbers):$ brew list --versions
Ruby and rbenv is already installed on Unix systems, but we don't want to mess around with that installation. More importantly, we want to be able to use the latest version of Ruby.Installation provides  and  which allow you to manage multiple versions of Ruby on the same machine.   adds the following line to your  file to initialize :eval ""$(rbenv init -)""
Usage uses  to download, compile, and install new versions of Ruby. You can see all versions available to download and install:$ ruby-build --definitions
To install a new version of Ruby:# list all available versions installed on the system:
$ rbenv install -l

# install a Ruby version:
$ rbenv install 2.2.3
To switch Ruby versions:# set a local application-specific Ruby version in the current directory
$ rbenv local 1.9.3

# set the global version of Ruby to be used in all shells
$ rbenv global 2.0.0

 by default will install Ruby versions into a directory of the same name under . Because your user owns this directory, you no longer need to use  to install gems.PythonOS X, like Linux, ships with  already installed. But you don't want to mess with the system Python (some system tools rely on it, etc.), so we'll install our own version with Homebrew. It will also allow us to get the very latest version of Python 2.7 and Python 3.InstallationThe  installs the latest versions of Python 2 and Python 3.Pip is the Python package manager.InstallationThe  installs pip.UsageHere are a couple Pip commands to get you started. To install a Python package:$ pip install <package>
To upgrade a package:$ pip install --upgrade <package>
To see what's installed:$ pip freeze
To uninstall a package:$ pip uninstall <package>
Virtualenv is a tool that creates an isolated Python environment for each of your projects. For a particular project, instead of installing required packages globally, it is best to install them in an isolated folder in the project (say a folder named ), that will be managed by virtualenv.The advantage is that different projects might require different versions of packages, and it would be hard to manage that if you install packages globally. It also allows you to keep your global  folder clean.InstallationThe  installs Virtualenv.UsageLet's say you have a project in a directory called . To set up virtualenv for that project:$ cd myproject/
$ virtualenv venv --distribute
If you want your virtualenv to also inherit globally installed packages (like IPython or Numpy mentioned above), use:$ virtualenv venv --distribute --system-site-packages
These commands create a  subdirectory in your project where everything is installed. You need to activate it first though (in every terminal where you are working on your project):$ source venv/bin/activate
You should see a  appear at the beginning of your terminal prompt indicating that you are working inside the virtualenv. Now when you install something:$ pip install <package>
It will get installed in the  folder, and not conflict with other projects.Important: Remember to add  to your project's  file so you don't include all of that in your source code!Virtualenvwrapper is a set of extensions that includes wrappers for creating and deleting virtual environments and otherwise managing your development workflow, making it easier to work on more than one project at a time without introducing conflicts in their dependencies.Main features include:InstallationThe  installs Virtualenvwrapper.UsageCreate a new virtual environment. When you create a new environment it automatically becomes the active environment:$ mkvirtualenv [env name]
Remove an existing virtual environment. The environment must be deactivated (see below) before it can be removed:$ rmvirtualenv [env name]
Activate a virtual environment. Will also list all existing virtual environments if no argument is passed:$ workon [env name]
Deactivate the currently active virtual environment. Note that workonwill automatically deactivate the current environment before activating a new one:$ deactivate
Section 3: Python Data AnalysisAnacondaAnaconda is a free distribution of the Python programming language for large-scale data processing, predictive analytics, and scientific computing that aims to simplify package management and deployment.InstallationThe  installs packages you need to run Python data applications.  Alternatively, you can install the more heavy-weight Anaconda instead.Follow instructions to install  or the more lightweight .IPython Notebook is an awesome project which provides a much better Python shell than the one you get from running  in the command-line. It has many cool functions (running Unix commands from the Python shell, easy copy & paste, creating Matplotlib charts in-line, etc.) and I'll let you refer to the  to discover them.IPython Notebook is a web-based interactive computational environment where you can combine code execution, text, mathematics, plots and rich media into a single document.InstallationThe  installs IPython Notebook.  If you prefer to install it separately, run:$ pip install ""ipython[notebook]""
If you run into an issue about pyzmq, refer to the following  and run:$ pip uninstall ipython
$ pip install ""ipython[all]""
Usage$ ipython notebook
If you'd like to see some examples here are a couple of my repos that use IPython Notebooks heavily:NumPyNumPy adds Python support for large, multi-dimensional arrays and matrices, along with a large library of high-level mathematical functions to operate on these arrays.InstallationThe  installs NumPy.  If you prefer to install it separately, run:$ pip install numpy
UsageRefer to the following .PandasPandas is a software library written for data manipulation and analysis in Python. Offers data structures and operations for manipulating numerical tables and time series.InstallationThe  installs Pandas.  If you prefer to install it separately, run:$ pip install pandas
UsageRefer to the following .MatplotlibMatplotlib is a Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms.InstallationThe  installs matplotlib.  If you prefer to install it separately, run:$ pip install matplotlib
UsageRefer to the following .SeabornSeaborn is a Python visualization library based on matplotlib. It provides a high-level interface for drawing attractive statistical graphics.InstallationThe  installs matplotlib.  If you prefer to install it separately, run:$ pip install seaborn
UsageRefer to the following .Scikit-learnScikit-learn adds Python support for large, multi-dimensional arrays and matrices, along with a large library of high-level mathematical functions to operate on these arrays.InstallationThe  installs Scikit-learn.  If you prefer to install it separately, run:$ pip install scikit-learn
UsageRefer to the following .SciPySciPy is a collection of mathematical algorithms and convenience functions built on the Numpy extension of Python. It adds significant power to the interactive Python session by providing the user with high-level commands and classes for manipulating and visualizing data.InstallationThe  installs SciPy.  If you prefer to install it separately, run:$ pip install scipy
UsageRefer to the following .FlaskFlask is a micro web application framework written in Python.InstallationThe  installs SciPy.  If you prefer to install it separately, run:$ pip install Flask
Usage[Coming Soon] Refer to the following .BokehBokeh is a Python interactive visualization library that targets modern web browsers for presentation. Its goal is to provide elegant, concise construction of novel graphics in the style of D3.js, but also deliver this capability with high-performance interactivity over very large or streaming datasets. Bokeh can help anyone who would like to quickly and easily create interactive plots, dashboards, and data applications.InstallationThe  installs Bokeh.  If you prefer to install it separately, run:$ pip install bokeh
Usage[Coming Soon] Refer to the following .Section 4: Big Data, AWS, and HerokuSparkSpark is an in-memory cluster computing framework, up to 100 times faster for certain applications and is well suited for machine learning algorithms.InstallationThe  installs Spark locally.  It also hooks up Spark to run within the IPython Notebook by configuring your  and adding the repo's  to .If you prefer to install it separately, run:$ brew install apache-spark
UsageRun Spark locally:$ pyspark
Run Spark within IPython Notebook:$ ipython notebook --profile=pyspark
Refer to the following .Spark is also supported on AWS Elastic MapReduce as described .  To create a cluster, run the following command with the , replacing  with the name of your keypair to SSH into your cluster:$ aws emr create-cluster --name ""Spark cluster"" --ami-version 3.8 --applications Name=Spark --ec2-attributes KeyName=myKeyPair --instance-type m3.xlarge --instance-count 3 --use-default-roles
MapReduceMrjob supports MapReduce jobs in Python, running them locally or on Hadoop clusters such as AWS Elastic MapReduce (EMR).InstallationMrjob is Python 2 only.The  installs mrjob locally.  If you prefer to install it separately, run:$ pip install mrjob
The aws.sh script also syncs the template  file to your home folder.  Note running the aws.sh script will overwrite any existing  file.  Update the config file with your credentials, keypair, region, and S3 bucket paths:runners:
  emr:
    aws_access_key_id: YOURACCESSKEY
    aws_secret_access_key: YOURSECRETKEY
    aws_region: us-east-1
    ec2_key_pair: YOURKEYPAIR
    ec2_key_pair_file: ~/.ssh/YOURKEYPAIR.pem
    ...
    s3_scratch_uri: s3://YOURBUCKETSCRATCH
    s3_log_uri: s3://YOURBUCKETLOG
    ...
UsageRefer to the following .Awesome AWS  is a curated list of awesome AWS libraries, open source repos, guides, blogs, and other resources.  It's a great way to stay up-to-date with the various aws-backed and community-led efforts geared towards AWS.The Fiery Meter of AWSome'Hot' repos in Awesome AWS are visually tagged based on their popularity:Repos not on AWS AccountTo start using AWS, you first need to sign up for an account.Sign up for AWSWhen you sign up for Amazon Web Services (AWS), your AWS account is automatically signed up for all services in AWS. You are charged only for the services that you use.  New users are eligible for 12 months of usage through the .To create an AWS account, open http://aws.amazon.com/, and then click Sign Up.  Follow the on-screen instructions.  Part of the sign-up procedure involves receiving a phone call and entering a PIN using the phone keypad.  Note your AWS account ID.AWS CLIThe AWS Command Line Interface is a unified tool to manage AWS services, allowing you to control multiple AWS services from the command line and to automate them through scripts.InstallationThe  installs the AWS CLI.  If you prefer to install it separately, run:$ pip install awscli
Run the following command to configure the AWS CLI:$ aws configure
Alternatively, the aws.sh script also syncs the template  folder to your home folder.  Note running the aws.sh script will overwrite any existing  folder.  Update the config file with your credentials and location:[default]
region = us-east-1
[default]
aws_access_key_id = YOURACCESSKEY
aws_secret_access_key = YOURSECRETKEY
Be careful you do not accidentally check in your credentials.  The .gitignore file is set to ignore files with credentials.UsageRefer to the following .SAWSAlthough the  is a great resource to manage your AWS-powered services, it's tough to remember usage of:SAWS: A Supercharged AWS CLI aims to supercharge the AWS CLI with features focusing on:Under the hood,  is powered by the AWS CLI and supports the same commands and command structure. and  Usage:aws <command> <subcommand> [parameters] [options]
 features: is available for Mac, Linux, Unix, and .Installation and Usage.Refer to the .BotoBoto is the official AWS SDK for Python.InstallationThe  installs boto.  If you prefer to install it separately, run:$ pip install boto
Boto uses the same configuration as described in the  section.UsageRefer to the following .S3cmdBefore I discovered , I had been using the  to do basic operations and  to do more of the heavy lifting.  However, sometimes I just want to hack away at a command line to do my work.I've found S3cmd to be a great command line tool for interacting with S3 on AWS.  S3cmd is written in Python, is open source, and is free even for commercial use.  It offers more advanced features than those found in the .InstallationS3cmd is Python 2 only.The  installs s3cmd.  If you prefer to install it separately, run:$ pip install s3cmd
Running the following command will prompt you to enter your AWS access and AWS secret keys. To follow security best practices, make sure you are using an IAM account as opposed to using the root account.I also suggest enabling GPG encryption which will encrypt your data at rest, and enabling HTTPS to encrypt your data in transit. Note this might impact performance.$ s3cmd --configure
Alternatively, the aws.sh script also syncs the template  file to your home folder.  Note running the aws.sh script will overwrite any existing  file.  Update the config file with your credentials and location:[Credentials]
aws_access_key_id = YOURACCESSKEY
aws_secret_access_key = YOURSECRETKEY
...
bucket_location = US
...
gpg_passphrase = YOURPASSPHRASE
Be careful you do not accidentally check in your credentials.  The .gitignore file is set to ignore files with credentials.UsageRefer to the following .S3DistCp is an extension of DistCp that is optimized to work with Amazon S3.  S3DistCp is useful for combining smaller files and aggregate them together, taking in a pattern and target file to combine smaller input files to larger ones.  S3DistCp can also be used to transfer large volumes of data from S3 to your Hadoop cluster.InstallationS3DistCp comes bundled with the AWS CLI.UsageRefer to the following .S3-parallel-put is a great tool for uploading multiple files to S3 in parallel.Installation$ git clone https://github.com/twpayne/s3-parallel-put.git
UsageRefer to the following .RedshiftRedshift is a fast data warehouse built on top of technology from massive parallel processing (MPP).SetupFollow these .UsageRefer to the following .KinesisKinesis streams data in real time with the ability to process thousands of data streams per second.SetupFollow these .UsageRefer to the following .LambdaLambda runs code in response to events, automatically managing compute resources.SetupFollow these .UsageRefer to the following .AWS Machine LearningAmazon Machine Learning is a service that makes it easy for developers of all skill levels to use machine learning technology. Amazon Machine Learning provides visualization tools and wizards that guide you through the process of creating machine learning (ML) models without having to learn complex ML algorithms and technology. Once your models are ready, Amazon Machine Learning makes it easy to obtain predictions for your application using simple APIs, without having to implement custom prediction generation code, or manage any infrastructure.SetupFollow these .Usage[Coming Soon] Refer to the following .Heroku, if you're not already familiar with it, is a  (PaaS) that makes it really easy to deploy your apps online. There are other similar solutions out there, but Heroku was among the first and is currently the most popular. Not only does it make a developer's life easier, but I find that having Heroku deployment in mind when building an app forces you to follow modern app development .InstallationAssuming that you have an account (sign up if you don't), let's install the  for the command-line. Heroku offers a Mac OS X installer, the , that includes the client. But for these kind of tools, I prefer using Homebrew. It allows us to keep better track of what we have installed. Luckily for us, Homebrew includes a  formula:$ brew install heroku-toolbelt
The formula might not have the latest version of the Heroku Client, which is updated pretty often. Let's update it now:$ brew upgrade heroku-toolbelt
Don't be afraid to run  every now and then to always have the most recent version.UsageLogin to your Heroku account using your email and password:$ heroku login
If this is a new account, and since you don't already have a public SSH key in your  directory, it will offer to create one for you. Say yes! It will also upload the key to your Heroku account, which will allow you to deploy apps from this computer.If it didn't offer create the SSH key for you (i.e. your Heroku account already has SSH keys associated with it), you can do so manually by running: $ mkdir ~/.ssh
 $ ssh-keygen -t rsa
Keep the default file name and skip the passphrase by just hitting Enter both times. Then, add the key to your Heroku account:$ heroku keys:add
Once the key business is done, you're ready to deploy apps! Heroku has a great  guide, so I'll let you refer to that (the one linked here is for Python, but there is one for every popular language). Heroku uses Git to push code for deployment, so make sure your app is under Git version control. A quick cheat sheet (if you've used Heroku before):$ cd myapp/
$ heroku create myapp
$ git push heroku master
$ heroku ps
$ heroku logs -t
The  is full of great resources, so be sure to check it out!Section 5: Data StoresMySQLInstallationThe  installs MySQL.  If you prefer to install it separately, run:$ brew update # Always good to do
$ brew install mysql
As you can see in the ouput from Homebrew, before we can use MySQL we first need to set it up with:$ unset TMPDIR
$ mkdir /usr/local/var
$ mysql_install_db --verbose --user=`whoami` --basedir=""$(brew --prefix mysql)"" --datadir=/usr/local/var/mysql --tmpdir=/tmp
UsageTo start the MySQL server, use the  tool:$ mysql.server start
To stop it when you are done, run:$ mysql.server stop
You can see the different commands available for  with:$ mysql.server --help
To connect with the command-line client, run:$ mysql -uroot
(Use  to quit the MySQL shell.)Note: By default, the MySQL user  has no password. It doesn't really matter for a local development database. If you wish to change it though, you can use .MySQL WorkbenchIn terms of a GUI client for MySQL, I'm used to the official and free . But feel free to use whichever you prefer.InstallationThe  installs MySQL Workbench.  If you prefer to install it separately, run:$ brew install caskroom/cask/brew-cask
$ brew cask install --appdir=""/Applications"" mysqlworkbench
You can also find the MySQL Workbench download . (Note: It will ask you to sign in, you don't need to, just click on ""No thanks, just start my download!"" at the bottom.)MongoDB is a popular  database.InstallationThe  installs MongoDB. If you prefer to install it separately, run:$ brew update
$ brew install mongo
UsageIn a terminal, start the MongoDB server:$ mongod
In another terminal, connect to the database with the Mongo shell using:$ mongo
I'll let you refer to MongoDB's  guide for more!Redis is a blazing fast, in-memory, key-value store, that uses the disk for persistence. It's kind of like a NoSQL database, but there are a lot of  that you can do with it that would be hard or inefficient with other database solutions. For example, it's often used as session management or caching by web apps, but it has many other uses.InstallationThe  installs Redis. If you prefer to install it separately, run:$ brew update
$ brew install redis
UsageStart a local Redis server using the default configuration settings with:$ redis-server
For advanced usage, you can tweak the configuration file at  (I suggest making a backup first), and use those settings with:$ redis-server /usr/local/etc/redis.conf
In another terminal, connect to the server with the Redis command-line interface using:$ redis-cli
I'll let you refer to Redis'  or other tutorials for more information.ElasticsearchAs it says on the box,  is a ""powerful open source, distributed real-time search and analytics engine"". It uses an HTTP REST API, making it really easy to work with from any programming language.You can use elasticsearch for such cool things as real-time search results, autocomplete, recommendations, machine learning, and more.InstallationThe  installs Elasticsearch.  If you prefer to install it separately, check out the following discussion.Elasticsearch runs on Java, so check if you have it installed by running:$ java -version
If Java isn't installed yet, a window will appear prompting you to install it. Go ahead and click ""Install"".Next, install elasticsearch with:$ brew install elasticsearch
Note: Elasticsearch also has a  program that gets moved to your . I find that too generic of a name, so I rename it to  by running (will need to do that again if you update elasticsearch):$ mv /usr/local/bin/plugin /usr/local/bin/elasticsearch-plugin
Below I will use , just replace it with  if you haven't followed this step.As you guessed, you can add plugins to elasticsearch. A popular one is , which gives you a web interface to the REST API. Install it with:$ elasticsearch-plugin --install mobz/elasticsearch-head
UsageStart a local elasticsearch server with:$ elasticsearch
Test that the server is working correctly by running:$ curl -XGET 'http://localhost:9200/'
If you installed the elasticsearch-head plugin, you can visit its interface at .Elasticsearch's  is more of a reference. To get started, I suggest reading some of the blog posts linked on this .Section 6: Web DevelopmentNode.jsInstallationThe  installs .  You can also install it manually with Homebrew:$ brew update
$ brew install node
The formula also installs the  package manager. However, as suggested by the Homebrew output, we need to add  to our path so that npm-installed modules with executables will have them picked up.To do so, add this line to your  file, before the  line:PATH=/usr/local/share/npm/bin:$PATH
Open a new terminal for the  changes to take effect.We also need to tell npm where to find the Xcode Command Line Tools, by running:$ sudo xcode-select -switch /usr/bin
(If Xcode Command Line Tools were installed by Xcode, try instead:)$ sudo xcode-select -switch /Applications/Xcode.app/Contents/Developer
Node modules are installed locally in the  folder of each project by default, but there are at least two that are worth installing globally. Those are  and :$ npm install -g coffee-script
$ npm install -g grunt-cli
Npm usageTo install a package:$ npm install <package> # Install locally
$ npm install -g <package> # Install globally
To install a package and save it in your project's  file:$ npm install <package> --save
To see what's installed:$ npm list # Local
$ npm list -g # Global
To find outdated packages (locally or globally):$ npm outdated [-g]
To upgrade all or a particular package:$ npm update [<package>]
To uninstall a package:$ npm uninstall <package>
JSHintJSHint is a JavaScript developer's best friend.If the extra credit assignment to install Sublime Package Manager was completed, JSHint can be run as part of Sublime Text.InstallationThe  installs JSHint.  You can also install it manually via via npm:$ npm install -g jshint
Follow additional instructions on the  or .LESSCSS preprocessors are becoming quite popular, the most popular processors are  and . Preprocessing is a lot like compiling code for CSS. It allows you to reuse CSS in many different ways. Let's start out with using LESS as a basic preprocessor, it's used by a lot of popular CSS frameworks like .InstallationThe  installs LESS.  To install LESS manually you have to use NPM / Node, which you installed earlier using Homebrew. In the terminal use:$ npm install -g less
Note: the  flag is optional but it prevents having to mess around with file paths. You can install without the flag, just know what you're doing.You can check that it installed properly by using:$ lessc --version
This should output some information about the compiler:lessc 1.5.1 (LESS Compiler) [JavaScript]
Okay, LESS is installed and running. Great!UsageThere's a lot of different ways to use LESS. Generally I use it to compile my stylesheet locally. You can do that by using this command in the terminal:$ lessc template.less template.css
The two options are the ""input"" and ""output"" files for the compiler. The command looks in the current directory for the LESS stylesheet, compiles it, and outputs it to the second file in the same directory. You can add in paths to keep your project files organized:$ lessc less/template.less css/template.css
Read more about LESS on their page here: http://lesscss.org/Section 7: Android Development[<marko.inline.RawText object at 0x000001592FE42AC8>]JavaInstallationThe  installs Java.If you prefer to install it separately, you can download the JDK  or run:$ brew update
$ brew install caskroom/cask/brew-cask
$ brew cask install --appdir=""~/Applications"" java
Android SDKThe  installs the Android SDK.If you prefer to install it separately, you can download it .Android StudioThe  installs Android Studio.If you prefer to install it separately, you can download it  or run:$ brew update
$ brew install caskroom/cask/brew-cask
$ brew cask install --appdir=""~/Applications"" android-studio
IntelliJ IDEAThe  installs Java.If you prefer to install it separately, you can download it  or run:$ brew update
$ brew install caskroom/cask/brew-cask
$ brew cask install --appdir=""~/Applications"" intellij-idea-ce
Section 8: MiscContributionsBug reports, suggestions, and pull requests are !CreditsSee the .Contact InfoFeel free to contact me to discuss any issues, questions, or comments.My contact info can be found on my .LicenseThis repository contains a variety of content; some developed by Donne Martin, and some from third-parties.  The third-party content is distributed under the license provided by those parties.The content developed by Donne Martin is distributed under the following license:I am providing code and resources in this repository to you under an open source license.  Because this is my personal repository, the license you receive to my code and resources is from me and not my employer (Facebook).Copyright 2015 Donne Martin

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
"
https://github.com/TheKingOfDuck/fuzzDicts,"Web Pentesting Fuzz 字典,一个就够了。","fuzzDictsWeb Pentesting Fuzz 字典,一个就够了。log不定期更新，使用前建议git pull一下，同步更新。分享字典建议直接提交PR 20210608:20201202:20200510:20200420:20200410:20200406:20200221:20200211:20200115:20200106:20200104:20191219:20191214:20191106:20191026:20191022:20190928:20190819:20190811：20190801：20190615：content工具推荐：,,,,如果有什么的好字典或是建议欢迎提交issue给我。https://github.com/TheKingOfDuck/fuzzDicts/blob/master/paramDict/parameter.txt
采集自,,,,,等常见PHP框架/CMS。使用技巧：如http://127.0.0.1/1.php ,视为可疑文件，进行fuzz param 选择GET,POST AND (POST JSON) AND (GET Route) AND cookie paramhttps://github.com/TheKingOfDuck/easyXssPayload/blob/master/easyXssPayload.txt
采集自。https://github.com/TheKingOfDuck/fuzzDicts/tree/master/userNameDict
https://github.com/TheKingOfDuck/fuzzDicts/tree/master/passwordDict
https://github.com/TheKingOfDuck/fuzzDicts/tree/master/directoryDicts
https://github.com/TheKingOfDuck/fuzzDicts/blob/master/sqlDict/sql.txt
https://github.com/TheKingOfDuck/fuzzDicts/blob/master/ssrfDicts
由师傅提供。https://github.com/TheKingOfDuck/fuzzDicts/tree/master/XXEDicts
收集自百度。https://github.com/TheKingOfDuck/fuzzDicts/tree/master/ctfDict
采集自，原先收集时百度直接下载的压缩包，没看到github链接，所以没标记来源，抱歉抱歉https://github.com/TheKingOfDuck/fuzzDicts/tree/master/apiDict/api.txt
钟馗采集的代码写得很cxk 我真弟弟。。。https://github.com/TheKingOfDuck/fuzzDicts/tree/master/routerDicts/pass.txt
https://github.com/TheKingOfDuck/fuzzDicts/tree/master/uploadFileExtDicts
采集自https://github.com/c0ny1/upload-fuzz-dic-builder采集自:https://github.com/7dog7/bottleneckOsmosis"
https://github.com/2020PB/police-brutality,Repository containing evidence of police brutality during the 2020 George Floyd protests,"Police Brutality During US ProtestsVisit us at: https://2020pb.com/Important Note on BranchesAs of September 15th at 3 PM EDT, the default branch is .Most tools will use the  branch, which will not be affected by the change.The  branch will no longer be updated. Please update forks off of  to reference , rebasing your work off of main if needed.New to GitHub? Just want to report an incident?&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;👇🏽 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;👇🏾👉🏿 [<marko.inline.RawText object at 0x000001592FE12508>] 👈🏻&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;👆🏼&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Want to report an incident, but don't have a Github account? Send it For more information on contributing to this repository please consult the .This repository exists to accumulate and contextualize evidence of police brutality during the 2020 George Floyd protests.Our goal in doing this is to assist journalists, politicians, prosecutors, activists and concerned citizens who can use the evidence accumulated here for political campaigns, news reporting, public education and prosecution of criminal police officers.This was first started as a megathread on the subreddit  but after being overwhelmed by people looking to contribute, we decided to make a github repository so that everyone who wants to can easily contribute to the project directly.If you are here looking for evidence of police brutality, please refer to the table of contents for links to incident reports sorted by state.If you wish to contribute, please start by reading the .LinksWays to use our incident files:Some projects built using our APIsBackgroundOn May 25, 2020 an African American man named George Floyd was murdered by police officer Derek Chauvin in Minneapolis, a city with a police department . George Floyd's murder sparked a wave of protests across the nation as citizens became outraged by another example of police misconduct in the midst of a historic financial and public health crisis caused by the Coronavirus pandemic.In response to the burgeoning civil unrest, numerous police departments violently cracked down on peaceful demonstrations and drastically escalated confrontations with protesters. As videos of these crack-downs circulated on social media over the following hours and days, the protests escalated nationally and every major city in the country saw mass protests, with some experiencing riots that resulted in mass property destruction and saw buildings burned down.As these protests have continued, hundreds of incidents have been recorded where police engaged in unprovoked violent assaults, outright police brutality and unprovoked arrests and harassment.Table of Contents"
https://github.com/pyeve/eve,REST API framework designed for human beings,"Eve.. image:: https://img.shields.io/pypi/v/eve.svg?style=flat-square:target: https://pypi.org/project/eve.. image:: https://github.com/pyeve/eve/workflows/CI/badge.svg:target: https://github.com/pyeve/eve/actions?query=workflow%3ACI.. image:: https://img.shields.io/pypi/pyversions/eve.svg?style=flat-square:target: https://pypi.org/project/eve.. image:: https://img.shields.io/badge/license-BSD-blue.svg?style=flat-square:target: https://en.wikipedia.org/wiki/BSD_License.. image:: https://img.shields.io/badge/code%20style-black-000000.svg:target: https://github.com/ambv/blackEve is an open source Python REST API framework designed for human beings. Itallows to effortlessly build and deploy highly customizable, fully featuredRESTful Web Services. Eve offers native support for MongoDB, and SQL backendsvia community extensions.Eve is Simple.. code-block:: pythonfrom eve import Eve

app = Eve()
app.run()
The API is now live, ready to be consumed:.. code-block:: console$ curl -i http://example.com/people
HTTP/1.1 200 OK
All you need to bring your API online is a database, a configuration file(defaults to ) and a launch script.  Overall, you will find thatconfiguring and fine-tuning your API is a very simple process._FeaturesFundingEve REST framework is a open source, collaboratively funded project. If you runa business and are using Eve in a revenue-generating product, it would makebusiness sense to sponsor Eve development: it ensures the project that yourproduct relies on stays healthy and actively maintained. Individual users arealso welcome to make a recurring pledge or a one time donation if Eve hashelped you in your work or personal projects.Every single sign-up makes a significant impact towards making Eve possible. Tolearn more, check out our _.LicenseEve is a _ open source project,distributed under the _... _: http://nicolaiarocci.com.. _: http://python-eve.org/funding.html"
https://github.com/frida/frida,Clone this repo to build Frida,"FridaDynamic instrumentation toolkit for developers, reverse-engineers, and securityresearchers. Learn more at .Two ways to install1. Install from prebuilt binariesThis is the recommended way to get started. All you need to do is:pip install frida-tools # CLI tools
pip install frida       # Python bindings
npm install frida       # Node.js bindings
You may also download pre-built binaries for various operating systems fromFrida's  page on GitHub.2. Build your own binariesDependenciesFor running the Frida CLI tools, e.g. , , ,, , , etc., you need Python plus afew packages:pip install colorama prompt-toolkit pygments
Linuxmake
Apple OSesFirst make a trusted code-signing certificate. You can use the guide athttps://sourceware.org/gdb/wiki/PermissionsDarwin in the sections“Create a certificate in the System Keychain” and “Trust the certificatefor code signing”. You can use the name  instead of if you'd like.Next export the name of the created certificate to relevant environmentvariables, and run :export MACOS_CERTID=frida-cert
export IOS_CERTID=frida-cert
export WATCHOS_CERTID=frida-cert
export TVOS_CERTID=frida-cert
make
To ensure that macOS accepts the newly created certificate, restart the daemon:sudo killall taskgated
Windowsfrida.sln
(Requires Visual Studio 2022.)See for details.Learn moreHave a look at our ."
https://github.com/ansible/ansible,"Ansible is a radically simple IT automation platform that makes your applications and systems easier to deploy and maintain. Automate everything from code deployment to network configuration to cloud management, in a language that approaches plain English, using SSH, with no agents to install on remote systems. https://docs.ansible.com.","AnsibleAnsible is a radically simple IT automation system. It handlesconfiguration management, application deployment, cloud provisioning,ad-hoc task execution, network automation, and multi-node orchestration. Ansible makes complexchanges like zero-downtime rolling updates with load balancers easy. More information on the Ansible .Design PrinciplesUse AnsibleYou can install a released version of Ansible with  or a package manager. See our for details on installing Ansibleon a variety of platforms.Power users and developers can run the  branch, which has the latestfeatures and fixes, directly. Although it is reasonably stable, you are more likely to encounterbreaking changes when running the  branch. We recommend getting involvedin the Ansible community if you want to run the  branch.Get InvolvedCoding GuidelinesWe document our Coding Guidelines in the . We particularly suggest you review:Branch InfoRoadmapBased on team and community feedback, an initial roadmap will be published for a major or minor version (ex: 2.7, 2.8).The  details what is planned and how to influence the roadmap.AuthorsAnsible was created by and has contributions from over 5000 users (and growing). Thanks everyone! is sponsored by LicenseGNU General Public License v3.0 or laterSee  to see the full text."
https://github.com/kemayo/sublime-text-git,Plugin for some git integration into sublime text,"Sublime Text plugin: gitGit integration: it's pretty handy. Who knew, right?For more information about what's supported, and how to install this, .InstallPackage ControlThe easiest way to install this is with .Package Control will automatically keep Git up to date with the latest version.Basic UsageThe restIf you don't want to use Package Control,  for other installation methods on various platforms.TroubleshootingThis package works by running commands as your system . As such, if you have problems with this package, first make sure that git is installed and configured correctly on your system.You may want to make sure that the  binary this plugin is using is the correct one, if you have multiple ones installed. Most git installation guides will be happy to walk you through configuring your system  appropriately.If necessary, set the  plugin preference to tell us where to look.Git isn't configured properly. Tell it who you are, by opening a command prompt and doing this:git config --global user.email ""you@example.com""
git config --global user.name ""Your Name""
If you've done this and it's still complaining, you probably have multiple copies of git on your system which have different configuration locations, and the one which runs on your command line isn't the one which the shell  exposes to Sublime Text.Git isn't configured to use a system-level ssh-agent, and so it's asking you for a username and password when you try to push / pull. The plugin doesn't know how to ask you for this information. and this will stop happening.AcknowledgementsThis package contains:"
https://github.com/Shougo/deoplete.nvim,:stars: Dark powered asynchronous completion framework for neovim/Vim8,"deoplete.nvimNote: The development of this plugin is finished. Accepts minor patches andissues but no new features. is the next generation autocompletion plugin. Consider migrating to it.Please read  for details.Note: If you need to understand what's different between deoplete and othersimilar plugins, please read ""deoplete-faq"" section in the documentation.Deoplete is the abbreviation of ""dark powered neo-completion"".  Itprovides an extensible and asynchronous completion framework forneovim/Vim8.deoplete will display completions via  by default.Here are some  specifically made for deoplete.nvim.InstallNote: deoplete requires Neovim (0.3.0+ and of course, latest isrecommended) or Vim8.2.1978+ with Python 3.6.1+ and timers enabled.  See if you aren't sure whether you have this.Note: deoplete requires msgpack package 1.0.0+.Please install/upgrade msgpack package by pip.https://github.com/msgpack/msgpack-pythonNote: If you really need to use older msgpack, please use deoplete ver.5.2instead.https://github.com/Shougo/deoplete.nvim/releases/tag/5.2For vim-plugif has('nvim')
  Plug 'Shougo/deoplete.nvim', { 'do': ':UpdateRemotePlugins' }
else
  Plug 'Shougo/deoplete.nvim'
  Plug 'roxma/nvim-yarp'
  Plug 'roxma/vim-hug-neovim-rpc'
endif
let g:deoplete#enable_at_startup = 1
For dein.vimcall dein#add('Shougo/deoplete.nvim')
if !has('nvim')
  call dein#add('roxma/nvim-yarp')
  call dein#add('roxma/vim-hug-neovim-rpc')
endif
let g:deoplete#enable_at_startup = 1
Vim >= 8 built-in package manager (not recommended)Requirementsdeoplete requires Neovim or Vim8 with .If  returns , then you have python 3 support;otherwise, see below.You can enable Python3 interface with pip:pip3 install --user pynvim
Please install nvim-yarp and vim-hug-neovim-rpc for Vim8.Note: Python3 must be enabled before updating remote pluginsIf Deoplete was installed prior to Python support being added to Neovim, should be executed manually in order to enableauto-completion.Note: deoplete needs pynvim ver.0.3.0+.You need update pynvim module.pip3 install --user --upgrade pynvim
If you want to read the Neovim-python/python3 interface install documentation,you should read  and the Wiki.Configuration"" Use deoplete.
let g:deoplete#enable_at_startup = 1
See  for a complete list of options.ScreenshotsDeoplete for JavaScript"
https://github.com/soumith/convnet-benchmarks,Easy benchmarking of all publicly accessible implementations of convnets,"convnet-benchmarksEasy benchmarking of all public open-source implementations of convnets.A summary is provided in the section below.Machine:  +  + Imagenet Winners BenchmarkingI pick some popular imagenet models, and I clock the time for a full forward + backward pass. I average my times over 10 runs. I ignored dropout and softmax layers.NotationInput is described as . Where  is the number of images used in a minibatch,  is the number of channels in an image,  is the width of the image, and  is the height of the image.One small note:The CuDNN benchmarks are done using Torch bindings. One can also do the same via Caffe bindings or bindings of any other library. This note is here to clarify that Caffe (native) and Torch (native) are the convolution kernels which are present as a default fallback. Some of the frameworks like TensorFlow and Chainer are benchmarked with CuDNN, but it is not explicitly mentioned, and hence one might think that these frameworks as a whole are faster, than for example Caffe, which might not be the case.[<marko.inline.RawText object at 0x000001592FD70D88>] - Input 128x3x224x224| Library         | Class                                                                                                                | Time (ms)  | forward (ms) | backward (ms) ||:------------------------:|:-----------------------------------------------------------------------------------------------------------:| ----------:| ------------:| -------------:|| CuDNN[R4]-fp16 (Torch)     |      |  71    |  25      |   46      || Nervana-neon-fp16    |                         |      78    |  25          |    52         || CuDNN[R4]-fp32 (Torch)      |     |      81    |  27          |   53          || TensorFlow               |                   |      81   |  26          |   55         || Nervana-neon-fp32        |                     |      87   |  28          |    58         || fbfft   (Torch)                  |                    |      104   |  31          |    72         || Chainer                 |      |      177   |  40          |   136         || cudaconvnet2*            |         |      177   |  42          |   135         || CuDNN[R2] *             |         |      231   |  70          |   161         || Caffe (native)           |                 |      324   | 121          |   203         || Torch-7 (native)         |                    |      342   | 132          |   210         || CL-nn (Torch)            |              |      963   | 388          |   574         || Caffe-CLGreenTea         |                                                         |      1442   | 210          |   1232         |[<marko.inline.RawText object at 0x000001592FEFAE88>] - Input 128x3x231x231| Library                  | Class                                                                                                                    | Time (ms)         | forward (ms)            | backward (ms)            ||:------------------------:|:------------------------------------------------------------------------------------------------------------------------:| -----------------:| -----------------------:| ------------------------:|| Nervana-neon-fp16          |                                  |         176       |  58                    |   118                    || Nervana-neon-fp32            |                                |         211       |  69                    |   141                    || CuDNN[R4]-fp16  (Torch)      |          |         242       |  86                    |  156             || CuDNN[R4]-fp32  (Torch)      |              |         268       |  94                    |   174                    || TensorFlow               |                             |         279       |  90                    |   189                    || fbfft  (Torch)                   |                              |         342       |  114                    |   227                    || Chainer                 |                |         620       |  135                    |   484                    || cudaconvnet2*            |                   |         723       |  176                    |   547                    || CuDNN[R2] *             |                   |         810       |  234                    |   576                    || Caffe                    |                              |         823       |  355                    |   468                    || Torch-7 (native)         |                                 |         878       |  379                    |   499                    || CL-nn (Torch)            |                           |         963       |  388                    |   574                    || Caffe-CLGreenTea         |                                                                      |      2857   | 616          |   2240         |[<marko.inline.RawText object at 0x000001592FEFA508>] - Input 64x3x224x224| Library                  | Class                                                                                                                    | Time (ms)         | forward (ms)            | backward (ms)            ||:------------------------:|:------------------------------------------------------------------------------------------------------------------------:| -----------------:| -----------------------:| ------------------------:|| Nervana-neon-fp16    |                                  |    254        |  82                |   171                || Nervana-neon-fp32        |                                  |        320        |  103                    |   217                    || CuDNN[R4]-fp16  (Torch)  |            |       471         |  140                    |   331                    || CuDNN[R4]-fp32  (Torch)     |                     |       529         |  162                    |   366                    || TensorFlow               |                                |      540         |  158                    |   382                    || Chainer                 |                     |    885 | 251 | 632 || fbfft    (Torch)                 |                                         |       1092        |  355                    |   737                    || cudaconvnet2*            |                      |       1229        |  408                    |   821                    || CuDNN[R2] *             |                     |       1099        |  342                    |   757                    || Caffe                    |                              |       1068        |  323                    |   745                    || Torch-7 (native)         |                                 |       1105        |  350                    |   755                    || CL-nn (Torch)            |                           |       3437        |  875                    |   2562                   || Caffe-CLGreenTea         |              |      5620   | 988          |   4632         |[<marko.inline.RawText object at 0x000001592FE3FF08>] - Input 128x3x224x224| Library                  | Class                                                                                                                    | Time (ms)         | forward (ms)            | backward (ms)            ||:------------------------:|:------------------------------------------------------------------------------------------------------------------------:| -----------------:| -----------------------:| ------------------------:|| Nervana-neon-fp16    |                                  |    230        |  72                 |   157                || Nervana-neon-fp32        |                                  |        270        |  84                     |   186                    || TensorFlow               |                                |      445         |  135                    |   310                    || CuDNN[R4]-fp16   (Torch)     |                     |       462         |  112                    |   349                    || CuDNN[R4]-fp32  (Torch)      |                     |       470         |  130                    |   340                    || Chainer                 |                |    687            |               189      |   497                       || Caffe                    |                              |       1935        |  786                    |   1148                   || CL-nn (Torch)            |                           |       7016        |  3027                   |   3988                   || Caffe-CLGreenTea         |                                                                      |      9462   | 746          |   8716         |Layer-wise Benchmarking (Last Updated April 2015)Spatial Convolution layer (3D input 3D output, densely connected)forward + backprop (wrt input and weights)| Original Library         | Class/Function Benchmarked                                                                                               | Time (ms)         | forward (ms)            | backward (ms)            ||:------------------------:|:------------------------------------------------------------------------------------------------------------------------:| -----------------:| -----------------------:| ------------------------:|| fbfft                |                                         |  256          |  101                | 155                  || cuda-convnet2 *          |                      | 977               |  201                    | 776                      || cuda-convnet**           |    | 1077              |  312                    | 765                      || CuDNN R2 *               |                     | 1019              |  269                    | 750                      || Theano                   | CorrMM                                                                                                                   | 1225              |  407                    | 818                      || Caffe                    |                              | 1231              |  396                    |   835                    || Torch-7                  |                                 | 1265              |  418                    | 877                      || DeepCL                   |                          |  6280             |  2648                   | 3632                     || cherry-picking****     | best per layer                                                                                                         | 235             |  79                   |   155                  |This table is [<marko.inline.RawText object at 0x000001592FE2F5C8>]. These numbers below were on Titan Black and are here only for informational and legacy purposes.| Original Library         | Class/Function Benchmarked | Time (ms)         | forward (ms)            | backward (ms)            ||:------------------------:|:------------------------------------------------------------------------------------------------------------------------:| -----------------:| -----------------------:| ------------------------:|| Theano (experimental)*** |                                 | 1178          |  304                | 874                  || Torch-7                  |                 | 1892              |  581                    | 1311                     || ccv                      |                                  | 809+bw            |  809                    |                          || Theano (legacy)          |                                   | 70774             |  3833                   | 66941                    |BreakdownforwardColumns L1, L2, L3, L4, L5, Total are times in milliseconds| Original Library         | Class/Function Benchmarked                                                                                                        |  L1 |   L2 |  L3 | L4 |  L5 | Total ||:------------------------:|:---------------------------------------------------------------------------------------------------------------------------------:| ---:| ----:| ---:| --:| ---:| -----:|| fbfft                    |                                                    | 57 |  27 |   6 |  2 |  9 | 101 || cuda-convnet2 *          |                               | 36 | 113 |  40 |  4 |  8 | 201 || cuda-convnet**           |             | 38 | 183 |  68 |  7 | 16 | 312 || CuDNN R2                 |                              | 56 | 143 |  53 |  6 | 11 | 269 || Theano                   | CorrMM                                                                                                                            | 91 | 143 | 121 | 24 | 28 | 407 || Caffe                    |                               | 93 | 136 | 116 | 24 | 27 | 396 || Torch-7                  |                                       | 94 | 149 | 123 | 24 | 28 | 418 || DeepCL                   |                                   | 738| 1241 | 518| 47 |104 |2648 || cherry-picking****     | best per layer                                                                                                                  |36|27 |  6| 2| 8|  79 |backward (gradInput + gradWeight)Columns L1, L2, L3, L4, L5, Total are times in milliseconds| Original Library         | Class/Function Benchmarked                                                                                                        |  L1 | L2  |  L3 | L4 |  L5| Total ||:------------------------:|:---------------------------------------------------------------------------------------------------------------------------------:| ---:| ---:| ---:| --:| --:| -----:|| fbfft                    |                                                    |  76 |  45 |  12 |  4 | 18 | 155   || cuda-convnet2 *          |                               | 103 | 467 | 162 | 15 | 29 | 776   || cuda-convnet**           |             | 136 | 433 | 147 | 15 | 34 | 765   || CuDNN R2                 |                              | 139 | 401 | 159 | 19 | 32 | 750   || Theano                   | CorrMM                                                                                                                            | 179 | 405 | 174 | 29 | 31 | 818   || Caffe                    |                               | 200 | 405 | 172 | 28 | 30 | 835   || Torch-7                  |                                       | 206 | 432 | 178 | 29 | 32 | 877   || DeepCL                   |                                   | 484 |2144 | 747 | 59 |198 |  3632 || cherry-picking****     | best per layer                                                                                                                  | 76| 45| 12|4 |18|155  |"
https://github.com/giampaolo/psutil,Cross-platform lib for process and system monitoring in Python,"|  |downloads| |stars| |forks| |contributors| |coverage||  |version| |py-versions| |packages| |license||  |github-actions-wheels|  |github-actions-bsd| |appveyor| |doc| |twitter| |tidelift|.. |downloads| image:: https://img.shields.io/pypi/dm/psutil.svg:target: https://pepy.tech/project/psutil:alt: Downloads.. |stars| image:: https://img.shields.io/github/stars/giampaolo/psutil.svg:target: https://github.com/giampaolo/psutil/stargazers:alt: Github stars.. |forks| image:: https://img.shields.io/github/forks/giampaolo/psutil.svg:target: https://github.com/giampaolo/psutil/network/members:alt: Github forks.. |contributors| image:: https://img.shields.io/github/contributors/giampaolo/psutil.svg:target: https://github.com/giampaolo/psutil/graphs/contributors:alt: Contributors.. |github-actions-wheels| image:: https://img.shields.io/github/actions/workflow/status/giampaolo/psutil/.github/workflows/build.yml?label=Linux%2C%20macOS%2C%20Windows:target: https://github.com/giampaolo/psutil/actions?query=workflow%3Abuild:alt: Linux, macOS, Windows.. |github-actions-bsd| image:: https://img.shields.io/github/actions/workflow/status/giampaolo/psutil/.github/workflows/bsd.yml?label=FreeBSD,%20NetBSD,%20OpenBSD:target: https://github.com/giampaolo/psutil/actions?query=workflow%3Absd-tests:alt: FreeBSD, NetBSD, OpenBSD.. |appveyor| image:: https://img.shields.io/appveyor/build/giampaolo/psutil/master.svg?maxAge=3600&label=Windows%20(py2):target: https://ci.appveyor.com/project/giampaolo/psutil:alt: Windows (Appveyor).. |coverage| image:: https://coveralls.io/repos/github/giampaolo/psutil/badge.svg?branch=master:target: https://coveralls.io/github/giampaolo/psutil?branch=master:alt: Test coverage (coverall.io).. |doc| image:: https://readthedocs.org/projects/psutil/badge/?version=latest:target: https://psutil.readthedocs.io/en/latest/:alt: Documentation Status.. |version| image:: https://img.shields.io/pypi/v/psutil.svg?label=pypi:target: https://pypi.org/project/psutil:alt: Latest version.. |py-versions| image:: https://img.shields.io/pypi/pyversions/psutil.svg:alt: Supported Python versions.. |packages| image:: https://repology.org/badge/tiny-repos/python:psutil.svg:target: https://repology.org/metapackage/python:psutil/versions:alt: Binary packages.. |license| image:: https://img.shields.io/pypi/l/psutil.svg:target: https://github.com/giampaolo/psutil/blob/master/LICENSE:alt: License.. |twitter| image:: https://img.shields.io/twitter/follow/grodola.svg?label=follow&style=flat&logo=twitter&logoColor=4FADFF:target: https://twitter.com/grodola:alt: Twitter Follow.. |tidelift| image:: https://tidelift.com/badges/github/giampaolo/psutil?style=flat:target: https://tidelift.com/subscription/pkg/pypi-psutil?utm_source=pypi-psutil&utm_medium=referral&utm_campaign=readme:alt: Tidelift.. raw:: html<div align=""center"">
    <a href=""https://github.com/giampaolo/psutil""><img src=""https://github.com/giampaolo/psutil/raw/master/docs/_static/psutil-logo.png"" /></a>
    <br />
    <br />
    <a href=""https://github.com/giampaolo/psutil""><b>Home</b></a>&nbsp;&nbsp;&nbsp;
    <a href=""https://github.com/giampaolo/psutil/blob/master/INSTALL.rst""><b>Install</b></a>&nbsp;&nbsp;&nbsp;
    <a href=""https://psutil.readthedocs.io/""><b>Documentation</b></a>&nbsp;&nbsp;&nbsp;
    <a href=""https://pypi.org/project/psutil/#files""><b>Download</b></a>&nbsp;&nbsp;&nbsp;
    <a href=""https://groups.google.com/g/psutil""><b>Forum</b></a>&nbsp;&nbsp;&nbsp;
    <a href=""https://gmpy.dev/tags/psutil""><b>Blog</b></a>&nbsp;&nbsp;&nbsp;
    <a href=""#funding""><b>Funding</b></a>&nbsp;&nbsp;&nbsp;
    <a href=""https://github.com/giampaolo/psutil/blob/master/HISTORY.rst""><b>What's new</b></a>&nbsp;&nbsp;&nbsp;
</div>
Summarypsutil (process and system utilities) is a cross-platform library forretrieving information on running processes and system utilization(CPU, memory, disks, network, sensors) in Python.It is useful mainly for system monitoring, profiling and limiting process and management of running processes.It implements many functionalities offered by classic UNIX command line toolssuch as ps, top, iotop, lsof, netstat, ifconfig, free and others.psutil currently supports the following platforms:Supported Python versions are 2.7, 3.6+ and__.FundingWhile psutil is free software and will always be, the project would benefitimmensely from some funding.Keeping up with bug reports and maintenance has become hardly sustainable forme alone in terms of time.If you're a company that's making significant use of psutil you can considerbecoming a sponsor via , or__and have your logo displayed in here and psutil __.Sponsors.. raw:: html<div>
    <a href=""https://tidelift.com/subscription/pkg/pypi-psutil?utm_source=pypi-psutil&utm_medium=referral&utm_campaign=readme"">
        <img width=""185"" src=""https://github.com/giampaolo/psutil/raw/master/docs/_static/tidelift-logo.svg"" />
    </a>
    &nbsp;&nbsp
    <a href=""https://sansec.io/"">
        <img src=""https://sansec.io/assets/images/logo.svg"" />
    </a>
</div>
<sup><a href=""https://github.com/sponsors/giampaolo"">add your logo</a></sup>
Supporters.. raw:: html<div>
  <a href=""https://github.com/dbwiddis""><img height=""40"" width=""40"" title=""Daniel Widdis"" src=""https://avatars1.githubusercontent.com/u/9291703?s=88&amp;v=4"" /></a>
  <a href=""https://github.com/aristocratos""><img height=""40"" width=""40"" title=""aristocratos"" src=""https://avatars3.githubusercontent.com/u/59659483?s=96&amp;v=4"" /></a>
  <a href=""https://github.com/cybersecgeek""><img height=""40"" width=""40"" title=""cybersecgeek"" src=""https://avatars.githubusercontent.com/u/12847926?v=4"" /></a>
  <a href=""https://github.com/scoutapm-sponsorships""><img height=""40"" width=""40"" title=""scoutapm-sponsorships"" src=""https://avatars.githubusercontent.com/u/71095532?v=4"" /></a>
  <a href=""https://opencollective.com/chenyoo-hao""><img height=""40"" width=""40"" title=""Chenyoo Hao"" src=""https://images.opencollective.com/chenyoo-hao/avatar/40.png"" /></a>
  <a href=""https://opencollective.com/alexey-vazhnov""><img height=""40"" width=""40"" title=""Alexey Vazhnov"" src=""https://images.opencollective.com/alexey-vazhnov/daed334/avatar/40.png"" /></a>
  <a href=""https://github.com/indeedeng""><img height=""40"" width=""40"" title=""indeedeng"" src=""https://avatars.githubusercontent.com/u/2905043?s=200&v=4"" /></a>
  <a href=""https://github.com/PySimpleGUI""><img height=""40"" width=""40"" title=""PySimpleGUI"" src=""https://avatars.githubusercontent.com/u/46163555?v=4"" /></a>
  <a href=""https://github.com/u93""><img height=""40"" width=""40"" title=""Eugenio E Breijo"" src=""https://avatars.githubusercontent.com/u/16807302?v=4"" /></a>
  <a href=""https://github.com/guilt""><img height=""40"" width=""40"" title=""Karthik Kumar Viswanathan"" src=""https://avatars.githubusercontent.com/u/195178?v=4"" /></a>
  <a href=""https://github.com/eallrich""><img height=""40"" width=""40"" title=""Evan Allrich"" src=""https://avatars.githubusercontent.com/u/17393?v=4"" /></a>
  <a href=""https://github.com/robusta-dev""><img height=""40"" width=""40"" title=""Robusta"" src=""https://avatars.githubusercontent.com/u/82757710?s=200&v=4"" /></a>
  <a href=""https://github.com/JeremyGrosser""><img height=""40"" width=""40"" title=""JeremyGrosser"" src=""https://avatars.githubusercontent.com/u/2151?v=4"" /></a>
  <a href=""https://github.com/getsentry""><img height=""40"" width=""40"" title=""getsentry"" src=""https://avatars.githubusercontent.com/u/1396951?s=200&v=4"" /></a>

</div>
<sup><a href=""https://github.com/sponsors/giampaolo"">add your avatar</a></sup>
ContributingSee __.Example usagesThis represents pretty much the whole psutil API.CPU.. code-block:: python>>> import psutil
>>>
>>> psutil.cpu_times()
scputimes(user=3961.46, nice=169.729, system=2150.659, idle=16900.540, iowait=629.59, irq=0.0, softirq=19.42, steal=0.0, guest=0, nice=0.0)
>>>
>>> for x in range(3):
...     psutil.cpu_percent(interval=1)
...
4.0
5.9
3.8
>>>
>>> for x in range(3):
...     psutil.cpu_percent(interval=1, percpu=True)
...
[4.0, 6.9, 3.7, 9.2]
[7.0, 8.5, 2.4, 2.1]
[1.2, 9.0, 9.9, 7.2]
>>>
>>> for x in range(3):
...     psutil.cpu_times_percent(interval=1, percpu=False)
...
scputimes(user=1.5, nice=0.0, system=0.5, idle=96.5, iowait=1.5, irq=0.0, softirq=0.0, steal=0.0, guest=0.0, guest_nice=0.0)
scputimes(user=1.0, nice=0.0, system=0.0, idle=99.0, iowait=0.0, irq=0.0, softirq=0.0, steal=0.0, guest=0.0, guest_nice=0.0)
scputimes(user=2.0, nice=0.0, system=0.0, idle=98.0, iowait=0.0, irq=0.0, softirq=0.0, steal=0.0, guest=0.0, guest_nice=0.0)
>>>
>>> psutil.cpu_count()
4
>>> psutil.cpu_count(logical=False)
2
>>>
>>> psutil.cpu_stats()
scpustats(ctx_switches=20455687, interrupts=6598984, soft_interrupts=2134212, syscalls=0)
>>>
>>> psutil.cpu_freq()
scpufreq(current=931.42925, min=800.0, max=3500.0)
>>>
>>> psutil.getloadavg()  # also on Windows (emulated)
(3.14, 3.89, 4.67)
Memory.. code-block:: python>>> psutil.virtual_memory()
svmem(total=10367352832, available=6472179712, percent=37.6, used=8186245120, free=2181107712, active=4748992512, inactive=2758115328, buffers=790724608, cached=3500347392, shared=787554304)
>>> psutil.swap_memory()
sswap(total=2097147904, used=296128512, free=1801019392, percent=14.1, sin=304193536, sout=677842944)
>>>
Disks.. code-block:: python>>> psutil.disk_partitions()
[sdiskpart(device='/dev/sda1', mountpoint='/', fstype='ext4', opts='rw,nosuid', maxfile=255, maxpath=4096),
 sdiskpart(device='/dev/sda2', mountpoint='/home', fstype='ext', opts='rw', maxfile=255, maxpath=4096)]
>>>
>>> psutil.disk_usage('/')
sdiskusage(total=21378641920, used=4809781248, free=15482871808, percent=22.5)
>>>
>>> psutil.disk_io_counters(perdisk=False)
sdiskio(read_count=719566, write_count=1082197, read_bytes=18626220032, write_bytes=24081764352, read_time=5023392, write_time=63199568, read_merged_count=619166, write_merged_count=812396, busy_time=4523412)
>>>
Network.. code-block:: python>>> psutil.net_io_counters(pernic=True)
{'eth0': netio(bytes_sent=485291293, bytes_recv=6004858642, packets_sent=3251564, packets_recv=4787798, errin=0, errout=0, dropin=0, dropout=0),
 'lo': netio(bytes_sent=2838627, bytes_recv=2838627, packets_sent=30567, packets_recv=30567, errin=0, errout=0, dropin=0, dropout=0)}
>>>
>>> psutil.net_connections(kind='tcp')
[sconn(fd=115, family=<AddressFamily.AF_INET: 2>, type=<SocketType.SOCK_STREAM: 1>, laddr=addr(ip='10.0.0.1', port=48776), raddr=addr(ip='93.186.135.91', port=80), status='ESTABLISHED', pid=1254),
 sconn(fd=117, family=<AddressFamily.AF_INET: 2>, type=<SocketType.SOCK_STREAM: 1>, laddr=addr(ip='10.0.0.1', port=43761), raddr=addr(ip='72.14.234.100', port=80), status='CLOSING', pid=2987),
 ...]
>>>
>>> psutil.net_if_addrs()
{'lo': [snicaddr(family=<AddressFamily.AF_INET: 2>, address='127.0.0.1', netmask='255.0.0.0', broadcast='127.0.0.1', ptp=None),
        snicaddr(family=<AddressFamily.AF_INET6: 10>, address='::1', netmask='ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff', broadcast=None, ptp=None),
        snicaddr(family=<AddressFamily.AF_LINK: 17>, address='00:00:00:00:00:00', netmask=None, broadcast='00:00:00:00:00:00', ptp=None)],
 'wlan0': [snicaddr(family=<AddressFamily.AF_INET: 2>, address='192.168.1.3', netmask='255.255.255.0', broadcast='192.168.1.255', ptp=None),
           snicaddr(family=<AddressFamily.AF_INET6: 10>, address='fe80::c685:8ff:fe45:641%wlan0', netmask='ffff:ffff:ffff:ffff::', broadcast=None, ptp=None),
           snicaddr(family=<AddressFamily.AF_LINK: 17>, address='c4:85:08:45:06:41', netmask=None, broadcast='ff:ff:ff:ff:ff:ff', ptp=None)]}
>>>
>>> psutil.net_if_stats()
{'lo': snicstats(isup=True, duplex=<NicDuplex.NIC_DUPLEX_UNKNOWN: 0>, speed=0, mtu=65536, flags='up,loopback,running'),
 'wlan0': snicstats(isup=True, duplex=<NicDuplex.NIC_DUPLEX_FULL: 2>, speed=100, mtu=1500, flags='up,broadcast,running,multicast')}
>>>
Sensors.. code-block:: python>>> import psutil
>>> psutil.sensors_temperatures()
{'acpitz': [shwtemp(label='', current=47.0, high=103.0, critical=103.0)],
 'asus': [shwtemp(label='', current=47.0, high=None, critical=None)],
 'coretemp': [shwtemp(label='Physical id 0', current=52.0, high=100.0, critical=100.0),
              shwtemp(label='Core 0', current=45.0, high=100.0, critical=100.0)]}
>>>
>>> psutil.sensors_fans()
{'asus': [sfan(label='cpu_fan', current=3200)]}
>>>
>>> psutil.sensors_battery()
sbattery(percent=93, secsleft=16628, power_plugged=False)
>>>
Other system info.. code-block:: python>>> import psutil
>>> psutil.users()
[suser(name='giampaolo', terminal='pts/2', host='localhost', started=1340737536.0, pid=1352),
 suser(name='giampaolo', terminal='pts/3', host='localhost', started=1340737792.0, pid=1788)]
>>>
>>> psutil.boot_time()
1365519115.0
>>>
Process management.. code-block:: python>>> import psutil
>>> psutil.pids()
[1, 2, 3, 4, 5, 6, 7, 46, 48, 50, 51, 178, 182, 222, 223, 224, 268, 1215,
 1216, 1220, 1221, 1243, 1244, 1301, 1601, 2237, 2355, 2637, 2774, 3932,
 4176, 4177, 4185, 4187, 4189, 4225, 4243, 4245, 4263, 4282, 4306, 4311,
 4312, 4313, 4314, 4337, 4339, 4357, 4358, 4363, 4383, 4395, 4408, 4433,
 4443, 4445, 4446, 5167, 5234, 5235, 5252, 5318, 5424, 5644, 6987, 7054,
 7055, 7071]
>>>
>>> p = psutil.Process(7055)
>>> p
psutil.Process(pid=7055, name='python3', status='running', started='09:04:44')
>>> p.pid
7055
>>> p.name()
'python3'
>>> p.exe()
'/usr/bin/python3'
>>> p.cwd()
'/home/giampaolo'
>>> p.cmdline()
['/usr/bin/python3', 'main.py']
>>>
>>> p.ppid()
7054
>>> p.parent()
psutil.Process(pid=4699, name='bash', status='sleeping', started='09:06:44')
>>> p.parents()
[psutil.Process(pid=4699, name='bash', started='09:06:44'),
 psutil.Process(pid=4689, name='gnome-terminal-server', status='sleeping', started='0:06:44'),
 psutil.Process(pid=1, name='systemd', status='sleeping', started='05:56:55')]
>>> p.children(recursive=True)
[psutil.Process(pid=29835, name='python3', status='sleeping', started='11:45:38'),
 psutil.Process(pid=29836, name='python3', status='waking', started='11:43:39')]
>>>
>>> p.status()
'running'
>>> p.create_time()
1267551141.5019531
>>> p.terminal()
'/dev/pts/0'
>>>
>>> p.username()
'giampaolo'
>>> p.uids()
puids(real=1000, effective=1000, saved=1000)
>>> p.gids()
pgids(real=1000, effective=1000, saved=1000)
>>>
>>> p.cpu_times()
pcputimes(user=1.02, system=0.31, children_user=0.32, children_system=0.1, iowait=0.0)
>>> p.cpu_percent(interval=1.0)
12.1
>>> p.cpu_affinity()
[0, 1, 2, 3]
>>> p.cpu_affinity([0, 1])  # set
>>> p.cpu_num()
1
>>>
>>> p.memory_info()
pmem(rss=10915840, vms=67608576, shared=3313664, text=2310144, lib=0, data=7262208, dirty=0)
>>> p.memory_full_info()  # ""real"" USS memory usage (Linux, macOS, Win only)
pfullmem(rss=10199040, vms=52133888, shared=3887104, text=2867200, lib=0, data=5967872, dirty=0, uss=6545408, pss=6872064, swap=0)
>>> p.memory_percent()
0.7823
>>> p.memory_maps()
[pmmap_grouped(path='/lib/x8664-linux-gnu/libutil-2.15.so', rss=32768, size=2125824, pss=32768, shared_clean=0, shared_dirty=0, private_clean=20480, private_dirty=12288, referenced=32768, anonymous=12288, swap=0),
 pmmap_grouped(path='/lib/x8664-linux-gnu/libc-2.15.so', rss=3821568, size=3842048, pss=3821568, shared_clean=0, shared_dirty=0, private_clean=0, private_dirty=3821568, referenced=3575808, anonymous=3821568, swap=0),
 pmmap_grouped(path='[heap]',  rss=32768, size=139264, pss=32768, shared_clean=0, shared_dirty=0, private_clean=0, private_dirty=32768, referenced=32768, anonymous=32768, swap=0),
 pmmap_grouped(path='[stack]', rss=2465792, size=2494464, pss=2465792, shared_clean=0, shared_dirty=0, private_clean=0, private_dirty=2465792, referenced=2277376, anonymous=2465792, swap=0),
 ...]
>>>
>>> p.io_counters()
pio(read_count=478001, write_count=59371, read_bytes=700416, write_bytes=69632, read_chars=456232, write_chars=517543)
>>>
>>> p.open_files()
[popenfile(path='/home/giampaolo/monit.py', fd=3, position=0, mode='r', flags=32768),
 popenfile(path='/var/log/monit.log', fd=4, position=235542, mode='a', flags=33793)]
>>>
>>> p.connections(kind='tcp')
[pconn(fd=115, family=<AddressFamily.AF_INET: 2>, type=<SocketType.SOCK_STREAM: 1>, laddr=addr(ip='10.0.0.1', port=48776), raddr=addr(ip='93.186.135.91', port=80), status='ESTABLISHED'),
 pconn(fd=117, family=<AddressFamily.AF_INET: 2>, type=<SocketType.SOCK_STREAM: 1>, laddr=addr(ip='10.0.0.1', port=43761), raddr=addr(ip='72.14.234.100', port=80), status='CLOSING')]
>>>
>>> p.threads()
[pthread(id=5234, user_time=22.5, system_time=9.2891),
 pthread(id=5237, user_time=0.0707, system_time=1.1)]
>>>
>>> p.num_threads()
4
>>> p.num_fds()
8
>>> p.num_ctx_switches()
pctxsw(voluntary=78, involuntary=19)
>>>
>>> p.nice()
0
>>> p.nice(10)  # set
>>>
>>> p.ionice(psutil.IOPRIO_CLASS_IDLE)  # IO priority (Win and Linux only)
>>> p.ionice()
pionice(ioclass=<IOPriority.IOPRIO_CLASS_IDLE: 3>, value=0)
>>>
>>> p.rlimit(psutil.RLIMIT_NOFILE, (5, 5))  # set resource limits (Linux only)
>>> p.rlimit(psutil.RLIMIT_NOFILE)
(5, 5)
>>>
>>> p.environ()
{'LC_PAPER': 'it_IT.UTF-8', 'SHELL': '/bin/bash', 'GREP_OPTIONS': '--color=auto',
'XDG_CONFIG_DIRS': '/etc/xdg/xdg-ubuntu:/usr/share/upstart/xdg:/etc/xdg',
 ...}
>>>
>>> p.as_dict()
{'status': 'running', 'num_ctx_switches': pctxsw(voluntary=63, involuntary=1), 'pid': 5457, ...}
>>> p.is_running()
True
>>> p.suspend()
>>> p.resume()
>>>
>>> p.terminate()
>>> p.kill()
>>> p.wait(timeout=3)
<Exitcode.EX_OK: 0>
>>>
>>> psutil.test()
USER         PID %CPU %MEM     VSZ     RSS TTY        START    TIME  COMMAND
root           1  0.0  0.0   24584    2240            Jun17   00:00  init
root           2  0.0  0.0       0       0            Jun17   00:00  kthreadd
...
giampaolo  31475  0.0  0.0   20760    3024 /dev/pts/0 Jun19   00:00  python2.4
giampaolo  31721  0.0  2.2  773060  181896            00:04   10:30  chrome
root       31763  0.0  0.0       0       0            00:05   00:00  kworker/0:1
>>>
Further process APIs.. code-block:: python>>> import psutil
>>> for proc in psutil.process_iter(['pid', 'name']):
...     print(proc.info)
...
{'pid': 1, 'name': 'systemd'}
{'pid': 2, 'name': 'kthreadd'}
{'pid': 3, 'name': 'ksoftirqd/0'}
...
>>>
>>> psutil.pid_exists(3)
True
>>>
>>> def on_terminate(proc):
...     print(""process {} terminated"".format(proc))
...
>>> # waits for multiple processes to terminate
>>> gone, alive = psutil.wait_procs(procs_list, timeout=3, callback=on_terminate)
>>>
Windows services.. code-block:: python>>> list(psutil.win_service_iter())
[<WindowsService(name='AeLookupSvc', display_name='Application Experience') at 38850096>,
 <WindowsService(name='ALG', display_name='Application Layer Gateway Service') at 38850128>,
 <WindowsService(name='APNMCP', display_name='Ask Update Service') at 38850160>,
 <WindowsService(name='AppIDSvc', display_name='Application Identity') at 38850192>,
 ...]
>>> s = psutil.win_service_get('alg')
>>> s.as_dict()
{'binpath': 'C:\\Windows\\System32\\alg.exe',
 'description': 'Provides support for 3rd party protocol plug-ins for Internet Connection Sharing',
 'display_name': 'Application Layer Gateway Service',
 'name': 'alg',
 'pid': None,
 'start_type': 'manual',
 'status': 'stopped',
 'username': 'NT AUTHORITY\\LocalService'}
Projects using psutilHere's some I find particularly interesting:Portings"
https://github.com/miguelgrinberg/microblog,A microblogging web application written in Python and Flask that I developed as part of my Flask Mega-Tutorial series.,Welcome to Microblog!This is an example application featured in my . See the tutorial for instructions on how to work with it.
https://github.com/nabla-c0d3/sslyze,Fast and powerful SSL/TLS scanning library.,"SSLyzeSSLyze is a fast and powerful SSL/TLS scanning tool and Python library.SSLyze can analyze the SSL/TLS configuration of a server by connecting to it, in order to ensure that it uses strongencryption settings (certificate, cipher suites, elliptic curves, etc.), and that it is not vulnerable to known TLSattacks (Heartbleed, ROBOT, OpenSSL CCS injection, etc.).Key featuresQuick startOn Windows, Linux (x86 or x64) and macOS, SSLyze can be installed directly via pip:$ pip install --upgrade pip setuptools wheel
$ pip install --upgrade sslyze
$ python -m sslyze www.yahoo.com www.google.com ""[2607:f8b0:400a:807::2004]:443""
It can also be used via Docker:$ docker run --rm -it nablac0d3/sslyze:5.0.0 www.google.com
Lastly, a pre-compiled Windows executable can be downloaded from .Python API DocumentationDocumentation for SSLyze's Python API is .Usage as a CI/CD stepBy default, SSLyze will check the server's scan results against Mozilla's recommended , and will return a non-zero exit code if the serveris not compliant. $ python -m sslyze mozilla.com
Checking results against Mozilla's ""intermediate"" configuration. See https://ssl-config.mozilla.org/ for more details.

mozilla.com:443: OK - Compliant.
The Mozilla configuration to check against can be configured via :$ python -m sslyze --mozilla_config=modern mozilla.com
Checking results against Mozilla's ""modern"" configuration. See https://ssl-config.mozilla.org/ for more details.

mozilla.com:443: FAILED - Not compliant.
    * certificate_types: Deployed certificate types are {'rsa'}, should have at least one of {'ecdsa'}.
    * certificate_signatures: Deployed certificate signatures are {'sha256WithRSAEncryption'}, should have at least one of {'ecdsa-with-SHA512', 'ecdsa-with-SHA256', 'ecdsa-with-SHA384'}.
    * tls_versions: TLS versions {'TLSv1.2'} are supported, but should be rejected.
    * ciphers: Cipher suites {'TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384', 'TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256', 'TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256'} are supported, but should be rejected.
This can be used to easily run an SSLyze scan as a CI/CD step.Development environmentTo setup a development environment:$ pip install --upgrade pip setuptools wheel
$ pip install -e . 
$ pip install -r requirements-dev.txt
The tests can then be run using:$ invoke test
LicenseCopyright (c) 2023 Alban DiquetSSLyze is made available under the terms of the GNU Affero General Public License (AGPL). See LICENSE.txt for details and exceptions."
https://github.com/yosinski/deep-visualization-toolbox,DeepVis Toolbox,"Deep Visualization ToolboxThis is the code required to run the Deep Visualization Toolbox, as well as to generate the neuron-by-neuron visualizations using regularized optimization.The toolbox and methods are described casually  and more formally in this paper:If you find this paper or code useful, we encourage you to cite the paper. BibTeX:@inproceedings{yosinski-2015-ICML-DL-understanding-neural-networks,
Author = {Jason Yosinski and Jeff Clune and Anh Nguyen and Thomas Fuchs and Hod Lipson},
Booktitle = {Deep Learning Workshop, International Conference on Machine Learning (ICML)},
Title = {Understanding Neural Networks Through Deep Visualization},
Year = {2015}}
FeaturesThe main toolbox window looks like this, here showing a convolutional unit that responds to automobile wheels:For a quick tour of the toolbox features, including what each pane of the above interface is showing, watch this . In addition to processing images files from disk, the toolbox can run off a webcam for live network visualization (below left).The toolbox comes bundled with the default  model weights and pre-computed per-unit visualizations shown in the paper. Weights, but not per-unit visualizations, for  (below right) and  can be downloaded by scripts in their respective directories.You can visualize your own model as well. However, note that the toolbox provides two rather separate sets of features; the first is easy to use with your own model, and the second is more involved:Summary:| Model          | Forward/Backward prop | Per-unit visualizations || -------------  | ---------------- | ----------------------- ||  | easy             | included            ||     | easy             | not-included,  if desired            ||      | easy             | not-included,  if desired            || your network | easy (just point to your model in )  | not-included,  if desired  |Setting up and running the toolboxStep 0: Compile master branch of caffe (optional but recommended)Checkout the master branch of  and compile it on yourmachine. If you've never used Caffe before, it can take a bit of time to get all the required libraries in place. Fortunately, the . When you're installing the OpenCV dependency, install the Python bindings as well (see Step 2 below).Note: When compiling Caffe, you can set  in your  to skip all the Cuda/GPU stuff. The Deep Visualization Toolbox can run with Caffe in either CPU or GPU mode, and it's simpler to get Caffe to compile for the first time in  mode. If Caffe is compiled with GPU options enabled, CPU vs. GPU may be switched at runtime via a setting in . Also, cuDNN may be enabled or disabled by recompiling Caffe with or without cuDNN.Step 1: Compile the deconv-deep-vis-toolbox branch of caffeInstead of using the master branch of Caffe, to use the demoyou'll need the slightly modified  (supporting deconv and a fewextra Python bindings). Getting the branch and switching to it is easy.Starting from your Caffe directory (that is, the directory where you've checked out Caffe, not the directory where you've checked out the DeepVis Toolbox), run:$ git remote add yosinski https://github.com/yosinski/caffe.git
$ git fetch --all
$ git checkout --track -b deconv-deep-vis-toolbox yosinski/deconv-deep-vis-toolbox
$ < edit Makefile.config to suit your system if not already done in Step 0 >
$ make clean
$ make -j
$ make -j pycaffe
As noted above, feel free to compile in  mode.Step 2: Install prerequisitesThe only prerequisites beyond those required for Caffe are , , and , which may be installed as follows (other install options exist as well):Ubuntu:$ sudo apt-get install python-opencv scipy python-skimage
Mac using :Install  using one of the following two lines, depending on whether you want to compile using Intel TBB to enable parallel operations:$ brew install opencv
$ brew install --with-tbb opencv
Install  either with OpenBLAS...$ brew install openblas
$ brew install --with-openblas scipy
...or without it$ brew install scipy
And install  using pip:$ pip install scikit-image
You may have already installed the  bindings as part of the Caffe setup process. If  works from Python, then you're all set. Similarly for  and .Step 3: Download and configure Deep Visualization Toolbox codeYou can put it wherever you like:$ git clone https://github.com/yosinski/deep-visualization-toolbox
$ cd deep-visualization-toolbox
The settings in the latest version of the toolbox (February 2016) work a bit differently than in earlier versions (April 2015). If you have the latest version (recommended!),the minimal steps are to create a  file using the template for the default  model:$ cp models/caffenet-yos/settings_local.template-caffenet-yos.py settings_local.py
And then edit the  file to make the  variable point to the directory where you've compiled caffe in Step 1:$ < edit settings_local.py >
Note on settings: Settings are now split into two files: a versioned  file that provides documentation and default values for all settings and an unversioned  file. This latter file allows you to override any default setting to tailor the toolbox to your specific setup (Caffe path, CPU vs. GPU, webcam device, etc) and model (model weights, prototxt, sizes of the various panels shown in the toolbox, etc). This also makes it easy to distribute settings tweaks alongside models: for example,  includes the appropriate window pane sizes and so on for the  model. To load a new model, just change the details in , perhaps by copying from the included template.Finally, download the default model weights and corresponding top-9 visualizations saved as jpg (downloads a 230MB model and 1.1GB of jpgs to show as visualization):$ cd models/caffenet-yos/
$ ./fetch.sh
$ cd ../..
Step 4: Run it!Simple:$ ./run_toolbox.py
Once the toolbox is running, push 'h' to show a help screen. You can also have a look at  to see what the various keys do. If the window is too large or too small for your screen, set the  and  variables in  to values smaller or larger than 1.0.TroubleshootingIf you have any problems running the Deep Vis Toolbox, here are a few things to try:Other ways of running the toolboxIf running the toolbox on a local Mac or Linux machine isn't working for you, you might want to try one of these other options:"
https://github.com/TheRook/subbrute,"A DNS meta-query spider that enumerates DNS records, and subdomains.","subdomain-bruteforcer (SubBrute)SubBrute is a community driven project with the goal of creating the fastest, and most accurate subdomain enumeration tool.  Some of the magic behind SubBrute is that it uses open resolvers as a kind of proxy to circumvent DNS rate-limiting (https://www.us-cert.gov/ncas/alerts/TA13-088A).  This design also provides a layer of anonymity, as SubBrute does not send traffic directly to the target's name servers.Whats new in v2.1?Better stablity. Better support for testing cloudflare domains.Thank you for the bug posts!Whats new in v1.2.1?The big news in this version is that SubBrute is now a recursive DNS-spider, and also a library,  more on this later. SubBrute should be easy to use, so the interface should be intuitive (like nmap!), if you would like the interface to change,  let us know.  In this version we are opening up SubBrute's fast DNS resolution pipeline for any DNS record type. Additionally, SubBrute now has a feature to detect subdomains were their resolution is intentionally blocked, which sometimes happens when a subdomain is intended for for use on an internal network.	./subbrute.py google.com -o google.names
		...162 subdomains found...

	./subbrute.py -s google.names google.com --type TXT
		google.com,""v=spf1 include:_spf.google.com ip4:216.73.93.70/31 ip4:216.73.93.72/31 ~all""
		adwords.google.com,""v=spf1 redirect=google.com""
		...

	./subbrute.py -s google.names google.com --type CNAME
		blog.google.com,www.blogger.com,blogger.l.google.com
		groups.google.com,groups.l.google.com
		...
	import subbrute

	for d in subbrute.run(""google.com""):
		print d 
Feedback welcome.Whats new in v1.1?This version merges pull requests from the community; changes from JordanMilne, KxCode and rc0r is in this release.  In SubBrute 1.1 we fixed bugs, improved  accuracy, and efficiency.  As requested, this project is now GPLv3.Accuracy and better wildcard detection:Faster:New output:More Informationnames.txt contains 101,010 subdomains.  subs_small.txt was stolen from fierce2 which contains 1896 subdomains.   If you find more subdomains to add,  open a bug report or pull request and I'll be happy to add them.No install required for Windows,  just cd into the 'windows' folder:Easy to install:You just need http://www.dnspython.org/ and python2.7 or python3.  This tool should work under any operating system:  bsd, osx, windows, linux...(On a side note giving a makefile root always bothers me,  it would be a great way to install a backdoor...)Under Ubuntu/Debian all you need is:On other operating systems you may have to install dnspython manually:http://www.dnspython.org/ Easy to use:Tests multiple domains:or a newline delimited list of domains:Also keep in mind that subdomains can have subdomains (example: _xmpp-server._tcp.gmail.com):Cheers!"
https://github.com/pallets/jinja,A very fast and expressive template engine.,"JinjaJinja is a fast, expressive, extensible templating engine. Specialplaceholders in the template allow writing code similar to Pythonsyntax. Then the template is passed data to render the final document.It includes:Jinja's philosophy is that while application logic belongs in Python ifpossible, it shouldn't make the template designer's job difficult byrestricting functionality too much.InstallingInstall and update using _:.. code-block:: text$ pip install -U Jinja2
.. _pip: https://pip.pypa.io/en/stable/getting-started/In A Nutshell.. code-block:: jinja{% extends ""base.html"" %}
{% block title %}Members{% endblock %}
{% block content %}
  <ul>
  {% for user in users %}
    <li><a href=""{{ user.url }}"">{{ user.username }}</a></li>
  {% endfor %}
  </ul>
{% endblock %}
DonateThe Pallets organization develops and supports Jinja and other popularpackages. In order to grow the community of contributors and users, andallow the maintainers to devote more time to the projects, _... _please donate today: https://palletsprojects.com/donateLinks"
https://github.com/bottlepy/bottle,bottle.py is a fast and simple micro-framework for python web-applications.,".. image:: http://bottlepy.org/docs/dev/_static/logo_nav.png:target: http://bottlepy.org/:alt: Bottle Logo:align: right.. image:: https://github.com/bottlepy/bottle/workflows/Tests/badge.svg:target: https://github.com/bottlepy/bottle/workflows/Tests:alt: Tests Status.. image:: https://img.shields.io/pypi/v/bottle.svg:target: https://pypi.python.org/pypi/bottle/:alt: Latest Version.. image:: https://img.shields.io/pypi/l/bottle.svg:target: https://pypi.python.org/pypi/bottle/:alt: License.. _mako: http://www.makotemplates.org/.. _cheetah: http://www.cheetahtemplate.org/.. _jinja2: http://jinja.pocoo.org/.. _paste: https://pythonpaste.readthedocs.io/.. _fapws3: https://github.com/william-os4y/fapws3.. _bjoern: https://github.com/jonashaag/bjoern.. _cherrypy: https://docs.cherrypy.dev/.. _WSGI: https://wsgi.readthedocs.io/.. _Python: http://python.org/============================Bottle: Python Web FrameworkBottle is a fast, simple and lightweight WSGI_ micro web-framework for Python_. It is distributed as a single file module and has no dependencies other than the _.Homepage and documentation: http://bottlepy.orgExample: ""Hello World"" in a bottle.. code-block:: pythonfrom bottle import route, run, template@route('/hello/')def index(name):return template('Hello {{name}}!', name=name)run(host='localhost', port=8080)Run this script or paste it into a Python console, then point your browser to _. That's it.Download and Install.. __: https://github.com/bottlepy/bottle/raw/master/bottle.pyInstall the latest stable release with  or download __ (unstable) into your project directory. There are no hard dependencies other than the Python standard library. Bottle runs with Python 2.7 and 3.6+.License.. __: https://github.com/bottlepy/bottle/raw/master/LICENSECode and documentation are available according to the MIT License (see LICENSE__).The Bottle logo however is NOT covered by that license. It is allowed to use the logo as a link to the bottle homepage or in direct context with the unmodified library. In all other cases, please ask first."
https://github.com/gelstudios/gitfiti,abusing github commit history for the lulz,"gitfiti noun : Carefully crafted graffiti in a github commit history calendar.  An example of gitfiti in the wild: is a tool to decorate your github account's commit history calendar by (blatantly) abusing git's ability to accept commits in the past.How?  generates a script (powershell or bash) that makes commits with the GIT_AUTHOR_DATE and GIT_COMMITTER_DATE environment variables set for each targeted pixel.Since this is likely to clobber repo's history, it is highly recommend that you create a new github repo when using gitfiti. Also, the generated script assumes you are using public-key authentication with git.Pixel ArtIncluded ""art"" from left to right: kitty, oneup, oneup2, hackerschool, octocat, octocat2UsageUser TemplatesThe file format for personal templates is the following:For example::center-blank
[[1,1,1,1,1,1,1],
[1,1,1,1,1,1,1],
[1,1,1,1,1,1,1],
[1,1,1,0,1,1,1],
[1,1,1,1,1,1,1],
[1,1,1,1,1,1,1],
[1,1,1,1,1,1,1]]
This would output a 7 x 7 light green square with a single blank center square.Once you have a file with templates, enter its name when prompted and the templates will be added to the list of options.RemovalFortunately if you regret your gitfiti in the morning, removing it is fairly easy: delete the repo you created for your gitfiti (and wait).Licensegitfiti is released under TodoNotable derivatives or mentions"
https://github.com/ipython/ipython,"Official repository for IPython itself. Other repos in the IPython organization contain things like the website, documentation builds, etc.",".. image:: https://codecov.io/github/ipython/ipython/coverage.svg?branch=main:target: https://codecov.io/github/ipython/ipython?branch=main.. image:: https://img.shields.io/pypi/v/IPython.svg:target: https://pypi.python.org/pypi/ipython.. image:: https://github.com/ipython/ipython/actions/workflows/test.yml/badge.svg:target: https://github.com/ipython/ipython/actions/workflows/test.yml.. image:: https://www.codetriage.com/ipython/ipython/badges/users.svg:target: https://www.codetriage.com/ipython/ipython/.. image:: https://raster.shields.io/badge/Follows-SPEC--0000-brightgreen.png:target: https://scientific-python.org/specs/spec-0000/.. image:: https://tidelift.com/badges/package/pypi/ipython?style=flat:target: https://tidelift.com/subscription/pkg/pypi-ipython===========================================IPython: Productive Interactive ComputingOverviewWelcome to IPython.  Our full documentation is available on _ and contains information on how to install, use, andcontribute to the project.IPython (Interactive Python) is a command shell for interactive computing in multiple programming languages, originally developed for the Python programming language, that offers introspection, rich media, shell syntax, tab completion, and history.IPython versions and Python SupportStarting after IPython 8.16, we will progressively transition to _.Starting with IPython 7.10, IPython follows _IPython 7.17+ requires Python version 3.7 and above.IPython 7.10+ requires Python version 3.6 and above.IPython 7.0 requires Python version 3.5 and above.IPython 6.x requires Python version 3.3 and above.IPython 5.x LTS is the compatible release for Python 2.7.If you require Python 2 support, you must use IPython 5.x LTS. Pleaseupdate your project configurations and requirements as necessary.The Notebook, Qt console and a number of other pieces are now parts of Jupyter.See the __if you want to use these.Main features of IPythonComprehensive object introspection.Input history, persistent across sessions.Caching of output results during a session with automatically generated references.Extensible tab completion, with support by default for completion of python variables and keywords, filenames and function keywords.Extensible system of ‘magic’ commands for controlling the environment and performing many tasks related to IPython or the operating system.A rich configuration system with easy switching between different setups (simpler than changing $PYTHONSTARTUP environment variables every time).Session logging and reloading.Extensible syntax processing for special purpose situations.Access to the system shell with user-extensible alias system.Easily embeddable in other Python programs and GUIs.Integrated access to the pdb debugger and the Python profiler.Development and Instant runningYou can find the latest version of the development documentation on _.You can run IPython from this directory without even installing it system-wideby typing at the terminal::$ python -m IPythonOr see the _for the latest revision on read the docs.Documentation and installation instructions for older version of IPython can befound on the _IPython requires Python version 3 or aboveStarting with version 6.0, IPython does not support Python 2.7, 3.0, 3.1, or3.2.For a version compatible with Python 2.7, please install the 5.x LTS Long TermSupport version.If you are encountering this error message you are likely trying to install oruse IPython from source. You need to checkout the remote 5.x branch. If you areusing git the following should work::$ git fetch origin$ git checkout 5.xIf you encounter this error message with a regular install of IPython, then youlikely need to update your package manager, for example if you are using check the version of pip with::$ pip --versionYou will need to update pip to the version 9.0.1 or greater. If you are not usingpip, please inquiry with the maintainers of the package for your packagemanager.For more information see one of our blog posts:https://blog.jupyter.org/release-of-ipython-5-0-8ce60b8d2e8e
As well as the following Pull-Request for discussion:https://github.com/ipython/ipython/pull/9900
This error does also occur if you are invoking  directly – which youshould not – or are using  If this is the case, use  instead of  , and  insteadof  If you are depending on IPython as a dependency you mayalso want to have a conditional dependency on IPython depending on the Pythonversion::install_req = ['ipython']
if sys.version_info[0] < 3 and 'bdist_wheel' not in sys.argv:
    install_req.remove('ipython')
    install_req.append('ipython<6')

setup(
    ...
    install_requires=install_req
)
Alternatives to IPythonIPython may not be to your taste; if that's the case there might be similarproject that you might want to use:Ignoring commits with git blame.ignoreRevsFileAs of git 2.23, it is possible to make formatting changes without breaking. See the _for more details.To use this feature you must:"
https://github.com/mkaz/termgraph,a python command-line tool which draws basic graphs in the terminal,"TermgraphA command-line tool that draws basic graphs in the terminal, written in Python.Graph types supported:Examplestermgraph data/ex1.dat

# Reading data from data/ex1.dat

2007: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 183.32
2008: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 231.23
2009: ▇ 16.43
2010: ▇▇▇▇ 50.21
2011: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 508.97
2012: ▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇ 212.05
2014: ▏ 1.00
An example using emoji as custom tick:termgraph data/ex1.dat --custom-tick ""🏃"" --width 20 --title ""Running Data""

# Running Data

2007: 🏃🏃🏃🏃🏃🏃🏃 183.32
2008: 🏃🏃🏃🏃🏃🏃🏃🏃🏃 231.23
2009:  16.43
2010: 🏃 50.21
2011: 🏃🏃🏃🏃🏃🏃🏃🏃🏃🏃🏃🏃🏃🏃🏃🏃🏃🏃🏃🏃 508.97
2012: 🏃🏃🏃🏃🏃🏃🏃🏃 212.05
2014:  1.00

An example using stdin and emoji:echo ""Label,3,9,1"" | termgraph --custom-tick ""😀"" --no-label


😀😀😀 3.00
😀😀😀😀😀😀😀😀😀 9.00
😀 1.00

Most results can be copied and pasted wherever you like, since they use standard block characters. However the color charts will not show, since they use terminal escape codes for color. A couple images to show color examples:termgraph data/ex4.dat --color {blue,red}
termgraph data/ex7.dat --color {yellow,magenta} --stacked --title ""Stacked Data""
Calendar Heatmap, expects first column to be date in yyyy-mm-ddtermgraph --calendar --start-dt 2017-07-01 data/cal.dat
InstallRequires Python 3.7+, install from python3 -m pip install termgraph
Note: Be sure your PATH includes the pypi install directory, for me it is Usageusage: termgraph.py [-h] [(optional arguments)] [filename]

draw basic graphs on terminal

positional arguments:
  filename              data file name (comma or space separated). Defaults to stdin.

optional arguments:
  -h, --help            show this help message and exit
  --title TITLE         Title of graph
  --width WIDTH         width of graph in characters default:50
  --format FORMAT       format specifier to use.
  --suffix SUFFIX       string to add as a suffix to all data points.
  --no-labels           Do not print the label column
  --no-values           Do not print the values at end
  --space-between       Print a new line after every field
  --color [COLOR ...]   Graph bar color( s )
  --vertical            Vertical graph
  --stacked             Stacked bar graph
  --histogram           Histogram
  --bins BINS           Bins of Histogram
  --different-scale     Categories have different scales.
  --calendar            Calendar Heatmap chart
  --start-dt START_DT   Start date for Calendar chart
  --custom-tick CUSTOM_TICK
                        Custom tick mark, emoji approved
  --delim DELIM         Custom delimiter, default , or space
  --verbose             Verbose output, helpful for debugging
  --label-before        Display the values before the bars
  --version             Display version and exit
BackgroundI wanted a quick way to visualize data stored in a simple text file. I initially created some scripts in R that generated graphs but this was a two step process of creating the graph and then opening the generated graph.After seeing  I figured I could do the same thing using block characters for bar charts.ContributeAll contributions are welcome, for feature requests or bug reports, use . Pull requests are welcome to help fix or add features.Code contributions: This repository uses the  to automatically format the code. A Github Action is setup to lint your code, to avoid failures it is recommended to .Thanks to all the !LicenseMIT License, see "
https://github.com/houtianze/bypy,Python client for Baidu Yun (Personal Cloud Storage) 百度云/百度网盘Python客户端,"bypy - Python client for Baidu Yun (Personal Cloud Storage) 百度云/百度网盘Python客户端极简说明TL;DR此项目已经进入维护状态：不会再有新的功能加入，只有在发现重大bug情况下才会有 This is project is now in ""maintenance"" mode: NO new features will be added, and 如果有人想帮助搭国内建授权服务器的话，请按以下步骤进行:中文说明 (English readme is at the bottom)这是一个百度云/百度网盘的Python客户端。主要的目的就是在Linux环境下（Windows下应该也可用，但没有仔细测试过）通过命令行来使用百度云盘的2TB的巨大空间。比如，你可以用在Raspberry Pi树莓派上。它提供文件列表、下载、上传、比较、向上同步、向下同步，等操作。由于百度PCS API权限限制，程序只能存取百度云端（已解决）~~据说百度PCS API最多返回目录下1000个文件（ #285 )，如果属实，百度云盘上若有超过1000个文件的目录，将有一部分文件无法被看到 / 下载~~特征: 支持Unicode/中文；失败重试；递归上传/下载；目录比较; 哈希缓存。界面是英文的，主要是因为这个是为了Raspberry Pi树莓派开发的。程序依赖重要：需要把系统的区域编码设置为UTF-8。（参见：安装运行简单的图形界面：运行 基本操作显示使用帮助和所有命令（英文）:bypy
第一次运行时需要授权，只需跑任何一个命令（比如 ）然后跟着说明（登陆等）来授权即可。授权只需一次，一旦成功，以后不会再出现授权提示.更详细的了解某一个命令：bypy help <command>
显示在云盘（程序的）根目录下文件列表：bypy list
把当前目录同步到云盘：bypy syncup
orbypy upload
把云盘内容同步到本地来：bypy syncdown
orbypy downdir /
比较本地当前目录和云盘（程序的）根目录（个人认为非常有用）：bypy compare
更多命令和详细解释请见运行的输出。调试整合测试（15 - 30分钟）直接在Python程序中调用from bypy import ByPy
bp=ByPy()
bp.list() # or whatever instance methods of ByPy class
经验分享请移步至，方便分享/交流。授权许可请阅: PCS API文档（已失效）:  (以前保存的离线版：  directory)IntroductionThis is a Python client for Baidu Yun (a.k.a PCS - Personal Cloud Storage), an online storage website offering 2 TB (fast) free personal storage. This main purpose is to be able to utilize this storage service under Linux environment (console), e.g. Raspberry Pi.Due to Baidu PC permission restrictions, this program can only access your (Fixed) ~~It's said the Baidu PCS API won't return more than 1000 items inside a directory ( #285 )，if this is true，you won't be able to see / download some files if you have a directory with more than 1000 files on Baidu Cloud~~Features: Unicode / Chinese support; Retry on failures; Recursive down/up-load; Directory comparison; Hash caching.PrerequisiteImportant: You need to set you system locale encoding to UTF-8 for this to work (You can refer here: InstallationUsageSimple GUI:Run Getting startedTo get help and a list of available commands:bypy
To authorize for first time use, run any commands e.g.  and follow the instructions (login etc). This is a one-time requirement only.To get more details about certain command:bypy help <command>
List files at (App's) root directory at Baidu PCS:bypy list
To sync up to the cloud (from the current directory):bypy syncup
orbypy upload
To sync down from the cloud (to the current directory):bypy syncdown
orbypy downdir /
To compare the current directory to (App's) root directory at Baidu PCS (which I think is very useful):bypy compare
To get more information about the commands, check the output of .DebugIntegration Test (15-30min)To call from Python codefrom bypy import ByPy
bp=ByPy()
bp.list() # or whatever instance methods of ByPy class
Tips / SharingPlease go to LicensePlease refer to PCS API Document (link dead 404):  (Offline pdf retrieved before:  directory)"
https://github.com/getsentry/sentry,Developer-first error tracking and performance monitoring,"What's Sentry?Sentry is a developer-first error tracking and performance monitoring platform that helps developers see what actually matters, solve quicker, and learn continuously about their applications.Official Sentry SDKsResources"
https://github.com/secdev/scapy,Scapy: the Python-based interactive packet manipulation program & library. Supports Python 2 & Python 3.,"&nbsp;&nbsp; Scapy <!-- ignore_ppi --> <!-- ignore_ppi --> <!-- ignore_ppi --> <!-- ignore_ppi --> Scapy is a powerful Python-based interactive packet manipulation program andlibrary.It is able to forge or decode packets of a wide number of protocols, send themon the wire, capture them, store or read them using pcap files, match requestsand replies, and much more. It is designed to allow fast packet prototyping byusing default values that work.It can easily handle most classical tasks like scanning, tracerouting, probing,unit tests, attacks or network discovery (it can replace , 85% of ,, , , , , , etc.). It alsoperforms very well at a lot of other specific tasks that most other tools can'thandle, like sending invalid frames, injecting your own 802.11 frames, combiningtechniques (VLAN hopping+ARP cache poisoning, VoIP decoding on WEP protectedchannel, ...), etc.Scapy supports Python 2.7 and Python 3 (3.4 to 3.9). It's intended tobe cross platform, and runs on many different platforms (Linux, OSX,BSD, and Windows).Getting startedScapy is usable either as a shell or as a library.For further details, please head over to , which is part of the documentation.Shell demoScapy can easily be used as an interactive shell to interact with the network.The following example shows how to send an ICMP Echo Request message to, then display the reply source IP address:sudo ./run_scapy
Welcome to Scapy
>>> p = IP(dst=""github.com"")/ICMP()
>>> r = sr1(p)
Begin emission:
.Finished to send 1 packets.
*
Received 2 packets, got 1 answers, remaining 0 packets
>>> r[IP].src
'192.30.253.113'
ResourcesThe  contains moreadvanced use cases, and examples.Other useful resources:Scapy works without any external Python modules on Linux and BSD like operatingsystems. On Windows, you need to install some mandatory dependencies asdescribed in .On most systems, using Scapy is as simple as running the following commands:git clone https://github.com/secdev/scapy
cd scapy
./run_scapy
To benefit from all Scapy features, such as plotting, you might want to installPython modules, such as  or . See the andfollow the instructions to install them.ContributingWant to contribute? Great! Please take a few minutes to!"
https://github.com/Yelp/elastalert,Easy & Flexible Alerting With ElasticSearch,"ElastAlert is no longer maintained. Please use ElastAlert - .Easy & Flexible Alerting With ElasticsearchElastAlert is a simple framework for alerting on anomalies, spikes, or other patterns of interest from data in Elasticsearch.ElastAlert works with all versions of Elasticsearch.At Yelp, we use Elasticsearch, Logstash and Kibana for managing our ever increasing amount of data and logs.Kibana is great for visualizing and querying data, but we quickly realized that it needed a companion tool for alertingon inconsistencies in our data. Out of this need, ElastAlert was created.If you have data being written into Elasticsearch in near real time and want to be alerted when that data matches certain patterns, ElastAlert is the tool for you. If you can see it in Kibana, ElastAlert can alert on it.OverviewWe designed ElastAlert to be reliable, highly modular, and easy to set up and configure.It works by combining Elasticsearch with two types of components, rule types and alerts.Elasticsearch is periodically queried and the data is passed to the rule type, which determines whena match is found. When a match occurs, it is given to one or more alerts, which take action based on the match.This is configured by a set of rules, each of which defines a query, a rule type, and a set of alerts.Several rule types with common monitoring paradigms are included with ElastAlert:Currently, we have built-in support for the following alert types:Additional rule types and alerts can be easily imported or written.In addition to this basic usage, there are many other features that make alerts more useful:To get started, check out  in the .Running ElastAlertYou can either install the latest released version of ElastAlert using pip:or you can clone the ElastAlert repository for the most recent changes:Install the module:The following invocation can be used to run ElastAlert after installing will print additional information to the screen as well as suppresses alerts and instead prints the alert body. Not compatible with . will print additional information without suppressing alerts. Not compatible with  will begin querying at the given timestamp. By default, ElastAlert will begin querying from the present.Timestamp format is  (Note the T between date and hour).Eg:  (UTC) or  will cause ElastAlert to stop querying at the given timestamp. By default, ElastAlert will continueto query indefinitely. will allow you to run only one rule. It must still be in the rules folder.Eg:  allows you to specify the location of the configuration. By default, it is will look for config.yaml in the current directory.Third Party Tools And ExtrasKibana pluginAvailable at the .DockerA  of ElastAlert including a REST api is build from  to .git clone https://github.com/bitsensor/elastalert.git; cd elastalert
docker run -d -p 3030:3030 \
    -v `pwd`/config/elastalert.yaml:/opt/elastalert/config.yaml \
    -v `pwd`/config/config.json:/opt/elastalert-server/config/config.json \
    -v `pwd`/rules:/opt/elastalert/rules \
    -v `pwd`/rule_templates:/opt/elastalert/rule_templates \
    --net=""host"" \
    --name elastalert bitsensor/elastalert:latest
DocumentationRead the documentation at .To build a html version of the docs locallypip install sphinx_rtd_theme sphinx
cd docs
make html
View in browser at build/html/index.htmlConfigurationSee config.yaml.example for details on configuration.Example rulesExamples of different types of rules can be found in example_rules/.Frequently Asked QuestionsMy rule is not getting any hits?So you've managed to set up ElastAlert, write a rule, and run it, but nothing happens, or it says . First of all, we recommend using the command  to debug. It will show you how many documents match your filters for the last 24 hours (or more, see ), and then shows you if any alerts would have fired. If you have a filter in your rule, remove it and try again. This will show you if the index is correct and that you have at least some documents. If you have a filter in Kibana and want to recreate it in ElastAlert, you probably want to use a query string. Your filter will look likefilter:
- query:
    query_string:
      query: ""foo: bar AND baz: abc*""
If you receive an error that Elasticsearch is unable to parse it, it's likely the YAML is not spaced correctly, and the filter is not in the right format. If you are using other types of filters, like , a common pitfall is not realizing that you may need to use the analyzed token. This is the default if you are using Logstash. For example,filter:
- term:
    foo: ""Test Document""
will not match even if the original value for  was exactly ""Test Document"". Instead, you want to use . If you are still having trouble troubleshooting why your documents do not match, try running ElastAlert with . This will log the queries made to Elasticsearch in full so that you can see exactly what is happening.I got hits, why didn't I get an alert?If you got logs that had , it depends on the  why you didn't get any alerts. If , a match will occur for every hit. If you are using ,  must occur within  of each other for a match to occur. Different rules apply for different rule types.If you see , this may occur for several reasons. If you set , the alert will not be sent until after that time has elapsed. If you have gotten an alert for this same rule before, that rule may be silenced for a period of time. The default is one minute between alerts. If a rule is silenced, you will see  in the logs.If you see  but didn't get any alert, it's probably related to the alert configuration. If you are using the  flag, you will not receive any alerts. Instead, the alert text will be written to the console. Use  to achieve the same affects without preventing alerts. If you are using email alert, make sure you have it configured for an SMTP server. By default, it will connect to localhost on port 25. It will also use the word ""elastalert"" as the ""From:"" address. Some SMTP servers will reject this because it does not have a domain while others will add their own domain automatically. See the email section in the documentation for how to configure this.Why did I only get one alert when I expected to get several?There is a setting called  which is the minimum time between two alerts for the same rule. Any alert that occurs within this time will simply be dropped. The default value for this is one minute. If you want to receive an alert for every single match, even if they occur right after each other, userealert:
  minutes: 0
You can of course set it higher as well.How can I prevent duplicate alerts?By setting , you will prevent the same rule from alerting twice in an amount of time.realert:
  days: 1
You can also prevent duplicates based on a certain field by using . For example, to prevent multiple alerts for the same user, you might userealert:
  hours: 8
query_key: user
Note that this will also affect the way many rule types work. If you are using  for example,  for a single value of  must occur before an alert will be sent. You can also use a compound of multiple fields for this key. For example, if you only wanted to receieve an alert once for a specific error and hostname, you could usequery_key: [error, hostname]
Internally, this works by creating a new field for each document called  with a value of  and using that as the .The data for when an alert will fire again is stored in Elasticsearch in the  index, with a  of  and also cached in memory.How can I change what's in the alert?You can use the field  to add custom text to an alert. By setting , it will be the entirety of the alert. You can also add different fields from the alert by using Python style string formatting and . For examplealert_text: ""Something happened with {0} at {1}""
alert_text_type: alert_text_only
alert_text_args: [""username"", ""@timestamp""]
You can also limit the alert to only containing certain fields from the document by using .include: [""ip_address"", ""hostname"", ""status""]
My alert only contains data for one event, how can I see more?If you are using , you can set the option  and every document will be included in the alert. An alternative, which works for every type, is . This will show the top counts for each value for certain fields. For example, if you havetop_count_keys: [""ip_address"", ""status""]
and 10 documents matched your alert, it may contain something likeip_address:
127.0.0.1: 7
10.0.0.1: 2
192.168.0.1: 1

status:
200: 9
500: 1
How can I make the alert come at a certain time?The  feature will take every alert that has occured over a period of time and send them together in one alert. You can use cron style syntax to send all alerts that have occured since the last once by usingaggregation:
  schedule: '2 4 * * mon,fri'
I have lots of documents and it's really slow, how can I speed it up?There are several ways to potentially speed up queries. If you are using , Elasticsearch will query all shards, even if they do not possibly contain data with the correct timestamp. Instead, you can use Python time format strings and set index: logstash-%Y.%m
use_strftime_index: true
Another thing you could change is . By default, ElastAlert will query large overlapping windows in order to ensure that it does not miss any events, even if they are indexed in real time. In config.yaml, you can adjust  to a smaller number to only query the most recent few minutes.buffer_time:
  minutes: 5
By default, ElastAlert will download every document in full before processing them. Instead, you can have ElastAlert simply get a count of the number of documents that have occured in between each query. To do this, set . This cannot be used if you use , because ElastAlert will not know the contents of each documents, just the total number of them. This also reduces the precision of alerts, because all events that occur between each query will be rounded to a single timestamp.If you are using  (a single key, not multiple keys) you can use . This will make ElastAlert perform a terms aggregation to get the counts for each value of a certain field. Both  and  also require  to be set to the  of the documents. They may not be compatible with all rule types.Can I perform aggregations?The only aggregation supported currently is a terms aggregation, by setting .I'm not using @timestamp, what do I do?You can use  to change which field ElastAlert will use as the timestamp. You can use  to change it between ISO 8601 and unix timestamps. You must have some kind of timestamp for ElastAlert to work. If your events are not in real time, you can use  and  to adjust when ElastAlert will look for documents.I'm using flatline but I don't see any alertsWhen using , ElastAlert must see at least one document before it will alert you that it has stopped seeing them.How can I get a ""resolve"" event?ElastAlert does not currently support stateful alerts or resolve events.Can I set a warning threshold?Currently, the only way to set a warning threshold is by creating a second rule with a lower threshold.LicenseElastAlert is licensed under the Apache License, Version 2.0: http://www.apache.org/licenses/LICENSE-2.0Read the documentation at .Questions? Drop by #elastalert on Freenode IRC."
https://github.com/n1nj4sec/pupy,"Pupy is an opensource, cross-platform (Windows, Linux, OSX, Android) C2 and post-exploitation framework written in python and C","PupyInstallationInstallation instructions are on the wiki, in addition to all other documentation. For maximum compatibility, it is recommended to use Docker Compose.DescriptionPupy is a cross-platform, multi function RAT and post-exploitation tool mainly written in python. It features an all-in-memory execution guideline and leaves a very low footprint. Pupy can communicate using multiple transports, migrate into processes using reflective injection, and load remote python code, python packages and python C-extensions from memory.Features| Format | Architecture | Short Name ||---|---|---|Android Package | x86 & ARMv7 | apkLinux Binary | x86 | lin_x86Linux Binary | x64 | lin_x64Linux Shared Object | x86 | so_x86Linux Shared Object | x64 | so_x64Windows PE Executable | x86 | exe_x86Windows PE Executable | x64 | exe_x64Windows DLL | x86 | dll_x86Windows DLL | x64 | dll_x64Python Script | x86 & x64 | pyPyInstaller | x86 & x64 | pyinstPython Oneliner | x86 & x64 | py_onelinerPowershell | x86 & x64 | ps1Powershell Oneliner | x86 & x64 | ps1_onelinerDucky Script | N/A | rubber_ducky| Platform | Support Status ||---|---|Windows XP | SupportedWindows 7 | SupportedWindows 8 | SupportedWindows 10 | SupportedLinux | SupportedMac OSX | Limited SupportAndroid | Limited SupportDocumentationAll documentation can be found on the wiki.FAQPupy has not been tested on Windows. Theoretically, it should work on any platform that supports Docker and Docker Compose. However, you will need to adapt the Docker Compose installation instructions for the Windows platform.If you do not follow these steps, you issue will be closed.Pupy has limited support for Android and OSX. These platforms may not be well maintained and may break intermittently. Some modules (i.e. keylogger) may be missing for these platforms.DevelopmentIf some of you want to participate to pupy development, don't hesitate! All help is greatly appreciated and all pull requests will be reviewed.Also there is small  about development. Please run flake8 before doing any commits. File with config is .Contact| Platform | Contact Info ||---|---|Email | contact@n1nj4.euTwitter | https://twitter.com/n1nj4secThis project is a , please respect its philosophy and don't use it for evil purposes!Special thanksSpecial thanks to all contributors that help improve pupy and make it a better tool! :)"
https://github.com/marshmallow-code/marshmallow,A lightweight library for converting complex objects to and from simple Python datatypes.,"marshmallow: simplified object serialization.. image:: https://badgen.net/pypi/v/marshmallow:target: https://pypi.org/project/marshmallow/:alt: Latest version.. image:: https://github.com/marshmallow-code/marshmallow/actions/workflows/build-release.yml/badge.svg:target: https://github.com/marshmallow-code/marshmallow/actions/workflows/build-release.yml:alt: Build status.. image:: https://results.pre-commit.ci/badge/github/marshmallow-code/marshmallow/dev.svg:target: https://results.pre-commit.ci/latest/github/marshmallow-code/marshmallow/dev:alt: pre-commit.ci status.. image:: https://readthedocs.org/projects/marshmallow/badge/:target: https://marshmallow.readthedocs.io/:alt: Documentation.. image:: https://badgen.net/badge/code%20style/black/000:target: https://github.com/ambv/black:alt: code style: blackmarshmallow is an ORM/ODM/framework-agnostic library for converting complex datatypes, such as objects, to and from native Python datatypes... code-block:: pythonfrom datetime import date
from pprint import pprint

from marshmallow import Schema, fields


class ArtistSchema(Schema):
    name = fields.Str()


class AlbumSchema(Schema):
    title = fields.Str()
    release_date = fields.Date()
    artist = fields.Nested(ArtistSchema())


bowie = dict(name=""David Bowie"")
album = dict(artist=bowie, title=""Hunky Dory"", release_date=date(1971, 12, 17))

schema = AlbumSchema()
result = schema.dump(album)
pprint(result, indent=2)
# { 'artist': {'name': 'David Bowie'},
#   'release_date': '1971-12-17',
#   'title': 'Hunky Dory'}
In short, marshmallow schemas can be used to:Get It Now::$ pip install -U marshmallow
DocumentationFull documentation is available at https://marshmallow.readthedocs.io/ .RequirementsEcosystemA list of marshmallow-related libraries can be found at the GitHub wiki here:https://github.com/marshmallow-code/marshmallow/wiki/EcosystemCreditsContributorsThis project exists thanks to all the people who contribute.You're highly encouraged to participate in marshmallow's development.Check out the _ to see how you can help.Thank you to all who have already contributed to marshmallow!.. image:: https://opencollective.com/marshmallow/contributors.svg?width=890&button=false:target: https://marshmallow.readthedocs.io/en/latest/authors.html:alt: ContributorsBackersIf you find marshmallow useful, please consider supporting the team witha donation. Your donation helps move marshmallow forward.Thank you to all our backers! [_].. _: https://opencollective.com/marshmallow#backer.. image:: https://opencollective.com/marshmallow/backers.svg?width=890:target: https://opencollective.com/marshmallow#backers:alt: BackersSponsorsSupport this project by becoming a sponsor (or ask your company to support this project by becoming a sponsor).Your logo will show up here with a link to your website. [_].. _: https://opencollective.com/marshmallow#sponsor.. image:: https://opencollective.com/marshmallow/sponsor/0/avatar.svg:target: https://opencollective.com/marshmallow/sponsor/0/website:alt: Sponsors.. image:: https://opencollective.com/static/images/become_sponsor.svg:target: https://opencollective.com/marshmallow#sponsor:alt: Become a sponsorProfessional SupportProfessionally-supported marshmallow is now available through the_.Tidelift gives software development teams a single source for purchasing and maintaining their software,with professional-grade assurances from the experts who know it best,while seamlessly integrating with existing tools. [_].. _: https://tidelift.com/subscription/pkg/pypi-marshmallow?utm_source=marshmallow&utm_medium=referral&utm_campaign=github.. image:: https://user-images.githubusercontent.com/2379650/45126032-50b69880-b13f-11e8-9c2c-abd16c433495.png:target: https://tidelift.com/subscription/pkg/pypi-marshmallow?utm_source=pypi-marshmallow&utm_medium=readme:alt: Get supported marshmallow with TideliftProject LinksLicenseMIT licensed. See the bundled _ file for more details."
https://github.com/facebook/chisel,Chisel is a collection of LLDB commands to assist debugging iOS apps.,"Chisel is a collection of  commands to assist in the debugging of iOS apps.[ &bull;  &bull;  &bull;   &bull; ]For a comprehensive overview of LLDB, and how Chisel complements it, read Ari Grant's  in issue 19 of .Installationbrew update
brew install chisel
if  file doesn't exist you can create it & open it by tapping on the terminaltouch .lldbinit
open .lldbinit
Then add the following line to your  file.# ~/.lldbinit
...
command script import /usr/local/opt/chisel/libexec/fbchisellldb.py
Alternatively, download chisel and add the following line to your ~/.lldbinit file.# ~/.lldbinit
...
command script import /path/to/fbchisellldb.py

The commands will be available the next time  starts.CommandsThere are many commands; here's a few:(Compatibility with iOS/Mac indicated at right)|Command          |Description     |iOS    |OS X   ||-----------------|----------------|-------|-------||pviews           |Print the recursive view description for the key window.|Yes|Yes||pvc              |Print the recursive view controller description for the key window.|Yes|No||visualize        |Open a , , , ,  (of an image), , , ,  or  in Preview.app on your Mac.|Yes|No||fv               |Find a view in the hierarchy whose class name matches the provided regex.|Yes|No||fvc              |Find a view controller in the hierarchy whose class name matches the provided regex.|Yes|No||show/hide        |Show or hide the given view or layer. You don't even have to continue the process to see the changes!|Yes|Yes||mask/unmask      |Overlay a view or layer with a transparent rectangle to visualize where it is.|Yes|No||border/unborder  |Add a border to a view or layer to visualize where it is.|Yes|Yes||caflush          |Flush the render server (equivalent to a ""repaint"" if no animations are in-flight).|Yes|Yes||bmessage         |Set a symbolic breakpoint on the method of a class or the method of an instance without worrying which class in the hierarchy actually implements the method.|Yes|Yes||wivar            |Set a watchpoint on an instance variable of an object.|Yes|Yes||presponder       |Print the responder chain starting from the given object.|Yes|Yes||...              |... and many more!|To see the list of all of the commands execute the help command in  or go to the .(lldb) help
The following is a list of built-in, permanent debugger commands:
...

The following is a list of your current user-defined commands:
...
The bottom list contains all the commands sourced from .You can also inspect a specific command by passing its name as an argument to the help command (as with all other  commands).(lldb) help border
Draws a border around <viewOrLayer>. Color and width can be optionally provided.

Arguments:
  <viewOrLayer>; Type: UIView*; The view to border.

Options:
  --color/-c <color>; Type: string; A color name such as 'red', 'green', 'magenta', etc.
  --width/-w <width>; Type: CGFloat; Desired width of border.

Syntax: border [--color=color] [--width=width] <viewOrLayer>
All of the commands provided by  come with verbose help. Be sure to read it when in doubt!Custom CommandsYou can add local, custom commands. Here's a contrived example.#!/usr/bin/python
# Example file with custom commands, located at /magical/commands/example.py

import lldb
import fbchisellldbbase as fb

def lldbcommands():
  return [ PrintKeyWindowLevel() ]

class PrintKeyWindowLevel(fb.FBCommand):
  def name(self):
    return 'pkeywinlevel'

  def description(self):
    return 'An incredibly contrived command that prints the window level of the key window.'

  def run(self, arguments, options):
    # It's a good habit to explicitly cast the type of all return
    # values and arguments. LLDB can't always find them on its own.
    lldb.debugger.HandleCommand('p (CGFloat)[(id)[(id)[UIApplication sharedApplication] keyWindow] windowLevel]')
Then all that's left is to source the commands in lldbinit.  has a python function just for this, loadCommandsInDirectory in the fbobjclldb.py module.# ~/.lldbinit
...
command script import /path/to/fbobjclldb.py
script fbobjclldb.loadCommandsInDirectory('/magical/commands/')

There's also builtin support to make it super easy to specify the arguments and options that a command takes. See the border and pinvocation commands for example use.Development WorkflowDeveloping commands, whether for local use or contributing to  directly, both follow the same workflow. Create a command as described in the  section and thenContributingPlease contribute any generic commands that you make. If it helps you then it will likely help many others! :D See  to learn how to contribute.License is MIT-licensed. See ."
https://github.com/lazyprogrammer/machine_learning_examples,A collection of machine learning examples and tutorials.,"machine_learning_examplesA collection of machine learning examples and tutorials.Find associated tutorials at https://lazyprogrammer.meFind associated courses at https://deeplearningcourses.comPlease note that not all code from all courses will be found in this repository. Some newer code examples (e.g. most of Tensorflow 2.0) were done in Google Colab. Therefore, you should check the instructions given in the lectures for the course you are taking.How to I find the code for a particular course?The code for each course is separated by folder. You can determine which folder corresponds with which course by watching the ""Where to get the code"" lecture inside the course (usually Lecture 2 or 3).Remember: one folder = one course.Why you should not fork this repoI've noticed that many people have out-of-date forks. Thus, I recommend not forking this repository if you take one of my courses. I am constantly updating my courses, and your fork will soon become out-of-date. You should clone the repository instead to make it easy to get updates (i.e. just ""git pull"" randomly and frequently).Where is the code for your latest courses?Beginning with Tensorflow 2, I started to use Google Colab. For those courses, unless otherwise noted, the code will be on Google Colab. Links to the notebooks are provided in the course. See the lecture ""Where to get the code"" for further details.VIP Course LinksData Science: Transformers for Natural Language Processinghttps://deeplearningcourses.com/c/data-science-transformers-nlpMachine Learning: Natural Language Processing in Python (V2)https://deeplearningcourses.com/c/natural-language-processing-in-pythonTime Series Analysis, Forecasting, and Machine Learninghttps://deeplearningcourses.com/c/time-series-analysisFinancial Engineering and Artificial Intelligence in Pythonhttps://deeplearningcourses.com/c/ai-financePyTorch: Deep Learning and Artificial Intelligencehttps://deeplearningcourses.com/c/pytorch-deep-learningTensorflow 2.0: Deep Learning and Artificial Intelligence (VIP Version)https://deeplearningcourses.com/c/deep-learning-tensorflow-2Deep Learning Courses ExclusivesData Science: Bayesian Linear Regression in Pythonhttps://deeplearningcourses.com/c/bayesian-linear-regression-in-pythonData Science: Bayesian Classification in Pythonhttps://deeplearningcourses.com/c/bayesian-classification-in-pythonClassical Statistical Inference and A/B Testing in Pythonhttps://deeplearningcourses.com/c/statistical-inference-in-pythonLinear Programming for Linear Regression in Pythonhttps://deeplearningcourses.com/c/linear-programming-pythonMATLAB for Students, Engineers, and Professionals in STEMhttps://deeplearningcourses.com/c/matlabOther Course LinksFinancial Analysis: Build a ChatGPT Pairs Trading Bothttps://deeplearningcourses.com/c/chatgpt-pairs-tradingMath 0-1: Calculus for Data Science & Machine Learninghttps://deeplearningcourses.com/c/calculus-data-scienceData Science & Machine Learning: Naive Bayes in Pythonhttps://deeplearningcourses.com/c/data-science-machine-learning-naive-bayes-in-pythonCutting-Edge AI: Deep Reinforcement Learning in Pythonhttps://deeplearningcourses.com/c/cutting-edge-artificial-intelligenceRecommender Systems and Deep Learning in Pythonhttps://deeplearningcourses.com/c/recommender-systemsMachine Learning and AI: Support Vector Machines in Pythonhttps://deeplearningcourses.com/c/support-vector-machines-in-pythonDeep Learning: Advanced Computer Visionhttps://deeplearningcourses.com/c/advanced-computer-visionDeep Learning: Advanced NLP and RNNshttps://deeplearningcourses.com/c/deep-learning-advanced-nlpDeep Learning: GANs and Variational Autoencodershttps://deeplearningcourses.com/c/deep-learning-gans-and-variational-autoencodersAdvanced AI: Deep Reinforcement Learning in Pythonhttps://deeplearningcourses.com/c/deep-reinforcement-learning-in-pythonArtificial Intelligence: Reinforcement Learning in Pythonhttps://deeplearningcourses.com/c/artificial-intelligence-reinforcement-learning-in-pythonNatural Language Processing with Deep Learning in Pythonhttps://deeplearningcourses.com/c/natural-language-processing-with-deep-learning-in-pythonDeep Learning: Recurrent Neural Networks in Pythonhttps://deeplearningcourses.com/c/deep-learning-recurrent-neural-networks-in-pythonUnsupervised Machine Learning: Hidden Markov Models in Pythonhttps://deeplearningcourses.com/c/unsupervised-machine-learning-hidden-markov-models-in-pythonDeep Learning Prerequisites: The Numpy Stack in Pythonhttps://deeplearningcourses.com/c/deep-learning-prerequisites-the-numpy-stack-in-pythonDeep Learning Prerequisites: Linear Regression in Pythonhttps://deeplearningcourses.com/c/data-science-linear-regression-in-pythonDeep Learning Prerequisites: Logistic Regression in Pythonhttps://deeplearningcourses.com/c/data-science-logistic-regression-in-pythonData Science: Deep Learning and Neural Networks in Pythonhttps://deeplearningcourses.com/c/data-science-deep-learning-in-pythonCluster Analysis and Unsupervised Machine Learning in Pythonhttps://deeplearningcourses.com/c/cluster-analysis-unsupervised-machine-learning-pythonData Science: Supervised Machine Learning in Pythonhttps://deeplearningcourses.com/c/data-science-supervised-machine-learning-in-pythonBayesian Machine Learning in Python: A/B Testinghttps://deeplearningcourses.com/c/bayesian-machine-learning-in-python-ab-testingData Science: Natural Language Processing in Pythonhttps://deeplearningcourses.com/c/data-science-natural-language-processing-in-pythonModern Deep Learning in Pythonhttps://deeplearningcourses.com/c/data-science-deep-learning-in-theano-tensorflowEnsemble Machine Learning in Python: Random Forest and AdaBoosthttps://deeplearningcourses.com/c/machine-learning-in-python-random-forest-adaboostDeep Learning: Convolutional Neural Networks in Pythonhttps://deeplearningcourses.com/c/deep-learning-convolutional-neural-networks-theano-tensorflowUnsupervised Deep Learning in Pythonhttps://deeplearningcourses.com/c/unsupervised-deep-learning-in-python"
https://github.com/geekan/scrapy-examples,Multifarious Scrapy examples. Spiders for alexa / amazon / douban / douyu / github / linkedin etc.,"scrapy-examplesMultifarious scrapy examples with integrated proxies and agents, which make you comfy to write a spider.Don't use it to do anything illegal!Real spider example: doubanbookTutorialgit clone https://github.com/geekan/scrapy-examples
cd scrapy-examples/doubanbook
scrapy crawl doubanbook
DepthThere are several depths in the spider, and the spider getsreal data from depth2.Example imageAvaiable SpidersAdvancedAdvanced UsageExample to hack  and Hacked  with additional fields  and :  from scrapy.item import Item, Field

class exampleItem(Item):
    url = Field()
    name = Field()
    description = Field()
Hacked  with start rules and css rules (here only display the class exampleSpider):  class exampleSpider(CommonSpider):
    name = ""dmoz""
    allowed_domains = [""dmoz.org""]
    start_urls = [
        ""http://www.dmoz.com/"",
    ]
    # Crawler would start on start_urls, and follow the valid urls allowed by below rules.
    rules = [
        Rule(sle(allow=[""/Arts/"", ""/Games/""]), callback='parse', follow=True),
    ]

    css_rules = {
        '.directory-url li': {
            '__use': 'dump', # dump data directly
            '__list': True, # it's a list
            'url': 'li > a::attr(href)',
            'name': 'a::text',
            'description': 'li::text',
        }
    }

    def parse(self, response):
        info('Parse '+response.url)
        # parse_with_rules is implemented here:
        #   https://github.com/geekan/scrapy-examples/blob/master/misc/spider.py
        self.parse_with_rules(response, self.css_rules, exampleItem)
"
https://github.com/docker/docker-py,A Python library for the Docker Engine API,"Docker SDK for PythonA Python library for the Docker Engine API. It lets you do anything the  command does, but from within Python apps – run containers, manage containers, manage Swarms, etc.InstallationThe latest stable version . Either add  to your  file or install with pip:pip install docker
UsageConnect to Docker using the default socket or the configuration in your environment:import docker
client = docker.from_env()
You can run containers:>>> client.containers.run(""ubuntu:latest"", ""echo hello world"")
'hello world\n'
You can run containers in the background:>>> client.containers.run(""bfirsh/reticulate-splines"", detach=True)
<Container '45e6d2de7c54'>
You can manage containers:>>> client.containers.list()
[<Container '45e6d2de7c54'>, <Container 'db18e4f20eaa'>, ...]

>>> container = client.containers.get('45e6d2de7c54')

>>> container.attrs['Config']['Image']
""bfirsh/reticulate-splines""

>>> container.logs()
""Reticulating spline 1...\n""

>>> container.stop()
You can stream logs:>>> for line in container.logs(stream=True):
...   print(line.strip())
Reticulating spline 2...
Reticulating spline 3...
...
You can manage images:>>> client.images.pull('nginx')
<Image 'nginx'>

>>> client.images.list()
[<Image 'ubuntu'>, <Image 'nginx'>, ...]
 to see everything you can do."
https://github.com/asciinema/asciinema,Terminal session recorder 📹,"asciinemaTerminal session recorder and the best companion of.asciinema [as-kee-nuh-muh] is a free and open source solution for recordingterminal sessions and sharing them on the web.Shout-out to our Platinum , whosefinancial support helps keep the project alive:Quick introasciinema lets you easily record terminal sessions and replaythem in a terminal as well as in a web browser.Install latest version ()using  (if you have it):pipx install asciinema
If you don't have pipx, install using pip with your preferred Python version:python3 -m pip install asciinema
Record your first session:asciinema rec first.cast
Now replay it with double speed:asciinema play -s 2 first.cast
Or with normal speed but with idle time limited to 2 seconds:asciinema play -i 2 first.cast
You can pass  to  as well, to set it permanently on arecording. Idle time limiting makes the recordings much more interesting towatch. Try it.If you want to watch and share it on the web, upload it:asciinema upload first.cast
The above uploads it to , which is adefault instance, and prints a secret link you can use to watch your recording in a webbrowser.You can record and upload in one step by omitting the filename:asciinema rec
You'll be asked to confirm the upload when the recording is done. Nothing issent anywhere without your consent.These are the basics, but there's much more you can do. The following sectionscover installation, usage and hosting of the recordings in more detail. Also,checkout  if you're interested in GIF generation.InstallationPython package from PyPIasciinema is available on  and can be installed with (if you have it) or with pip (Python 3with setuptools required):pipx install asciinema
Or with pip (using your preferred Python version):python3 -m pip install asciinema
Installing from  is the recommended way of installation, which gives you the latest released version.Native packagesasciinema is included in repositories of most popular package managers on Mac OSX, Linux and FreeBSD. Look for package named . See the.Running latest version from source code checkoutIf you can't use Python package or native package for your OS is outdated youcan clone the repo and run asciinema straight from the checkout.Clone the repo:git clone https://github.com/asciinema/asciinema.git
cd asciinema
If you want latest stable version:git checkout master
If you want current development version:git checkout develop
Then run it with:python3 -m asciinema --version
Docker imageasciinema Docker image is based on  and has the latest version ofasciinema recorder pre-installed.docker pull ghcr.io/asciinema/asciinema
When running it don't forget to allocate a pseudo-TTY (), keep STDIN open() and mount config directory volume ():docker run --rm -it -v ""${HOME}/.config/asciinema:/root/.config/asciinema"" ghcr.io/asciinema/asciinema rec
Container's entrypoint is set to  so you can run thecontainer with any arguments you would normally pass to  binary (seeUsage section for commands and options).There's not much software installed in this image though. In most cases you maywant to install extra programs before recording. One option is to derive newimage from this one (start your custom Dockerfile with ). Another option is to start the container with as the entrypoint, install extra packages and manually start:docker run --rm -it -v ""${HOME}/.config/asciinema:/root/.config/asciinema"" --entrypoint=/bin/bash ghcr.io/asciinema/asciinema rec
root@6689517d99a1:~# apt-get install foobar
root@6689517d99a1:~# asciinema rec
It is also possible to run the docker container as a non-root user, which hassecurity benefits. You can specify a user and group id at runtime to give theapplication permission similar to the calling user on your host.docker run --rm -it \
    --env=ASCIINEMA_CONFIG_HOME=""/run/user/$(id -u)/.config/asciinema"" \
    --user=""$(id -u):$(id -g)"" \
    --volume=""${HOME}/.config/asciinema:/run/user/$(id -u)/.config/asciinema:rw"" \
    --volume=""${PWD}:/data:rw"" \
    --workdir='/data' \
    ghcr.io/asciinema/asciinema rec
Usageasciinema is composed of multiple commands, similar to ,  or.When you run  with no arguments help message is displayed, listingall available commands with their options.Record terminal session.By running  you start a new recording session. Thecommand (process) that is recorded can be specified with  option (seebelow), and defaults to  which is what you want in most cases.You can temporarily pause the capture of your terminal by pressingCtrl+.  This is useful when you want to execute some commands duringthe recording session that should not be captured (e.g. pasting secrets). Resumeby pressing Ctrl+ again. When pausing desktop notification isdisplayed so you're sure the sensitive output won't be captured in therecording.Recording finishes when you exit the shell (hit Ctrl+D or type). If the recorded process is not a shell then recording finishes whenthe process exits.If the  argument is omitted then (after asking for confirmation) theresulting asciicast is uploaded to (by default toasciinema.org), where it can be watched and shared.If the  argument is given then the resulting recording (called) is saved to a local file. It can later bereplayed with  and/or uploaded to asciinema serverwith . is added to recorded process environment variables. Thiscan be used by your shell's config file (, ) to alter theprompt or play a sound when the shell is being recorded.Available options:Stdin recording allows for capturing of all characters typed in by the user inthe currently recorded shell. This may be used by a player (e.g.) to displaypressed keys. Because it's basically key-logging (scoped to a single shellinstance), it's disabled by default, and has to be explicitly enabled via option.Replay recorded asciicast in a terminal.This command replays given asciicast (as recorded by  command) directly inyour terminal.Following keyboard shortcuts are available by default:See ""Configuration file"" section for information on how to customize thekeyboard shortcuts.Playing from a local file:asciinema play /path/to/asciicast.cast
Playing from HTTP(S) URL:asciinema play https://asciinema.org/a/22124.cast
asciinema play http://example.com/demo.cast
Playing from asciicast page URL (requires  in page's HTML):asciinema play https://asciinema.org/a/22124
asciinema play http://example.com/blog/post.html
Playing from stdin:cat /path/to/asciicast.cast | asciinema play -
ssh user@host cat asciicast.cast | asciinema play -
Playing from IPFS:asciinema play dweb:/ipfs/QmNe7FsYaHc9SaDEAEXbaagAzNw9cH7YbzN4xV7jV1MCzK/ascii.cast
Available options:By default the output stream () is played. This is what you want in mostcases.  If you recorded the input stream () with  thenyou can replay it with .By default the selected stream is written to stdout in original, raw data form.This is also what you want in majority of cases. However you can change theoutput format to asciicast (newline delimited JSON) with . This allows delegating actual rendering toanother place (e.g. outside of your terminal) by piping output of  to a tool of your choice.Print full output of recorded asciicast to a terminal.While  replays the recorded session using timinginformation saved in the asciicast,  dumps the fulloutput (including all escape sequences) to a terminal immediately. gives the same result as recording via.Upload recorded asciicast to asciinema.org site.This command uploads given asciicast (recorded by  command) toasciinema.org, where it can be watched and shared. +  +  is a nice combo if you want to review an asciicast beforepublishing it on asciinema.org.Link your install ID with your asciinema.org user account.If you want to manage your recordings (change title/theme, delete) atasciinema.org you need to link your ""install ID"" with asciinema.org useraccount.This command displays the URL to open in a web browser to do that. You may beasked to log in first.Install ID is a random ID () generatedlocally when you run asciinema for the first time, and saved at. Its purpose is to connect local machinewith uploaded recordings, so they can later be associated with asciinema.orgaccount. This way we decouple uploading from account creation, allowing them tohappen in any order.MarkersMarkers allow marking specific time locations in a recording, which can be usedfor navigation, as well as for automatic pausing of the playback.Markers can be added to a recording in several ways:When replaying a recording with  you can enableauto-pause-on-marker behaviour with / option (it's offby default). When a marker is encountered, the playback automatically pauses andcan be resumed by pressing space bar key. The playback continues until nextmarker is encountered. You can also fast-forward to the next marker by pressing key (when paused).Markers can be useful in e.g. live demos: you can create a recording withmarkers, then play it back during presentation, and have it stop wherever youwant to explain terminal contents in more detail.Hosting the recordings on the webAs mentioned in the  section above, if the  argument to is omitted then the recorded asciicast is uploaded to. You can watch it there and share it viasecret URL.If you prefer to host the recordings yourself, you can do so by either:Configuration fileYou can configure asciinema by creating config file at.Configuration is split into sections (, , ). Here's alist of all available options for each section:[api]

; API server URL, default: https://asciinema.org
; If you run your own instance of asciinema-server then set its address here
; It can also be overriden by setting ASCIINEMA_API_URL environment variable
url = https://asciinema.example.com

[record]

; Command to record, default: $SHELL
command = /bin/bash -l

; Enable stdin (keyboard) recording, default: no
stdin = yes

; List of environment variables to capture, default: SHELL,TERM
env = SHELL,TERM,USER

; Limit recorded terminal inactivity to max n seconds, default: off
idle_time_limit = 2

; Answer ""yes"" to all interactive prompts, default: no
yes = true

; Be quiet, suppress all notices/warnings, default: no
quiet = true

; Define hotkey for pausing recording (suspending capture of output),
; default: C-\ (control + backslash)
pause_key = C-p

; Define hotkey for adding a marker, default: none
add_marker_key = C-x

; Define hotkey prefix key - when defined other recording hotkeys must
; be preceeded by it, default: no prefix
prefix_key = C-a

[play]

; Playback speed (can be fractional), default: 1
speed = 2

; Limit replayed terminal inactivity to max n seconds, default: off
idle_time_limit = 1

; Define hotkey for pausing/resuming playback,
; default: space
pause_key = p

; Define hotkey for stepping through playback, a frame at a time,
; default: . (dot)
step_key = s

; Define hotkey for jumping to the next marker,
; default: ]
next_marker_key = m

[notifications]
; Desktop notifications are displayed on certain occasions, e.g. when
; pausing/resuming the capture of terminal with C-\ keyboard shortcut.

; Should desktop notifications be enabled, default: yes
enabled = no

; Custom notification command
; asciinema automatically detects available desktop notification system
; (notify-send on GNU/Linux, osacript/terminal-notifier on macOS). Custom
; command can be used if needed.
; When invoked, environment variable $TEXT contains notification text, while
; $ICON_PATH contains path to the asciinema logo image.
command = tmux display-message ""$TEXT""
A very minimal config file could look like that:[record]
idle_time_limit = 2
Config directory location can be changed by setting environment variable.If  is set on Linux then asciinema uses instead of .Sponsorsasciinema is sponsored by:ConsultingI offer consulting services for asciinema project. See https://asciinema.org/consulting for more information.ContributingIf you want to contribute to this project check out page.AuthorsDeveloped with passion by  and great opensource .License© 2011 Marcin Kulik.All code is licensed under the GPL, v3 or later. See  filefor details."
https://github.com/bokeh/bokeh,"Interactive Data Visualization in the browser, from  Python"," is an interactive visualization library for modern web browsers. It provides elegant, concise construction of versatile graphics and affords high-performance interactivity across large or streaming datasets. Bokeh can help anyone who wants to create interactive plots, dashboards, and data applications quickly and easily.Consider InstallationTo install Bokeh and its required dependencies using , enter the following command at a Bash or Windows command prompt:pip install bokeh
To install , enter the following command at a Bash or Windows command prompt:conda install bokeh
Refer to the  for more details.ResourcesOnce Bokeh is installed, check out the .Visit the  to view the  or  to learn about Bokeh in live Jupyter Notebooks.Community support is available on the .If you would like to contribute to Bokeh, please review the  and .Note: Everyone who engages in the Bokeh project's discussion forums, codebases, and issue trackers is expected to follow the Follow usFollow us on Twitter SupportFiscal SupportThe Bokeh project is grateful for , as well as for monetary support from the organizations and companies listed below:If your company uses Bokeh and is able to sponsor the project, please contact info@bokeh.orgBokeh is a Sponsored Project of NumFOCUS, a 501(c)(3) nonprofit charity in the United States. NumFOCUS provides Bokeh with fiscal, legal, and administrative support to help ensure the health and sustainability of the project. Visit Donations to Bokeh are managed by NumFOCUS. For donors in the United States, your gift is tax-deductible to the extent provided by law. As with any donation, you should consult with your tax adviser about your particular tax situation.In-kind SupportNon-monetary support can help with development, collaboration, infrastructure, security, and vulnerability management. The Bokeh project is grateful to the following companies for their donation of services:"
https://github.com/guohongze/adminset,自动化运维平台：CMDB、CD、DevOps、资产管理、任务编排、持续交付、系统监控、运维管理、配置管理,"AdminSet QuickStartAdminset基于DevOps理念开发，以整合全部运维场景为己任。Adminset是一个真正的基于运维思维而开发的全自动化运维平台。v0.50 新功能全新用户权限系统
基于用户角色的部署权限关联
基于用户权限的功能按钮自动显示隐藏
基于用户的WEBSSH授权
django安全更新
开发环境centos 7.2(1511) django 1.11.16 python 2.7
服务端安装生产服务器建议 4核CPU，6G内存以上.
学习测试建议 2核CPU，2G内存以上.
服务器操作系统版本要求 centos7.2 centos7.4
安装之前请关闭防火墙
git clone https://github.com/guohongze/adminset.git
adminset/install/server/auto_install.sh
说明：手动自定义安装请使用adminset/install/server/server_install.sh客户端安装客户端脚本目前rhel/centos6、centos7,ubuntu16.04
客户端python版本支持2.6.6及以上
说明：为保证注册IP是管理IP（后续会被ansible等调用），客户端的IP抓取目前使用主机名解析，否则报错。
如：主机名为cn-bj-web01 请在/etc/hosts中加入相应的解析 192.168.x.x cn-bj-web01，这样再执行adminset_agent.py 可以保证正常运行。
step1: 修改文件install/client/adminset_agent.py :客户端正常使用需要修改脚本中的两个字段：
token = 'HPcWR7l4NJNJ'        #token是上传到服务器的密钥可以在WEB界面的系统配置中自定义
server_ip = '192.168.47.130'  #此项目为adminset server的IP地址
step2: 拷贝install/client/ 目录到客户机的任意位置并执行:cd client
/bin/bash install.sh
step3: 客户端管理service adminsetd start|stop|restart|status
注意：客户端全部功能需要配置服务器到客户端的ssh免密登录。访问关闭防火墙或开通80端口
http://your_server_ip
自动安装的用户名admin 密码Adminset123
手动安装使用自定义创建的super admin用户名密码
说明使用手册，使用手册详细使用说明：自动化安装完成后打开 http://your_server_ip/static/docs/ FAQ参考，常见问题demo每2小时重置一次数据
http://adminset.cn
用户名admin 密码Adminset123
安全强烈建议您不要将程序对公网开放
如果需要公网访问请使用VPN
建议生产环境中使用https配置服务器，并对命令执行、webssh等模块进行安全强化
由于开发方便，在django的settings中开启了DEBUG，在生产中需要关闭并指定自己的域名。
开发者交流请加入开发者群
3号群 730232593
"
https://github.com/mps-youtube/yewtube,"yewtube, forked from mps-youtube , is a Terminal based YouTube player and downloader. No Youtube API key required.","  yewtube, forked from mps-youtube , is a Terminal based YouTube player and downloader. No Youtube API key required. Visit  page if you want to support maintainers of this project.InstallationStable VersionUsing pipUsing pipx (Recommended)Latest VersionUsing pipUsing pipxWhat's new in yewtube?See complete and up-to-date changelog .These features are still inherited from .This project is based on  and mps-youtube is based on , a terminal based program to search, stream and download music. Thisimplementation uses YouTube as a source of content and can play anddownload video as well as audio. The   library handles interfacing with YouTube. ScreenshotsSearch:A standard search is performed by entering  followed by search terms.You can play all of the search results by giving  as inputRepeating song/songs can be done with , for example: or Local Playlists:Search result items can easily be stored in local playlists.YouTube Playlists:YouTube playlists can be searched and played or saved as localplaylists.A playlist search is performed by  followed by search term.Download:Content can be downloaded in various formats and resolutions.Comments:A basic comments browser is available to view YouTube user comments.Music Album Matching:An album title can be specified and yewtube will attempt to findmatches for each track of the album, based on title and duration. Type for more info.Customisation:Search results can be customised to display additional fields andordered by various criteria.This configuration was set up using the following commandsset order views
set columns user:14 date comments rating likes dislikes category:9 views
Type  for help on configuration optionsUpgradingIf installed using pipxOptionally with upgrading all dependencies:If installed using pipOptionally with upgrading all dependencies:Usageyewtube is run on the command line using the command:Enter  from within the program for help.Using yewtube with mpris> # recommended
> pipx install 'yewtube[mpris]'
> # or
> pip install 'yewtube[mpris]'
> yt --version
yewtube version    : 2.8.2
yt_dlp version     : 2022.02.04
Python version     : 3.9.7 (default, Nov  7 2021, 15:17:57)
[GCC 11.2.0]
Processor          : x86_64
Machine type       : x86_64
Architecture       : 64bit, ELF
Platform           : Linux-5.13.0-35-generic-x86_64-with-glibc2.34
sys.stdout.enc     : utf-8
default enc        : utf-8
Config dir         : /home/user/.config/mps-youtube
dbus               : 1.2.18
glib               : True
env:TERM           : tmux-256color
env:SHELL          : /usr/bin/zsh
env:LANG           : en_US.UTF-8
If everything working correctly, dbug and glib would have similar result as above text> playerctl -l
mps-youtube.instance567867
Check also the  if you are having problem with yewtube.How to ContributeContributions are warmly welcomed! However, please check out the  before making a contribution."
https://github.com/facebookresearch/demucs,Code for the paper Hybrid Spectrogram and Waveform Source Separation,"Demucs Music Source SeparationThis is the 4th release of Demucs (v4), featuring Hybrid Transformer based source separation.For the classic Hybrid Demucs (v3): .If you are experiencing issues and want the old Demucs back, please fill an issue, and then you can get back to the v3 with. You can also go .Demucs is a state-of-the-art music source separation model, currently capable of separatingdrums, bass, and vocals from the rest of the accompaniment.Demucs is based on a U-Net convolutional architecture inspired by .The v4 version features , a hybrid spectrogram/waveform separation model using Transformers.It is based on  (also provided in this repo) with the innermost layers arereplaced by a cross-domain Transformer Encoder. This Transformer uses self-attention within each domain,and cross-attention across domains.The model achieves a SDR of 9.00 dB on the MUSDB HQ test set. Moreover, when using sparse attentionkernels to extend its receptive field and per source fine-tuning, we achieve state-of-the-art 9.20 dB of SDR.Samples are available .Checkout  for more information.It has been trained on the  dataset + an extra training dataset of 800 songs.This model separates drums, bass and vocals and other stems for any song.As Hybrid Transformer Demucs is brand new, it is not activated by default, you can activate it in the usualcommands described hereafter with .The single, non fine-tuned model is provided as , and the retrained baselineas . The Sparse Hybrid Transformer model decribed in our paper is not provided as itsrequires custom CUDA code that is not ready for release yet.We are also releasing an experimental 6 sources model, that adds a  and  source.Quick testing seems to show okay quality for , but a lot of bleeding and artifacts for the  source.Important news if you are already using DemucsSee the  for more details.Comparison with other modelsWe provide hereafter a summary of the different metrics presented in the paper.You can also compare Hybrid Demucs (v3), , , Open-Unmix, Demucs (v1), and Conv-Tasnet on one of my favoritesongs on my .Comparison of accuracy is the mean of the SDR for each of the 4 sources,  is a rating from 1 to 5of the naturalness and absence of artifacts given by human listeners (5 = no artifacts), is a rating from 1 to 5 with 5 being zero contamination by other sources. We refer the reader to our ,for more details.| Model                        | Domain      | Extra data?       | Overall SDR | MOS Quality | MOS Contamination ||------------------------------|-------------|-------------------|-------------|-------------|-------------------||        | waveform    | no                | 3.2         | -           | -                 ||       | spectrogram | no                | 5.3         | -           | -                 ||                | spectrogram | no                | 6.0         | -           | -                 ||      | waveform    | no                | 5.7         | -           |                   ||      | waveform    | no                | 6.3         | 2.37        | 2.36              ||  | spectrogram | no                | 6.7         | -           | -                 ||    | hybrid      | no                | 7.5         | 2.86    | 2.55              ||    | spectrogram | no                | 8.2     | -           | -                 || Hybrid Demucs (v3)       | hybrid      | no                | 7.7         | 2.83    | 3.04          ||    | spectrogram | 804 songs         | 6.0         | -           | -                 ||                | spectrogram | 1.5k songs        | 6.7         | -           | -                 ||          | spectrogram | 25k songs         | 5.9         | -           | -                 ||    | spectrogram | 1.7k (mixes only) | 9.0     | -           | -                 || HT Demucs f.t. (v4)      | hybrid      | 800 songs         | 9.0     | -           | -                 |RequirementsYou will need at least Python 3.8. See  for requirements for separation only,and  (or ) if you want to train a new model.For Windows usersEverytime you see , replace it with . You should always run commands from theAnaconda console.For musiciansIf you just want to use Demucs to separate tracks, you can install it withpython3 -m pip install -U demucs
For bleeding edge versions, you can install directly from this repo usingpython3 -m pip install -U git+https://github.com/facebookresearch/demucs#egg=demucs
Advanced OS support are provided on the following page, you must read the page for your OS before posting an issues:For machine learning scientistsIf you have anaconda installed, you can run from the root of this repository:conda env update -f environment-cpu.yml  # if you don't have GPUs
conda env update -f environment-cuda.yml # if you have GPUs
conda activate demucs
pip install -e .
This will create a  environment with all the dependencies installed.You will also need to install : on Mac OSX you can do ,and on Ubuntu . This is used for thepitch/tempo augmentation.Running in DockerThanks to @xserrat, there is now a Docker image definition ready for using Demucs. This can ensure all libraries are correctly installed without interfering with the host OS. See his repo  for more information.Running from ColabI made a Colab to easily separate track with Demucs. Note thattransfer speeds with Colab are a bit slow for large media files,but it will allow you to use Demucs without installing anything.Web DemoIntegrated to  with . See demo: Graphical Interface@CarlGao4 has released a GUI for Demucs: . Downloads for Windows and macOS is available . Use  to speed up your download.@Anjok07 is providing a self contained GUI in  that supports Demucs.Other providersAudiostrip is providing free online separation with Demucs on their website . also provides free online separation, select  for the best quality. provides a realtime Demucs model in their free VST/AU plugin that can be used in your favorite DAW.Separating tracksIn order to try Demucs, you can just run from any folder (as long as you properly installed it)demucs PATH_TO_AUDIO_FILE_1 [PATH_TO_AUDIO_FILE_2 ...]   # for Demucs
# If you used `pip install --user` you might need to replace demucs with python3 -m demucs
python3 -m demucs --mp3 --mp3-bitrate BITRATE PATH_TO_AUDIO_FILE_1  # output files saved as MP3
        # use --mp3-preset to change encoder preset, 2 for best quality, 7 for fastest
# If your filename contain spaces don't forget to quote it !!!
demucs ""my music/my favorite track.mp3""
# You can select different models with `-n` mdx_q is the quantized model, smaller but maybe a bit less accurate.
demucs -n mdx_q myfile.mp3
# If you only want to separate vocals out of an audio, use `--two-stems=vocals` (You can also set to drums or bass)
demucs --two-stems=vocals myfile.mp3
If you have a GPU, but you run out of memory, please use  to reduce length of each split.  should be changed to a integer. Personally recommend not less than 10 (the bigger the number is, the more memory is required, but quality may increase). Create an environment variable  is also helpful. If this still cannot help, please add  to the command line. See the section hereafter for more details on the memory requirements for GPU acceleration.Separated tracks are stored in the  folder. There you will find four stereo wav files sampled at 44.1 kHz: , ,,  (or  if you used the  option).All audio formats supported by  can be processed (i.e. wav, mp3, flac, ogg/vorbis on Linux/Mac OS X etc.). On Windows,  has limited support, so we rely on , which should support pretty much anything.Audio is resampled on the fly if necessary.The output will be a wave file encoded as int16.You can save as float32 wav files with , or 24 bits integer wav with .You can pass  to save as mp3 instead, and set the bitrate with  (default is 320kbps).It can happen that the output would need clipping, in particular due to some separation artifacts.Demucs will automatically rescale each output stem so as to avoid clipping. This can however breakthe relative volume between stems. If instead you prefer hard clipping, pass .You can also try to reduce the volume of the input mixture before feeding it to Demucs.Other pre-trained models can be selected with the  flag.The list of pre-trained models is:The  option allows to separate vocals from the rest (e.g. karaoke mode). can be changed into any source in the selected model.This will mix the files after separating the mix fully, so this won't be faster or use less memory.The  performs multiple predictions with random shifts (a.k.a the shift trick) of the input and average them. This makes prediction  timesslower. Don't use it unless you have a GPU.The  option controls the amount of overlap between prediction windows. Default is 0.25 (i.e. 25%) which is probably fine.It can probably be reduced to 0.1 to improve a bit speed.The  flag allow to specify a number of parallel jobs (e.g. ).This will multiply by the same amount the RAM used so be careful!Memory requirements for GPU accelerationIf you want to use GPU acceleration, you will need at least 3GB of RAM on your GPU for . However, about 7GB of RAM will be required if you use the default arguments. Add  to change size of each split. If you only have 3GB memory, set SEGMENT to 8 (though quality may be worse if this argument is too small). Creating an environment variable  can help users with even smaller RAM such as 2GB (I separated a track that is 4 minutes but only 1.5GB is used), but this would make the separation slower.If you do not have enough memory on your GPU, simply add  to the command line to use the CPU. With Demucs, processing time should be roughly equal to 1.5 times the duration of the track.Calling from another Python programThe main function provides a  parameter as a simple API. You can just pass the parsed command line as this parameter: # Assume that your command is `demucs --mp3 --two-stems vocals -n mdx_extra ""track with space.mp3""`
# The following codes are same as the command above:
import demucs.separate
demucs.separate.main([""--mp3"", ""--two-stems"", ""vocals"", ""-n"", ""mdx_extra"", ""track with space.mp3""])

# Or like this
import demucs.separate
import shlex
demucs.separate.main(shlex.split('--mp3 --two-stems vocals -n mdx_extra ""track with space.mp3""'))
To use more complicated APIs, see Training DemucsIf you want to train (Hybrid) Demucs, please follow the .MDX Challenge reproductionIn order to reproduce the results from the Track A and Track B submissions, checkout the .How to cite@inproceedings{rouard2022hybrid,
  title={Hybrid Transformers for Music Source Separation},
  author={Rouard, Simon and Massa, Francisco and D{\'e}fossez, Alexandre},
  booktitle={ICASSP 23},
  year={2023}
}

@inproceedings{defossez2021hybrid,
  title={Hybrid Spectrogram and Waveform Source Separation},
  author={D{\'e}fossez, Alexandre},
  booktitle={Proceedings of the ISMIR 2021 Workshop on Music Source Separation},
  year={2021}
}
LicenseDemucs is released under the MIT license as found in the  file."
https://github.com/roytseng-tw/Detectron.pytorch,A pytorch implementation of Detectron. Both training from scratch and inferring directly from pretrained Detectron weights are available.,"Use this instead: https://github.com/facebookresearch/maskrcnn-benchmarkA Pytorch Implementation of DetectronThis code follows the implementation architecture of Detectron. Only part of the functionality is supported. Check  for more information.With this code, you can...This repository is originally built on . However, after many modifications, the structure changes a lot and it's now more similar to . I deliberately make everything similar or identical to Detectron's implementation, so as to reproduce the result directly from official pretrained weight files.This implementation has the following features:NewsGetting StartedClone the repo:git clone https://github.com/roytseng-tw/mask-rcnn.pytorch.git
RequirementsTested under python3.CompilationCompile the CUDA code:cd lib  # please change to this directory
sh make.sh
If your are using Volta GPUs, uncomment this  in  and remember to postpend a backslash at the line above.  defaults to . If you want to use a CUDA library on different path, change this  accordingly.It will compile all the modules you need, including NMS, ROI_Pooing, ROI_Crop and ROI_Align. (Actually gpu nms is never used ...)Note that, If you use  to set gpus, make sure at least one gpu is visible when compile the code.Data PreparationCreate a data folder under the repo,cd {repo_root}
mkdir data
Pretrained ModelI use ImageNet pretrained weights from Caffe for the backbone networks.Download them and put them into the .You can the following command to download them all:python tools/download_imagenet_weights.py
NOTE: Caffe pretrained weights have slightly better performance than Pytorch pretrained. Suggest to use Caffe pretrained models from the above link to reproduce the results. By the way, Detectron also use pretrained weights from Caffe.If you want to use pytorch pre-trained models, please remember to transpose images from BGR to RGB, and also use the same data preprocessing (minus mean and normalize) as used in Pytorch pretrained model.ImageNet Pretrained Model provided by DetectronBesides of using the pretrained weights for ResNet above, you can also use the weights from Detectron by changing the corresponding line in model config file as follows:RESNETS:
  IMAGENET_PRETRAINED_WEIGHTS: 'data/pretrained_model/R-50.pkl'
R-50-GN.pkl and R-101-GN.pkl are required for gn_baselines.X-101-32x8d.pkl, X-101-64x4d.pkl and X-152-32x8d-IN5k.pkl are required for ResNeXt backbones.TrainingDO NOT CHANGE anything in the provided config files(configs/Use the environment variable  to control which GPUs to use.Adapative config adjustmentLet's define some terms first&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; batch_size:             x &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; effective_batch_size:  batch_size x &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; change of somethining: Following config options will be adjusted automatically according to actual training setups: 1) number of GPUs , 2) batch size per GPU , 3) update period Train from scratchTake mask-rcnn with res50 backbone for example.python tools/train_net_step.py --dataset coco2017 --cfg configs/baselines/e2e_mask_rcnn_R-50-C4.yml --use_tfboard --bs {batch_size} --nw {num_workers}
Use  to overwrite the default batch size to a proper value that fits into your GPUs. Simliar for , number of data loader threads defaults to 4 in config.py.Specify  to log the losses on Tensorboard.NOTE: use  when training for keypoint-rcnn.The use of As in Caffe, update network once () every  iterations (forward + backward). This way to have a larger effective batch size for training. Notice that, step count is only increased after network update.python tools/train_net_step.py --dataset coco2017 --cfg configs/baselines/e2e_mask_rcnn_R-50-C4.yml --bs 4 --iter_size 4
 defaults to 1.Finetune from a pretrained checkpointpython tools/train_net_step.py ... --load_ckpt {path/to/the/checkpoint}
or using Detectron's checkpoint filepython tools/train_net_step.py ... --load_detectron {path/to/the/checkpoint}
Resume training with the same dataset and batch sizepython tools/train_net_step.py ... --load_ckpt {path/to/the/checkpoint} --resume
When resume the training, step count and optimizer state will also be restored from the checkpoint. For SGD optimizer, optimizer state contains the momentum for each trainable parameter.NOTE:  is not yet supported for Set config options in command line  python tools/train_net_step.py ... --no_save --set {config.name1} {value1} {config.name2} {value2} ...
Show command line help messagespython train_net_step.py --help
Two Training ScriptsIn short, use .In :(Deprecated) In  some config options have no effects and worth noticing:InferenceEvaluate the training resultsFor example, test mask-rcnn on coco2017 val setpython tools/test_net.py --dataset coco2017 --cfg config/baselines/e2e_mask_rcnn_R-50-FPN_1x.yaml --load_ckpt {path/to/your/checkpoint}
Use  to load Detectron's checkpoint. If multiple gpus are available, add .Specify a different output directry, use . Defaults to Visualize the training results on imagespython tools/infer_simple.py --dataset coco --cfg cfgs/baselines/e2e_mask_rcnn_R-50-C4.yml --load_ckpt {path/to/your/checkpoint} --image_dir {dir/of/input/images}  --output_dir {dir/to/save/visualizations}
 defaults to .Supported Network modulesNOTE: the naming is similar to the one used in Detectron. Just remove any prepending .Supported DatasetsOnly COCO is supported for now. However, the whole dataset library implementation is almost identical to Detectron's, so it should be easy to add more datasets supported by Detectron.Configuration OptionsArchitecture specific configuration files are put under . The general configuration file  has almost all the options with same default values as in Detectron's, so it's effortless to transform the architecture specific configs from Detectron.Some options from Detectron are not used because the corresponding functionalities are not implemented yet. For example, data augmentation on testing.Extra optionsHow to transform configuration files from DetectronMy nn.DataParallelBenchmark"
https://github.com/google/seq2seq,A general-purpose encoder-decoder framework for Tensorflow,"[<marko.inline.RawText object at 0x000001592FE2F0C8>][<marko.inline.RawText object at 0x000001592FE2F3C8>]A general-purpose encoder-decoder framework for Tensorflow that can be used for Machine Translation, Text Summarization, Conversational Modeling, Image Captioning, and more.The official code used for the  paper.If you use this code for academic purposes, please cite it as:@ARTICLE{Britz:2017,
  author          = {{Britz}, Denny and {Goldie}, Anna and {Luong}, Thang and {Le}, Quoc},
  title           = ""{Massive Exploration of Neural Machine Translation Architectures}"",
  journal         = {ArXiv e-prints},
  archivePrefix   = ""arXiv"",
  eprinttype      = {arxiv},
  eprint          = {1703.03906},
  primaryClass    = ""cs.CL"",
  keywords        = {Computer Science - Computation and Language},
  year            = 2017,
  month           = mar,
}
This is not an official Google product."
https://github.com/Nuitka/Nuitka,"Nuitka is a Python compiler written in Python.  It's fully compatible with Python 2.6, 2.7, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9, 3.10, and 3.11. You feed it your Python app, it does a lot of clever things, and spits out an executable or extension module. ","####################Nuitka User Manual####################OverviewThis document is the recommended first read if you are interested inusing Nuitka, understand its use cases, check what you can expect,license, requirements, credits, etc.Nuitka is the Python compiler. It is written in Python. It is aseamless replacement or extension to the Python interpreter and compilesevery construct that CPython 2.6, 2.7, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8,3.9, 3.10, 3.11 have, when itself run with that Python version.It then executes uncompiled code and compiled code together in anextremely compatible manner.You can use all Python library modules and all extension modules freely.Nuitka translates the Python modules into a C level program that thenuses  and static C files of its own to execute in the sameway as CPython does.All optimization is aimed at avoiding overhead, where it's unnecessary.None is aimed at removing compatibility, although slight improvementswill occasionally be done, where not every bug of standard Python isemulated, e.g. more complete error messages are given, but there is afull compatibility mode to disable even that.RequirementsC CompilerYou need a C compiler with support for C11 or alternatively a C++compiler for C++03 [#]_.Currently this means, you need to use one of these compilers:.. [#]Support for this C11 is given with gcc 5.x or higher or any clangversion.The MSVC compiler doesn't do it yet. But as a workaround, as the C++03language standard is very overlapping with C11, it is then used insteadwhere the C compiler is too old. Nuitka used to require a C++ compilerin the past, but it changed... [#]Download for free fromhttps://www.visualstudio.com/en-us/downloads/download-visual-studio-vs.aspx(the community editions work just fine).The latest version is recommended but not required. On the other hand,there is no need to except to support pre-Windows 10 versions, and theymight work for you, but support of these configurations is onlyavailable to commercial users.PythonPython Version 2.6, 2.7 or 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9, 3.10, 3.11are supported. If at any moment, there is a stable Python release thatis not in this list, rest assured it is being worked on and will beadded... important::For Python 3.3/3.4 and only those, we need other Python version asa compile time dependency.Nuitka itself is fully compatible with all listed versions, but Sconsas an internally used tool is not.For these versions, you need a Python2 or Python 3.5 or higherinstalled as well, but only during the compile time only. That is foruse with Scons (which orchestrates the C compilation), which does notsupport the same Python versions as Nuitka.In addition, on Windows, Python2 cannot be used because does not work with it, there a Python 3.5 or higher needs to beinstalled.Nuitka finds these needed Python versions (e.g. on Windows viaregistry) and you shouldn't notice it as long as they are installed.Increasingly, other functionality is available when another Pythonhas a certain package installed. For example, onefile compressionwill work for a Python 2.x when another Python is found that has the package installed... admonition:: Moving binaries to other machinesThe created binaries can be made executable independent of the Pythoninstallation, with  and  options... admonition:: Binary filename suffixThe created binaries have an  suffix on Windows. On otherplatforms they have no suffix for standalone mode, or suffix, that you are free to remove or change, or specify with the option.The suffix for acceleration mode is added just to be sure that theoriginal script name and the binary name do not ever collide, so wecan safely do an overwrite without destroying the original sourcefile... admonition:: It has to be CPython, Anaconda Python, or HomebrewYou need the standard Python implementation, called ""CPython"", toexecute Nuitka, because it is closely tied to implementation detailsof it... admonition:: It cannot be from Windows app storeIt is known that Windows app store Python definitely does not work,it's checked against... admonition:: It cannot be pyenv on macOSIt is known that macOS ""pyenv"" does not work. Use Homebrewinstead for self compiled Python installations. But note thatstandalone mode will be worse on these platforms and not be asbackward compatible with older macOS versions.Operating SystemSupported Operating Systems: Linux, FreeBSD, NetBSD, macOS X, andWindows (32bits/64 bits/ARM).Others will work as well. The portability is expected to be generallygood, but the e.g. Nuitka's internal Scons usage may have to be adaptedor need flags passed. Make sure to match Python and C compilerarchitecture, or else you will get cryptic error messages.ArchitectureSupported Architectures are x86, x86_64 (amd64), and arm, likely many,many more.Other architectures are expected to also work, out of the box, as Nuitkais generally not using any hardware specifics. These are just the onestested and known to be good. Feedback is welcome. Generally, thearchitectures that Debian supports can be considered good and testedtoo.UsageCommand LineThe recommended way of executing Nuitka is  to be absolutely certain which Python interpreter you areusing, so it is easier to match with what Nuitka has.The next best way of executing Nuitka bare that is from a sourcecheckout or archive, with no environment variable changes, mostnoteworthy, you do not have to mess with  at all forNuitka. You just execute the  and  scriptsdirectly without any changes to the environment. You may want to add the directory to your  for your convenience, but that stepis optional.Moreover, if you want to execute with the right interpreter, in thatcase, be sure to execute  and be good... admonition:: Pick the right InterpreterIf you encounter a  you absolutely most certainly havepicked the wrong interpreter for the program you are compiling.Nuitka has a  option to output what it can do:.. code:: bashnuitka --helpThe  command is the same as , but with adifferent default. It tries to compile and directly execute a Pythonscript:.. code:: bashnuitka-run --helpThis option that is different is , and passing on argumentsafter the first non-option to the created binary, so it is somewhat moresimilar to what plain  will do.InstallationFor most systems, there will be packages on the __ of Nuitka. But you can alsoinstall it from source code as described above, but also like any otherPython program it can be installed via the normal  routine.LicenseNuitka is licensed under the Apache License, Version 2.0; you may notuse it except in compliance with the License.You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an ""AS IS"" BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.Tutorial Setup and build on WindowsThis is basic steps if you have nothing installed, of course if you haveany of the parts, just skip it.SetupInstall PythonInstall NuitkaWrite some code and testCreate a folder for the Python code.. code:: pythondef talk(message):return ""Talk "" + messagedef main():print(talk(""Hello World""))if name == ""main"":main()Test your programDo as you normally would. Running Nuitka on code that works incorrectlyis not easier to debug... code:: bashpython hello.pyBuild it using.. code:: bashpython -m nuitka hello.py.. note::This will prompt you to download a C caching tool (to speed uprepeated compilation of generated C code) and a MinGW64 based Ccompiler unless you have a suitable MSVC installed. Say  toboth those questions.Run itExecute the  created near .DistributeTo distribute, build with  option, which will not outputa single executable, but a whole folder. Copy the resulting folder to the other machine and run it.You may also try  which does create a single file, but makesure that the mere standalone is working, before turning to it, as itwill make the debugging only harder, e.g. in case of missing data files.Use CasesUse Case 1 - Program compilation with all modules embeddedIf you want to compile a whole program recursively, and not only thesingle file that is the main program, do it like this:.. code:: bashpython -m nuitka --follow-imports program.py.. note::There are more fine grained controls than available. Consider the output of . Including lessmodules into the compilation, but instead using normal Python for itwill make it faster to compile.In case you have a source directory with dynamically loaded files, i.e.one which cannot be found by recursing after normal import statementsvia the  (which would be the recommended way), you canalways require that a given directory shall also be included in theexecutable:.. code:: bashpython -m nuitka --follow-imports --include-plugin-directory=plugin_dir program.py.. note::If you don't do any dynamic imports, simply setting your at compilation time is what you should do.Use  only if you make calls that Nuitka cannot predict, because they e.g. depend on commandline parameters. Nuitka also warns about these, and point to theoption... note::The resulting filename will be  on Windows, on other platforms... note::The resulting binary still depends on CPython and used C extensionmodules being installed.If you want to be able to copy it to another machine, use and copy the created  directory andexecute the  (Windows) or  (otherplatforms) put inside.Use Case 2 - Extension Module compilationIf you want to compile a single extension module, all you have to do isthis:.. code:: bashpython -m nuitka --module some_module.pyThe resulting file  can then be used instead of... important::The filename of the produced extension module must not be changed asPython insists on a module name derived function as an entry point,in this case  and renaming the file will notchange that. Match the filename of the source code to what the binaryname should be... note::If both the extension module and the source code of it are in thesame directory, the extension module is loaded. Changes to the sourcecode only have effect once you recompile... note::The option  and work as well, but the includedmodules will only become importable after you imported the name. If these kinds of imports are invisible toNuitka, e.g. dynamically created, you can use  or in that case, but for static imports it shouldnot be needed... note::An extension module can never include other extension modules. Youwill have to create a wheel for this to be doable... note::The resulting extension module can only be loaded into a CPython ofthe same version and doesn't include other extension modules.Use Case 3 - Package compilationIf you need to compile a whole package and embed all modules, that isalso feasible, use Nuitka like this:.. code:: bashpython -m nuitka --module some_package --include-package=some_package.. note::The inclusion of the package contents needs to be provided manually,otherwise, the package is mostly empty. You can be more specific ifyou want, and only include part of it, or exclude part of it, e.g.with  you would not include theunused test part of your code... note::Data files located inside the package will not be embedded by thisprocess, you need to copy them yourself with this approach.Alternatively you can use the __.Use Case 4 - Program DistributionFor distribution to other systems, there is the standalone mode whichproduces a folder for which you can specify ... code:: bashpython -m nuitka --standalone program.pyFollowing all imports is default in this mode. You can selectivelyexclude modules by specifically saying , butthen an  will be raised when import of it is attempted atprogram run time. This may cause different behavior, but it may alsoimprove your compile time if done wisely.For data files to be included, use the option where the source is a filesystem path, but target has to be specified relative. For standalone youcan also copy them manually, but this can do extra checks, and foronefile mode, there is no manual copying possible.To copy some or all file in a directory, use the option where you get to specify shellpatterns for the files, and a subdirectory where to put them, indicatedby the trailing slash.To copy a whole folder with all files, you can use which will copy all filesincluding a potential subdirectory structure. You cannot filter here,i.e. if you want only a partial copy, remove the files beforehand.For package data, there is a better way, using which detects data files of packagesautomatically and copies them over. It even accepts patterns in shellstyle. It spares you the need to find the package directory yourself andshould be preferred whenever available.With data files, you are largely on your own. Nuitka keeps track of onesthat are needed by popular packages, but it might be incomplete. Raiseissues if you encounter something in these.When that is working, you can use the onefile mode if you so desire... code:: bashpython -m nuitka --onefile program.pyThis will create a single binary, that extracts itself on the target,before running the program. But notice, that accessing files relative toyour program is impacted, make sure to read the section _ as well... code:: bashCreate a binary that unpacks into a temporary folderpython -m nuitka --onefile program.py.. note::There are more platform specific options, e.g. related to icons,splash screen, and version information, consider the output for the details of these and check the section .For the unpacking, by default a unique user temporary path one is used,and then deleted, however this default can beoverridden with a path specification that is using then using a cachedpath, avoiding repeated unpacking, e.g. withwhich uses version information, and user specific cache directory... note::Using cached paths will e.g. be relevant too, when Windows Firewallcomes into play, because otherwise, the binary will be a differentone to it each time it is run.Currently these expanded tokens are available:+----------------+-----------------------------------------------------------+---------------------------------------+| Token          | What this Expands to                                      | Example                               |+================+===========================================================+=======================================+| %TEMP%         | User temporary file directory                             | C:Users...AppDataLocalsTemp |+----------------+-----------------------------------------------------------+---------------------------------------+| %PID%          | Process ID                                                | 2772                                  |+----------------+-----------------------------------------------------------+---------------------------------------+| %TIME%         | Time in seconds since the epoch.                          | 1299852985                            |+----------------+-----------------------------------------------------------+---------------------------------------+| %PROGRAM%      | Full program run-time filename of executable.             | C:SomeWhereYourOnefile.exe        |+----------------+-----------------------------------------------------------+---------------------------------------+| %PROGRAM_BASE% | No-suffix of run-time filename of executable.             | C:SomeWhereYourOnefile            |+----------------+-----------------------------------------------------------+---------------------------------------+| %CACHE_DIR%    | Cache directory for the user.                             | C:UsersSomeBodyAppDataLocal   |+----------------+-----------------------------------------------------------+---------------------------------------+| %COMPANY%      | Value given as                          | YourCompanyName                       |+----------------+-----------------------------------------------------------+---------------------------------------+| %PRODUCT%      | Value given as                          | YourProductName                       |+----------------+-----------------------------------------------------------+---------------------------------------+| %VERSION%      | Combination of  &  | 3.0.0.0-1.0.0.0                       |+----------------+-----------------------------------------------------------+---------------------------------------+| %HOME%         | Home directory for the user.                              | /home/somebody                        |+----------------+-----------------------------------------------------------+---------------------------------------+| %NONE%         | When provided for file outputs,  is used          | see notice below                      |+----------------+-----------------------------------------------------------+---------------------------------------+| %NULL%         | When provided for file outputs,  is used    | see notice below                      |+----------------+-----------------------------------------------------------+---------------------------------------+.. important::It is your responsibility to make the path provided unique, onWindows a running program will be locked, and while using a fixedfolder name is possible, it can cause locking issues in that case,where the program gets restarted.Usually you need to use  or at least  to make apath unique, and this is mainly intended for use cases, where e.g.you want things to reside in a place you choose or abide your namingconventions... important::For disabling output and stderr with  and the values  and  achieveit, but with different effect. With the correspondinghandle becomes . As a result e.g.  will be which is different from  where it will be backedby a file pointing to , i.e. you can write to it.With  you may get  in caseit does get used, with  that never happens. However, somelibraries handle this as input for their logging mechanism, and onWindows this is how you are compatible with  which isbehaving like .Use Case 5 - Setuptools WheelsIf you have a ,  or  drivencreation of wheels for your software in place, putting Nuitka to use isextremely easy.Lets start with the most common  approach, you can -having Nuitka installed of course, simply execute the target rather than the . It takes all theoptions and allows you to specify some more, that are specific toNuitka... code:: pythonFor setup.py if not you't use other build systems:setup(# Data files are to be handled by setuptools and not Nuitkapackage_data={""some_package"": [""some_file.txt""]},...,# This is to pass Nuitka options.command_options={'nuitka': {# boolean option, e.g. if you cared for C compilation commands'--show-scons': True,# options without value, e.g. enforce using Clang'--clang': None,# options with single values, e.g. enable a plugin of Nuitka'--enable-plugin': ""pyside2"",# options with several values, e.g. avoiding including modules'--nofollow-import-to' : ["".tests"", "".distutils""],},},)For setup.py with other build systems:The tuple nature of the arguments is required by the dark nature of""setuptools"" and plugins to it, that insist on full compatibility,e.g. ""setuptools_rust""setup(# Data files are to be handled by setuptools and not Nuitkapackage_data={""some_package"": [""some_file.txt""]},...,# This is to pass Nuitka options....,command_options={'nuitka': {# boolean option, e.g. if you cared for C compilation commands'--show-scons': (""setup.py"", True),# options without value, e.g. enforce using Clang'--clang': (""setup.py"", None),# options with single values, e.g. enable a plugin of Nuitka'--enable-plugin': (""setup.py"", ""pyside2""),# options with several values, e.g. avoiding including modules'--nofollow-import-to' : (""setup.py"", ["".tests"", "".distutils""]),}},)If for some reason, you cannot or do not want to change the target, youcan add this to your ... code:: pythonFor setup.pysetup(...,build_with_nuitka=True).. note::To temporarily disable the compilation, you could remove above line,or edit the value to  by or take its value from anenvironment variable if you so choose, e.g.. This is up to you.Or you could put it in your .. code:: toml[metadata]build_with_nuitka = TrueAnd last, but not least, Nuitka also supports the new  meta, sowhen you have a  already, simple replace or add thisvalue:.. code:: toml[build-system]requires = [""setuptools>=42"", ""wheel"", ""nuitka"", ""toml""]build-backend = ""nuitka.distutils.Build""Data files are to be handled by setuptools and not Nuitka[tool.setuptools.package-data]some_package = ['data_file.txt'][nuitka]These are not recommended, but they make it obvious to have effect.boolean option, e.g. if you cared for C compilation commands, leadingdashes are omittedshow-scons = trueoptions with single values, e.g. enable a plugin of Nuitkaenable-plugin = pyside2options with several values, e.g. avoiding including modules, acceptslist argument.nofollow-import-to = ["".tests"", "".distutils""].. note::For the  requirement above absolute paths like will also work on Linux, use an absolute pathwith two leading slashes, e.g. ... note::Whatever approach you take, data files in these wheels are nothandled by Nuitka at all, but by setuptools. You can however use thedata file embedding of Nuitka commercial. In that case you actuallywould embed the files inside the extension module itself, and not asa file in the wheel.Use Case 6 - MultidistIf you have multiple programs, that each should be executable, in thepast you had to compile multiple times, and deploy all of these. Withstandalone mode, this of course meant that you were fairly wasteful, assharing the folders could be done, but wasn't really supported byNuitka.Enter . There is an option  that replaces or addsto the positional argument given. And it can be given multiple times.When given multiple times, Nuitka will create a binary that contains thecode of all the programs given, but sharing modules used in them. Theytherefore do not have to be distributed multiple times.Lets call the basename of the main path, and entry point. The names ofthese must of course be different. Then the created binary can executeeither entry point, and will react to what  appears toit. So if executed in the right way (with something like or OS API you can control this name), or by renaming or copying thebinary, or symlinking to it, you can then achieve the miracle.This allows to combine very different programs into one... note::This feature is still experimental. Use with care and report yourfindings should you encounter anything that is undesirable behaviorThis mode works with standalone, onefile, and mere acceleration. It doesnot work with module mode.TweaksIconsFor good looks, you may specify icons. On Windows, you can provide anicon file, a template executable, or a PNG file. All of these will workand may even be combined:.. code:: bashThese create binaries with icons on Windowspython -m nuitka --onefile --windows-icon-from-ico=your-icon.png program.pypython -m nuitka --onefile --windows-icon-from-ico=your-icon.ico program.pypython -m nuitka --onefile --windows-icon-template-exe=your-icon.ico program.pyThese create application bundles with icons on macOSpython -m nuitka --macos-create-app-bundle --macos-app-icon=your-icon.png program.pypython -m nuitka --macos-create-app-bundle --macos-app-icon=your-icon.icns program.py.. note::With Nuitka, you do not have to create platform specific icons, butinstead it will convert e.g. PNG, but also other format on the flyduring the build.MacOS EntitlementsEntitlements for an macOS application bundle can be added with theoption, , all values are listed on__An example value would be for requesting access to a Microphone. After the colon, thedescriptive text is to be given... note::Beware that in the likely case of using spaces in the descriptionpart, you need to quote it for your shell to get through to Nuitkaand not be interpreted as Nuitka arguments.Console WindowOn Windows, the console is opened by programs unless you say so. Nuitkadefaults to this, effectively being only good for terminal programs, orprograms where the output is requested to be seen. There is a differencein  and  along those lines. This isreplicated in Nuitka with the option . Nuitkarecommends you to consider this in case you are using  e.g.and other GUI packages, e.g. , but it leaves the decision up toyou. In case, you know your program is console application, just using which will get rid of these kinds of outputs fromNuitka... note::The  is never good to be used with Nuitka, as youcannot see its output.Splash screenSplash screens are useful when program startup is slow. Onefile startupitself is not slow, but your program may be, and you cannot really knowhow fast the computer used will be, so it might be a good idea to havethem. Luckily with Nuitka, they are easy to add for Windows.For splash screen, you need to specify it as a PNG file, and then makesure to disable the splash screen when your program is ready, e.g. hascomplete the imports, prepared the window, connected to the database,and wants the splash screen to go away. Here we are using the projectsyntax to combine the code with the creation, compile this:.. code:: pythonnuitka-project: --onefilenuitka-project: --onefile-windows-splash-screen-image={MAIN_DIRECTORY}/Splash-Screen.pngWhatever this is obviouslyprint(""Delaying startup by 10s..."")import time, tempfile, ostime.sleep(10)Use this code to signal the splash screen removal.if ""NUITKA_ONEFILE_PARENT"" in os.environ:splash_filename = os.path.join(tempfile.gettempdir(),""onefile_%d_splash_feedback.tmp"" % int(os.environ[""NUITKA_ONEFILE_PARENT""]),)  if os.path.exists(splash_filename):
     os.unlink(splash_filename)
print(""Done... splash should be gone."")...Rest of your program goes here.ReportsFor analysis of your program and Nuitka packaging, there is the_ available. You can also make custom reportsproviding your own template, with a few of them built-in to Nuitka.These reports carry all the detail information, e.g. when a module wasattempted to be imported, but not found, you can see where that happens.For bug reporting, it is very much recommended to provide the report.Version InformationYou can attach copyright and trademark information, company name,product name, and so on to your compilation. This is then used inversion information for the created binary on Windows, or applicationbundle on macOS. If you find something that is lacking, let us know.Typical ProblemsDeployment ModeBy default, Nuitka compiles without  which leaves a setof safe guards and helpers on, that are aimed at debugging wrong uses ofNuitka.This is a new feature, and implements a bunch of protections andhelpers, that are documented here.Fork bombs (self-execution)So after compilation,  is the compiled binary. In caseof packages like , , or  what thesetypically do is to expect to run from a full  with and then to be able to use its options like  or  and then be able to launch other codetemporarily or permanently as a service daemon.With Nuitka however, this executes your program again, and puts thesearguments, in  where you maybe ignore them, and then youfork yourself again to launch the helper daemons. Sometimes this ends upspawning CPU count processes that spawn CPU count processes that... thisis called a fork bomb, and with almost all systems, that freezes themeasily to death.That is why e.g. this happens with default Nuitka:.. code::./hello.dist/hello.bin -l fooL -m fooM -n fooN -o fooO -pError, the program tried to call itself with '-m' argument. Disable with '--no-deployment-flag=self-execution'.Your program may well have its own command line parsing, and not use anunsupported package that does attempt to re-execute. In this case, youneed at compile time to use which disables this specific guard.Misleading MessagesSome packages output what they think is helpful information about whatthe reason of a failed import might me. With compiled programs there arevery often just plain wrong. We try and repair those in non-deploymentmode. Here is an example, where we change a message that asks to pipinstall (which is not the issue) to point the user to the includecommand that makes an  plugin work... code:: yamlAnd much moreThe deployment mode is relatively new and has constantly more featuresadded, e.g. something for  should be coming soon.Disabling AllAll these helpers can of course be disabled at once with but keep in mind that for debugging, you may want tore-enable it. You might want to use Nuitka Project options and anenvironment variable to make this conditional.Should you disable them all?We believe, disabling should only happen selectively, but with PyPIupgrades, your code changes, all of these issues can sneak back in. Thespace saving of deployment mode is currently negligible, so attempt tonot do it, but review what exists, and if you know that it cannot affectyou, or if it does, you will not need it. Some of the future ones, willclearly be geared at beginner level usage.Windows Virus scannersBinaries compiled on Windows with default settings of Nuitka and nofurther actions taken might be recognized by some AV vendors as malware.This is avoidable, but only in Nuitka commercial there is actual supportand instructions for how to do it, seeing this as a typical commercialonly need. https://nuitka.net/doc/commercial.htmlLinux StandaloneFor Linux standalone it is pretty difficult to build a binary that workson other Linux versions. This is mainly because on Linux, much softwareis built specifically targeted to concrete DLLs. Things like glibc used,are then encoded into the binary built, and it will not run with anolder glibc, just to give one critical example.The solution is to build on the oldest OS that you want to seesupported. Picking that and setting it up can be tedious, so can belogin, and keeping it secure, as it's something you put your source codeon.To aid that, Nuitka commercial has container based builds, that you canuse. This uses dedicated optimized Python builds, targets CentOS 7 andsupports even newest Pythons and very old OSes that way using recent Ccompiler chains all turn key solution. The effort needs to becompensated to support Nuitka development for Linux, there you need topurchase it https://nuitka.net/doc/commercial.html but even a sponsorlicense will be cheaper than doing it yourself.Memory issues and compiler bugsSometimes the C compilers will crash saying they cannot allocate memoryor that some input was truncated, or similar error messages, clearlyfrom it. There are several options you can explore here:Ask Nuitka to use less memoryThere is a dedicated option  which influences decisionsof Nuitka, such that it avoids high usage of memory during compilationat the cost of increased compile time.Avoid 32 bit C compiler/assembler memory limitsDo not use a 32 bits compiler, but a 64 bit one. If you are using Pythonwith 32 bits on Windows, you most definitely ought to use MSVC as the Ccompiler, and not MinGW64. The MSVC is a cross compiler, and can usemore memory than gcc on that platform. If you are not on Windows, thatis not an option of course. Also using the 64 bits Python will work.Use a minimal virtualenvWhen you compile from a living installation, that may well have manyoptional dependencies of your software installed. Some software, willthen have imports on these, and Nuitka will compile them as well. Notonly may these be just the trouble makers, they also require morememory, so get rid of that. Of course you do have to check that yourprogram has all needed dependencies before you attempt to compile, orelse the compiled program will equally not run.Use LTO compilation or notWith  or  you can switch the C compilation toonly produce bytecode, and not assembler code and machine code directly,but make a whole program optimization at the end. This will change thememory usage pretty dramatically, and if your error is coming from theassembler, using LTO will most definitely avoid that.Switch the C compiler to clangPeople have reported that programs that fail to compile with gcc due toits bugs or memory usage work fine with clang on Linux. On Windows, thiscould still be an option, but it needs to be implemented first for theautomatic downloaded gcc, that would contain it. Since MSVC is known tobe more memory effective anyway, you should go there, and if you want touse Clang, there is support for the one contained in MSVC.Add a larger swap file to your embedded LinuxOn systems with not enough RAM, you need to use swap space. Running outof it is possibly a cause, and adding more swap space, or one at all,might solve the issue, but beware that it will make things extremelyslow when the compilers swap back and forth, so consider the next tipfirst or on top of it.Limit the amount of compilation jobsWith the  option of Nuitka, it will not start many C compilerinstances at once, each competing for the scarce resource of RAM. Bypicking a value of one, only one C compiler instance will be running,and on a 8 core system, that reduces the amount of memory by factor 8,so that's a natural choice right there.Dynamic If your script modifies  to e.g. insert directories withsource code relative to it, Nuitka will not be able to see those.However, if you set the  to the resulting value, it willbe able to compile it and find the used modules from these paths aswell.Manual Python File LoadingA very frequent pattern with private code is that it scans plugindirectories of some kind, and e.g. uses , then considersPython filenames, and then opens a file and does  on them. Thisapproach is working for Python code, but for compiled code, you shoulduse this much cleaner approach, that works for pure Python code and is alot less vulnerable... code:: pythonUsing a package name, to locate the plugins. This is also a saneway to organize them into a directory.scan_path = scan_package.pathfor item in pkgutil.iter_modules(scan_path):importlib.import_module(scan_package.name + ""."" + item.name)  # You may want to do it recursively, but we don't do this here in
  # this example. If you want to, handle that in this kind of branch.
  if item.ispkg:
     ...
Missing data files in standaloneIf your program fails to file data, it can cause all kinds of differentbehaviors, e.g. a package might complain it is not the right version,because a  file check defaulted to unknown. The absence oficon files or help texts, may raise strange errors.Often the error paths for files not being present are even buggy andwill reveal programming errors like unbound local variables. Please lookcarefully at these exceptions keeping in mind that this can be thecause. If your program works without standalone, chances are data filesmight be cause.The most common error indicating file absence is of course an uncaught with a filename. You should figure out whatpackage is missing files and then use (preferably), or / toinclude them.Missing DLLs/EXEs in standaloneNuitka has plugins that deal with copying DLLs. For NumPy, SciPy,Tkinter, etc.These need special treatment to be able to run on other systems.Manually copying them is not enough and will given strange errors.Sometimes newer version of packages, esp. NumPy can be unsupported. Inthis case you will have to raise an issue, and use the older one.If you want to manually add a DLL or an EXE, because it is your projectonly, you will have to use user Yaml files describing where they can befound. This is described in detail with examples in the __page.Dependency creep in standaloneSome packages are a single import, but to Nuitka mean that more than athousand packages (literally) are to be included. The prime example ofPandas, which does want to plug and use just about everything you canimagine. Multiple frameworks for syntax highlighting everythingimaginable take time.Nuitka will have to learn effective caching to deal with this in thefuture. Right now, you will have to deal with huge compilation times forthese.A major weapon in fighting dependency creep should be applied, namelythe  plugin, which offers interesting abilities, that canbe put to use and block unneeded imports, giving an error for where theyoccur. Use it e.g. like this  and e.g. also to get the compiler toerror out for a specific package. Make sure to check its help output. Itcan take for each module of your choice, e.g. forcing also that e.g. is considered uninstalled for standalone mode.It's also driven by a configuration file,  that youcan contribute to, removing typical bloat from packages. Feel free toenhance it and make PRs towards Nuitka with it.Standalone: Finding filesThe standard code that normally works, also works, you should refer to or use all the packages like ,,  to locate data files near thestandalone binary... important::What you should not do, is use the current directory, or assume that this is the script directory, e.g. withpaths like .If you did that, it was never good code. Links, to a program,launching from another directory, etc. will all fail in bad ways. Donot make assumptions about the directory your program is startedfrom.Onefile: Finding filesThere is a difference between  and  of themain module for onefile mode, that is caused by using a bootstrap to atemporary location. The first one will be the original executable path,whereas the second one will be the temporary or permanent path thebootstrap executable unpacks to. Data files will be in the laterlocation, your original environment files will be in the formerlocation.Given 2 files, one which you expect to be near your executable, and onewhich you expect to be inside the onefile binary, access them like this... code:: pythonThis will find a file near your onefile.exeopen(os.path.join(os.path.dirname(sys.argv[0]), ""user-provided-file.txt""))This will find a file inside your onefile.exeopen(os.path.join(os.path.dirname(file), ""user-provided-file.txt""))Windows Programs without console give no errorsFor debugging purposes, remove  or use the options and  with paths asdocumented for  above. These can be relativeto the program or absolute, so you can see the outputs given.Deep copying uncompiled functionsSometimes people use this kind of code, which for packages on PyPI, wedeal with by doing source code patches on the fly. If this is in yourown code, here is what you can do:.. code:: pythondef binder(func, name):result = types.FunctionType(func.code, func.globals, name=func.name, argdefs=func.defaults, closure=func.closure)result = functools.update_wrapper(result, func)result.kwdefaults = func.kwdefaultsresult.name = namereturn resultCompiled functions cannot be used to create uncompiled ones from, so theabove code will not work. However, there is a dedicated method, that is specific to them, so use this instead... code:: pythondef binder(func, name):try:result = func.clone()except AttributeError:result = types.FunctionType(func.code, func.globals, name=func.name, argdefs=func.defaults, closure=func.closure)result = functools.update_wrapper(result, func)result.kwdefaults = func.kwdefaults  result.__name__ = name
  return result
Modules: Extension modules are not executable directlyA package can be compiled with Nuitka, no problem, but when it comes toexecuting it,  is not going to work andgive the error  because thecompiled module is not source code, and Python will not just load it.The closest would be  and you mighthave to call the main function yourself.To support this, the CPython  and/or would need improving such that Nuitka could supply its compiled moduleobject for Python to use.TipsNuitka Options in the codeThere is support for conditional options, and options using pre-definedvariables, this is an example:.. code:: pythonCompilation mode, support OS specific optionsnuitka-project-if: {OS} in (""Windows"", ""Linux"", ""Darwin"", ""FreeBSD""):nuitka-project: --onefilenuitka-project-else:nuitka-project: --standaloneThe PySide2 plugin covers qt-pluginsnuitka-project: --enable-plugin=pyside2nuitka-project: --include-qt-plugins=sensible,qmlDebugging togglenuitka-project-if: os.environ.get(""DEBUG_COMPILATION"", ""no"") == ""yes""nuitka-project: --debugnuitka-project-else:nuitka-project: --deploymentThe comments must be a start of line, and indentation is to be used, toend a conditional block, much like in Python. There are currently noother keywords than the used ones demonstrated above.You can put arbitrary Python expressions there, and if you wanted toe.g. access a version information of a package, you could simply use if that would be required toe.g. enable or disable certain Nuitka settings. The only thing Nuitkadoes that makes this not Python expressions, is expanding for a pre-defined set of variables:Table with supported variables:+------------------+--------------------------------+------------------------------------------+| Variable         | What this Expands to           | Example                                  |+==================+================================+==========================================+| {OS}             | Name of the OS used            | Linux, Windows, Darwin, FreeBSD, OpenBSD |+------------------+--------------------------------+------------------------------------------+| {Version}        | Version of Nuitka              | e.g. (1, 6, 0)                           |+------------------+--------------------------------+------------------------------------------+| {Commercial}     | Version of Nuitka Commercial   | e.g. (2, 1, 0)                           |+------------------+--------------------------------+------------------------------------------+| {Arch}           | Architecture used              | x86_64, arm64, etc.                      |+------------------+--------------------------------+------------------------------------------+| {MAIN_DIRECTORY} | Directory of the compiled file | some_dir/maybe_relative                  |+------------------+--------------------------------+------------------------------------------+| {Flavor}         | Variant of Python              | e.g. Debian Python, Anaconda Python      |+------------------+--------------------------------+------------------------------------------+The use of  is recommended when you want to specifya filename relative to the main script, e.g. for use in data fileoptions or user package configuration yaml files,.. code:: pythonnuitka-project: --include-data-files={MAIN_DIRECTORY}/my_icon.png=my_icon.pngnuitka-project: --user-package-configuration-file={MAIN_DIRECTORY}/user.nuitka-package.config.ymlPython command line flagsFor passing things like  or  to Python, to your compiledprogram, there is a command line option name  whichmakes Nuitka emulate these options.The most important ones are supported, more can certainly be added.Caching compilation resultsThe C compiler, when invoked with the same input files, will take a longtime and much CPU to compile over and over. Make sure you are having installed and configured when using gcc (even on Windows). Itwill make repeated compilations much faster, even if things are not yetnot perfect, i.e. changes to the program can cause many C files tochange, requiring a new compilation instead of using the cached result.On Windows, with gcc Nuitka supports using  which it willoffer to download from an official source and it automatically. This isthe recommended way of using it on Windows, as other versions can e.g.hang.Nuitka will pick up  if it's in found in system , andit will also be possible to provide if by setting to the full path of the binary, this is for usein CI systems where things might be non-standard.For the MSVC compilers and ClangCL setups, using the  isautomatic and included in Nuitka.On macOS and Intel, there is an automatic download of a binary from our site, for arm64 arches, it's recommended to use thissetup, which installs Homebrew and ccache in there. Nuitka picks thatone up automatically if it on that kind of machine. You need and shouldnot use Homebrew with Nuitka otherwise, it's not the best for standalonedeployments, but we can take  from there... code:: bashexport HOMEBREW_INSTALL_FROM_API=1/bin/bash -c ""$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)""eval $(/opt/homebrew/bin/brew shellenv)brew install ccacheControl where Caches liveThe storage for cache results of all kinds, downloads, cachedcompilation results from C and Nuitka, is done in a platform dependentdirectory as determined by the  package. However, you canoverride it with setting the environment variable to a base directory. This is for use in environments where the homedirectory is not persisted, but other paths are.RunnersAvoid running the  binary, doing  willmake a 100% sure you are using what you think you are. Using the wrongPython will make it give you  for good code or for installed modules. That is happening, when you runNuitka with Python2 on Python3 code and vice versa. By explicitlycalling the same Python interpreter binary, you avoid that issueentirely.Fastest C CompilersThe fastest binaries of  on Windows with 64 bits Pythonproved to be significantly faster with MinGW64, roughly 20% betterscore. So it is recommended for use over MSVC. Using  ofClang7 was faster than MSVC, but still significantly slower thanMinGW64, and it will be harder to use, so it is not recommended.On Linux for  the binary produced by  wasfaster than , but not by a significant margin. Since gcc ismore often already installed, that is recommended to use for now.Differences in C compilation times have not yet been examined.Unexpected SlowdownsUsing the Python DLL, like standard CPython does can lead to unexpectedslowdowns, e.g. in uncompiled code that works with Unicode strings. Thisis because calling to the DLL rather than residing in the DLL causesoverhead, and this even happens to the DLL with itself, being slower,than a Python all contained in one binary.So if feasible, aim at static linking, which is currently only possiblewith Anaconda Python on non-Windows, Debian Python2, self compiledPythons (do not activate , not needed), and installscreated with ... note::On Anaconda, you may need to execute Standalone executables and dependenciesThe process of making standalone executables for Windows traditionallyinvolves using an external dependency walker in order to copy necessarylibraries along with the compiled executables to the distributionfolder.There is plenty of ways to find that something is missing. Do notmanually copy things into the folder, esp. not DLLs, as that's not goingto work. Instead make bug reports to get these handled by Nuitkaproperly.Windows errors with resourcesOn Windows, the Windows Defender tool and the Windows Indexing Serviceboth scan the freshly created binaries, while Nuitka wants to work withit, e.g. adding more resources, and then preventing operations randomlydue to holding locks. Make sure to exclude your compilation stage fromthese services.Windows standalone program redistributionWhether compiling with MingW or MSVC, the standalone programs haveexternal dependencies to Visual C Runtime libraries. Nuitka tries toship those dependent DLLs by copying them from your system.Beginning with Microsoft Windows 10, Microsoft ships (Universal C Runtime libraries) which handles calls to.With earlier Windows platforms (and wine/ReactOS), you should considerinstalling Visual C runtime libraries before executing a Nuitkastandalone compiled program.Depending on the used C compiler, you'll need the following redistversions on the target machines. However notice that compilation usingthe 14.3 based version is recommended.+------------------+-------------+-------------------------------+| Visual C version | Redist Year | CPython                       |+==================+=============+===============================+| 14.3             | 2022        | 3.11                          |+------------------+-------------+-------------------------------+| 14.2             | 2019        | 3.5, 3.6, 3.7, 3.8, 3.9, 3.10 |+------------------+-------------+-------------------------------+| 14.1             | 2017        | 3.5, 3.6, 3.7, 3.8            |+------------------+-------------+-------------------------------+| 14.0             | 2015        | 3.5, 3.6, 3.7, 3.8            |+------------------+-------------+-------------------------------+| 10.0             | 2010        | 3.3, 3.4                      |+------------------+-------------+-------------------------------+| 9.0              | 2008        | 2.6, 2.7                      |+------------------+-------------+-------------------------------+When using MingGW64, you'll need the following redist versions:+------------------+-------------+-------------------------------------+| MingGW64 version | Redist Year | CPython                             |+==================+=============+=====================================+| 8.1.0            | 2015        | 3.5, 3.6, 3.7, 3.8, 3.9, 3.10, 3.11 |+------------------+-------------+-------------------------------------+Once the corresponding runtime libraries are installed on the targetsystem, you may remove all  files from your Nuitkacompiled dist folder.Detecting Nuitka at run timeNuitka does not  unlike other tools, because it usuallytriggers inferior code for no reason. For Nuitka, we have the moduleattribute  to test if a specific module was compiled,and the function attribute  to test if a specificfunction was compiled.Providing extra Options to Nuitka C compilationNuitka will apply values from the environment variables , during the compilation on top of what it determines to benecessary. Beware of course, that is this is only useful if you knowwhat you are doing, so should this pose an issues, raise them only withperfect information.Producing a 32 bit binary on a 64 bit Windows systemNuitka will automatically target the architecture of the Python you areusing. If this is 64 bits, it will create a 64 bits binary, if it is 32bits, it will create a 32 bits binary. You have the option to select thebits when you download the Python. In the output of  there is a line for the architecture. It for 64 bits, and just  for 32 bits.The C compiler will be picked to match that more or less automatically.If you specify it explicitly and it mismatches, you will get a warningabout the mismatch and informed that your compiler choice was rejected.Compilation ReportWhen you use  Nuitka will create anXML file with detailed information about the compilation and packagingprocess. This is growing in completeness with every release and exposesmodule usage attempts, timings of the compilation, plugin influences,data file paths, DLLs, and reasons why things are included or not.At this time, the report contains absolute paths in some places, withyour private information. The goal is to make this blended out bydefault, because we also want to become able to compare compilationreports from different setups, e.g. with updated packages, and see thechanges to Nuitka. The report is however recommended for your bugreporting.Also, another form is available, where the report is free form andaccording to a Jinja2 template of yours, and one that is included inNuitka. The same information as used to produce the XML file isaccessible. However, right now this is not yet documented, but we planto add a table with the data. For reader of the source code that isfamiliar with Jinja2, however, it will be easy to do it now already.If you have a template, you can use it like this and ofcourse, the usage of restructured text, is only an example. You can usemarkdown, your own XML, or whatever you see fit. Nuitka will just expandthe template with the compilation report data.Currently the follow reports are included in Nuitka. You just use thename as a filename, and Nuitka will pick that one instead.+---------------+--------------+--------------------------------------------------------+| Report Name   | Status       | Purpose                                                |+===============+==============+========================================================+| LicenseReport | experimental | Distributions used in a compilation with license texts |+---------------+--------------+--------------------------------------------------------+.. note::The community can and should contribute more report types and helpenhancing the existing ones for good looks.PerformanceThis chapter gives an overview, of what to currently expect in terms ofperformance from Nuitka. It's a work in progress and is updated as wego. The current focus for performance measurements is Python 2.7, but3.x is going to follow later.pystone resultsThe results are the top value from this kind of output, running pystone1000 times and taking the minimal value. The idea is that the fastestrun is most meaningful, and eliminates usage spikes... code:: bashecho ""Uncompiled Python2""for i in {1..100}; do BENCH=1 python2 tests/benchmarks/pystone.py ; done | sort -rn | head -n 1python2 -m nuitka --lto=yes --pgo tests/benchmarks/pystone.pyecho ""Compiled Python2""for i in {1..100}; do BENCH=1 ./pystone.bin ; done | sort -n | head -rn 1echo ""Uncompiled Python3""for i in {1..100}; do BENCH=1 python3 tests/benchmarks/pystone3.py ; done | sort -rn | head -n 1python3 -m nuitka --lto=yes --pgo tests/benchmarks/pystone3.pyecho ""Compiled Python3""for i in {1..100}; do BENCH=1 ./pystone3.bin ; done | sort -rn | head -n 1+-------------------+-------------------+----------------------+---------------------+| Python            | Uncompiled        | Compiled LTO         | Compiled PGO        |+===================+===================+======================+=====================+| Debian Python 2.7 | 137497.87 (1.000) | 460995.20 (3.353)    | 503681.91 (3.663)   |+-------------------+-------------------+----------------------+---------------------+| Nuitka Python 2.7 | 144074.78 (1.048) | 479271.51 (3.486)    | 511247.44 (3.718)   |+-------------------+-------------------+----------------------+---------------------+Where to go nextRemember, this project needs constant work. Although the Pythoncompatibility is insanely high, and test suite works near perfectly,there is still more work needed, esp. to make it do more optimization.Try it out, and when popular packages do not work, please make reportson GitHub.Follow me on Mastodon and TwitterNuitka announcements and interesting stuff is pointed to on both theMastodon and Twitter accounts, but obviously with not too many details,usually pointing to the website, but sometimes I also ask questionsthere.. .Report issues or bugsShould you encounter any issues, bugs, or ideas, please visit the__ andreport them.Best practices for reporting bugs:Word of WarningConsider using this software with caution. Even though many tests areapplied before releases, things are potentially breaking. Your feedbackand patches to Nuitka are very welcome.Join NuitkaYou are more than welcome to join Nuitka development and help tocomplete the project in all minor and major ways.The development of Nuitka occurs in git. We currently have these 3branches:.. note::The __ explains the codingrules, branching model used, with feature branches and hotfixreleases, the Nuitka design and much more. Consider reading it tobecome a contributor. This document is intended for Nuitka users.DonationsShould you feel that you cannot help Nuitka directly, but still want tosupport, please consider __ and help this way.Unsupported functionalityThe  attribute of code objectsThe code objects are empty for native compiled functions. There is nobytecode with Nuitka's compiled function objects, so there is no way toprovide it.PDBThere is no tracing of compiled functions to attach a debugger to.OptimizationConstant FoldingThe most important form of optimization is the constant folding. This iswhen an operation can be fully predicted at compile time. Currently,Nuitka does these for some built-ins (but not all yet, somebody to lookat this more closely will be very welcome!), and it does it e.g. forbinary/unary operations and comparisons.Constants currently recognized:.. code:: python5 + 6  # binary operationsnot 7  # unary operations5 < 6  # comparisonsrange(3)  # built-insLiterals are the one obvious source of constants, but also most likelyother optimization steps like constant propagation or function inliningwill be. So this one should not be underestimated and a very importantstep of successful optimizations. Every option to produce a constant mayimpact the generated code quality a lot... admonition:: StatusThe folding of constants is considered implemented, but it might beincomplete in that not all possible cases are caught. Please reportit as a bug when you find an operation in Nuitka that has onlyconstants as input and is not folded.Constant PropagationAt the core of optimizations, there is an attempt to determine thevalues of variables at run time and predictions of assignments. Itdetermines if their inputs are constants or of similar values. Anexpression, e.g. a module variable access, an expensive operation, maybe constant across the module of the function scope and then there needsto be none or no repeated module variable look-up.Consider e.g. the module attribute  which likely is onlyever read, so its value could be predicted to a constant string known atcompile time. This can then be used as input to the constant folding... code:: pythonif name == ""main"":# Your test code might be hereuse_something_not_use_by_program().. admonition:: StatusFrom modules attributes, only  is currently actuallyoptimized. Also possible would be at least . In thefuture, this may improve as SSA is expanded to module variables.Built-in Name LookupsAlso, built-in exception name references are optimized if they are usedas a module level read-only variables:.. code:: pythontry:something()except ValueError:  # The ValueError is a slow global name lookup normally.pass.. admonition:: StatusThis works for all built-in names. When an assignment is done to sucha name, or it's even local, then, of course, it is not done.Built-in Call PredictionFor built-in calls like , , or  it is oftenpossible to predict the result at compile time, esp. for constant inputsthe resulting value often can be precomputed by Nuitka. It can simplydetermine the result or the raised exception and replace the built-incall with that value, allowing for more constant folding or code pathreduction... code:: pythontype(""string"")  # predictable result, builtin type str.len([1, 2])  # predictable resultrange(3, 9, 2)  # predictable resultrange(3, 9, 0)  # predictable exception, range raises due to 0... admonition:: StatusThe built-in call prediction is considered implemented. We can simplyduring compile time emulate the call and use its result or raisedexception. But we may not cover all the built-ins there are yet.Sometimes the result of a built-in should not be predicted when theresult is big. A  call e.g. may give too big values toinclude the result in the binary. Then it is not done... code:: pythonrange(100000)  # We do not want this one to be expanded.. admonition:: StatusThis is considered mostly implemented. Please file bugs for built-insthat are pre-computed, but should not be computed by Nuitka atcompile time with specific values.Conditional Statement PredictionFor conditional statements, some branches may not ever be taken, becauseof the condition truth value being possible to predict. In these cases,the branch not taken and the condition check is removed.This can typically predict code like this:.. code:: pythonif name == ""main"":# Your test code might be hereuse_something_not_use_by_program()or.. code:: pythonif False:# Your deactivated code might be hereuse_something_not_use_by_program()It will also benefit from constant propagations, or enable them becauseonce some branches have been removed, other things may become morepredictable, so this can trigger other optimization to become possible.Every branch removed makes optimization more likely. With some codebranches removed, access patterns may be more friendly. Imagine e.g.that a function is only called in a removed branch. It may be possibleto remove it entirely, and that may have other consequences too... admonition:: StatusThis is considered implemented, but for the maximum benefit, moreconstants need to be determined at compile time.Exception PropagationFor exceptions that are determined at compile time, there is anexpression that will simply do raise the exception. These can bepropagated upwards, collecting potentially ""side effects"", i.e. parts ofexpressions that were executed before it occurred, and still have to beexecuted.Consider the following code:.. code:: pythonprint(side_effect_having() + (1 / 0))print(something_else())The  can be predicted to raise a exception, which will be propagated through the  operation. Thatpart is just Constant Propagation as normal.The call  will have to be retained though, butthe  does not and can be turned into an explicit raise. Thestatement sequence can then be aborted and as such the call needs no code generation or considerationanymore.To that end, Nuitka works with a special node that raises an exceptionand is wrapped with a so-called ""side_effects"" expression, but yet canbe used in the code as an expression having a value... admonition:: StatusThe propagation of exceptions is mostly implemented but needshandling in every kind of operations, and not all of them might do italready. As work progresses or examples arise, the coverage will beextended. Feel free to generate bug reports with non-workingexamples.Exception Scope ReductionConsider the following code:.. code:: pythontry:b = 8print(range(3, b, 0))print(""Will not be executed"")except ValueError as e:print(e)The  block is bigger than it needs to be. The statement cannot cause a  to be raised. As such it can be moved tooutside the try without any risk... code:: pythonb = 8try:print(range(3, b, 0))print(""Will not be executed"")except ValueError as e:print(e).. admonition:: StatusThis is considered done. For every kind of operation, we trace if itmay raise an exception. We do however not track properly yet, whatcan do a  and what cannot.Exception Block InliningWith the exception propagation, it then becomes possible to transformthis code:.. code:: pythontry:b = 8print(range(3, b, 0))print(""Will not be executed!"")except ValueError as e:print(e).. code:: pythontry:raise ValueError(""range() step argument must not be zero"")except ValueError as e:print(e)Which then can be lowered in complexity by avoiding the raise and catchof the exception, making it:.. code:: pythone = ValueError(""range() step argument must not be zero"")print(e).. admonition:: StatusThis is not implemented yet.Empty Branch RemovalFor loops and conditional statements that contain only code withouteffect, it should be possible to remove the whole construct:.. code:: pythonfor i in range(1000):passThe loop could be removed, at maximum, it should be considered anassignment of variable  to  and no more... admonition:: StatusThis is not implemented yet, as it requires us to track iterators,and their side effects, as well as loop values, and exit conditions.Too much yet, but we will get there.Another example:.. code:: pythonif side_effect_free:passThe condition check should be removed in this case, as its evaluation isnot needed. It may be difficult to predict that  hasno side effects, but many times this might be possible... admonition:: StatusThis is considered implemented. The conditional statement nature isremoved if both branches are empty, only the condition is evaluatedand checked for truth (in cases that could raise an exception).Unpacking PredictionWhen the length of the right-hand side of an assignment to a sequencecan be predicted, the unpacking can be replaced with multipleassignments... code:: pythona, b, c = 1, side_effect_free(), 3.. code:: pythona = 1b = side_effect_free()c = 3This is of course only really safe if the left-hand side cannot raise anexception while building the assignment targets.We do this now, but only for constants, because we currently have noability to predict if an expression can raise an exception or not... admonition:: StatusThis is partially implemented. We are working on unpackingenhancements, that will recognize where index access is available.This faster access will then avoid tuples and iteration, then thiswill be perfect.Built-in Type InferenceWhen a construct like  or  is used, it ispossible to know what the iteration does and represent that so thatiterator users can use that instead.I consider that:.. code:: pythonfor i in xrange(1000):something(i)could translate  into an object of a special class thatdoes the integer looping more efficiently. In case  is onlyassigned from there, this could be a nice case for a dedicated class... admonition:: StatusFuture work, not even started.Quicker Function CallsFunctions are structured so that their parameter parsing and interface is separate from the actual function code. This way the callcan be optimized away. One problem is that the evaluation order candiffer... code:: pythondef f(a, b, c):return a, b, cf(c=get1(), b=get2(), a=get3())This will have to evaluate first , then  and onlythen  and then make the function call with these values.Therefore it will be necessary to have a staging of the parametersbefore making the actual call, to avoid a re-ordering of the calls to, , and ... admonition:: StatusNot even started. A re-formulation that avoids the dictionary to callthe function, and instead uses temporary variables appears to berelatively straightforward once we do that kind of parameteranalysis.Lowering of iterated Container TypesIn some cases, accesses to  constants can become constants instead.Consider that:.. code:: pythonfor x in [a, b, c]:something(x)Can be optimized into this:.. code:: pythonfor x in (a, b, c):something(x)This allows for simpler, faster code to be generated, and fewer checksneeded, because e.g. the  is clearly immutable, whereas the needs a check to assert that. This is also possible for sets... admonition:: StatusImplemented, even works for non-constants. Needs other optimizationto become generally useful, and will itself help other optimizationto become possible. This allows us to e.g. only treat iteration overtuples, and not care about sets.In theory, something similar is also possible for . For thelater, it will be non-trivial though to maintain the order of executionwithout temporary values introduced. The same thing is done for pureconstants of these types, they change to  values when iterated.Metadata calls at compile timeNuitka does not include metadata in the distribution. It's rather large,and the goal is to use it at compile time. Therefore information aboutentry points, version checks, etc. are all done at compile time ratherthan at run time. Not only is that faster, it also recognized problemssooner... code:: pythonpkg_resources.require(""lxml"")importlib.metadata.version(""lxml"")..... admonition:: StatusThis is considered complete. The coverage of the APIs is very good,but naturally this will always have to be code that uses compile timevalues, but that is nearly never an issue, and where it happens, weuse ""anti-bloat"" patches to deal with these in 3rd party packages.Updates for this ManualThis document is written in REST. That is an ASCII format which isreadable to human, but easily used to generate PDF or HTML documents.You will find the current version at:https://nuitka.net/doc/user-manual.html"
https://github.com/bregman-arie/devops-exercises,"Linux, Jenkins, AWS, SRE, Prometheus, Docker, Python, Ansible, Git, Kubernetes, Terraform, OpenStack, SQL, NoSQL, Azure, GCP, DNS, Elastic, Network, Virtualization. DevOps Interview Questions",":information_source: &nbsp;This repo contains questions and exercises on various technical topics, sometimes related to DevOps and SRE:bar_chart: &nbsp;There are currently 2624 exercises and questions:warning: &nbsp;You can use these for preparing for an interview but most of the questions and exercises don't represent an actual interview. Please read  for more details:stop_sign: &nbsp;If you are interested in pursuing a career as DevOps engineer, learning some of the concepts mentioned here would be useful, but you should know it's not about learning all the topics and technologies mentioned in this repository:pencil: &nbsp;You can add more exercises by submitting pull requests :) Read about contribution guidelines NetworkA set of protocols that define how two or more devices can communicate with each other.To learn more about TCP/IP, read Ethernet simply refers to the most common type of Local Area Network (LAN) used today. A LAN—in contrast to a WAN (Wide Area Network), which spans a larger geographical area—is a connected network of computers in a small area, like your office, college campus, or even home.A MAC address is a unique identification number or code used to identify individual devices on the network.Packets that are sent on the ethernet are always coming from a MAC address and sent to a MAC address. If a network adapter is receiving a packet, it is comparing the packet’s destination MAC address to the adapter’s own MAC address.When a device sends a packet to the broadcast MAC address (FF:FF:FF:FF:FF:FF​), it is delivered to all stations on the local network. Ethernet broadcasts are used to resolve IP addresses to MAC addresses (by ARP) at the data link layer.An Internet Protocol address (IP address) is a numerical label assigned to each device connected to a computer network that uses the Internet Protocol for communication.An IP address serves two main functions: host or network interface identification and location addressing.A Subnet mask is a 32-bit number that masks an IP address and divides the IP addresses into network addresses and host addresses. Subnet Mask is made by setting network bits to all ""1""s and setting host bits to all ""0""s. Within a given network, out of the total usable host addresses, two are always reserved for specific purposes and cannot be allocated to any host. These are the first address, which is reserved as a network address (a.k.a network ID), and the last address used for network broadcast.You can read more about the OSI model in Unicast: One-to-one communication where there is one sender and one receiver.Broadcast: Sending a message to everyone in the network. The address ff:ff:ff:ff:ff:ff is used for broadcasting.Two common protocols which use broadcast are ARP and DHCP.Multicast: Sending a message to a group of subscribers. It can be one-to-many or many-to-many.CSMA/CD stands for Carrier Sense Multiple Access / Collision Detection.Its primary focus is to manage access to a shared medium/bus where only one host can transmit at a given point in time.CSMA/CD algorithm:A router, switch, and hub are all network devices used to connect devices in a local area network (LAN). However, each device operates differently and has its specific use cases. Here is a brief description of each device and the differences between them:Three collision domains and one broadcast domainA router is a physical or virtual appliance that passes information between two or more packet-switched computer networks. A router inspects a given data packet's destination Internet Protocol address (IP address), calculates the best way for it to reach its destination, and then forwards it accordingly.Network Address Translation (NAT) is a process in which one or more local IP addresses are translated into one or more Global IP address and vice versa in order to provide Internet access to the local hosts.A proxy server acts as a gateway between you and the internet. It’s an intermediary server separating end users from the websites they browse.If you’re using a proxy server, internet traffic flows through the proxy server on its way to the address you requested. The request then comes back through that same proxy server (there are exceptions to this rule), and then the proxy server forwards the data received from the website to you.Proxy servers provide varying levels of functionality, security, and privacy depending on your use case, needs, or company policy.TCP 3-way handshake or three-way handshake is a process that is used in a TCP/IP network to make a connection between server and client.A three-way handshake is primarily used to create a TCP socket connection. It works when:From : ""the length of time it takes for a signal to be sent plus the length of time it takes for an acknowledgment of that signal to be received""Bonus question: what is the RTT of LAN?TCP establishes a connection between the client and the server to guarantee the order of the packages, on the other hand, UDP does not establish a connection between the client and server and doesn't handle package orders. This makes UDP more lightweight than TCP and a perfect candidate for services like streaming. provides a good explanation.A default gateway serves as an access point or IP router that a networked computer uses to send information to a computer in another network or the internet.ARP stands for Address Resolution Protocol. When you try to ping an IP address on your local network, say 192.168.1.1, your system has to turn the IP address 192.168.1.1 into a MAC address. This involves using ARP to resolve the address, hence its name.Systems keep an ARP look-up table where they store information about what IP addresses are associated with what MAC addresses. When trying to send a packet to an IP address, the system will first consult this table to see if it already knows the MAC address. If there is a value cached, ARP is not used.It stands for Dynamic Host Configuration Protocol and allocates IP addresses, subnet masks, and gateways to hosts. This is how it works:Read more It is possible to have two DHCP servers on the same network, however, it is not recommended, and it is important to configure them carefully to prevent conflicts and configuration problems.Here's how SSL tunneling works:There are several reasons why we should consider using IPv6 over IPv4:MTU stands for Maximum Transmission Unit. It's the size of the largest PDU (protocol Data Unit) that can be sent in a single transaction.With the IPv4 protocol, the router can fragment the PDU and then send all the fragmented PDU through the transaction.With IPv6 protocol, it issues an error to the user's computer.False. Ping is actually using ICMP (Internet Control Message Protocol) which is a network protocol used to send diagnostic messages and control messages related to network communication.ICMP messages are used for a variety of purposes, including:NAT stands for Network Address Translation. It’s a way to map multiple local private addresses to a public one before transferring the information. Organizations that want multiple devices to employ a single IP address use NAT, as do most home routers.For example, your computer's private IP could be 192.168.1.100, but your router maps the traffic to its public IP (e.g. 1.1.1.1). Any device on the internet would see the traffic coming from your public IP (1.1.1.1) instead of your private IP (192.168.1.100).Several factors can affect network performance, including:APIPA is a set of IP addresses that devices are allocatedwhen the main DHCP server is not reachableAPIPA uses the IP range: 169.254.0.1 - 169.254.255.254.Control Plane and Data PlaneThe control plane is a part of the network that decides how to route and forward packets to a different location.The data plane is a part of the network that actually forwards the data/packets.It refers to monitoring and management functions.Control Plane.OSPF (Open Shortest Path First) is a routing protocol that can be implemented on various types of routers. In general, OSPF is supported on most modern routers, including those from vendors such as Cisco, Juniper, and Huawei. The protocol is designed to work with IP-based networks, including both IPv4 and IPv6. Also, it uses a hierarchical network design, where routers are grouped into areas, with each area having its own topology map and routing table. This design helps to reduce the amount of routing information that needs to be exchanged between routers and improve network scalability.The OSPF 4 Types of routers are:Learn more about OSPF router types: https://www.educba.com/ospf-router-types/Latency is the time taken for information to reach its destination from the source.Bandwidth is the capacity of a communication channel to measure how much data the latter can handle over a specific time period. More bandwidth would imply more traffic handling and thus more data transfer.Throughput refers to the measurement of the real amount of data transferred over a certain period of time across any transmission channel.Latency. To have good latency, a search query should be forwarded to the closest data center.Throughput. To have good throughput, the upload stream should be routed to an underutilized link.Network congestion occurs when there is too much data to transmit on a network and it doesn't have enough capacity to handle the demand. This can lead to increased latency and packet loss. The causes can be multiple, such as high network usage, large file transfers, malware, hardware issues, or network design problems. To prevent network congestion, it's important to monitor your network usage and implement strategies to limit or manage the demand.00110011110100011101Read more [here](https://www.globalsign.com/en/blog/what-is-hsts-and-how-do-i-use-it#:~:text=HTTP%20Strict%20Transport%20Security%20(HSTS,and%20back%20to%20the%20browser.)Network - MiscThe internet refers to a network of networks, transferring huge amounts of data around the globe.The World Wide Web is an application running on millions of servers, on top of the internet, accessed through what is known as the web browserISP (Internet Service Provider) is the local internet company provider.Operating SystemOperating System Exercises|Name|Topic|Objective & Instructions|Solution|Comments||--------|--------|------|----|----||Fork 101|Fork|||Fork 102|Fork||Operating System - Self AssessmentFrom the book ""Operating Systems: Three Easy Pieces"":""responsible for making it easy to run programs (even allowing you to seemingly run many at the same time), allowing programs to share memory, enabling programs to interact with devices, and other fun stuff like that"".Operating System - ProcessA process is a running program. A program is one or more instructions and the program (or process) is executed by the operating system.It would support the following:False. It was true in the past but today's operating systems perform lazy loading which means only the relevant pieces required for the process to run are loaded first.Even when using a system with one physical CPU, it's possible to allow multiple users to work on it and run programs. This is possible with time sharing where computing resources are shared in a way it seems to the user the system has multiple CPUs but in fact it's simply one CPU shared by applying multiprogramming and multi-tasking.Somewhat the opposite of time sharing. While in time sharing a resource is used for a while by one entity and then the same resource can be used by another resource, in space sharing the space is shared by multiple entities but in a way where it's not being transferred between them.It's used by one entity until this entity decides to get rid of it. Take for example storage. In storage, a file is yours until you decide to delete it.CPU schedulerOperating System - MemoryVirtual memory combines your computer's RAM with temporary space on your hard disk. When RAM runs low, virtual memory helps to move data from RAM to a space called a paging file. Moving data to paging file can free up the RAM so your computer can complete its work. In general, the more RAM your computer has, the faster the programs run.https://www.minitool.com/lib/virtual-memory.htmlThe idea:The kernel is part of the operating system and is responsible for tasks like:TrueBuffer: Reserved place in RAM which is used to hold data for temporary purposesCache: Cache is usually used when processes reading and writing to the disk to make the process faster by making similar data used by different programs easily accessible.VirtualizationVirtualization uses software to create an abstraction layer over computer hardware that allows the hardware elements of a single computer—processors, memory, storage and more - to be divided into multiple virtual computers, commonly called virtual machines (VMs).Red Hat: ""A hypervisor is software that creates and runs virtual machines (VMs). A hypervisor, sometimes called a virtual machine monitor (VMM), isolates the hypervisor operating system and resources from the virtual machines and enables the creation and management of those VMs.""Read more Hosted hypervisors and bare-metal hypervisors.Due to having its own drivers and a direct access to hardware components, a baremetal hypervisor will often have better performances along with stability and scalability.On the other hand, there will probably be some limitation regarding loading (any) drivers so a hosted hypervisor will usually benefit from having a better hardware compatibility.Operating system virtualizationNetwork functions virtualizationDesktop virtualizationYes, it's a operating-system-level virtualization, where the kernel is shared and allows to use multiple isolated user-spaces instances.The introduction of virtual machines allowed companies to deploy multiple business applications on the same hardware while each application is separated from each other in secured way, where each is running on its own separate operating system.Virtual MachinesYes, virtual machines are still relevant even in the age of containers. While containers provide a lightweight and portable alternative to virtual machines, they do have certain limitations. Virtual machines still matter because they offer isolation and security, can run different operating systems, and are good for legacy apps. Containers limitations for example are sharing the host kernel.PrometheusPrometheus is a popular open-source systems monitoring and alerting toolkit, originally developed at SoundCloud. It is designed to collect and store time-series data, and to allow for querying and analysis of that data using a powerful query language called PromQL. Prometheus is frequently used to monitor cloud-native applications, microservices, and other modern infrastructure.Some of the main features of Prometheus include:1. Data model: Prometheus uses a flexible data model that allows users to organize and label their time-series data in a way that makes sense for their particular use case. Labels are used to identify different dimensions of the data, such as the source of the data or the environment in which it was collected.

2. Pull-based architecture: Prometheus uses a pull-based model to collect data from targets, meaning that the Prometheus server actively queries its targets for metrics data at regular intervals. This architecture is more scalable and reliable than a push-based model, which would require every target to push data to the server.

3. Time-series database: Prometheus stores all of its data in a time-series database, which allows users to perform queries over time ranges and to aggregate and analyze their data in various ways. The database is optimized for write-heavy workloads, and can handle a high volume of data with low latency.

4. Alerting: Prometheus includes a powerful alerting system that allows users to define rules based on their metrics data and to send alerts when certain conditions are met. Alerts can be sent via email, chat, or other channels, and can be customized to include specific details about the problem.

5. Visualization: Prometheus has a built-in graphing and visualization tool, called PromDash, which allows users to create custom dashboards to monitor their systems and applications. PromDash supports a variety of graph types and visualization options, and can be customized using CSS and JavaScript.
Overall, Prometheus is a powerful and flexible tool for monitoring and analyzing systems and applications, and is widely used in the industry for cloud-native monitoring and observability.From Prometheus documentation: ""if you need 100% accuracy, such as for per-request billing"".The Prometheus architecture consists of four major components:1. Prometheus Server: The Prometheus server is responsible for collecting and storing metrics data. It has a simple built-in storage layer that allows it to store time-series data in a time-ordered database.

2. Client Libraries: Prometheus provides a range of client libraries that enable applications to expose their metrics data in a format that can be ingested by the Prometheus server. These libraries are available for a range of programming languages, including Java, Python, and Go.

3. Exporters: Exporters are software components that expose existing metrics from third-party systems and make them available for ingestion by the Prometheus server. Prometheus provides exporters for a range of popular technologies, including MySQL, PostgreSQL, and Apache.

4. Alertmanager: The Alertmanager component is responsible for processing alerts generated by the Prometheus server. It can handle alerts from multiple sources and provides a range of features for deduplicating, grouping, and routing alerts to appropriate channels.
Overall, the Prometheus architecture is designed to be highly scalable and resilient. The server and client libraries can be deployed in a distributed fashion to support monitoring across large-scale, highly dynamic environmentsCompared to other monitoring solutions, such as InfluxDB, Prometheus is known for its high performance and scalability. It can handle large volumes of data and can easily be integrated with other tools in the monitoring ecosystem. InfluxDB, on the other hand, is known for its ease of use and simplicity. It has a user-friendly interface and provides easy-to-use APIs for collecting and querying data.Another popular solution, Nagios, is a more traditional monitoring system that relies on a push-based model for collecting data. Nagios has been around for a long time and is known for its stability and reliability. However, compared to Prometheus, Nagios lacks some of the more advanced features, such as multi-dimensional data model and powerful query language.Overall, the choice of a monitoring solution depends on the specific needs and requirements of the organization. While Prometheus is a great choice for large-scale monitoring and alerting, InfluxDB may be a better fit for smaller environments that require ease of use and simplicity. Nagios remains a solid choice for organizations that prioritize stability and reliability over advanced features.In Prometheus, an instance refers to a single target that is being monitored. For example, a single server or service. A job is a set of instances that perform the same function, such as a set of web servers serving the same application. Jobs allow you to define and manage a group of targets together.In essence, an instance is an individual target that Prometheus collects metrics from, while a job is a collection of similar instances that can be managed as a group.1. Counter: A monotonically increasing value used for tracking counts of events or samples. Examples include the number of requests processed or the total number of errors encountered.

2. Gauge: A value that can go up or down, such as CPU usage or memory usage. Unlike counters, gauge values can be arbitrary, meaning they can go up and down based on changes in the system being monitored.

3. Histogram: A set of observations or events that are divided into buckets based on their value. Histograms help in analyzing the distribution of a metric, such as request latencies or response sizes.

4. Summary: A summary is similar to a histogram, but instead of buckets, it provides a set of quantiles for the observed values. Summaries are useful for monitoring the distribution of request latencies or response sizes over time.
Prometheus also supports various functions and operators for aggregating and manipulating metrics, such as sum, max, min, and rate. These features make it a powerful tool for monitoring and alerting on system metrics.The exporter acts as a server, listening on a specific network port for requests from Prometheus to scrape metrics. It collects metrics from the third-party system or application and transforms them into a format that can be understood by Prometheus. The exporter then exposes these metrics to Prometheus via an HTTP endpoint, making them available for collection and analysis.Exporters are commonly used to monitor various types of infrastructure components such as databases, web servers, and storage systems. For example, there are exporters available for monitoring popular databases such as MySQL and PostgreSQL, as well as web servers like Apache and Nginx.Overall, exporters are a critical component of the Prometheus ecosystem, allowing for the monitoring of a wide range of systems and applications, and providing a high degree of flexibility and extensibility to the platform.1. Label carefully: Careful and consistent labeling of metrics is crucial for effective querying and alerting. Labels should be clear, concise, and include all relevant information about the metric.

2. Keep metrics simple: The metrics exposed by exporters should be simple and focus on a single aspect of the system being monitored. This helps avoid confusion and ensures that the metrics are easily understandable by all members of the team.

3. Use alerting sparingly: While alerting is a powerful feature of Prometheus, it should be used sparingly and only for the most critical issues. Setting up too many alerts can lead to alert fatigue and result in important alerts being ignored. It is recommended to set up only the most important alerts and adjust the thresholds over time based on the actual frequency of alerts.
sum(rate(http_requests_total[1h]))
In this query, http_requests_total is the name of the metric that tracks the total number of HTTP requests, and the rate function calculates the per-second rate of requests over the last hour. The sum function then adds up all of the requests to give you the total number of requests in the last hour.You can adjust the time range by changing the duration in the rate function. For example, if you wanted to get the total number of requests in the last day, you could change the function to rate(http_requests_total[1d]).HA stands for High Availability. This means that the system is designed to be highly reliable and always available, even in the face of failures or other issues. In practice, this typically involves setting up multiple instances of Prometheus and ensuring that they are all synchronized and able to work together seamlessly. This can be achieved through a variety of techniques, such as load balancing, replication, and failover mechanisms. By implementing HA in Prometheus, users can ensure that their monitoring data is always available and up-to-date, even in the face of hardware or software failures, network issues, or other problems that might otherwise cause downtime or data loss.Here's an example of how to join two metrics using the join() function:sum_series(
  join(
    on(service, instance) request_count_total,
    on(service, instance) error_count_total,
  )
)
In this example, the join() function combines the request_count_total and error_count_total time series based on their service and instance label values. The sum_series() function then calculates the sum of the resulting time seriesFor example, if you have a metric called http_requests_total with a label called method, and you want to return all the values of the method label, you can use the following query:label_values(http_requests_total, method)
This will return a list of all the values for the method label in the http_requests_total metric. You can then use this list in further queries or to filter your data.100 * sum(rate(process_cpu_user_seconds_total{job=""<job-name>""}[<time-period>])) by (instance) / (<time-period> * <num-cpu-cores>)
Here, <job-name> is the name of the job you want to query, <time-period> is the time range you want to query (e.g. 5m, 1h), and <num-cpu-cores> is the number of CPU cores on the machine you are querying.For example, to get the CPU usage in percentage for the last 5 minutes for a job named my-job running on a machine with 4 CPU cores, you can use the following query:100 * sum(rate(process_cpu_user_seconds_total{job=""my-job""}[5m])) by (instance) / (5m * 4)
GoGo also has good community.The result is the same, a variable with the value 2.With var x int = 2 we are setting the variable type to integer while with x := 2 we are letting Go figure out by itself the type.False. We can't redeclare variables but yes, we must used declared variables.This should be answered based on your usage but some examples are:func main() {
    var x float32 = 13.5
    var y int
    y = x
}
package main

import ""fmt""

func main() {
    var x int = 101
    var y string
    y = string(x)
    fmt.Println(y)
}
It looks what unicode value is set at 101 and uses it for converting the integer to a string.If you want to get ""101"" you should use the package ""strconv"" and replace y = string(x) with y = strconv.Itoa(x)package main

func main() {
    var x = 2
    var y = 3
    const someConst = x + y
}
Constants in Go can only be declared using constant expressions.But ,  and their sum is variable.const initializer x + y is not a constantpackage main

import ""fmt""

const (
	x = iota
	y = iota
)
const z = iota

func main() {
	fmt.Printf(""%v\n"", x)
	fmt.Printf(""%v\n"", y)
	fmt.Printf(""%v\n"", z)
}
Go's iota identifier is used in const declarations to simplify definitions of incrementing numbers. Because it can be used in expressions, it provides a generality beyond that of simple enumerations. and  in the first iota group,  in the second.It avoids having to declare all the variables for the returns values.It is called the .package main

import ""fmt""

const (
	_ = iota + 3
	x
)

func main() {
	fmt.Printf(""%v\n"", x)
}
Since the first iota is declared with the value  (), the next one has the value package main

import (
	""fmt""
	""sync""
	""time""
)

func main() {
	var wg sync.WaitGroup

	wg.Add(1)
	go func() {
		time.Sleep(time.Second * 2)
		fmt.Println(""1"")
		wg.Done()
	}()

	go func() {
		fmt.Println(""2"")
	}()

	wg.Wait()
	fmt.Println(""3"")
}
Output: 2 1 3package main

import (
	""fmt""
)

func mod1(a []int) {
	for i := range a {
		a[i] = 5
	}

	fmt.Println(""1:"", a)
}

func mod2(a []int) {
	a = append(a, 125) // !

	for i := range a {
		a[i] = 5
	}

	fmt.Println(""2:"", a)
}

func main() {
	s1 := []int{1, 2, 3, 4}
	mod1(s1)
	fmt.Println(""1:"", s1)

	s2 := []int{1, 2, 3, 4}
	mod2(s2)
	fmt.Println(""2:"", s2)
}
Output: 1 [5 5 5 5]1 [5 5 5 5]2 [5 5 5 5 5]2 [1 2 3 4]In  a is link, and when we're using , we're changing  value to.But in ,  creates new slice, and we're changing only  value, not .,package main

import (
	""container/heap""
	""fmt""
)

// An IntHeap is a min-heap of ints.
type IntHeap []int

func (h IntHeap) Len() int           { return len(h) }
func (h IntHeap) Less(i, j int) bool { return h[i] < h[j] }
func (h IntHeap) Swap(i, j int)      { h[i], h[j] = h[j], h[i] }

func (h *IntHeap) Push(x interface{}) {
	// Push and Pop use pointer receivers because they modify the slice's length,
	// not just its contents.
	*h = append(*h, x.(int))
}

func (h *IntHeap) Pop() interface{} {
	old := *h
	n := len(old)
	x := old[n-1]
	*h = old[0 : n-1]
	return x
}

func main() {
	h := &IntHeap{4, 8, 3, 6}
	heap.Init(h)
	heap.Push(h, 7)

  fmt.Println((*h)[0])
}
Output: 3MongoMongoDB advantages are as following:The main difference is that SQL databases are structured (data is stored in the form oftables with rows and columns - like an excel spreadsheet table) while NoSQL isunstructured, and the data storage can vary depending on how the NoSQL DB is set up, suchas key-value pair, document-oriented, etc.QueriesSQLSQL Exercises|Name|Topic|Objective & Instructions|Solution|Comments||--------|--------|------|----|----|| Functions vs. Comparisons | Query Improvements |  | SQL Self AssessmentSQL (Structured Query Language) is a standard language for relational databases (like MySQL, MariaDB, ...).It's used for reading, updating, removing and creating data in a relational database.The main difference is that SQL databases are structured (data is stored in the form oftables with rows and columns - like an excel spreadsheet table) while NoSQL isunstructured, and the data storage can vary depending on how the NoSQL DB is set up, suchas key-value pair, document-oriented, etc.SQL - Best used when data integrity is crucial. SQL is typically implemented with manybusinesses and areas within the finance field due to it's ACID compliance.NoSQL - Great if you need to scale things quickly. NoSQL was designed with web applicationsin mind, so it works great if you need to quickly spread the same information around tomultiple serversAdditionally, since NoSQL does not adhere to the strict table with columns and rows structurethat Relational Databases require, you can store different data types together.Practical SQL - BasicsFor these questions, we will be using the Customers and Orders tables shown below:CustomersCustomer_ID | Customer_Name | Items_in_cart | Cash_spent_to_Date------------ | ------------- | ------------- | -------------100204 | John Smith | 0 | 20.00100205 | Jane Smith | 3 | 40.00100206 | Bobby Frank | 1 | 100.20ORDERSCustomer_ID | Order_ID | Item | Price | Date_sold------------ | ------------- | ------------- | ------------- | -------------100206 | A123 | Rubber Ducky | 2.20 | 2019-09-18100206 | A123 | Bubble Bath | 8.00 | 2019-09-18100206 | Q987 | 80-Pack TP | 90.00 | 2019-09-20100205 | Z001 | Cat Food - Tuna Fish | 10.00 | 2019-08-05100205 | Z001 | Cat Food - Chicken | 10.00 | 2019-08-05100205 | Z001 | Cat Food - Beef | 10.00 | 2019-08-05100205 | Z001 | Cat Food - Kitty quesadilla | 10.00 | 2019-08-05100204 | X202 | Coffee | 20.00 | 2019-04-29Select * From Customers;Select Items_in_cart From Customers Where Customer_Name = ""John Smith"";Select SUM(Cash_spent_to_Date) as SUM_CASH From Customers;Select count(1) as Number_of_People_w_items From Customers where Items_in_cart > 0;You would join them on the unique key. In this case, the unique key is Customer_ID inboth the Customers table and Orders tableSelect c.Customer_Name, o.Item From Customers c Left Join Orders o On c.Customer_ID = o.Customer_ID;with cat_food as ( Select Customer_ID, SUM(Price) as TOTAL_PRICE From Orders Where Item like ""%Cat Food%"" Group by Customer_ID ) Select Customer_name, TOTAL_PRICE From Customers c Inner JOIN cat_food f ON c.Customer_ID = f.Customer_ID where c.Customer_ID in (Select Customer_ID from cat_food);Although this was a simple statement, the ""with"" clause really shines whena complex query needs to be run on a table before joining to another. With statements are nice,because you create a pseudo temp when running your query, instead of creating a whole new table.The Sum of all the purchases of cat food weren't readily available, so we used a with statement to createthe pseudo table to retrieve the sum of the prices spent by each customer, then join the table normally.SELECT count(*)                             SELECT count(*)
FROM shawarma_purchases                     FROM shawarma_purchases
WHERE                               vs.     WHERE
  YEAR(purchased_at) == '2017'              purchased_at >= '2017-01-01' AND
                                            purchased_at <= '2017-31-12'
SELECT count(*)
FROM shawarma_purchases
WHERE
  purchased_at >= '2017-01-01' AND
  purchased_at <= '2017-31-12'
When you use a function () it has to scan the whole database as opposed to using indexes and basically the column as it is, in its natural state.OpenStackOpenStack Deployment & TripleOYou can read about TripleO right OpenStack ComputeOpenStack Networking (Neutron)There are many reasons for that. One for example: you can't remove router if there are active ports assigned to it.OpenStack - GlanceOpenStack - SwiftNot by default. Object Storage API limits the maximum to 5GB per object but it can be adjusted.False. Two objects can have the same name if they are in different containers.OpenStack - CinderOpenStack - KeystoneUsing:A list of services and their endpointsOpenStack Advanced - ServicesOpenStack Advanced - KeystoneOpenStack Advanced - Compute (Nova)OpenStack Advanced - Networking (Neutron)OpenStack Advanced - HorizonPuppetElasticThe Elastic Stack consists of:Elasticsearch, Logstash and Kibana are also known as the ELK stack.From the official :""Elasticsearch is a distributed document store. Instead of storing information as rows of columnar data, Elasticsearch stores complex data structures that have been serialized as JSON documents""From the :""Logstash is a powerful, flexible pipeline that collects, enriches and transports data. It works as an extract, transform & load (ETL) tool for collecting log messages.""Beats are lightweight data shippers. These data shippers installed on the client where the data resides.Examples of beats: Filebeat, Metricbeat, Auditbeat. There are much more.From the official docs:""Kibana is an open source analytics and visualization platform designed to work with Elasticsearch. You use Kibana to search, view, and interact with data stored in Elasticsearch indices. You can easily perform advanced data analysis and visualize your data in a variety of charts, tables, and maps.""The process may vary based on the chosen architecture and the processing you may want to apply to the logs. One possible workflow is:ElasticsearchThis is where data is stored and also where different processing takes place (e.g. when you search for a data).Part of a master node responsibilities:While there can be multiple master nodes in reality only of them is the elected master node.A node which responsible for processing the data according to ingest pipeline. In case you don't need to uselogstash then this node can receive data from beats and process it, similarly to how it can be processedin Logstash.From the official docs:Coordinating only nodes can benefit large clusters by offloading the coordinating node role from data and master-eligible nodes. They join the cluster and receive the full cluster state, like every other node, and they use the cluster state to route requests directly to the appropriate place(s).Index in Elasticsearch is in most cases compared to a whole database from the SQL/NoSQL world.You can choose to have one index to hold all the data of your app or have multiple indices where each index holds different type of your app (e.g. index for each service your app is running).The official docs also offer a great explanation (in general, it's really good documentation, as every project should have):""An index can be thought of as an optimized collection of documents and each document is a collection of fields, which are the key-value pairs that contain your data""An index is split into shards and documents are hashed to a particular shard. Each shard may be on a different node in a cluster and each one of the shards is a self contained index.This allows Elasticsearch to scale to an entire cluster of servers.From the official docs:""An inverted index lists every unique word that appears in any document and identifies all of the documents each word occurs in.""Continuing with the comparison to SQL/NoSQL a Document in Elasticsearch is a row in table in the case of SQL or a document in a collection in the case of NoSQL.As in NoSQL a document is a JSON object which holds data on a unit in your app. What is this unit depends on the your app. If your app related to book then each document describes a book. If you are app is about shirts then each document is a shirt.Red means some data is unavailable in your cluster. Some shards of your indices are unassigned.There are some other states for the cluster.Yellow means that you have unassigned shards in the cluster. You can be in this state if you have single node and your indices have replicas.Green means that all shards in the cluster are assigned to nodes and your cluster is healthy.False.From the official docs:""Each indexed field has a dedicated, optimized data structure. For example, text fields are stored in inverted indices, and numeric and geo fields are stored in BKD trees.""In a network/cloud environment where failures can be expected any time, it is very useful and highly recommended to have a failover mechanism in case a shard/node somehow goes offline or disappears for whatever reason.To this end, Elasticsearch allows you to make one or more copies of your index’s shards into what are called replica shards, or replicas for short.Term Frequency is how often a term appears in a given document and Document Frequency is how often a term appears in all documents. They both are used for determining the relevance of a term by calculating Term Frequency / Document Frequency.""The index is actively being written to"".More about the phases It creates customer index if it doesn't exists and adds a new document with the field name which is set to ""John Dow"". Also, if it's the first document it will get the ID 1.Bulk API is used when you need to index multiple documents. For high number of documents it would be significantly faster to use rather than individual requests since there are less network roundtrips.Query DSLFrom the official docs:""In the query context, a query clause answers the question “How well does this document match this query clause?” Besides deciding whether or not the document matches, the query clause also calculates a relevance score in the _score meta-field.""""In a filter context, a query clause answers the question “Does this document match this query clause?” The answer is a simple Yes or No — no scores are calculated. Filter context is mostly used for filtering structured data""There are several possible answers for this question. One of them is as follows:A small-scale architecture of elastic will consist of the elastic stack as it is. This means we will have beats, logstash, elastcsearch and kibana.A production environment with large amounts of data can include some kind of buffering component (e.g. Reddis or RabbitMQ) and also security component such as Nginx.LogstashA logstash plugin which modifies information in one format and immerse it in another.KibanaThe raw data as it is stored in the index. You can search and filter it.Total number of documents matching the search results. If not query used then simply the total number of documents.""Visualize"" is where you can create visual representations for your data (pie charts, graphs, ...)FilebeatFilebeat is used to monitor the logging directories inside of VMs or mounted as a sidecar if exporting logs from containers, and then forward these logs onward for further processing, usually to logstash.Filebeat is a typical component of the ELK stack, since it was developed by Elastic to work with the other products (Logstash and Kibana). It's possible to send logs directly to logstash, though this often requires coding changes for the application. Particularly for legacy applications with little test coverage, it might be a better option to use filebeat, since you don't need to make any changes to the application code.Read False. One harvester harvests one file.These are pre-configured modules for specific types of logging locations (eg, Traefik, Fargate, HAProxy) to make it easy to configure forwarding logs using filebeat. They have different configurations based on where you're collecting logs from.Elastic StackYou can generate certificates with the provided elastic utils and change configuration to enable security using certificates model.DistributedAccording to Martin Kleppmann:""Many processes running on many machines...only message-passing via an unreliable network with variable delays, and the system may suffer from partial failures, unreliable clocks, and process pauses.""Another definition: ""Systems that are physically separated, but logically connected""According to the CAP theorem, it's not possible for a distributed data store to provide more than two of the following at the same time:Ways to improve:It's an architecture in which data is and retrieved from a single, non-shared, source usually exclusively connected to one node as opposed to architectures where the request can get to one of many nodes and the data will be retrieved from one shared location (storage, memory, ...).Misc|Name|Topic|Objective & Instructions|Solution|Comments||--------|--------|------|----|----|| Highly Available ""Hello World"" |  | TODO: add more details!APII like this definition from :""An explicitly and purposefully defined interface designed to be invoked over a network that enables software developers to get programmatic access to data and functionality within an organization in a controlled and comfortable way.""From :""An API specification provides a broad understanding of how an API behaves and how the API links with other APIs. It explains how the API functions and the results to expect when using the API""False. From :""An API definition is similar to an API specification in that it provides an understanding of how an API is organized and how the API functions. But the API definition is aimed at machine consumption instead of human consumption of APIs.""An API gateway is like the gatekeeper that controls how different parts talk to each other and how information is exchanged between them.The API gateway provides a single point of entry for all clients, and it can perform several tasks, including routing requests to the appropriate backend service, load balancing, security and authentication, rate limiting, caching, and monitoring.By using an API gateway, organizations can simplify the management of their APIs, ensure consistent security and governance, and improve the performance and scalability of their backend services. They are also commonly used in microservices architectures, where there are many small, independent services that need to be accessed by different clients.Advantages:Automation is the act of automating tasks to reduce human intervention or interaction in regards to IT technology and systems.While automation focuses on a task level, Orchestration is the process of automating processes and/or workflows which consists of multiple tasks that usually across multiple systems.Data about data. Basically, it describes the type of information that an underlying data will hold.I can't answer this for you :)Domain Specific Language (DSLs) are used to create a customised language that represents the domain such that domain experts can easily interpret it.YAMLData serialization language used by many technologies today like Kubernetes, Ansible, etc.True. Because YAML is superset of JSON.{
    applications: [
        {
            name: ""my_app"",
            language: ""python"",
            version: 20.17
        }
    ]
}
applications:
  - app: ""my_app""
    language: ""python""
    version: 20.17
someMultiLineString: |
  look mama
  I can write a multi-line string
  I love YAML
It's good for use cases like writing a shell script where each line of the script is a different command.using  will make the multi-line string to fold into a single linesomeMultiLineString: >
  This is actually
  a single line
  do not let appearances fool you
They allow you reference values instead of directly writing them and it is used like this:username: {{ my.user_name }}
Using this: For Examples:document_number: 1
---
document_number: 2
Firmware: ""In computing, firmware is a specific class of computer software that provides the low-level control for a device's specific hardware. Firmware, such as the BIOS of a personal computer, may contain basic functions of a device, and may provide hardware abstraction services to higher-level software such as operating systems.""CassandraHTTP: HTTP stands for Hypertext Transfer Protocol. HTTP uses TCP port 80 to enable internet communication. It is part of the Application Layer (L7) in OSI Model.False. It doesn't maintain state for incoming request.It consists of:HTTPS is a secure version of the HTTP protocol used to transfer data between a web browser and a web server. It encrypts the communication using SSL/TLS encryption to ensure that the data is private and secure.Learn more: https://www.cloudflare.com/learning/ssl/why-is-http-not-secure/HTTP is stateless. To share state, we can use Cookies.TODO: explain what is actually a CookieThe server didn't receive a response from another server it communicates with in a timely manner.A proxy is a server that acts as a middleman between a client device and a destination server. It can help improve privacy, security, and performance by hiding the client's IP address, filtering content, and caching frequently accessed data. A reverse proxy is a type of proxy server that sits between a client and a server, but it is used to manage traffic going in the opposite direction of a traditional forward proxy. In a forward proxy, the client sends requests to the proxy server, which then forwards them to the destination server. However, in a reverse proxy, the client sends requests to the destination server, but the requests are intercepted by the reverse proxy before they reach the server. : ""The X-Forwarded-For (XFF) HTTP header field is a common method for identifying the originating IP address of a client connecting to a web server through an HTTP proxy or load balancer.""Load BalancersA load balancer accepts (or denies) incoming network traffic from a client, and based on some criteria (application related, network, etc.) it distributes those communications out to servers (at least one).L4 and L7Yes, you can use DNS for performing load balancing.Load Balancers - Sticky SessionsRecommended read:Cons:You would like to make sure the user doesn't lose the current session data.Cookies. There are application based cookies and duration based cookies.Load Balancers - Load Balancing AlgorithmsThe maximum timeout value can be set between 1 and 3,600 seconds on both GCP and AWS.LicensesThe Creative Commons license is a set of copyright licenses that allow creators to share their work with the public while retaining some control over how it can be used. The license was developed as a response to the restrictive standards of traditional copyright laws, which limited access of creative works. Its creators to choose the terms under which their works can be shared, distributed, and used by others. They're six main types of Creative Commons licenses, each with different levels of restrictions and permissions, the six licenses are:Simply stated, the Creative Commons licenses are a way for creators to share their work with the public while retaining some control over how it can be used. The licenses promote creativity, innovation, and collaboration, while also respecting the rights of creators while still encouraging the responsible use of creative works.More information: https://creativecommons.org/licenses/In Copyleft, any derivative work must use the same licensing while in permissive licensing there are no such condition. GPL-3 is an example of copyleft license while BSD is an example of permissive license.RandomCPU cache.A memory leak is a programming error that occurs when a program fails to release memory that is no longer needed, causing the program to consume increasing amounts of memory over time.The leaks can lead to a variety of problems, including system crashes, performance degradation, and instability. Usually occurring after failed maintenance on older systems and compatibility with new components over time.SSHHTTPDHCPDNS...https://idiallo.com/blog/c10k-2016StoragePros:Pros:Local filesystemDropboxGoogle DriveA file system is a way for computers and other electronic devices to organize and store data files. It provides a structure that helps to organize data into files and directories, making it easier to find and manage information. A file system is crucial for providing a way to store and manage data in an organized manner.Commonly used filed systems:Windows:Mac OS:Questions you CAN askA list of questions you as a candidate can ask the interviewer during or after the interview.These are only a suggestion, use them carefully. Not every interviewer will be able to answer these (or happy to) which should be perhaps a red flag warning for your regarding working in such place but that's really up to you.Be careful when asking this question - all companies, regardless of size, have some level of tech debt.Phrase the question in the light that all companies have the deal with this, but you want to see the currentpain points they are dealing with This is a great way to figure how managers deal with unplanned work, and how good they are atsetting expectations with projects.This can give you insights in some of the cool projects a company is working on, and ifyou would enjoy working on projects like these. This is also a good way to see ifthe managers are allowing employees to learn and grow with projects outside of thenormal work you'd do.Similar to the tech debt question, this helps you identify any pain points with the company.Additionally, it can be a great way to show how you'd be an asset to the team.For Example, if they mention they have problem X, and you've solved that in the past,you can show how you'd be able to mitigate that problem.Not only this will tell you what is expected from you, it will also provide big hint on the type of work you are going to do in the first months of your job.TestingUnit test are a software testing technique that involves systimatically breaking down a system and testing each individual part of the assembly. These tests are automated and can be run repeatedly to allow developers to catch edge case scenarios or bugs quickly while developing.The main objective of unit tests are to verify each function is producing proper outputs given a set of inputs.RegexGiven a text file, perform the following exercisesExtractReplaceSystem DesignCDN (Content Delivery Network) responsible for distributing content geographically. Part of it, is what is known as edge locations, aka cache proxies, that allows users to get their content quickly due to cache features and geographical distribution.In single CDN, the whole content is originated from content delivery network.In multi-CDN, content is distributed across multiple different CDNs, each might be on a completely different provider/cloud. provides a great explanation.ScalabilityThe ability easily grow in size and capacity based on demand and usage.The ability to grow but also to reduce based on what is requiredDisaster recovery is the process of restoring critical business systems and data after a disruptive event. The goal is to minimize the impact and resume normal business activities quickly. This involves creating a plan, testing it, backing up critical data, and storing it in safe locations. In case of a disaster, the plan is then executed, backups are restored, and systems are hopefully brought back online. The recovery process may take hours or days depending on the damages of infrastructure. This makes business planning important, as a well-designed and tested disaster recovery plan can minimize the impact of a disaster and keep operations going.Fault Tolerance - The ability to self-heal and return to normal capacity. Also the ability to withstand a failure and remain functional.High Availability - Being able to access a resource (in some use cases, using different platforms): ""High availability, simply put, is eliminating single points of failure and disaster recovery is the process of getting a system back to an operational state when a system is rendered inoperative. In essence, disaster recovery picks up when high availability fails, so HA first.""Vertical Scaling is the process of adding resources to increase power of existing servers. For example, adding more CPUs, adding more RAM, etc.With vertical scaling alone, the component still remains a single point of failure.In addition, it has hardware limit where if you don't have more resources, you might not be able to scale vertically.Databases, cache. It's common mostly for non-distributed systems.Horizontal Scaling is the process of adding more resources that will be able handle requests as one unitA load balancer. You can add more resources, but if you would like them to be part of the process, you have to serve them the requests/responses.Also, data inconsistency is a concern with horizontal scaling.The load on the producers or consumers may be high which will then cause them to hang or crash.Instead of working in ""push mode"", the consumers can pull tasks only when they are ready to handle them. It can be fixed by using a streaming platform like Kafka, Kinesis, etc. This platform will make sure to handle the high load/traffic and pass tasks/messages to consumers only when the ready to get them.CacheTake a look You can find a list Read about it Caching is used to speed up read operations by storing frequently accessed data in memory or on a fast storage medium. By keeping data close to the application, caching reduces the latency and overhead of accessing data from a slower, more distant storage system such as a database or disk.On the other hand, databases are optimized for storing and managing persistent data. Databases are designed to handle concurrent read and write operations, enforce consistency and integrity constraints, and provide features such as indexing and querying.MigrationsYou can mention:roll-back & roll-forwardcut overdress rehearsalsDNS redirectionDesign a systemMore System Design QuestionsAdditional exercises can be found in .HardwareA central processing unit (CPU) performs basic arithmetic, logic, controlling, and input/output (I/O) operations specified by the instructions in the program. This contrasts with external components such as main memory and I/O circuitry, and specialized processors such as graphics processing units (GPUs).RAM (Random Access Memory) is the hardware in a computing device where the operating system (OS), application programs and data in current use are kept so they can be quickly reached by the device's processor. RAM is the main memory in a computer. It is much faster to read from and write to than other kinds of storage, such as a hard disk drive (HDD), solid-state drive (SSD) or optical drive.An embedded system is a computer system - a combination of a computer processor, computer memory, and input/output peripheral devices—that has a dedicated function within a larger mechanical or electronic system. It is embedded as part of a complete device often including electrical or electronic hardware and mechanical parts.A common example of an embedded system is a microwave oven's digital control panel, which is managed by a microcontroller.When committed to a certain goal, Raspberry Pi can serve as an embedded system.There are several types of storage, including hard disk drives (HDDs), solid-state drives (SSDs), and optical drives (CD/DVD/Blu-ray). Other types of storage include USB flash drives, memory cards, and network-attached storage (NAS).Choosing the right DevOps hardware is essential for ensuring streamlined CI/CD pipelines, timely feedback loops, and consistent service availability. Here's a distilled guide on what DevOps teams should consider:In essence, DevOps teams should choose hardware that is compatible with their tasks, versatile, gives good performance, and stays within their budget. Furthermore, long-term considerations such as maintenance, potential upgrades, and compatibility with impending technological shifts must be prioritized.Hardware is critical in disaster recovery (DR) solutions. While the broader scope of DR includes things like standard procedures, norms, and human roles, it's the hardware that keeps business processes running smoothly. Here's an outline of how hardware works with DR:In summary, while software and human interventions are important in disaster recovery operations, it is the hardware that provides the underlying support. It is critical for efficient disaster recovery plans to keep this hardware resilient, duplicated, and routinely assessed.Direct memory access (DMA) is a feature of computer systems that allows certain hardware subsystems to access main system memory independently of the central processing unit (CPU).DMA enables devices to share and receive data from the main memory in a computer. It does this while still allowing the CPU to perform other tasks.A real-time operating system (RTOS) is an operating system (OS) for real-time computing applications that processes data and events that have critically defined time constraints. An RTOS is distinct from a time-sharing operating system, such as Unix, which manages the sharing of system resources with a scheduler, data buffers, or fixed task prioritization in a multitasking or multiprogramming environment. Processing time requirements need to be fully understood and bound rather than just kept as a minimum. All processing must occur within the defined constraints. Real-time operating systems are event-driven and preemptive, meaning the OS can monitor the relevant priority of competing tasks, and make changes to the task priority. Event-driven systems switch between tasks based on their priorities, while time-sharing systems switch the task based on clock interrupts.There are six classes of interrupts possible:Big DataAs defined by Doug Laney:DataOps seeks to reduce the end-to-end cycle time of data analytics, from the origin of ideas to the literal creation of charts, graphs and models that create value.DataOps combines Agile development, DevOps and statistical process controls and applies them to data analytics.An answer from :""Data architecture is the process of standardizing how organizations collect, store, transform, distribute, and use data. The goal is to deliver relevant data to people who need it, when they need it, and help them make sense of it.""Apache HadoopResponsible for managing the compute resources in clusters and scheduling users' applicationsA programming model for large-scale data processingCephPackerIn general, Packer automates machine images creation.It allows you to focus on configuration prior to deployment while making the images. This allows you start the instances much faster in most cases.A configuration->deployment which has some advantages like:Release page explains it perfectly:Given a version number MAJOR.MINOR.PATCH, increment the:

MAJOR version when you make incompatible API changes
MINOR version when you add functionality in a backwards compatible manner
PATCH version when you make backwards compatible bug fixes
Additional labels for pre-release and build metadata are available as extensions to the MAJOR.MINOR.PATCH format.
CertificatesIf you are looking for a way to prepare for a certain exam this is the section for you. Here you'll find a list of certificates, each references to a separate file with focused questions that will help you to prepare to the exam. Good luck :)AWSAzureKubernetesAdditional DevOps and SRE ProjectsCreditsThanks to all of our amazing  who make it easy for everyone to learn new things :)Logos credits can be found License"
https://github.com/feast-dev/feast,Feature Store for Machine Learning,"OverviewFeast (Feature Store) is an open source feature store for machine learning. Feast is the fastest path to manage existing infrastructure to productionize analytic data for model training and online inference.Feast allows ML platform teams to:Please see our  for more information about the project.📐 ArchitectureThe above architecture is the minimal Feast deployment. Want to run the full Feast on Snowflake/GCP/AWS? Click .🐣 Getting Started1. Install Feastpip install feast
2. Create a feature repositoryfeast init my_feature_repo
cd my_feature_repo/feature_repo
3. Register your feature definitions and set up your feature storefeast apply
4. Explore your data in the web UI (experimental)feast ui
5. Build a training datasetfrom feast import FeatureStore
import pandas as pd
from datetime import datetime

entity_df = pd.DataFrame.from_dict({
    ""driver_id"": [1001, 1002, 1003, 1004],
    ""event_timestamp"": [
        datetime(2021, 4, 12, 10, 59, 42),
        datetime(2021, 4, 12, 8,  12, 10),
        datetime(2021, 4, 12, 16, 40, 26),
        datetime(2021, 4, 12, 15, 1 , 12)
    ]
})

store = FeatureStore(repo_path=""."")

training_df = store.get_historical_features(
    entity_df=entity_df,
    features = [
        'driver_hourly_stats:conv_rate',
        'driver_hourly_stats:acc_rate',
        'driver_hourly_stats:avg_daily_trips'
    ],
).to_df()

print(training_df.head())

# Train model
# model = ml.fit(training_df)
            event_timestamp  driver_id  conv_rate  acc_rate  avg_daily_trips
0 2021-04-12 08:12:10+00:00       1002   0.713465  0.597095              531
1 2021-04-12 10:59:42+00:00       1001   0.072752  0.044344               11
2 2021-04-12 15:01:12+00:00       1004   0.658182  0.079150              220
3 2021-04-12 16:40:26+00:00       1003   0.162092  0.309035              959

6. Load feature values into your online storeCURRENT_TIME=$(date -u +""%Y-%m-%dT%H:%M:%S"")
feast materialize-incremental $CURRENT_TIME
Materializing feature view driver_hourly_stats from 2021-04-14 to 2021-04-15 done!
7. Read online features at low latencyfrom pprint import pprint
from feast import FeatureStore

store = FeatureStore(repo_path=""."")

feature_vector = store.get_online_features(
    features=[
        'driver_hourly_stats:conv_rate',
        'driver_hourly_stats:acc_rate',
        'driver_hourly_stats:avg_daily_trips'
    ],
    entity_rows=[{""driver_id"": 1001}]
).to_dict()

pprint(feature_vector)

# Make prediction
# model.predict(feature_vector)
{
    ""driver_id"": [1001],
    ""driver_hourly_stats__conv_rate"": [0.49274],
    ""driver_hourly_stats__acc_rate"": [0.92743],
    ""driver_hourly_stats__avg_daily_trips"": [72]
}
📦 Functionality and RoadmapThe list below contains the functionality that contributors are planning to develop for Feast.🎓 Important ResourcesPlease refer to the official documentation at 👋 ContributingFeast is a community project and is still under active development. Please have a look at our contributing and development guides if you want to contribute to the project:✨ ContributorsThanks goes to these incredible people:"
https://github.com/facebookresearch/pifuhd,High-Resolution 3D Human Digitization from A Single Image.," News:This repository contains a pytorch implementation of ""Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization"".This codebase provides: See our  to learn more about our work at CVPR2020!Demo on Google ColabIn case you don't have an environment with GPUs to run PIFuHD, we offer Google Colab demo. You can also upload your own images and reconstruct 3D geometry together with visualization. Try our Colab demo using the following notebook: RequirementsFor visualizationNote: At least 8GB GPU memory is recommended to run PIFuHD model. Run the following code to install all pip packages:pip install -r requirements.txt 
Download Pre-trained modelRun the following script to download the pretrained model. The checkpoint is saved under .sh ./scripts/download_trained_model.sh
A Quick TestingTo process images under , run the following code:sh ./scripts/demo.sh
The resulting obj files and rendering will be saved in . You may use meshlab (http://www.meshlab.net/) to visualize the 3D mesh output (obj file). Testingpython apps/batch_openpose.py -d {openpose_root_path} -i {path_of_images} -o {path_of_images}
python -m apps.simple_test
python apps/clean_mesh.py -f {path_of_objs}
VisualizationTo render results with turn-table, run the following code. The rendered animation (.mp4) will be stored under .python -m apps.render_turntable -f {path_of_objs} -ww {rendering_width} -hh {rendering_height} 
# add -g for geometry rendering. default is normal visualization.
Citation@inproceedings{saito2020pifuhd,
  title={PIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization},
  author={Saito, Shunsuke and Simon, Tomas and Saragih, Jason and Joo, Hanbyul},
  booktitle={CVPR},
  year={2020}
}
Relevant Projects[<marko.inline.RawText object at 0x000001592FDE5288>]Ruilong LiThe first real-time PIFu by accelerating reconstruction and rendering!![<marko.inline.RawText object at 0x000001592FDE52C8>]Shunsuke SaitoThe original work of Pixel-Aligned Implicit Function for geometry and texture reconstruction, unifying sigle-view and multi-view methods.[<marko.inline.RawText object at 0x000001592FDDADC8>]Shichen Liu, Shunsuke Saito, Weikai Chen, Hao LiWe answer to the question of ""how can we learn implicit function if we don't have 3D ground truth?""[<marko.inline.RawText object at 0x000001592FE1B708>]Ryota NatsumeOur first attempt to reconstruct 3D clothed human body with texture from a single image!Other Relevant Works[<marko.inline.RawText object at 0x000001592D93BF48>]Zeng Huang, Yuanlu Xu, Christoph Lassner, Hao Li, Tony TungLearning PIFu in canonical space for animatable avatar generation![<marko.inline.RawText object at 0x000001592FF26F88>]Zhe Li, Tao Yu, Chuanyu Pan, Zerong Zheng, Yebin LiuThey extend PIFu to RGBD + introduce ""PIFusion"" utilizing PIFu reconstruction for non-rigid fusion.[<marko.inline.RawText object at 0x000001592FF26108>]Zeng Huang, Tianye Li, Weikai Chen, Yajie Zhao, Jun Xing, Chloe LeGendre, Linjie Luo, Chongyang Ma, Hao LiImplict surface learning for sparse view human performance capture!License.See the  file. "
https://github.com/linkedin/qark,Tool to look for several security related Android application vulnerabilities,"Quick Android Review KitThis tool is designed to look for several security related Android application vulnerabilities, either in source code or packaged APKs. The tool is also capable of creating ""Proof-of-Concept"" deployable APKs and/or ADB commands, capable of exploiting many of the vulnerabilities it finds. There is no need to root the test device, as this tool focuses on vulnerabilities that can be exploited under otherwise secure conditions.RequirementsTested on Python 2.7.13 and 3.6Tested on OSX, Linux, and WindowsUsageFor more options please see the  command.APK::~ qark --apk path/to/my.apkJava source code files::~ qark --java path/to/parent/java/folder~ qark --java path/to/specific/java/file.javaResultsA report is generated in JSON and can be built into other format types, to change the report type please use the  flag.InstallationWith pip (no security checks on requirements)::~ pip install --user qark  # --user is only needed if not using a virtualenv~ qark --helpWith  (security checks on requirements)::~ git clone https://github.com/linkedin/qark~ cd qark~ pip install -r requirements.txt~ pip install . --user  # --user is only needed if not using a virtualenv~ qark --helpExploit APKQARK can generate a basic exploit APK for a few of the vulnerabilities that have been found.To generate the exploit APK there are a few steps to follow. You need to have the Android SDK v21 and build-tools v21.1.2ChecksQARK is an easy to use tool capable of finding common security vulnerabilities in Android applications. Unlike commercial products, it is 100% free to use. QARK features educational information allowing security reviewers to locate precise, in-depth explanations of the vulnerabilities. QARK automates the use of multiple decompilers, leveraging their combined outputs, to produce superior results, when decompiling APKs. Finally, the major advantage QARK has over traditional tools, that just point you to possible vulnerabilities, is that it can produce ADB commands, or even fully functional APKs, that turn hypothetical vulnerabilities into working ""POC"" exploits.Included in the types of security vulnerabilities this tool attempts to find are:NoticeNote: QARK decompiles Android applications back to raw source code. Please do not use this tool if this may be considered illegal in your juristdiction. If you are unsure, seek legal counsel.If you run into issues on OSX, especially relating to the outbound call to the Play Store, or the downloading of the SDK, it islikely due to your Python/OpenSSL configuration and the fact that recent changes in OSX impacted Python installed via brew. Nuking yourPython installation(s) and re-installing from source may fix your issues.LicenseCopyright 2015 LinkedIn Corp.  All rights reserved.Copyright 2015 LinkedIn Corp. Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License. You may obtain a copy of the License _.Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied."
https://github.com/anishathalye/neural-style,Neural style in TensorFlow! 🎨,"neural-style An implementation of  in TensorFlow.This implementation is a lot simpler than a lot of the other ones out there,thanks to TensorFlow's really nice API and .TensorFlow doesn't support  (which is what the original authorsused), so we use . This may require a little bit morehyperparameter tuning to get nice results.RunningRun  to see a list of all options.Use  and  to save checkpoint images.Use  to change the number of iterations (default 1000).  For a 512×512 pixel content file, 1000 iterations take 60 seconds on a GTX 1080 Ti, 90 seconds on a Maxwell Titan X, or 60 minutes on an Intel Core i7-5930K. Using a GPU is highly recommended due to the huge speedup.Example 1Running it for 500-2000 iterations seems to produce nice results. With certainimages or output sizes, you might need some hyperparameter tuning (especially, , and ).The following example was run for 1000 iterations to produce the result (withdefault parameters):These were the input images used (me sleeping at a hackathon and Starry Night):Example 2The following example demonstrates style blending, and was run for 1000iterations to produce the result (with style blend weight parameters 0.8 and0.2):The content input image was a picture of the Stata Center at MIT:The style input images were Picasso's ""Dora Maar"" and Starry Night, with thePicasso image having a style blend weight of 0.8 and Starry Night having astyle blend weight of 0.2:Tweaking command line argument could be used to tweak how ""abstract""the style transfer should be. Lower values mean that style transfer of a finer featureswill be favored over style transfer of a more coarse features, and vice versa. Defaultvalue is 1.0 - all layers treated equally. Somewhat extreme examples of what you can achieve:(left: 0.2 - finer features style transfer; right: 2.0 - coarser features style transfer) specifies the coefficient of content transfer layers. Default value -1.0, style transfer tries to preserve finer grain content details. The value should bein range [0.0; 1.0].(left: 1.0 - default value; right: 0.1 - more abstract picture) allows to select which pooling layers to use (specify either  or ).Original VGG topology uses max pooling, but the  suggestsreplacing it with average pooling. The outputs are perceptually different, max pool ingeneral tends to have finer detail style transfer, but could have troubles atlower-freqency detail level:(left: max pooling; right: average pooling) boolean command line argument adds post-processing step, whichcombines colors from the original image and luma from the stylized image (YCbCr colorspace), thus producing color-preserving style transfer:(left: original stylized image; right: color-preserving style transfer)RequirementsData FilesDependenciesYou can install Python dependencies using ,and it should just work. If you want to install the packages manually, here's alist:Related ProjectsSee  for an implementation of  in TensorFlow.[<marko.inline.RawText object at 0x000001592FE50148>]CitationIf you use this implementation in your work, please cite the following:@misc{athalye2015neuralstyle,
  author = {Anish Athalye},
  title = {Neural Style},
  year = {2015},
  howpublished = {\url{https://github.com/anishathalye/neural-style}},
}
LicenseCopyright (c) Anish Athalye. Released under GPLv3. See for details."
https://github.com/malwaredllc/byob,"An open-source post-exploitation framework for students, researchers and developers.","Questions? Check out the  or join our Disclaimer: This project should be used for authorized testing or educational purposes only.BYOB is an open-source post-exploitation framework for students, researchers and developers. It includes features such as:It is designed to allow students and developers to easily implement their own code and add cool newfeatures without having to write a C2 server or Remote Administration Tool from scratch.This project has 2 main parts: the original console-based application () and the web GUI ().Web GUIDashboardA control panel for your C2 server with a point-and-click interface for executing post-exploitation modules. The control panel includes an interactive map of client machines and a dashboard which allows efficient, intuitive administration of client machines.Payload GeneratorThe payload generator uses black magic involving Docker containers & Wine servers to compile executable payloads for any platform/architecture you select. These payloads spawn reverse TCP shells with communication over the network encrypted via AES-256 after generating a secure symmetric key using the .Terminal EmulatorThe web app includes an in-browser terminal emulator so you can still have direct shell access even when using the web GUI.Console ApplicationClientGenerate fully-undetectable clients with staged payloads, remote imports, and unlimited post-exploitation modulesModulesPost-exploitation modules that are remotely importable by clientsServerCommand & control server with persistent database and consoleCoreCore framework modules used by the generator and the serverTo DoContributors welcome! Feel free to issue pull-requests with any new features or improvements you have come up with!"
https://github.com/yunjey/pytorch-tutorial,PyTorch Tutorial for Deep Learning Researchers,"This repository provides tutorial code for deep learning researchers to learn . In the tutorial, most of the models were implemented with less than 30 lines of code. Before starting this tutorial, it is recommended to finish .Table of Contents1. Basics2. Intermediate3. Advanced4. UtilitiesGetting Started$ git clone https://github.com/yunjey/pytorch-tutorial.git
$ cd pytorch-tutorial/tutorials/PATH_TO_PROJECT
$ python main.py
Dependencies"
https://github.com/eriklindernoren/ML-From-Scratch,Machine Learning From Scratch. Bare bones NumPy implementations of machine learning models and algorithms with a focus on accessibility. Aims to cover everything from linear regression to deep learning.,"Machine Learning From ScratchAboutPython implementations of some of the fundamental Machine Learning models and algorithms from scratch.The purpose of this project is not to produce as optimized and computationally efficient algorithms as possiblebut rather to present the inner workings of them in a transparent and accessible way.Table of ContentsInstallation$ git clone https://github.com/eriklindernoren/ML-From-Scratch
$ cd ML-From-Scratch
$ python setup.py install
ExamplesPolynomial Regression$ python mlfromscratch/examples/polynomial_regression.py
Classification With CNN$ python mlfromscratch/examples/convolutional_neural_network.py

+---------+
| ConvNet |
+---------+
Input Shape: (1, 8, 8)
+----------------------+------------+--------------+
| Layer Type           | Parameters | Output Shape |
+----------------------+------------+--------------+
| Conv2D               | 160        | (16, 8, 8)   |
| Activation (ReLU)    | 0          | (16, 8, 8)   |
| Dropout              | 0          | (16, 8, 8)   |
| BatchNormalization   | 2048       | (16, 8, 8)   |
| Conv2D               | 4640       | (32, 8, 8)   |
| Activation (ReLU)    | 0          | (32, 8, 8)   |
| Dropout              | 0          | (32, 8, 8)   |
| BatchNormalization   | 4096       | (32, 8, 8)   |
| Flatten              | 0          | (2048,)      |
| Dense                | 524544     | (256,)       |
| Activation (ReLU)    | 0          | (256,)       |
| Dropout              | 0          | (256,)       |
| BatchNormalization   | 512        | (256,)       |
| Dense                | 2570       | (10,)        |
| Activation (Softmax) | 0          | (10,)        |
+----------------------+------------+--------------+
Total Parameters: 538570

Training: 100% [------------------------------------------------------------------------] Time: 0:01:55
Accuracy: 0.987465181058
Density-Based Clustering$ python mlfromscratch/examples/dbscan.py
Generating Handwritten Digits$ python mlfromscratch/unsupervised_learning/generative_adversarial_network.py

+-----------+
| Generator |
+-----------+
Input Shape: (100,)
+------------------------+------------+--------------+
| Layer Type             | Parameters | Output Shape |
+------------------------+------------+--------------+
| Dense                  | 25856      | (256,)       |
| Activation (LeakyReLU) | 0          | (256,)       |
| BatchNormalization     | 512        | (256,)       |
| Dense                  | 131584     | (512,)       |
| Activation (LeakyReLU) | 0          | (512,)       |
| BatchNormalization     | 1024       | (512,)       |
| Dense                  | 525312     | (1024,)      |
| Activation (LeakyReLU) | 0          | (1024,)      |
| BatchNormalization     | 2048       | (1024,)      |
| Dense                  | 803600     | (784,)       |
| Activation (TanH)      | 0          | (784,)       |
+------------------------+------------+--------------+
Total Parameters: 1489936

+---------------+
| Discriminator |
+---------------+
Input Shape: (784,)
+------------------------+------------+--------------+
| Layer Type             | Parameters | Output Shape |
+------------------------+------------+--------------+
| Dense                  | 401920     | (512,)       |
| Activation (LeakyReLU) | 0          | (512,)       |
| Dropout                | 0          | (512,)       |
| Dense                  | 131328     | (256,)       |
| Activation (LeakyReLU) | 0          | (256,)       |
| Dropout                | 0          | (256,)       |
| Dense                  | 514        | (2,)         |
| Activation (Softmax)   | 0          | (2,)         |
+------------------------+------------+--------------+
Total Parameters: 533762
Deep Reinforcement Learning$ python mlfromscratch/examples/deep_q_network.py

+----------------+
| Deep Q-Network |
+----------------+
Input Shape: (4,)
+-------------------+------------+--------------+
| Layer Type        | Parameters | Output Shape |
+-------------------+------------+--------------+
| Dense             | 320        | (64,)        |
| Activation (ReLU) | 0          | (64,)        |
| Dense             | 130        | (2,)         |
+-------------------+------------+--------------+
Total Parameters: 450
Image Reconstruction With RBM$ python mlfromscratch/examples/restricted_boltzmann_machine.py
Evolutionary Evolved Neural Network$ python mlfromscratch/examples/neuroevolution.py

+---------------+
| Model Summary |
+---------------+
Input Shape: (64,)
+----------------------+------------+--------------+
| Layer Type           | Parameters | Output Shape |
+----------------------+------------+--------------+
| Dense                | 1040       | (16,)        |
| Activation (ReLU)    | 0          | (16,)        |
| Dense                | 170        | (10,)        |
| Activation (Softmax) | 0          | (10,)        |
+----------------------+------------+--------------+
Total Parameters: 1210

Population Size: 100
Generations: 3000
Mutation Rate: 0.01

[0 Best Individual - Fitness: 3.08301, Accuracy: 10.5%]
[1 Best Individual - Fitness: 3.08746, Accuracy: 12.0%]
...
[2999 Best Individual - Fitness: 94.08513, Accuracy: 98.5%]
Test set accuracy: 96.7%
Genetic Algorithm$ python mlfromscratch/examples/genetic_algorithm.py

+--------+
|   GA   |
+--------+
Description: Implementation of a Genetic Algorithm which aims to produce
the user specified target string. This implementation calculates each
candidate's fitness based on the alphabetical distance between the candidate
and the target. A candidate is selected as a parent with probabilities proportional
to the candidate's fitness. Reproduction is implemented as a single-point
crossover between pairs of parents. Mutation is done by randomly assigning
new characters with uniform probability.

Parameters
----------
Target String: 'Genetic Algorithm'
Population Size: 100
Mutation Rate: 0.05

[0 Closest Candidate: 'CJqlJguPlqzvpoJmb', Fitness: 0.00]
[1 Closest Candidate: 'MCxZxdr nlfiwwGEk', Fitness: 0.01]
[2 Closest Candidate: 'MCxZxdm nlfiwwGcx', Fitness: 0.01]
[3 Closest Candidate: 'SmdsAklMHn kBIwKn', Fitness: 0.01]
[4 Closest Candidate: '  lotneaJOasWfu Z', Fitness: 0.01]
...
[292 Closest Candidate: 'GeneticaAlgorithm', Fitness: 1.00]
[293 Closest Candidate: 'GeneticaAlgorithm', Fitness: 1.00]
[294 Answer: 'Genetic Algorithm']
Association Analysis$ python mlfromscratch/examples/apriori.py
+-------------+
|   Apriori   |
+-------------+
Minimum Support: 0.25
Minimum Confidence: 0.8
Transactions:
    [1, 2, 3, 4]
    [1, 2, 4]
    [1, 2]
    [2, 3, 4]
    [2, 3]
    [3, 4]
    [2, 4]
Frequent Itemsets:
    [1, 2, 3, 4, [1, 2], [1, 4], [2, 3], [2, 4], [3, 4], [1, 2, 4], [2, 3, 4]]
Rules:
    1 -> 2 (support: 0.43, confidence: 1.0)
    4 -> 2 (support: 0.57, confidence: 0.8)
    [1, 4] -> 2 (support: 0.29, confidence: 1.0)
ImplementationsSupervised LearningUnsupervised LearningReinforcement LearningDeep LearningContactIf there's some implementation you would like to see here or if you're just feeling social,feel free to  me or connect with me on ."
https://github.com/lancopku/pkuseg-python,pkuseg多领域中文分词工具; The pkuseg toolkit for multi-domain Chinese word segmentation,"pkuseg：一个多领域中文分词工具包 pkuseg 是基于论文[]的工具包。其简单易用，支持细分领域分词，有效提升了分词准确度。目录主要亮点pkuseg具有如下几个特点：编译和安装注意：安装方式1和2目前仅支持linux(ubuntu)、mac、windows 64 位的python3版本。如果非以上系统，请使用安装方式3进行本地编译安装。各类分词工具包的性能对比我们选择jieba、THULAC等国内代表分词工具包与pkuseg做性能比较，详细设置可参考。细领域训练及测试结果以下是在不同数据集上的对比结果：| MSRA   | Precision | Recall |   F-score || :----- | --------: | -----: | --------: || jieba  |     87.01 |  89.88 |     88.42 || THULAC |     95.60 |  95.91 |     95.71 || pkuseg |     96.94 |  96.81 | 96.88 || WEIBO  | Precision | Recall |   F-score || :----- | --------: | -----: | --------: || jieba  |     87.79 |  87.54 |     87.66 || THULAC |     93.40 |  92.40 |     92.87 || pkuseg |     93.78 |  94.65 | 94.21 |默认模型在不同领域的测试效果考虑到很多用户在尝试分词工具的时候，大多数时候会使用工具包自带模型测试。为了直接对比“初始”性能，我们也比较了各个工具包的默认模型在不同领域的测试效果。请注意，这样的比较只是为了说明默认情况下的效果，并不一定是公平的。| Default | MSRA  | CTB8  | PKU   | WEIBO | All Average || ------- | :---: | :---: | :---: | :---: | :---------: || jieba  | 81.45 | 79.58 | 81.83 | 83.56 | 81.61       || THULAC |	85.55 | 87.84 | 92.29 | 86.65 | 88.08 || pkuseg | 87.29 | 91.77 | 92.68 | 93.43 | 91.29   |其中，显示的是在所有测试集上F-score的平均。更多详细比较可参见。使用方式代码示例以下代码示例适用于python交互式环境。代码示例1：使用默认配置进行分词（如果用户无法确定分词领域，推荐使用默认模型分词）import pkuseg

seg = pkuseg.pkuseg()           # 以默认配置加载模型
text = seg.cut('我爱北京天安门')  # 进行分词
print(text)
代码示例2：细领域分词（如果用户明确分词领域，推荐使用细领域模型分词）import pkuseg

seg = pkuseg.pkuseg(model_name='medicine')  # 程序会自动下载所对应的细领域模型
text = seg.cut('我爱北京天安门')              # 进行分词
print(text)
代码示例3：分词同时进行词性标注，各词性标签的详细含义可参考 import pkuseg

seg = pkuseg.pkuseg(postag=True)  # 开启词性标注功能
text = seg.cut('我爱北京天安门')    # 进行分词和词性标注
print(text)
代码示例4：对文件分词import pkuseg

# 对input.txt的文件分词输出到output.txt中
# 开20个进程
pkuseg.test('input.txt', 'output.txt', nthread=20)     
其他使用示例可参见。参数说明模型配置pkuseg.pkuseg(model_name = ""default"", user_dict = ""default"", postag = False)
	model_name		模型路径。
			        ""default""，默认参数，表示使用我们预训练好的混合领域模型(仅对pip下载的用户)。
				""news"", 使用新闻领域模型。
				""web"", 使用网络领域模型。
				""medicine"", 使用医药领域模型。
				""tourism"", 使用旅游领域模型。
			        model_path, 从用户指定路径加载模型。
	user_dict		设置用户词典。
				""default"", 默认参数，使用我们提供的词典。
				None, 不使用词典。
				dict_path, 在使用默认词典的同时会额外使用用户自定义词典，可以填自己的用户词典的路径，词典格式为一行一个词（如果选择进行词性标注并且已知该词的词性，则在该行写下词和词性，中间用tab字符隔开）。
	postag		        是否进行词性分析。
				False, 默认参数，只进行分词，不进行词性标注。
				True, 会在分词的同时进行词性标注。
对文件进行分词pkuseg.test(readFile, outputFile, model_name = ""default"", user_dict = ""default"", postag = False, nthread = 10)
	readFile		输入文件路径。
	outputFile		输出文件路径。
	model_name		模型路径。同pkuseg.pkuseg
	user_dict		设置用户词典。同pkuseg.pkuseg
	postag			设置是否开启词性分析功能。同pkuseg.pkuseg
	nthread			测试时开的进程数。
模型训练pkuseg.train(trainFile, testFile, savedir, train_iter = 20, init_model = None)
	trainFile		训练文件路径。
	testFile		测试文件路径。
	savedir			训练模型的保存路径。
	train_iter		训练轮数。
	init_model		初始化模型，默认为None表示使用默认初始化，用户可以填自己想要初始化的模型的路径如init_model='./models/'。
多进程分词当将以上代码示例置于文件中运行时，如涉及多进程功能，请务必使用保护全局语句，详见。预训练模型从pip安装的用户在使用细领域分词功能时，只需要设置model_name字段为对应的领域即可，会自动下载对应的细领域模型。从github下载的用户则需要自己下载对应的预训练模型，并设置model_name字段为预训练模型路径。预训练模型可以在部分下载。以下是对预训练模型的说明：我们还通过领域自适应的方法，利用维基百科的未标注数据实现了几个细领域预训练模型的自动构建以及通用模型的优化，这些模型目前仅可以在release中下载：欢迎更多用户可以分享自己训练好的细分领域模型。版本历史详见。开源协议论文引用该代码包主要基于以下科研论文，如使用了本工具，请引用以下论文：
@article{pkuseg,
  author = {Luo, Ruixuan and Xu, Jingjing and Zhang, Yi and Zhang, Zhiyuan and Ren, Xuancheng and Sun, Xu},
  journal = {CoRR},
  title = {PKUSEG: A Toolkit for Multi-Domain Chinese Word Segmentation.},
  url = {https://arxiv.org/abs/1906.11455},
  volume = {abs/1906.11455},
  year = 2019
}
其他相关论文常见问题及解答致谢感谢俞士汶教授（北京大学计算语言所）与邱立坤博士提供的训练数据集！作者Ruixuan Luo （罗睿轩）,  Jingjing Xu（许晶晶）, Xuancheng Ren（任宣丞）, Yi Zhang（张艺）, Zhiyuan Zhang（张之远）, Bingzhen Wei（位冰镇）， Xu Sun （孙栩）  北京大学 "
https://github.com/shadowsocksr-backup/shadowsocksr,Python port of ShadowsocksR,"shadowsocksA fast tunnel proxy that helps you bypass firewalls.ServerInstallDebian / Ubuntu:apt-get install python-pip
pip install shadowsocks
CentOS:yum install python-setuptools && easy_install pip
pip install shadowsocks
Windows:See Usagessserver -p 443 -k password -m aes-256-cfb
To run in the background:sudo ssserver -p 443 -k password -m aes-256-cfb --user nobody -d start
To stop:sudo ssserver -d stop
To check the log:sudo less /var/log/shadowsocks.log
Check all the options via . You can also use a  fileinstead.ClientUse GUI clients on your local PC/phones. Check the README of your clientfor more information.DocumentationYou can find all the documentation in the .LicenseCopyright 2015 clowwindyLicensed under the Apache License, Version 2.0 (the ""License""); you maynot use this file except in compliance with the License. You may obtaina copy of the License athttp://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an ""AS IS"" BASIS, WITHOUTWARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See theLicense for the specific language governing permissions and limitationsunder the License.Bugs and Issues"
https://github.com/encode/starlette,The little ASGI framework that shines. 🌟,"Documentation: StarletteStarlette is a lightweight  framework/toolkit,which is ideal for building async web services in Python.It is production-ready, and gives you the following:RequirementsPython 3.8+Installation$ pip3 install starlette
You'll also want to install an ASGI server, such as , , or .$ pip3 install uvicorn
Exampleexample.py:from starlette.applications import Starlette
from starlette.responses import JSONResponse
from starlette.routing import Route


async def homepage(request):
    return JSONResponse({'hello': 'world'})

routes = [
    Route(""/"", endpoint=homepage)
]

app = Starlette(debug=True, routes=routes)
Then run the application using Uvicorn:$ uvicorn example:app
For a more complete example, see .DependenciesStarlette only requires , and the following are optional:You can install all of these with .Framework or ToolkitStarlette is designed to be used either as a complete framework, or asan ASGI toolkit. You can use any of its components independently.from starlette.responses import PlainTextResponse


async def app(scope, receive, send):
    assert scope['type'] == 'http'
    response = PlainTextResponse('Hello, world!')
    await response(scope, receive, send)
Run the  application in :$ uvicorn example:app
INFO: Started server process [11509]
INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
Run uvicorn with  to enable auto-reloading on code changes.ModularityThe modularity that Starlette is designed on promotes building re-usablecomponents that can be shared between any ASGI framework. This should enablean ecosystem of shared middleware and mountable applications.The clean API separation also means it's easier to understand each componentin isolation."
https://github.com/1adrianb/face-alignment,:fire: 2D and 3D Face alignment library build using pytorch ,"Face RecognitionDetect facial landmarks from Python using the world's most accurate face alignment network, capable of detecting points in both 2D and 3D coordinates.Build using 's state-of-the-art deep learning based face alignment method. Note: The lua version is available .For numerical evaluations it is highly recommended to use the lua version which uses indentical models with the ones evaluated in the paper. More models will be added soon.   FeaturesDetect 2D facial landmarks in picturesimport face_alignment
from skimage import io

fa = face_alignment.FaceAlignment(face_alignment.LandmarksType.TWO_D, flip_input=False)

input = io.imread('../test/assets/aflw-test.jpg')
preds = fa.get_landmarks(input)
Detect 3D facial landmarks in picturesimport face_alignment
from skimage import io

fa = face_alignment.FaceAlignment(face_alignment.LandmarksType.THREE_D, flip_input=False)

input = io.imread('../test/assets/aflw-test.jpg')
preds = fa.get_landmarks(input)
Process an entire directory in one goimport face_alignment
from skimage import io

fa = face_alignment.FaceAlignment(face_alignment.LandmarksType.TWO_D, flip_input=False)

preds = fa.get_landmarks_from_directory('../test/assets/')
Detect the landmarks using a specific face detector.By default the package will use the SFD face detector. However the users can alternatively use dlib, BlazeFace, or pre-existing ground truth bounding boxes.import face_alignment

# sfd for SFD, dlib for Dlib and folder for existing bounding boxes.
fa = face_alignment.FaceAlignment(face_alignment.LandmarksType.TWO_D, face_detector='sfd')
Running on CPU/GPUIn order to specify the device (GPU or CPU) on which the code will run one can explicitly pass the device flag:import torch
import face_alignment

# cuda for CUDA, mps for Apple M1/2 GPUs.
fa = face_alignment.FaceAlignment(face_alignment.LandmarksType.TWO_D, device='cpu')

# running using lower precision
fa = fa = face_alignment.FaceAlignment(face_alignment.LandmarksType.TWO_D, dtype=torch.bfloat16, device='cuda')
Please also see the  folderSupported face detectors
# dlib (fast, may miss faces)
model = FaceAlignment(landmarks_type= LandmarksType.TWO_D, face_detector='dlib')

# SFD (likely best results, but slowest)
model = FaceAlignment(landmarks_type= LandmarksType.TWO_D, face_detector='sfd')

# Blazeface (front camera model)
model = FaceAlignment(landmarks_type= LandmarksType.TWO_D, face_detector='blazeface')

# Blazeface (back camera model)
model = FaceAlignment(landmarks_type= LandmarksType.TWO_D, face_detector='blazeface', face_detector_kwargs={'back_model': True})

InstallationRequirementsWhile not required, for optimal performance(especially for the detector) it is highly recommended to run the code using a CUDA enabled GPU.BinariesThe easiest way to install it is using either pip or conda:| Using pip                | Using conda                            ||------------------------------|--------------------------------------------||  |  ||                              |                                            |Alternatively, bellow, you can find instruction to build it from source.From sourceInstall pytorch and pytorch dependencies. Please check the  for this.Get the Face Alignment source codegit clone https://github.com/1adrianb/face-alignment
Install the Face Alignment libpip install -r requirements.txt
python setup.py install
Docker imageA Dockerfile is provided to build images with cuda support and cudnn. For more instructions about running and building a docker image check the orginal Docker documentation.docker build -t face-alignment .
How does it work?While here the work is presented as a black-box, if you want to know more about the intrisecs of the method please check the original paper either on arxiv or my .ContributionsAll contributions are welcomed. If you encounter any issue (including examples of images where it fails) feel free to open an issue. If you plan to add a new features please open an issue to discuss this prior to making a pull request.Citation@inproceedings{bulat2017far,
  title={How far are we from solving the 2D \& 3D Face Alignment problem? (and a dataset of 230,000 3D facial landmarks)},
  author={Bulat, Adrian and Tzimiropoulos, Georgios},
  booktitle={International Conference on Computer Vision},
  year={2017}
}
For citing dlib, pytorch or any other packages used here please check the original page of their respective authors.Acknowledgements"
https://github.com/jcjohnson/pytorch-examples,Simple examples to introduce PyTorch,"This repository introduces the fundamental concepts ofthrough self-contained examples.At its core, PyTorch provides two main features:We will use a fully-connected ReLU network as our running example. The networkwill have a single hidden layer, and will be trained with gradient descent tofit random data by minimizing the Euclidean distance between the network outputand the true output.NOTE: These examples have been update for PyTorch 0.4, which made severalmajor changes to the core PyTorch API. Most notably, prior to 0.4 Tensors hadto be wrapped in Variable objects to use autograd; this functionality has nowbeen added directly to Tensors, and Variables are now deprecated.Table of ContentsWarm-up: numpyBefore introducing PyTorch, we will first implement the network using numpy.Numpy provides an n-dimensional array object, and many functions for manipulatingthese arrays. Numpy is a generic framework for scientific computing; it does notknow anything about computation graphs, or deep learning, or gradients. Howeverwe can easily use numpy to fit a two-layer network to random data by manuallyimplementing the forward and backward passes through the network using numpyoperations:# Code in file tensor/two_layer_net_numpy.py
import numpy as np

# N is batch size; D_in is input dimension;
# H is hidden dimension; D_out is output dimension.
N, D_in, H, D_out = 64, 1000, 100, 10

# Create random input and output data
x = np.random.randn(N, D_in)
y = np.random.randn(N, D_out)

# Randomly initialize weights
w1 = np.random.randn(D_in, H)
w2 = np.random.randn(H, D_out)

learning_rate = 1e-6
for t in range(500):
  # Forward pass: compute predicted y
  h = x.dot(w1)
  h_relu = np.maximum(h, 0)
  y_pred = h_relu.dot(w2)
  
  # Compute and print loss
  loss = np.square(y_pred - y).sum()
  print(t, loss)
  
  # Backprop to compute gradients of w1 and w2 with respect to loss
  grad_y_pred = 2.0 * (y_pred - y)
  grad_w2 = h_relu.T.dot(grad_y_pred)
  grad_h_relu = grad_y_pred.dot(w2.T)
  grad_h = grad_h_relu.copy()
  grad_h[h < 0] = 0
  grad_w1 = x.T.dot(grad_h)
 
  # Update weights
  w1 -= learning_rate * grad_w1
  w2 -= learning_rate * grad_w2
PyTorch: TensorsNumpy is a great framework, but it cannot utilize GPUs to accelerate itsnumerical computations. For modern deep neural networks, GPUs often providespeedups of , sounfortunately numpy won't be enough for modern deep learning.Here we introduce the most fundamental PyTorch concept: the Tensor. A PyTorchTensor is conceptually identical to a numpy array: a Tensor is an n-dimensionalarray, and PyTorch provides many functions for operating on these Tensors.Any computation you might want to perform with numpy can also be accomplishedwith PyTorch Tensors; you should think of them as a generic tool for scientificcomputing.However unlike numpy, PyTorch Tensors can utilize GPUs to accelerate theirnumeric computations. To run a PyTorch Tensor on GPU, you use the argument when constructing a Tensor to place the Tensor on a GPU.Here we use PyTorch Tensors to fit a two-layer network to random data. Like thenumpy example above we manually implement the forward and backwardpasses through the network, using operations on PyTorch Tensors:# Code in file tensor/two_layer_net_tensor.py
import torch

device = torch.device('cpu')
# device = torch.device('cuda') # Uncomment this to run on GPU

# N is batch size; D_in is input dimension;
# H is hidden dimension; D_out is output dimension.
N, D_in, H, D_out = 64, 1000, 100, 10

# Create random input and output data
x = torch.randn(N, D_in, device=device)
y = torch.randn(N, D_out, device=device)

# Randomly initialize weights
w1 = torch.randn(D_in, H, device=device)
w2 = torch.randn(H, D_out, device=device)

learning_rate = 1e-6
for t in range(500):
  # Forward pass: compute predicted y
  h = x.mm(w1)
  h_relu = h.clamp(min=0)
  y_pred = h_relu.mm(w2)

  # Compute and print loss; loss is a scalar, and is stored in a PyTorch Tensor
  # of shape (); we can get its value as a Python number with loss.item().
  loss = (y_pred - y).pow(2).sum()
  print(t, loss.item())

  # Backprop to compute gradients of w1 and w2 with respect to loss
  grad_y_pred = 2.0 * (y_pred - y)
  grad_w2 = h_relu.t().mm(grad_y_pred)
  grad_h_relu = grad_y_pred.mm(w2.t())
  grad_h = grad_h_relu.clone()
  grad_h[h < 0] = 0
  grad_w1 = x.t().mm(grad_h)

  # Update weights using gradient descent
  w1 -= learning_rate * grad_w1
  w2 -= learning_rate * grad_w2
PyTorch: AutogradIn the above examples, we had to manually implement both the forward andbackward passes of our neural network. Manually implementing the backward passis not a big deal for a small two-layer network, but can quickly get very hairyfor large complex networks.Thankfully, we can useto automate the computation of backward passes in neural networks.The autograd package in PyTorch provides exactly this functionality.When using autograd, the forward pass of your network will define acomputational graph; nodes in the graph will be Tensors, and edges will befunctions that produce output Tensors from input Tensors. Backpropagating throughthis graph then allows you to easily compute gradients.This sounds complicated, it's pretty simple to use in practice. If we want tocompute gradients with respect to some Tensor, then we set when constructing that Tensor. Any PyTorch operations on that Tensor will causea computational graph to be constructed, allowing us to later perform backpropagationthrough the graph. If  is a Tensor with , then afterbackpropagation  will be another Tensor holding the gradient of  withrespect to some scalar value.Sometimes you may wish to prevent PyTorch from building computational graphs whenperforming certain operations on Tensors with ; for examplewe usually don't want to backpropagate through the weight update steps whentraining a neural network. In such scenarios we can use the context manager to prevent the construction of a computational graph.Here we use PyTorch Tensors and autograd to implement our two-layer network;now we no longer need to manually implement the backward pass through thenetwork:# Code in file autograd/two_layer_net_autograd.py
import torch

device = torch.device('cpu')
# device = torch.device('cuda') # Uncomment this to run on GPU

# N is batch size; D_in is input dimension;
# H is hidden dimension; D_out is output dimension.
N, D_in, H, D_out = 64, 1000, 100, 10

# Create random Tensors to hold input and outputs
x = torch.randn(N, D_in, device=device)
y = torch.randn(N, D_out, device=device)

# Create random Tensors for weights; setting requires_grad=True means that we
# want to compute gradients for these Tensors during the backward pass.
w1 = torch.randn(D_in, H, device=device, requires_grad=True)
w2 = torch.randn(H, D_out, device=device, requires_grad=True)

learning_rate = 1e-6
for t in range(500):
  # Forward pass: compute predicted y using operations on Tensors. Since w1 and
  # w2 have requires_grad=True, operations involving these Tensors will cause
  # PyTorch to build a computational graph, allowing automatic computation of
  # gradients. Since we are no longer implementing the backward pass by hand we
  # don't need to keep references to intermediate values.
  y_pred = x.mm(w1).clamp(min=0).mm(w2)
  
  # Compute and print loss. Loss is a Tensor of shape (), and loss.item()
  # is a Python number giving its value.
  loss = (y_pred - y).pow(2).sum()
  print(t, loss.item())

  # Use autograd to compute the backward pass. This call will compute the
  # gradient of loss with respect to all Tensors with requires_grad=True.
  # After this call w1.grad and w2.grad will be Tensors holding the gradient
  # of the loss with respect to w1 and w2 respectively.
  loss.backward()

  # Update weights using gradient descent. For this step we just want to mutate
  # the values of w1 and w2 in-place; we don't want to build up a computational
  # graph for the update steps, so we use the torch.no_grad() context manager
  # to prevent PyTorch from building a computational graph for the updates
  with torch.no_grad():
    w1 -= learning_rate * w1.grad
    w2 -= learning_rate * w2.grad

    # Manually zero the gradients after running the backward pass
    w1.grad.zero_()
    w2.grad.zero_()
PyTorch: Defining new autograd functionsUnder the hood, each primitive autograd operator is really two functions thatoperate on Tensors. The forward function computes output Tensors from inputTensors. The backward function receives the gradient of the output Tensorswith respect to some scalar value, and computes the gradient of the input Tensorswith respect to that same scalar value.In PyTorch we can easily define our own autograd operator by defining a subclassof  and implementing the  and  functions.We can then use our new autograd operator by constructing an instance and calling itlike a function, passing Tensors containing input data.In this example we define our own custom autograd function for performing the ReLUnonlinearity, and use it to implement our two-layer network:# Code in file autograd/two_layer_net_custom_function.py
import torch

class MyReLU(torch.autograd.Function):
  """"""
  We can implement our own custom autograd Functions by subclassing
  torch.autograd.Function and implementing the forward and backward passes
  which operate on Tensors.
  """"""
  @staticmethod
  def forward(ctx, x):
    """"""
    In the forward pass we receive a context object and a Tensor containing the
    input; we must return a Tensor containing the output, and we can use the
    context object to cache objects for use in the backward pass.
    """"""
    ctx.save_for_backward(x)
    return x.clamp(min=0)

  @staticmethod
  def backward(ctx, grad_output):
    """"""
    In the backward pass we receive the context object and a Tensor containing
    the gradient of the loss with respect to the output produced during the
    forward pass. We can retrieve cached data from the context object, and must
    compute and return the gradient of the loss with respect to the input to the
    forward function.
    """"""
    x, = ctx.saved_tensors
    grad_x = grad_output.clone()
    grad_x[x < 0] = 0
    return grad_x


device = torch.device('cpu')
# device = torch.device('cuda') # Uncomment this to run on GPU

# N is batch size; D_in is input dimension;
# H is hidden dimension; D_out is output dimension.
N, D_in, H, D_out = 64, 1000, 100, 10

# Create random Tensors to hold input and output
x = torch.randn(N, D_in, device=device)
y = torch.randn(N, D_out, device=device)

# Create random Tensors for weights.
w1 = torch.randn(D_in, H, device=device, requires_grad=True)
w2 = torch.randn(H, D_out, device=device, requires_grad=True)

learning_rate = 1e-6
for t in range(500):
  # Forward pass: compute predicted y using operations on Tensors; we call our
  # custom ReLU implementation using the MyReLU.apply function
  y_pred = MyReLU.apply(x.mm(w1)).mm(w2)
 
  # Compute and print loss
  loss = (y_pred - y).pow(2).sum()
  print(t, loss.item())

  # Use autograd to compute the backward pass.
  loss.backward()

  with torch.no_grad():
    # Update weights using gradient descent
    w1 -= learning_rate * w1.grad
    w2 -= learning_rate * w2.grad

    # Manually zero the gradients after running the backward pass
    w1.grad.zero_()
    w2.grad.zero_()

TensorFlow: Static GraphsPyTorch autograd looks a lot like TensorFlow: in both frameworks we definea computational graph, and use automatic differentiation to compute gradients.The biggest difference between the two is that TensorFlow's computational graphsare static and PyTorch uses dynamic computational graphs.In TensorFlow, we define the computational graph once and then execute the samegraph over and over again, possibly feeding different input data to the graph.In PyTorch, each forward pass defines a new computational graph.Static graphs are nice because you can optimize the graph up front; for examplea framework might decide to fuse some graph operations for efficiency, or tocome up with a strategy for distributing the graph across many GPUs or manymachines. If you are reusing the same graph over and over, then this potentiallycostly up-front optimization can be amortized as the same graph is rerun overand over.One aspect where static and dynamic graphs differ is control flow. For some modelswe may wish to perform different computation for each data point; for example arecurrent network might be unrolled for different numbers of time steps for eachdata point; this unrolling can be implemented as a loop. With a static graph theloop construct needs to be a part of the graph; for this reason TensorFlowprovides operators such as  for embedding loops into the graph. Withdynamic graphs the situation is simpler: since we build graphs on-the-fly foreach example, we can use normal imperative flow control to perform computationthat differs for each input.To contrast with the PyTorch autograd example above, here we use TensorFlow tofit a simple two-layer net:# Code in file autograd/tf_two_layer_net.py
import tensorflow as tf
import numpy as np

# First we set up the computational graph:

# N is batch size; D_in is input dimension;
# H is hidden dimension; D_out is output dimension.
N, D_in, H, D_out = 64, 1000, 100, 10

# Create placeholders for the input and target data; these will be filled
# with real data when we execute the graph.
x = tf.placeholder(tf.float32, shape=(None, D_in))
y = tf.placeholder(tf.float32, shape=(None, D_out))

# Create Variables for the weights and initialize them with random data.
# A TensorFlow Variable persists its value across executions of the graph.
w1 = tf.Variable(tf.random_normal((D_in, H)))
w2 = tf.Variable(tf.random_normal((H, D_out)))

# Forward pass: Compute the predicted y using operations on TensorFlow Tensors.
# Note that this code does not actually perform any numeric operations; it
# merely sets up the computational graph that we will later execute.
h = tf.matmul(x, w1)
h_relu = tf.maximum(h, tf.zeros(1))
y_pred = tf.matmul(h_relu, w2)

# Compute loss using operations on TensorFlow Tensors
loss = tf.reduce_sum((y - y_pred) ** 2.0)

# Compute gradient of the loss with respect to w1 and w2.
grad_w1, grad_w2 = tf.gradients(loss, [w1, w2])

# Update the weights using gradient descent. To actually update the weights
# we need to evaluate new_w1 and new_w2 when executing the graph. Note that
# in TensorFlow the the act of updating the value of the weights is part of
# the computational graph; in PyTorch this happens outside the computational
# graph.
learning_rate = 1e-6
new_w1 = w1.assign(w1 - learning_rate * grad_w1)
new_w2 = w2.assign(w2 - learning_rate * grad_w2)

# Now we have built our computational graph, so we enter a TensorFlow session to
# actually execute the graph.
with tf.Session() as sess:
  # Run the graph once to initialize the Variables w1 and w2.
  sess.run(tf.global_variables_initializer())

  # Create numpy arrays holding the actual data for the inputs x and targets y
  x_value = np.random.randn(N, D_in)
  y_value = np.random.randn(N, D_out)
  for _ in range(500):
    # Execute the graph many times. Each time it executes we want to bind
    # x_value to x and y_value to y, specified with the feed_dict argument.
    # Each time we execute the graph we want to compute the values for loss,
    # new_w1, and new_w2; the values of these Tensors are returned as numpy
    # arrays.
    loss_value, _, _ = sess.run([loss, new_w1, new_w2],
                                feed_dict={x: x_value, y: y_value})
    print(loss_value)
PyTorch: nnComputational graphs and autograd are a very powerful paradigm for definingcomplex operators and automatically taking derivatives; however for largeneural networks raw autograd can be a bit too low-level.When building neural networks we frequently think of arranging the computationinto layers, some of which have learnable parameters which will beoptimized during learning.In TensorFlow, packages like ,,and  provide higher-level abstractions overraw computational graphs that are useful for building neural networks.In PyTorch, the  package serves this same purpose. The  package defines a set ofModules, which are roughly equivalent to neural network layers. A Module receivesinput Tensors and computes output Tensors, but may also hold internal state such asTensors containing learnable parameters. The  package also defines a set of usefulloss functions that are commonly used when training neural networks.In this example we use the  package to implement our two-layer network:# Code in file nn/two_layer_net_nn.py
import torch

device = torch.device('cpu')
# device = torch.device('cuda') # Uncomment this to run on GPU

# N is batch size; D_in is input dimension;
# H is hidden dimension; D_out is output dimension.
N, D_in, H, D_out = 64, 1000, 100, 10

# Create random Tensors to hold inputs and outputs
x = torch.randn(N, D_in, device=device)
y = torch.randn(N, D_out, device=device)

# Use the nn package to define our model as a sequence of layers. nn.Sequential
# is a Module which contains other Modules, and applies them in sequence to
# produce its output. Each Linear Module computes output from input using a
# linear function, and holds internal Tensors for its weight and bias.
# After constructing the model we use the .to() method to move it to the
# desired device.
model = torch.nn.Sequential(
          torch.nn.Linear(D_in, H),
          torch.nn.ReLU(),
          torch.nn.Linear(H, D_out),
        ).to(device)

# The nn package also contains definitions of popular loss functions; in this
# case we will use Mean Squared Error (MSE) as our loss function. Setting
# reduction='sum' means that we are computing the *sum* of squared errors rather
# than the mean; this is for consistency with the examples above where we
# manually compute the loss, but in practice it is more common to use mean
# squared error as a loss by setting reduction='elementwise_mean'.
loss_fn = torch.nn.MSELoss(reduction='sum')

learning_rate = 1e-4
for t in range(500):
  # Forward pass: compute predicted y by passing x to the model. Module objects
  # override the __call__ operator so you can call them like functions. When
  # doing so you pass a Tensor of input data to the Module and it produces
  # a Tensor of output data.
  y_pred = model(x)

  # Compute and print loss. We pass Tensors containing the predicted and true
  # values of y, and the loss function returns a Tensor containing the loss.
  loss = loss_fn(y_pred, y)
  print(t, loss.item())
  
  # Zero the gradients before running the backward pass.
  model.zero_grad()

  # Backward pass: compute gradient of the loss with respect to all the learnable
  # parameters of the model. Internally, the parameters of each Module are stored
  # in Tensors with requires_grad=True, so this call will compute gradients for
  # all learnable parameters in the model.
  loss.backward()

  # Update the weights using gradient descent. Each parameter is a Tensor, so
  # we can access its data and gradients like we did before.
  with torch.no_grad():
    for param in model.parameters():
      param.data -= learning_rate * param.grad
PyTorch: optimUp to this point we have updated the weights of our models by manually mutatingTensors holding learnable parameters. This is not a huge burdenfor simple optimization algorithms like stochastic gradient descent, but in practicewe often train neural networks using more sophisiticated optimizers like AdaGrad,RMSProp, Adam, etc.The  package in PyTorch abstracts the idea of an optimization algorithm andprovides implementations of commonly used optimization algorithms.In this example we will use the  package to define our model as before, but wewill optimize the model using the Adam algorithm provided by the  package:# Code in file nn/two_layer_net_optim.py
import torch

# N is batch size; D_in is input dimension;
# H is hidden dimension; D_out is output dimension.
N, D_in, H, D_out = 64, 1000, 100, 10

# Create random Tensors to hold inputs and outputs.
x = torch.randn(N, D_in)
y = torch.randn(N, D_out)

# Use the nn package to define our model and loss function.
model = torch.nn.Sequential(
          torch.nn.Linear(D_in, H),
          torch.nn.ReLU(),
          torch.nn.Linear(H, D_out),
        )
loss_fn = torch.nn.MSELoss(reduction='sum')

# Use the optim package to define an Optimizer that will update the weights of
# the model for us. Here we will use Adam; the optim package contains many other
# optimization algorithms. The first argument to the Adam constructor tells the
# optimizer which Tensors it should update.
learning_rate = 1e-4
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
for t in range(500):
  # Forward pass: compute predicted y by passing x to the model.
  y_pred = model(x)

  # Compute and print loss.
  loss = loss_fn(y_pred, y)
  print(t, loss.item())
  
  # Before the backward pass, use the optimizer object to zero all of the
  # gradients for the Tensors it will update (which are the learnable weights
  # of the model)
  optimizer.zero_grad()

  # Backward pass: compute gradient of the loss with respect to model parameters
  loss.backward()

  # Calling the step function on an Optimizer makes an update to its parameters
  optimizer.step()
PyTorch: Custom nn ModulesSometimes you will want to specify models that are more complex than a sequence ofexisting Modules; for these cases you can define your own Modules by subclassing and defining a  which receives input Tensors and producesoutput Tensors using other modules or other autograd operations on Tensors.In this example we implement our two-layer network as a custom Module subclass:# Code in file nn/two_layer_net_module.py
import torch

class TwoLayerNet(torch.nn.Module):
  def __init__(self, D_in, H, D_out):
    """"""
    In the constructor we instantiate two nn.Linear modules and assign them as
    member variables.
    """"""
    super(TwoLayerNet, self).__init__()
    self.linear1 = torch.nn.Linear(D_in, H)
    self.linear2 = torch.nn.Linear(H, D_out)

  def forward(self, x):
    """"""
    In the forward function we accept a Tensor of input data and we must return
    a Tensor of output data. We can use Modules defined in the constructor as
    well as arbitrary (differentiable) operations on Tensors.
    """"""
    h_relu = self.linear1(x).clamp(min=0)
    y_pred = self.linear2(h_relu)
    return y_pred

# N is batch size; D_in is input dimension;
# H is hidden dimension; D_out is output dimension.
N, D_in, H, D_out = 64, 1000, 100, 10

# Create random Tensors to hold inputs and outputs
x = torch.randn(N, D_in)
y = torch.randn(N, D_out)

# Construct our model by instantiating the class defined above.
model = TwoLayerNet(D_in, H, D_out)

# Construct our loss function and an Optimizer. The call to model.parameters()
# in the SGD constructor will contain the learnable parameters of the two
# nn.Linear modules which are members of the model.
loss_fn = torch.nn.MSELoss(reduction='sum')
optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)
for t in range(500):
  # Forward pass: Compute predicted y by passing x to the model
  y_pred = model(x)

  # Compute and print loss
  loss = loss_fn(y_pred, y)
  print(t, loss.item())

  # Zero gradients, perform a backward pass, and update the weights.
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()

PyTorch: Control Flow + Weight SharingAs an example of dynamic graphs and weight sharing, we implement a very strangemodel: a fully-connected ReLU network that on each forward pass chooses a randomnumber between 1 and 4 and uses that many hidden layers, reusing the same weightsmultiple times to compute the innermost hidden layers.For this model can use normal Python flow control to implement the loop, and wecan implement weight sharing among the innermost layers by simply reusing thesame Module multiple times when defining the forward pass.We can easily implement this model as a Module subclass:# Code in file nn/dynamic_net.py
import random
import torch

class DynamicNet(torch.nn.Module):
  def __init__(self, D_in, H, D_out):
    """"""
    In the constructor we construct three nn.Linear instances that we will use
    in the forward pass.
    """"""
    super(DynamicNet, self).__init__()
    self.input_linear = torch.nn.Linear(D_in, H)
    self.middle_linear = torch.nn.Linear(H, H)
    self.output_linear = torch.nn.Linear(H, D_out)

  def forward(self, x):
    """"""
    For the forward pass of the model, we randomly choose either 0, 1, 2, or 3
    and reuse the middle_linear Module that many times to compute hidden layer
    representations.

    Since each forward pass builds a dynamic computation graph, we can use normal
    Python control-flow operators like loops or conditional statements when
    defining the forward pass of the model.

    Here we also see that it is perfectly safe to reuse the same Module many
    times when defining a computational graph. This is a big improvement from Lua
    Torch, where each Module could be used only once.
    """"""
    h_relu = self.input_linear(x).clamp(min=0)
    for _ in range(random.randint(0, 3)):
      h_relu = self.middle_linear(h_relu).clamp(min=0)
    y_pred = self.output_linear(h_relu)
    return y_pred


# N is batch size; D_in is input dimension;
# H is hidden dimension; D_out is output dimension.
N, D_in, H, D_out = 64, 1000, 100, 10

# Create random Tensors to hold inputs and outputs.
x = torch.randn(N, D_in)
y = torch.randn(N, D_out)

# Construct our model by instantiating the class defined above
model = DynamicNet(D_in, H, D_out)

# Construct our loss function and an Optimizer. Training this strange model with
# vanilla stochastic gradient descent is tough, so we use momentum
criterion = torch.nn.MSELoss(reduction='sum')
optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)
for t in range(500):
  # Forward pass: Compute predicted y by passing x to the model
  y_pred = model(x)

  # Compute and print loss
  loss = criterion(y_pred, y)
  print(t, loss.item())

  # Zero gradients, perform a backward pass, and update the weights.
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()
"
https://github.com/pallets-eco/flask-sqlalchemy,Adds SQLAlchemy support to Flask,"Flask-SQLAlchemyFlask-SQLAlchemy is an extension for _ that adds support for_ to your application. It aims to simplify using SQLAlchemywith Flask by providing useful defaults and extra helpers that make iteasier to accomplish common tasks... _Flask: https://palletsprojects.com/p/flask/.. _SQLAlchemy: https://www.sqlalchemy.orgInstallingInstall and update using _:.. code-block:: text$ pip install -U Flask-SQLAlchemy.. _pip: https://pip.pypa.io/en/stable/getting-started/A Simple Example.. code-block:: pythonfrom flask import Flask
from flask_sqlalchemy import SQLAlchemy
from sqlalchemy.orm import DeclarativeBase, Mapped, mapped_column

app = Flask(__name__)
app.config[""SQLALCHEMY_DATABASE_URI""] = ""sqlite:///example.sqlite""

class Base(DeclarativeBase):
  pass

db = SQLAlchemy(app, model_class=Base)

class User(db.Model):
    id: Mapped[int] = mapped_column(db.Integer, primary_key=True)
    username: Mapped[str] = mapped_column(db.String, unique=True, nullable=False)

with app.app_context():
    db.create_all()

    db.session.add(User(username=""example""))
    db.session.commit()

    users = db.session.execute(db.select(User)).scalars()
ContributingFor guidance on setting up a development environment and how to make acontribution to Flask-SQLAlchemy, see the _... _contributing guidelines: https://github.com/pallets-eco/flask-sqlalchemy/blob/main/CONTRIBUTING.rstDonateThe Pallets organization develops and supports Flask-SQLAlchemy andother popular packages. In order to grow the community of contributorsand users, and allow the maintainers to devote more time to theprojects, _... _please donate today: https://palletsprojects.com/donateLinks"
https://github.com/microsoft/cascadia-code,"This is a fun, new monospaced font that includes programming ligatures and is designed to enhance the modern look and feel of the Windows Terminal.","About Cascadia CodeCascadia is a fun new coding font that comes bundled with , and is now the default font in Visual Studio as well. Font VariantsFor the italic, there is a standard  and a  variant accessible via  (see ). Font featuresEnabling stylistic sets will . For example, in VS Code, you can enable stylistic sets (and other OpenType features) via :""editor.fontLigatures"": ""'ss01', 'ss02', 'ss03', 'ss04', 'ss05', 'ss06', 'zero', 'onum'""
To enable the Cursive form of the italic, here's the code you should use:""editor.fontLigatures"": ""'calt', 'ss01'"",
If you're using an environment that does not support the  OT feature, one option to consider is .Character SetsInstallationYou can download the latest version of Cascadia Code from the releases page here: Font formats:Once unzipped, right-click the font file and click . This will install the font onto your machine. 👉 Note: If you have previously installed a version of Cascadia Code, please uninstall the previous version prior to installing a new version. Not doing so can result in improper rendering. For more details and app-specific instructions, . Get involvedInstructions on how to modify and submit an update to the Cascadia Code source is .Communicating with the TeamThe easiest way to communicate with the team is via GitHub Issues. Please file new issues, feature requests and suggestions, but DO search for similar open/closed pre-existing issues before you do.Please help us keep this repository clean, inclusive, and fun! We will not tolerate any abusive, rude, disrespectful or inappropriate behavior. Read our  for more details.If you would like to ask a question that you feel doesn't warrant an issue (yet), please reach out to us via Twitter:Aaron Bell, Font Designer: Kayla Cinnamon, Program Manager: Rich Turner, Program Manager: Special thanks to:Code of ConductThis project has adopted the . For more information see the or contact  with any additional questions or comments."
https://github.com/clovaai/stargan-v2,StarGAN v2 - Official PyTorch Implementation (CVPR 2020),"StarGAN v2 - Official PyTorch ImplementationTeaser videoClick the figure to watch the teaser video. TensorFlow implementationThe TensorFlow implementation of StarGAN v2 by our team member junho can be found at .Software installationClone this repository:git clone https://github.com/clovaai/stargan-v2.git
cd stargan-v2/
Install the dependencies:conda create -n stargan-v2 python=3.6.7
conda activate stargan-v2
conda install -y pytorch=1.4.0 torchvision=0.5.0 cudatoolkit=10.0 -c pytorch
conda install x264=='1!152.20180717' ffmpeg=4.0.2 -c conda-forge
pip install opencv-python==4.1.2.30 ffmpeg-python==0.2.0 scikit-image==0.16.2
pip install pillow==7.0.0 scipy==1.2.1 tqdm==4.43.0 munch==2.5.0
Datasets and pre-trained networksWe provide a script to download datasets used in StarGAN v2 and the corresponding pre-trained networks. The datasets and network checkpoints will be downloaded and stored in the  and  directories, respectively.CelebA-HQ. To download the  dataset and the pre-trained network, run the following commands:bash download.sh celeba-hq-dataset
bash download.sh pretrained-network-celeba-hq
bash download.sh wing
AFHQ. To download the  dataset and the pre-trained network, run the following commands:bash download.sh afhq-dataset
bash download.sh pretrained-network-afhq
Generating interpolation videosAfter downloading the pre-trained networks, you can synthesize output images reflecting diverse styles (e.g., hairstyle) of reference images. The following commands will save generated images and interpolation videos to the  directory. CelebA-HQ. To generate images and interpolation videos, run the following command:python main.py --mode sample --num_domains 2 --resume_iter 100000 --w_hpf 1 \
               --checkpoint_dir expr/checkpoints/celeba_hq \
               --result_dir expr/results/celeba_hq \
               --src_dir assets/representative/celeba_hq/src \
               --ref_dir assets/representative/celeba_hq/ref
To transform a custom image, first crop the image manually so that the proportion of face occupied in the whole is similar to that of CelebA-HQ. Then, run the following command for additional fine rotation and cropping. All custom images in the  directory will be aligned and stored in the  directory.python main.py --mode align \
               --inp_dir assets/representative/custom/female \
               --out_dir assets/representative/celeba_hq/src/female
AFHQ. To generate images and interpolation videos, run the following command:python main.py --mode sample --num_domains 3 --resume_iter 100000 --w_hpf 0 \
               --checkpoint_dir expr/checkpoints/afhq \
               --result_dir expr/results/afhq \
               --src_dir assets/representative/afhq/src \
               --ref_dir assets/representative/afhq/ref
Evaluation metricsTo evaluate StarGAN v2 using  and , run the following commands:# celeba-hq
python main.py --mode eval --num_domains 2 --w_hpf 1 \
               --resume_iter 100000 \
               --train_img_dir data/celeba_hq/train \
               --val_img_dir data/celeba_hq/val \
               --checkpoint_dir expr/checkpoints/celeba_hq \
               --eval_dir expr/eval/celeba_hq

# afhq
python main.py --mode eval --num_domains 3 --w_hpf 0 \
               --resume_iter 100000 \
               --train_img_dir data/afhq/train \
               --val_img_dir data/afhq/val \
               --checkpoint_dir expr/checkpoints/afhq \
               --eval_dir expr/eval/afhq
Note that the evaluation metrics are calculated using random latent vectors or reference images, both of which are selected by the . In the paper, we reported the average of values from 10 measurements using different seed numbers. The following table shows the calculated values for both latent-guided and reference-guided synthesis.| Dataset  |  FID (latent)   |  LPIPS (latent)  |  FID (reference)  | LPIPS (reference) |  Elapsed time   || :---------- | :------------: | :----: | :-----: | :----: | :----------:||  | 13.73 &pm; 0.06 | 0.4515 &pm; 0.0006  | 23.84  &pm; 0.03 | 0.3880 &pm; 0.0001 | 49min 51s|  | 16.18 &pm; 0.15 | 0.4501 &pm; 0.0007 | 19.78 &pm; 0.01 | 0.4315 &pm; 0.0002 | 64min 49sTraining networksTo train StarGAN v2 from scratch, run the following commands. Generated images and network checkpoints will be stored in the  and  directories, respectively. Training takes about three days on a single Tesla V100 GPU. Please see  for training arguments and a description of them. # celeba-hq
python main.py --mode train --num_domains 2 --w_hpf 1 \
               --lambda_reg 1 --lambda_sty 1 --lambda_ds 1 --lambda_cyc 1 \
               --train_img_dir data/celeba_hq/train \
               --val_img_dir data/celeba_hq/val

# afhq
python main.py --mode train --num_domains 3 --w_hpf 0 \
               --lambda_reg 1 --lambda_sty 1 --lambda_ds 2 --lambda_cyc 1 \
               --train_img_dir data/afhq/train \
               --val_img_dir data/afhq/val
Animal Faces-HQ dataset (AFHQ)We release a new dataset of animal faces, Animal Faces-HQ (AFHQ), consisting of 15,000 high-quality images at 512×512 resolution. The figure above shows example images of the AFHQ dataset. The dataset includes three domains of cat, dog, and wildlife, each providing about 5000 images. By having multiple (three) domains and diverse images of various breeds per each domain, AFHQ sets a challenging image-to-image translation problem. For each domain, we select 500 images as a test set and provide all remaining images as a training set. To download the dataset, run the following command:bash download.sh afhq-dataset
[Update: 2021.07.01] We rebuild the original AFHQ dataset by using high-quality resize filtering (i.e., Lanczos resampling). Please see the  that brings attention to the unfortunate software library situation for downsampling. We thank to  authors for their suggestion and contribution to the updated AFHQ dataset. If you use the updated dataset, we recommend to cite not only our paper but also their paper.The differences from the original dataset are as follows:To download the updated dataset, run the following command:bash download.sh afhq-v2-dataset
LicenseThe source code, pre-trained models, and dataset are available under  license by NAVER Corporation. You can use, copy, tranform and build upon the material for non-commercial purposes as long as you give appropriate credit by citing our paper, and indicate if changes were made. For business inquiries, please contact clova-jobs@navercorp.com.	For technical and other inquires, please contact yunjey.choi@navercorp.com.CitationIf you find this work useful for your research, please cite our paper:@inproceedings{choi2020starganv2,
  title={StarGAN v2: Diverse Image Synthesis for Multiple Domains},
  author={Yunjey Choi and Youngjung Uh and Jaejun Yoo and Jung-Woo Ha},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year={2020}
}
AcknowledgementsWe would like to thank the full-time and visiting Clova AI Research (now NAVER AI Lab) members for their valuable feedback and an early review: especially Seongjoon Oh, Junsuk Choe, Muhammad Ferjad Naeem, and Kyungjune Baek. We also thank Alias-Free GAN authors for their contribution to the updated AFHQ dataset."
https://github.com/WeblateOrg/weblate,Web based localization tool with tight version control integration.,".. image:: https://s.weblate.org/cdn/Logo-Darktext-borders.png:alt: Weblate:target: https://weblate.org/:height: 80pxWeblate is libre software web-based continuous localization system,Install it, or use the Hosted Weblate service at _... image:: https://img.shields.io/badge/website-weblate.org-blue.svg:alt: Website:target: https://weblate.org/.. image:: https://hosted.weblate.org/widgets/weblate/-/svg-badge.svg:alt: Translation status:target: https://hosted.weblate.org/engage/weblate/.. image:: https://bestpractices.coreinfrastructure.org/projects/552/badge:alt: CII Best Practices:target: https://bestpractices.coreinfrastructure.org/projects/552.. image:: https://api.reuse.software/badge/github.com/WeblateOrg/weblate:alt: REUSE status:target: https://api.reuse.software/info/github.com/WeblateOrg/weblate.. image:: https://img.shields.io/pypi/v/weblate.svg:target: https://pypi.org/project/Weblate/.. image:: https://readthedocs.org/projects/weblate/badge/:target: https://docs.weblate.org/.. image:: https://img.shields.io/github/license/WeblateOrg/weblate.svg:alt: License:target: https://github.com/WeblateOrg/weblate/blob/main/COPYINGSupportWeblate is libre software with optional professional support and cloudhosting offerings. Check out https://weblate.org/hosting/ for more information.DocumentationTo be found in the  directory of the source code, orviewed online on https://docs.weblate.org/InstallationSetup instructions:https://docs.weblate.org/en/latest/admin/install.htmlBugsPlease report feature requests and problems to:https://github.com/WeblateOrg/weblate/issuesLive chatLive chat about Weblate is available at _ IRC network. The channel name is . This can be accessed by, for example, https://web.libera.chat/#weblate or an IRC client installed on your computer.LicenseCopyright © Michal Čihař michal@weblate.orgThis program is free software: you can redistribute it and/or modify it underthe terms of the GNU General Public License as published by the Free SoftwareFoundation, either version 3 of the License, or (at your option) any laterversion.This program is distributed in the hope that it will be useful, but WITHOUT ANYWARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR APARTICULAR PURPOSE. See the _ for more details... _weblate.org: https://weblate.org/.. _GNU General Public License: https://www.gnu.org/licenses/gpl-3.0.html"
https://github.com/yenchenlin/DeepLearningFlappyBird,Flappy Bird hack using Deep Reinforcement Learning (Deep Q-learning).,"Using Deep Q-Network to Learn How To Play Flappy Bird7 mins version: OverviewThis project follows the description of the Deep Q Learning algorithm described in Playing Atari with Deep Reinforcement Learning [2] and shows that this learning algorithm can be further generalized to the notorious Flappy Bird.Installation Dependencies:How to Run?git clone https://github.com/yenchenlin1994/DeepLearningFlappyBird.git
cd DeepLearningFlappyBird
python deep_q_network.py
What is Deep Q-Network?It is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards.For those who are interested in deep reinforcement learning, I highly recommend to read the following post:Deep Q-Network AlgorithmThe pseudo-code for the Deep Q Learning algorithm, as given in [1], can be found below:Initialize replay memory D to size N
Initialize action-value function Q with random weights
for episode = 1, M do
    Initialize state s_1
    for t = 1, T do
        With probability ϵ select random action a_t
        otherwise select a_t=max_a  Q(s_t,a; θ_i)
        Execute action a_t in emulator and observe r_t and s_(t+1)
        Store transition (s_t,a_t,r_t,s_(t+1)) in D
        Sample a minibatch of transitions (s_j,a_j,r_j,s_(j+1)) from D
        Set y_j:=
            r_j for terminal s_(j+1)
            r_j+γ*max_(a^' )  Q(s_(j+1),a'; θ_i) for non-terminal s_(j+1)
        Perform a gradient step on (y_j-Q(s_j,a_j; θ_i))^2 with respect to θ
    end for
end for
ExperimentsEnvironmentSince deep Q-network is trained on the raw pixel values observed from the game screen at each time step, [3] finds that remove the background appeared in the original game can make it converge faster. This process can be visualized as the following figure:Network ArchitectureAccording to [1], I first preprocessed the game screens with following steps:The architecture of the network is shown in the figure below. The first layer convolves the input image with an 8x8x4x32 kernel at a stride size of 4. The output is then put through a 2x2 max pooling layer. The second layer convolves with a 4x4x32x64 kernel at a stride of 2. We then max pool again. The third layer convolves with a 3x3x64x64 kernel at a stride of 1. We then max pool one more time. The last hidden layer consists of 256 fully connected ReLU nodes.The final output layer has the same dimensionality as the number of valid actions which can be performed in the game, where the 0th index always corresponds to doing nothing. The values at this output layer represent the Q function given the input state for each valid action. At each time step, the network performs whichever action corresponds to the highest Q value using a ϵ greedy policy.TrainingAt first, I initialize all weight matrices randomly using a normal distribution with a standard deviation of 0.01, then set the replay memory with a max size of 500,00 experiences.I start training by choosing actions uniformly at random for the first 10,000 time steps, without updating the network weights. This allows the system to populate the replay memory before training begins.Note that unlike [1], which initialize ϵ = 1, I linearly anneal ϵ from 0.1 to 0.0001 over the course of the next 3000,000 frames. The reason why I set it this way is that agent can choose an action every 0.03s (FPS=30) in our game, high ϵ will make it flap too much and thus keeps itself at the top of the game screen and finally bump the pipe in a clumsy way. This condition will make Q function converge relatively slow since it only start to look other conditions when ϵ is low.However, in other games, initialize ϵ to 1 is more reasonable.During training time, at each time step, the network samples minibatches of size 32 from the replay memory to train on, and performs a gradient step on the loss function described above using the Adam optimization algorithm with a learning rate of 0.000001. After annealing finishes, the network continues to train indefinitely, with ϵ fixed at 0.001.FAQCheckpoint not foundChange  to How to reproduce?OBSERVE = 10000
EXPLORE = 3000000
FINAL_EPSILON = 0.0001
INITIAL_EPSILON = 0.1
References[1] Mnih Volodymyr, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level Control through Deep Reinforcement Learning. Nature, 529-33, 2015.[2] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing Atari with Deep Reinforcement Learning. NIPS, Deep Learning workshop[3] Kevin Chen. Deep Reinforcement Learning for Flappy Bird  | DisclaimerThis work is highly based on the following repos:"
https://github.com/lk-geimfari/awesomo,Cool open source projects. Choose your project and get involved in Open Source development now.,"If you're interested in Open Source and thinking about joining the community of developers, you might find a suitable project here.LanguagesWant to add an interesting project?:point_up: However, keep in mind that we don't accept mammoth's shit. Only active and interesting projects with good documentation are added. Dead and abandoned projects will be removed.Want to support us?Just share this list with your friends on , ,  or somewhere else.License by To the extent possible under law, the person who associated CC0 with  has waived all copyright and related or neighboring rights to .You should have received a copy of the CC0 legalcode along with this work. If not, see https://creativecommons.org/publicdomain/zero/1.0/."
https://github.com/utkuozbulak/pytorch-cnn-visualizations,Pytorch implementation of convolutional neural network visualization techniques,"Convolutional Neural Network VisualizationsThis repository contains a number of convolutional neural network visualization techniques implemented in PyTorch.Note: I removed cv2 dependencies and moved the repository towards PIL. A few things might be broken (although I tested all methods), I would appreciate if you could create an issue if something does not work.Note: The code in this repository was tested with torch version 0.4.1 and some of the functions may not work as intended in later versions. Although it shouldn't be too much of an effort to make it work, I have no plans at the moment to make the code in this repository compatible with the latest version because I'm still using 0.4.1.Implemented TechniquesGeneral InformationDepending on the technique, the code uses pretrained AlexNet or VGG from the model zoo. Some of the code also assumes that the layers in the model are separated into two sections; features, which contains the convolutional layers and classifier, that contains the fully connected layer (after flatting out convolutions). If you want to port this code to use it on your model that does not have such separation, you just need to do some editing on parts where it calls model.features and model.classifier.Every technique has its own python file (e.g. gradcam.py) which I hope will make things easier to understand. misc_functions.py contains functions like image processing and image recreation which is shared by the implemented techniques.All images are pre-processed with mean and std of the ImageNet dataset before being fed to the model. None of the code uses GPU as these operations are quite fast for a single image (except for deep dream because of the example image that is used for it is huge). You can make use of gpu with very little effort. The example pictures below include numbers in the brackets after the description, like Mastiff (243), this number represents the class id in the ImageNet dataset.I tried to comment on the code as much as possible, if you have any issues understanding it or porting it, don't hesitate to send an email or create an issue.Below, are some sample results for each operation.Gradient VisualizationHierarchical Gradient VisualizationLayerCAM [16] is a simple modification of Grad-CAM [3], which can generate reliable class activation maps from different layers. For the examples provided below, a pre-trained VGG16 was used.Grad Times ImageAnother technique that is proposed is simply multiplying the gradients with the image itself. Results obtained with the usage of multiple gradient techniques are below.Smooth GradSmooth grad is adding some Gaussian noise to the original image and calculating gradients multiple times and averaging the results [8]. There are two examples at the bottom which use vanilla and guided backpropagation to calculate the gradients. Number of images (n) to average over is selected as 50. σ is shown at the bottom of the images.Convolutional Neural Network Filter VisualizationCNN filters can be visualized when we optimize the input image with respect to output of the specific convolution operation. For this example I used a pre-trained VGG16. Visualizations of layers start with basic color and direction filters at lower levels. As we approach towards the final layer the complexity of the filters also increase. If you employ external techniques like blurring, gradient clipping etc. you will probably produce better images.Another way to visualize CNN layers is to to visualize activations for a specific input on a specific layer and filter. This was done in [1] Figure 3. Below example is obtained from layers/filters of VGG16 for the first image using guided backpropagation. The code for this opeations is in layer_activation_with_guided_backprop.py. The method is quite similar to guided backpropagation but instead of guiding the signal from the last layer and a specific target, it guides the signal from a specific layer and filter. Inverted Image RepresentationsI think this technique is the most complex technique in this repository in terms of understanding what the code does. It is mainly because of complex regularization. If you truly want to understand how this is implemented I suggest you read the second and third page of the paper [5], specifically, the regularization part. Here, the aim is to generate original image after nth layer. The further we go into the model, the harder it becomes. The results in the paper are incredibly good (see Figure 6) but here, the result quickly becomes messy as we iterate through the layers. This is because the authors of the paper tuned the parameters for each layer individually. You can tune the parameters just like the to ones that are given in the paper to optimize results for each layer. The inverted examples from several layers of AlexNet with the previous Snake picture are below.Deep DreamDeep dream is technically the same operation as layer visualization the only difference is that you don't start with a random image but use a real picture. The samples below were created with VGG19, the produced result is entirely up to the filter so it is kind of hit or miss. The more complex models produce mode high level features. If you replace VGG19 with an Inception variant you will get more noticable shapes when you target higher conv layers. Like layer visualization, if you employ additional techniques like gradient clipping, blurring etc. you might get better visualizations.Class Specific Image GenerationThis operation produces different outputs based on the model and the applied regularization method. Below, are some samples produced with VGG19 incorporated with Gaussian blur every other iteration (see [14] for details). The quality of generated images also depend on the model, AlexNet generally has green(ish) artifacts but VGGs produce (kind of) better images. Note that these images are generated with regular CNNs with optimizing the input and not with GANs.The samples below show the produced image with no regularization, l1 and l2 regularizations on target class: flamingo (130) to show the differences between regularization methods. These images are generated with a pretrained AlexNet. Produced samples can further be optimized to resemble the desired target class, some of the operations you can incorporate to improve quality are; blurring, clipping gradients that are below a certain treshold, random color swaps on some parts, random cropping the image, forcing generated image to follow a path to force continuity.Some of these techniques are implemented in generate_regularized_class_specific_samples.py (courtesy of ).Requirements:torch == 0.4.1
torchvision >= 0.1.9
numpy >= 1.13.0
matplotlib >= 1.5
PIL >= 1.1.7
CitationIf you find the code in this repository useful for your research consider citing it.@misc{uozbulak_pytorch_vis_2022,
  author = {Utku Ozbulak},
  title = {PyTorch CNN Visualizations},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/utkuozbulak/pytorch-cnn-visualizations}},
  commit = {b7e60adaf64c9be97b480509285718603d1e9ba4}
}
References:[1] J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller. Striving for Simplicity: The All Convolutional Net, https://arxiv.org/abs/1412.6806[2] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, A. Torralba. Learning Deep Features for Discriminative Localization, https://arxiv.org/abs/1512.04150[3] R. R. Selvaraju, A. Das, R. Vedantam, M. Cogswell, D. Parikh, and D. Batra. Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization, https://arxiv.org/abs/1610.02391[4] K. Simonyan, A. Vedaldi, A. Zisserman. Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps, https://arxiv.org/abs/1312.6034[5] A. Mahendran, A. Vedaldi. Understanding Deep Image Representations by Inverting Them, https://arxiv.org/abs/1412.0035[6] H. Noh, S. Hong, B. Han,  Learning Deconvolution Network for Semantic Segmentation https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Noh_Learning_Deconvolution_Network_ICCV_2015_paper.pdf[7] A. Nguyen, J. Yosinski, J. Clune.  Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable  Images https://arxiv.org/abs/1412.1897[8] D. Smilkov, N. Thorat, N. Kim, F. Viégas, M. Wattenberg. SmoothGrad: removing noise by adding noise https://arxiv.org/abs/1706.03825[9] D. Erhan, Y. Bengio, A. Courville, P. Vincent. Visualizing Higher-Layer Features of a Deep Network https://www.researchgate.net/publication/265022827_Visualizing_Higher-Layer_Features_of_a_Deep_Network[10] A. Mordvintsev, C. Olah, M. Tyka. Inceptionism: Going Deeper into Neural Networks https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html[11] I. J. Goodfellow, J. Shlens, C. Szegedy. Explaining and Harnessing Adversarial Examples https://arxiv.org/abs/1412.6572[12] A. Shrikumar, P. Greenside, A. Shcherbina, A. Kundaje. Not Just a Black Box: Learning Important Features Through Propagating Activation Differences https://arxiv.org/abs/1605.01713[13] M. Sundararajan, A. Taly, Q. Yan. Axiomatic Attribution for Deep Networks https://arxiv.org/abs/1703.01365[14] J. Yosinski, J. Clune, A. Nguyen, T. Fuchs, Hod Lipson, Understanding Neural Networks Through Deep Visualization https://arxiv.org/abs/1506.06579[15] H. Wang, Z. Wang, M. Du, F. Yang, Z. Zhang, S. Ding, P. Mardziel, X. Hu. Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks https://arxiv.org/abs/1910.01279[16] P. Jiang, C. Zhang, Q. Hou, M. Cheng, Y. Wei. LayerCAM: Exploring Hierarchical Class Activation Maps for Localization http://mmcheng.net/mftp/Papers/21TIP_LayerCAM.pdf[17] G. Montavon1, A. Binder, S. Lapuschkin, W. Samek, and K. Muller. Layer-Wise Relevance Propagation: An Overview https://www.researchgate.net/publication/335708351_Layer-Wise_Relevance_Propagation_An_Overview"
https://github.com/instaloader/instaloader,Download pictures (or videos) along with their captions and other metadata from Instagram.,".. image:: https://raw.githubusercontent.com/instaloader/instaloader/master/docs/logo_heading.png.. badges-start|pypi| |pyversion| |license| |aur| |contributors| |downloads|.. |pypi| image:: https://img.shields.io/pypi/v/instaloader.svg:alt: Instaloader PyPI Project Page:target: https://pypi.org/project/instaloader/.. |license| image:: https://img.shields.io/github/license/instaloader/instaloader.svg:alt: MIT License:target: https://github.com/instaloader/instaloader/blob/master/LICENSE.. |pyversion| image:: https://img.shields.io/pypi/pyversions/instaloader.svg:alt: Supported Python Versions.. |contributors| image:: https://img.shields.io/github/contributors/instaloader/instaloader.svg:alt: Contributor Count:target: https://github.com/instaloader/instaloader/graphs/contributors.. |aur| image:: https://img.shields.io/aur/version/instaloader.svg:alt: Arch User Repository Package:target: https://aur.archlinux.org/packages/instaloader/.. |downloads| image:: https://pepy.tech/badge/instaloader/month:alt: PyPI Download Count:target: https://pepy.tech/project/instaloader.. badges-end::$ pip3 install instaloader

$ instaloader profile [profile ...]
Instaloader::instaloader [--comments] [--geotags]
            [--stories] [--highlights] [--tagged] [--igtv]
            [--login YOUR-USERNAME] [--fast-update]
            profile | ""#hashtag"" | :stories | :feed | :saved
__How to Automatically Download Pictures from InstagramTo download all pictures and videos of a profile, as well as theprofile picture, do::instaloader profile [profile ...]
where  is the name of a profile you want to download. Insteadof only one profile, you may also specify a list of profiles.To later update your local copy of that profiles, you may run::instaloader --fast-update profile [profile ...]
If  is given, Instaloader stops when arriving at thefirst already-downloaded picture.Alternatively, you can use  to have Instaloader storethe time each profile was last downloaded and only download newer media:::instaloader --latest-stamps -- profile [profile ...]
With this option it's possible to move or delete downloaded media and still keepthe archive updated.When updating profiles, Instaloaderautomatically detects profile name changes and renames the target directoryaccordingly.Instaloader can also be used to download private profiles. To do so,invoke it with::instaloader --login=your_username profile [profile ...]
When logging in, Instaloader stores the session cookies in a file in yourtemporary directory, which will be reused later the next time is given.  So you can download private profiles non-interactively when youalready have a valid session cookie file.__ContributingAs an open source project, Instaloader heavily depends on the contributions fromits community. See__for how you may help Instaloader to become an even greater tool.Supporters.. current-sponsors-start| Instaloader is proudly sponsored by|  __|  __See __ page forhow you can sponsor the development of Instaloader!.. current-sponsors-endIt is a pleasure for us to share our Instaloader to the world, and we are proudto have attracted such an active and motivating community, with so many userswho share their suggestions and ideas with us. Buying a community-sponsored beeror coffee from time to time is very likely to further raise our passion for thedevelopment of Instaloader.| For Donations, we provide GitHub Sponsors page, a PayPal.Me link and a Bitcoin address.|  GitHub Sponsors: __|  PayPal: __|  BTC: 1Nst4LoadeYzrKjJ1DX9CpbLXBYE9RKLwYDisclaimer.. disclaimer-startInstaloader is in no way affiliated with, authorized, maintained or endorsed by Instagram or any of its affiliates orsubsidiaries. This is an independent and unofficial project. Use at your own risk.Instaloader is licensed under an MIT license. Refer to  file for more information... disclaimer-end"
https://github.com/automl/auto-sklearn,Automated Machine Learning with scikit-learn,"auto-sklearnauto-sklearn is an automated machine learning toolkit and a drop-in replacement for a  estimator.Find the documentation [<marko.inline.RawText object at 0x000001592FDEC848>]. Quick links:auto-sklearn in one imageauto-sklearn in four lines of codeimport autosklearn.classification
cls = autosklearn.classification.AutoSklearnClassifier()
cls.fit(X_train, y_train)
predictions = cls.predict(X_test)
Relevant publicationsIf you use auto-sklearn in scientific publications, we would appreciate citations.Efficient and Robust Automated Machine LearningMatthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Springenberg, Manuel Blum and Frank HutterAdvances in Neural Information Processing Systems 28 (2015) to publication.@inproceedings{feurer-neurips15a,
    title     = {Efficient and Robust Automated Machine Learning},
    author    = {Feurer, Matthias and Klein, Aaron and Eggensperger, Katharina and Springenberg, Jost and Blum, Manuel and Hutter, Frank},
    booktitle = {Advances in Neural Information Processing Systems 28 (2015)},
    pages     = {2962--2970},
    year      = {2015}
}
Auto-Sklearn 2.0: The Next GenerationMatthias Feurer, Katharina Eggensperger, Stefan Falkner, Marius Lindauer and Frank Hutter*arXiv:2007.04074 [cs.LG], 2020 to publication.@article{feurer-arxiv20a,
    title     = {Auto-Sklearn 2.0: Hands-free AutoML via Meta-Learning},
    author    = {Feurer, Matthias and Eggensperger, Katharina and Falkner, Stefan and Lindauer, Marius and Hutter, Frank},
    booktitle = {arXiv:2007.04074 [cs.LG]},
    year      = {2020}
}
Also, have a look at the blog on  where we regularly release blogposts."
https://github.com/tortoise/tortoise-orm,"Familiar asyncio ORM for python, built with relations in mind","============Tortoise ORM.. image:: https://img.shields.io/pypi/v/tortoise-orm.svg?style=flat:target: https://pypi.python.org/pypi/tortoise-orm.. image:: https://pepy.tech/badge/tortoise-orm/month:target: https://pepy.tech/project/tortoise-orm.. image:: https://github.com/tortoise/tortoise-orm/workflows/gh-pages/badge.svg:target: https://github.com/tortoise/tortoise-orm/actions?query=workflow:gh-pages.. image:: https://github.com/tortoise/tortoise-orm/workflows/ci/badge.svg:target: https://github.com/tortoise/tortoise-orm/actions?query=workflow:ci.. image:: https://coveralls.io/repos/github/tortoise/tortoise-orm/badge.svg:target: https://coveralls.io/github/tortoise/tortoise-orm.. image:: https://app.codacy.com/project/badge/Grade/844030d0cb8240d6af92c71bfac764ff:target: https://www.codacy.com/gh/tortoise/tortoise-orm/dashboard?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=tortoise/tortoise-orm&amp;utm_campaign=Badge_GradeIntroductionTortoise ORM is an easy-to-use  ORM (Object Relational Mapper) inspired by Django.Tortoise ORM was built with relations in mind and admiration for the excellent and popular Django ORM.It's engraved in its design that you are working not with just tables, you work with relational data.You can find the docs at _.. note::Tortoise ORM is a young project and breaking changes are to be expected.We keep a _ and it will have possible breakage clearly documented.Tortoise ORM is supported on CPython >= 3.8 for SQLite, MySQL and PostgreSQL and Microsoft SQL Server and Oracle.Why was Tortoise ORM built?Python has many existing and mature ORMs, unfortunately they are designed with an opposing paradigm of how I/O gets processed. is relatively new technology that has a very different concurrency model, and the largest change is regarding how I/O is handled.However, Tortoise ORM is not the first attempt of building an  ORM. While there are many cases of developers attempting to map synchronous Python ORMs to the async world, initial attempts did not have a clean API.Hence we started Tortoise ORM.Tortoise ORM is designed to be functional, yet familiar, to ease the migration of developers wishing to switch to .It also performs well when compared to other Python ORMs. In _, where we measure different read and write operations (rows/sec, more is better), it's trading places with Pony ORM:.. image:: https://raw.githubusercontent.com/tortoise/tortoise-orm/develop/docs/ORM_Perf.png:target: https://github.com/tortoise/orm-benchmarksHow is an ORM useful?When you build an application or service that uses a relational database, there is a point where you can't get away with just using parameterized queries or even query builder. You just keep repeating yourself, writing slightly different code for each entity.Code has no idea about relations between data, so you end up concatenating your data almost manually.It is also easy to make mistakes in how you access your database, which can be exploited by SQL-injection attacks.Your data rules are also distributed, increasing the complexity of managing your data, and even worse, could lead to those rules being applied inconsistently.An ORM (Object Relational Mapper) is designed to address these issues, by centralising your data model and data rules, ensuring that your data is managed safely (providing immunity to SQL-injection) and keeping track of relationships so you don't have to.Getting StartedInstallationFirst you have to install Tortoise ORM like this:.. code-block:: bashpip install tortoise-orm
You can also install with your db driver ( is builtin):.. code-block:: bashpip install tortoise-orm[asyncpg]
For :.. code-block:: bashpip install tortoise-orm[asyncmy]
For / (not fully tested):.. code-block:: bashpip install tortoise-orm[asyncodbc]
Quick TutorialThe primary entity of tortoise is .You can start writing models like this:.. code-block:: python3from tortoise.models import Model
from tortoise import fields

class Tournament(Model):
    id = fields.IntField(pk=True)
    name = fields.TextField()

    def __str__(self):
        return self.name


class Event(Model):
    id = fields.IntField(pk=True)
    name = fields.TextField()
    tournament = fields.ForeignKeyField('models.Tournament', related_name='events')
    participants = fields.ManyToManyField('models.Team', related_name='events', through='event_team')

    def __str__(self):
        return self.name


class Team(Model):
    id = fields.IntField(pk=True)
    name = fields.TextField()

    def __str__(self):
        return self.name
After you defined all your models, tortoise needs you to init them, in order to create backward relations between models and match your db client with the appropriate models.You can do it like this:.. code-block:: python3from tortoise import Tortoise

async def init():
    # Here we connect to a SQLite DB file.
    # also specify the app name of ""models""
    # which contain models from ""app.models""
    await Tortoise.init(
        db_url='sqlite://db.sqlite3',
        modules={'models': ['app.models']}
    )
    # Generate the schema
    await Tortoise.generate_schemas()
Here we create a connection to an SQLite database in the local directory called . Then we discover and initialise the models.Tortoise ORM currently supports the following databases: generates the schema on an empty database. Tortoise generates schemas in safe mode by default whichincludes the  clause, so you may include it in your main code.After that you can start using your models:.. code-block:: python3# Create instance by save
tournament = Tournament(name='New Tournament')
await tournament.save()

# Or by .create()
await Event.create(name='Without participants', tournament=tournament)
event = await Event.create(name='Test', tournament=tournament)
participants = []
for i in range(2):
    team = await Team.create(name='Team {}'.format(i + 1))
    participants.append(team)

# M2M Relationship management is quite straightforward
# (also look for methods .remove(...) and .clear())
await event.participants.add(*participants)

# You can query a related entity with async for
async for team in event.participants:
    pass

# After making a related query you can iterate with regular for,
# which can be extremely convenient when using it with other packages,
# for example some kind of serializers with nested support
for team in event.participants:
    pass


# Or you can make a preemptive call to fetch related objects
selected_events = await Event.filter(
    participants=participants[0].id
).prefetch_related('participants', 'tournament')

# Tortoise supports variable depth of prefetching related entities
# This will fetch all events for Team and in those events tournaments will be prefetched
await Team.all().prefetch_related('events__tournament')

# You can filter and order by related models too
await Tournament.filter(
    events__name__in=['Test', 'Prod']
).order_by('-events__participants__name').distinct()
MigrationTortoise ORM uses _ as its database migration tool, see more detail at its _.ContributingPlease have a look at the _.ThanksToPowerful Python IDE _from _... image:: https://resources.jetbrains.com/storage/products/company/brand/logos/jb_beam.svg:target: https://jb.gg/OpenSourceSupportLicenseThis project is licensed under the Apache License - see the _ file for details."
https://github.com/xingyizhou/CenterNet,"Object detection, 3D detection, and pose estimation using center point detection: ","Objects as PointsObject detection, 3D detection, and pose estimation using center point detection:Contact: . Any questions or discussions are welcomed! UpdatesAbstractDetection identifies objects as axis-aligned boxes in an image. Most successful object detectors enumerate a nearly exhaustive list of potential object locations and classify each. This is wasteful, inefficient, and requires additional post-processing. In this paper, we take a different approach. We model an object as a single point -- the center point of its bounding box. Our detector uses keypoint estimation to find center points and regresses to all other object properties, such as size, 3D location, orientation, and even pose. Our center point based approach, CenterNet, is end-to-end differentiable, simpler, faster, and more accurate than corresponding bounding box based detectors. CenterNet achieves the best speed-accuracy trade-off on the MS COCO dataset, with 28.1% AP at 142 FPS, 37.4% AP at 52 FPS, and 45.1% AP with multi-scale testing at 1.4 FPS. We use the same approach to estimate 3D bounding box in the KITTI benchmark and human pose on the COCO keypoint dataset. Our method performs competitively with sophisticated multi-stage methods and runs in real-time.HighlightsMain resultsObject Detection on COCO validation| Backbone     |  AP / FPS | Flip AP / FPS|  Multi-scale AP / FPS ||--------------|-----------|--------------|-----------------------||Hourglass-104 | 40.3 / 14 | 42.2 / 7.8   | 45.1 / 1.4            ||DLA-34        | 37.4 / 52 | 39.2 / 28    | 41.7 / 4              ||ResNet-101    | 34.6 / 45 | 36.2 / 25    | 39.3 / 4              ||ResNet-18     | 28.1 / 142| 30.0 / 71    | 33.2 / 12             |Keypoint detection on COCO validation| Backbone     |  AP       |  FPS         ||--------------|-----------|--------------||Hourglass-104 | 64.0      |    6.6       ||DLA-34        | 58.9      |    23        |3D bounding box detection on KITTI validation|Backbone|FPS|AP-E|AP-M|AP-H|AOS-E|AOS-M|AOS-H|BEV-E|BEV-M|BEV-H||--------|---|----|----|----|-----|-----|-----|-----|-----|-----||DLA-34  |32 |96.9|87.8|79.2|93.9 |84.3 |75.7 |34.0 |30.5 |26.8 |All models and details are available in our .InstallationPlease refer to  for installation instructions.Use CenterNetWe support demo for image/ image folder, video, and webcam. First, download the models (By default,  for detection and for human pose estimation)from the  and put them in .For object detection on images/ video, run:python demo.py ctdet --demo /path/to/image/or/folder/or/video --load_model ../models/ctdet_coco_dla_2x.pth
We provide example images in  (from ). If set up correctly, the output should look likeFor webcam demo, run     python demo.py ctdet --demo webcam --load_model ../models/ctdet_coco_dla_2x.pth
Similarly, for human pose estimation, run:python demo.py multi_pose --demo /path/to/image/or/folder/or/video/or/webcam --load_model ../models/multi_pose_dla_3x.pth
The result for the example images should look like:You can add  to visualize the heatmap outputs.You can add  for flip test.To use this CenterNet in your own project, you can import sys
CENTERNET_PATH = /path/to/CenterNet/src/lib/
sys.path.insert(0, CENTERNET_PATH)

from detectors.detector_factory import detector_factory
from opts import opts

MODEL_PATH = /path/to/model
TASK = 'ctdet' # or 'multi_pose' for human pose estimation
opt = opts().init('{} --load_model {}'.format(TASK, MODEL_PATH).split(' '))
detector = detector_factory[opt.task](opt)

img = image/or/path/to/your/image/
ret = detector.run(img)['results']
 will be a python dict: Benchmark Evaluation and TrainingAfter , follow the instructions in  to setup the datasets. Then check  to reproduce the results in the paper.We provide scripts for all the experiments in the  folder.DevelopIf you are interested in training CenterNet in a new dataset, use CenterNet in a new task, or use a new network architecture for CenterNet, please refer to . Also feel free to send us emails for discussions or suggestions.Third-party resourcesLicenseCenterNet itself is released under the MIT License (refer to the LICENSE file for details).Portions of the code are borrowed from  (image transform, resnet),  (hourglassnet, loss functions),  (DLA network), (deformable convolutions), (Pascal VOC evaluation) and  (KITTI dataset evaluation). Please refer to the original License of these projects (See ).CitationIf you find this project useful for your research, please use the following BibTeX entry.@inproceedings{zhou2019objects,
  title={Objects as Points},
  author={Zhou, Xingyi and Wang, Dequan and Kr{\""a}henb{\""u}hl, Philipp},
  booktitle={arXiv preprint arXiv:1904.07850},
  year={2019}
}
"
https://github.com/WZMIAOMIAO/deep-learning-for-image-processing,deep learning for image processing including classification and object-detection etc.,深度学习在图像处理中的应用教程前言教程目录，点击跳转相应视频（后期会根据学习内容增加）[<marko.inline.RawText object at 0x000001592FD3B248>]所需环境欢迎大家关注下我的微信公众号（阿喆学习小记），平时会总结些相关学习博文。    如果有什么问题，也可以到我的CSDN中一起讨论。我的bilibili频道：
https://github.com/taki0112/Tensorflow-Cookbook,Simple Tensorflow Cookbook for easy-to-use,"ContributionsIn now, this repo contains general architectures and functions that are useful for the GAN and classificstion.I will continue to add useful things to other areas.Also, your pull requests and issues are always welcome.And write what you want to implement on the issue. I'll implement it.How to useImportNetwork templatedef network(x, is_training=True, reuse=False, scope=""network""):
    with tf.variable_scope(scope, reuse=reuse):
        x = conv(...)
        
        ...
        
        return logit
Insert data to network using DatasetAPIImage_Data_Class = ImageData(img_size, img_ch, augment_flag)

trainA_dataset = ['./dataset/cat/trainA/a.jpg', 
                  './dataset/cat/trainA/b.png', 
                  './dataset/cat/trainA/c.jpeg', 
                  ...]
trainA = tf.data.Dataset.from_tensor_slices(trainA_dataset)
trainA = trainA.map(Image_Data_Class.image_processing, num_parallel_calls=16)
trainA = trainA.shuffle(buffer_size=10000).prefetch(buffer_size=batch_size).batch(batch_size).repeat()

trainA_iterator = trainA.make_one_shot_iterator()
data_A = trainA_iterator.get_next()

logit = network(data_A)
OptionCautionWeightweight_init = tf.truncated_normal_initializer(mean=0.0, stddev=0.02)
weight_regularizer = tf.contrib.layers.l2_regularizer(0.0001)
weight_regularizer_fully = tf.contrib.layers.l2_regularizer(0.0001)
InitializationRegularizationConvolutionbasic convx = conv(x, channels=64, kernel=3, stride=2, pad=1, pad_type='reflect', use_bias=True, sn=True, scope='conv')
partial conv (NVIDIA )x = partial_conv(x, channels=64, kernel=3, stride=2, use_bias=True, padding='SAME', sn=True, scope='partial_conv')
dilated convx = dilate_conv(x, channels=64, kernel=3, rate=2, use_bias=True, padding='VALID', sn=True, scope='dilate_conv')
Deconvolutionbasic deconvx = deconv(x, channels=64, kernel=3, stride=1, padding='SAME', use_bias=True, sn=True, scope='deconv')
Fully-connectedx = fully_connected(x, units=64, use_bias=True, sn=True, scope='fully_connected')
Pixel shufflex = conv_pixel_shuffle_down(x, scale_factor=2, use_bias=True, sn=True, scope='pixel_shuffle_down')
x = conv_pixel_shuffle_up(x, scale_factor=2, use_bias=True, sn=True, scope='pixel_shuffle_up')
Blockresidual blockx = resblock(x, channels=64, is_training=is_training, use_bias=True, sn=True, scope='residual_block')
x = resblock_down(x, channels=64, is_training=is_training, use_bias=True, sn=True, scope='residual_block_down')
x = resblock_up(x, channels=64, is_training=is_training, use_bias=True, sn=True, scope='residual_block_up')
dense blockx = denseblock(x, channels=64, n_db=6, is_training=is_training, use_bias=True, sn=True, scope='denseblock')
residual-dense blockx = res_denseblock(x, channels=64, n_rdb=20, n_rdb_conv=6, is_training=is_training, use_bias=True, sn=True, scope='res_denseblock')
attention blockx = self_attention(x, use_bias=True, sn=True, scope='self_attention')
x = self_attention_with_pooling(x, use_bias=True, sn=True, scope='self_attention_version_2')

x = squeeze_excitation(x, ratio=16, use_bias=True, sn=True, scope='squeeze_excitation')

x = convolution_block_attention(x, ratio=16, use_bias=True, sn=True, scope='convolution_block_attention')

x = global_context_block(x, use_bias=True, sn=True, scope='gc_block')

x = srm_block(x, use_bias=False, is_training=is_training, scope='srm_block')
Normalizationx = batch_norm(x, is_training=is_training, scope='batch_norm')
x = layer_norm(x, scope='layer_norm')
x = instance_norm(x, scope='instance_norm')
x = group_norm(x, groups=32, scope='group_norm')

x = pixel_norm(x)

x = batch_instance_norm(x, scope='batch_instance_norm')
x = layer_instance_norm(x, scope='layer_instance_norm')
x = switch_norm(x, scope='switch_norm')

x = condition_batch_norm(x, z, is_training=is_training, scope='condition_batch_norm'):

x = adaptive_instance_norm(x, gamma, beta)
x = adaptive_layer_instance_norm(x, gamma, beta, smoothing=True, scope='adaLIN')

Activationx = relu(x)
x = lrelu(x, alpha=0.2)
x = tanh(x)
x = sigmoid(x)
x = swish(x)
x = elu(x)
Pooling & Resizex = nearest_up_sample(x, scale_factor=2)
x = bilinear_up_sample(x, scale_factor=2)
x = nearest_down_sample(x, scale_factor=2)
x = bilinear_down_sample(x, scale_factor=2)

x = max_pooling(x, pool_size=2)
x = avg_pooling(x, pool_size=2)

x = global_max_pooling(x)
x = global_avg_pooling(x)

x = flatten(x)
x = hw_flatten(x)
Lossclassification lossloss, accuracy = classification_loss(logit, label)

loss = dice_loss(n_classes=10, logit, label)
regularization lossg_reg_loss = regularization_loss('generator')
d_reg_loss = regularization_loss('discriminator')
pixel lossloss = L1_loss(x, y)
loss = L2_loss(x, y)
loss = huber_loss(x, y)
loss = histogram_loss(x, y)

loss = gram_style_loss(x, y)

loss = color_consistency_loss(x, y)
gan lossd_loss = discriminator_loss(Ra=True, loss_func='wgan-gp', real=real_logit, fake=fake_logit)
g_loss = generator_loss(Ra=True, loss_func='wgan-gp', real=real_logit, fake=fake_logit)
d_bottleneck_loss = vdb_loss(real_mu, real_logvar, i_c) + vdb_loss(fake_mu, fake_logvar, i_c)
kl-divergence (z ~ N(0, 1))loss = kl_loss(mean, logvar)
Author"
https://github.com/jazzband/tablib,"Python Module for Tabular Datasets in XLS, CSV, JSON, YAML, &c.","Tablib: format-agnostic tabular dataset library_____         ______  ___________ ______
__  /_______ ____  /_ ___  /___(_)___  /_
_  __/_  __ `/__  __ \__  / __  / __  __ \
/ /_  / /_/ / _  /_/ /_  /  _  /  _  /_/ /
\__/  \__,_/  /_.___/ /_/   /_/   /_.___/
Tablib is a format-agnostic tabular dataset library, written in Python.Output formats supported:Note that tablib purposefully excludes XML support. It always will. (Note: This is ajoke. Pull requests are welcome.)Tablib documentation is graciously hosted on https://tablib.readthedocs.ioIt is also available in the  directory of the source distribution.Make sure to check out !ContributePlease see the ."
https://github.com/spyder-ide/spyder,Official repository for Spyder - The Scientific Python Development Environment,"Copyright © 2009–  and others (see AUTHORS.txt)Some source files and icons may be under other authorship/licenses; seeProject statusBuild statusTry Spyder online :point_left: Click on this link to run the  in your browser. :point_left: Click on this link to check the next Spyder 5 version. :point_left: Click on this link to test changes in our  branch.How Spyder looksHelp support Spyder, the community-developed scientific IDE!Spyder development is made possible by contributions from our global usercommunity, along with organizations like and .There are numerous , many ofwhich don't require any programming. If you'd like to make a  to help fund further improvements,we're on .Thanks for all you do to make the Spyder project thrive! OverviewSpyder is a powerful scientific environment written in Python, for Python,and designed by and for scientists, engineers and data analysts. It offers aunique combination of the advanced editing, analysis, debugging, and profilingfunctionality of a comprehensive development tool with the data exploration,interactive execution, deep inspection, and beautiful visualizationcapabilities of a scientific package.Beyond its many built-in features, its abilities can be extended even furthervia its plugin system and API. Furthermore, Spyder can also be used as a PyQt5extension library, allowing you to build upon its functionality and embedits components, such as the interactive console, in your own software.For more general information about Spyder and to stay up to date on thelatest Spyder news and information, please check out .Core componentsDocumentationYou can read the Spyder documentation online on .InstallationFor a detailed guide to installing Spyder, please refer to our.The easiest way to install Spyder on any of our supported platformsis to download it as part of the distribution, and use the  package and environment manager to keep itand your other packages installed and up to date.If in doubt, you should always install Spyder via this method to avoidunexpected issues we are unable to help you with; it generally has theleast likelihood of potential pitfalls for non-experts, and we may beable to provide limited assistance if you do run into trouble.Other installation options exist, including:However, we lack the resources to provide individual support for users whoinstall via these methods, and they may be out of date or contain bugs outsideour control, so we recommend the Anaconda version instead if you run into issues.TroubleshootingBefore posting a report, please carefully read our [<marko.inline.RawText object at 0x000001592FDB6508>]and search the for your error message and problem description, as the great majority of bugsare either duplicates, or can be fixed on the user side with a few easy steps.Thanks!Contributing and CreditsSpyder was originally created by , and is currently maintained by and an internationalcommunity of volunteers.You can join us—everyone is welcome to help with Spyder!Please read our to get started!Certain source files are distributed under other compatible permissive licensesand/or originally by other authors.The icons for the Spyder 3 theme are derived from  4.7 (© 2016 David Gandy; SIL OFL 1.1).Most Spyder 2 theme icons are sourced from the  (© 2006-2007 Everaldo Coelho; LGPL 2.1+).Other Spyder 2 icons are from  (© 2013 Yusuke Kamiyamane; CC-BY 3.0),the (© 2006 Mark James; CC-BY 2.5), and the  (© 2007 KDE Artists; LGPL 3.0+).See for full legal information.Running from a git clonePlease see the instructions in ourto learn how to do run Spyder after cloning its repo from Github.DependenciesImportant Note: Most or all of the dependencies listed below comewith Anaconda and other scientific Python distributions, so you don't needto install them separately in those cases.Build dependenciesWhen installing Spyder from its source package, the only requirement is to havea Python version equal or greater than 3.8.Runtime dependenciesThe basic dependencies to run Spyder are:The rest our dependencies (both required and optional) are declared in.SponsorsSpyder is funded thanks to the generous support ofand the donations we have received from our users around the world through :More information"
https://github.com/motioneye-project/motioneye,A web frontend for the motion daemon.,"motionEye is a web-based frontend for . Check out the  for more details. Changelog is available on the _.Due to personal reasons _ can no longer be actively involved with this project. It has been transferred to the new _ GitHub organization with a new . Please check out the  branch for current development, which includes:The original code is maintained in the _ and bug fixes may be backported if there is still a need for a Python 2 compatible version of motionEye.If you want to be involved, please leave a comment here: https://github.com/motioneye-project/motioneye/issues/2307Any contribution is highly appreciated, also have a look at open _ and _ if you are looking for a way to help."
https://github.com/scrapinghub/portia,Visual scraping for Scrapy,"PortiaPortia is a tool that allows you to visually scrape websites without any programming knowledge required. With Portia you can annotate a web page to identify the data you wish to extract, and Portia will understand based on these annotations how to scrape data from similar pages.Running PortiaThe easiest way to run Portia is using :You can run Portia using Docker & official Portia-image by running:docker run -v ~/portia_projects:/app/data/projects:rw -p 9001:9001 scrapinghub/portia
You can also set up a local instance with  by cloning this repo & running from the root of the folder:docker-compose up
For more detailed instructions, and alternatives to using Docker, see the  docs.DocumentationDocumentation can be found from . Source files can be found in the  directory."
https://github.com/Tribler/tribler,Privacy enhanced BitTorrent client with P2P content discovery,"Tribler|Pytest| |docs| |Codacy| |Coverage| |contributors| |pr_closed| |issues_closed||python_3_8| |python_3_9||downloads_7_0| |downloads_7_1| |downloads_7_2| |downloads_7_3| |downloads_7_4||downloads_7_5| |downloads_7_6| |downloads_7_7| |downloads_7_8| |downloads_7_9||downloads_7_10| |downloads_7_11| |downloads_7_12| |downloads_7_13||doi| |openhub| |discord|Towards making Bittorrent anonymous and impossible to shut down.We use our own dedicated Tor-like network for anonymous torrent downloading.We implemented and enhanced the ..Tribler aims to give anonymous access to content. We are trying to make privacy, strong cryptography, and authentication the Internet norm.For the past 11 years we have been building a very robust Peer-to-Peer system.Today Tribler is robust: ""the only way to take Tribler down is to take The Internet down"" (but a single software bug could end everything).Obtaining the latest releaseJust click __ and download the latest package for your OS.Obtaining supportIf you found a bug or have a feature request, please make sure you read _ and then _. We will have a look at it ASAP.ContributingContributions are very welcome!If you are interested in contributing code or otherwise, please have a look at . if you are looking for inspiration :).Running Tribler from the repository###################################We support development on Linux, macOS and Windows. We have writtendocumentation that guides you through installing the required packages whensetting up a Tribler development environment.Packaging TriblerWe have written guides on how to package Tribler for distribution on various systems.Docker supportDockerfile is provided with the source code which can be used to build the docker image.To build the docker image:.. code-block:: bashdocker build -t triblercore/triblercore:latest .
To run the built docker image:.. code-block:: bashdocker run -p 20100:20100 --net=""host"" triblercore/triblercore:latest
Note that by default, the REST API is bound to localhost inside the container so toaccess the APIs, network needs to be set to host (--net=""host"").The REST APIs are now accessible at: http://localhost:20100/docsDocker ComposeTribler core can also be started using Docker Compose. For that, a  file is availableon the project root directory.To run via docker compose:.. code-block:: bashdocker-compose up
To run in detached mode:.. code-block:: bashdocker-compose up -d
To stop Tribler:.. code-block:: bashdocker-compose down
Get in touch!We like to hear your feedback and suggestions. To reach out to us, you can join _ or create a post on _... |jenkins_build| image:: http://jenkins-ci.tribler.org/job/Test_tribler_main/badge/icon:target: http://jenkins-ci.tribler.org/job/Test_tribler_main/:alt: Build status on Jenkins.. |pr_closed| image:: https://img.shields.io/github/issues-pr-closed/tribler/tribler.svg?style=flat:target: https://github.com/Tribler/tribler/pulls:alt: Pull Requests.. |issues_closed| image:: https://img.shields.io/github/issues-closed/tribler/tribler.svg?style=flat:target: https://github.com/Tribler/tribler/issues:alt: Issues.. |openhub| image:: https://www.openhub.net/p/tribler/widgets/project_thin_badge.gif?style=flat:target: https://www.openhub.net/p/tribler.. |downloads_7_0| image:: https://img.shields.io/github/downloads/tribler/tribler/v7.0.2/total.svg?style=flat:target: https://github.com/Tribler/tribler/releases:alt: Downloads(7.0.2).. |downloads_7_1| image:: https://img.shields.io/github/downloads/tribler/tribler/v7.1.3/total.svg?style=flat:target: https://github.com/Tribler/tribler/releases:alt: Downloads(7.1.3).. |downloads_7_2| image:: https://img.shields.io/github/downloads/tribler/tribler/v7.2.2/total.svg?style=flat:target: https://github.com/Tribler/tribler/releases:alt: Downloads(7.2.2).. |downloads_7_3| image:: https://img.shields.io/github/downloads/tribler/tribler/v7.3.2/total.svg?style=flat:target: https://github.com/Tribler/tribler/releases:alt: Downloads(7.3.2).. |downloads_7_4| image:: https://img.shields.io/github/downloads/tribler/tribler/v7.4.1/total.svg?style=flat:target: https://github.com/Tribler/tribler/releases:alt: Downloads(7.4.1).. |downloads_7_5| image:: https://img.shields.io/github/downloads/tribler/tribler/v7.5.1/total.svg?style=flat:target: https://github.com/Tribler/tribler/releases:alt: Downloads(7.5.1).. |downloads_7_6| image:: https://img.shields.io/github/downloads/tribler/tribler/v7.6.1/total.svg?style=flat:target: https://github.com/Tribler/tribler/releases:alt: Downloads(7.6.1).. |downloads_7_7| image:: https://img.shields.io/github/downloads/tribler/tribler/v7.7.0/total.svg?style=flat:target: https://github.com/Tribler/tribler/releases:alt: Downloads(7.7.0).. |downloads_7_8| image:: https://img.shields.io/github/downloads/tribler/tribler/v7.8.0/total.svg?style=flat:target: https://github.com/Tribler/tribler/releases:alt: Downloads(7.8.0).. |downloads_7_9| image:: https://img.shields.io/github/downloads/tribler/tribler/v7.9.0/total.svg?style=flat:target: https://github.com/Tribler/tribler/releases:alt: Downloads(7.9.0).. |downloads_7_10| image:: https://img.shields.io/github/downloads/tribler/tribler/v7.10.0/total.svg?style=flat:target: https://github.com/Tribler/tribler/releases:alt: Downloads(7.10.0).. |downloads_7_11| image:: https://img.shields.io/github/downloads/tribler/tribler/v7.11.0/total.svg?style=flat:target: https://github.com/Tribler/tribler/releases:alt: Downloads(7.11.0).. |downloads_7_12| image:: https://img.shields.io/github/downloads/tribler/tribler/v7.12.1/total.svg?style=flat:target: https://github.com/Tribler/tribler/releases:alt: Downloads(7.12.1).. |downloads_7_13| image:: https://img.shields.io/github/downloads/tribler/tribler/v7.13.0/total.svg?style=flat:target: https://github.com/Tribler/tribler/releases:alt: Downloads(7.13.0).. |contributors| image:: https://img.shields.io/github/contributors/tribler/tribler.svg?style=flat:target: https://github.com/Tribler/tribler/graphs/contributors:alt: Contributors.. |doi| image:: https://zenodo.org/badge/8411137.svg:target: https://zenodo.org/badge/latestdoi/8411137:alt: DOI number.. |docs| image:: https://readthedocs.org/projects/tribler/badge/?version=latest:target: https://tribler.readthedocs.io/en/latest/?badge=latest:alt: Documentation Status.. |discord| image:: https://img.shields.io/badge/discord-join%20chat-blue.svg:target: https://discord.gg/UpPUcVGESe:alt: Join Discord chat.. |python_3_8| image:: https://img.shields.io/badge/python-3.8-blue.svg:target: https://www.python.org/.. |python_3_9| image:: https://img.shields.io/badge/python-3.9-blue.svg:target: https://www.python.org/.. |Pytest| image:: https://github.com/Tribler/tribler/actions/workflows/pytest.yml/badge.svg?branch=main:target: https://github.com/Tribler.. |Codacy| image:: https://app.codacy.com/project/badge/Grade/35785b4de0b84724bffdd2598eea3276:target: https://www.codacy.com/gh/Tribler/tribler/dashboard?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=Tribler/tribler&amp;utm_campaign=Badge_Grade.. |Coverage| image:: https://app.codacy.com/project/badge/Coverage/35785b4de0b84724bffdd2598eea3276:target: https://www.codacy.com/gh/Tribler/tribler/dashboard?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=Tribler/tribler&amp;utm_campaign=Badge_Coverage"
https://github.com/jmcnamara/XlsxWriter,A Python module for creating Excel XLSX files.,"XlsxWriterXlsxWriter is a Python module for writing files in the Excel 2007+ XLSXfile format.XlsxWriter can be used to write text, numbers, formulas and hyperlinks tomultiple worksheets and it supports features such as formatting and many more,including:It supports Python 3.4+ and PyPy3 and uses standard libraries only.Here is a simple example:.. code-block:: pythonimport xlsxwriterCreate an new Excel file and add a worksheet.workbook = xlsxwriter.Workbook('demo.xlsx')worksheet = workbook.add_worksheet()Widen the first column to make the text clearer.worksheet.set_column('A:A', 20)Add a bold format to use to highlight cells.bold = workbook.add_format({'bold': True})Write some simple text.worksheet.write('A1', 'Hello')Text with formatting.worksheet.write('A2', 'World', bold)Write some numbers, with row/column notation.worksheet.write(2, 0, 123)worksheet.write(3, 0, 123.456)Insert an image.worksheet.insert_image('B5', 'logo.png')workbook.close().. image:: https://raw.github.com/jmcnamara/XlsxWriter/master/dev/docs/source/_images/demo.pngSee the full documentation at: https://xlsxwriter.readthedocs.ioRelease notes: https://xlsxwriter.readthedocs.io/changes.html"
https://github.com/lisa-lab/DeepLearningTutorials,Deep Learning Tutorial notes and code. See the wiki for more info.,"Deep Learning TutorialsDeep Learning is a new area of Machine Learning research, which has beenintroduced with the objective of moving Machine Learning closer to one of itsoriginal goals: Artificial Intelligence.  Deep Learning is about learningmultiple levels of representation and abstraction that help to make sense ofdata such as images, sound, and text.  The tutorials presented here willintroduce you to some of the most important deep learning algorithms and willalso show you how to run them using Theano.  Theano is a python library thatmakes writing deep learning models easy, and gives the option of training themon a GPU.The easiest way to follow the tutorials is to _._of this project... image:: https://secure.travis-ci.org/lisa-lab/DeepLearningTutorials.png:target: http://travis-ci.org/lisa-lab/DeepLearningTutorialsProject LayoutSubdirectories:Build instructionsTo build the html version of the tutorials, run python doc/scripts/docgen.py"
https://github.com/dwyl/english-words,:memo: A text file containing 479k English words for all your dictionary/word-based projects e.g: auto-completion / autosuggestion,List Of English WordsA text file containing over 466k English words.While searching for a list of english words (for an auto-complete tutorial)I found: https://stackoverflow.com/questions/2213607/how-to-get-english-language-word-database which refers to  (archived).No idea why infochimps put the word list inside an excel (.xls) file.I pulled out the words into a simple new-line-delimited text file.Which is more useful when building apps or importing into databases etc.Copyright still belongs to them.Files you may be interested in:See  for example usage.
https://github.com/shimohq/chinese-programmer-wrong-pronunciation,中国程序员容易发音错误的单词,"Words Commonly Mispronounced by Chinese Programmers中国程序员容易发音错误的单词(点击🔊收听正确读音)重要更新| 单词 | 正确发音（英音）| 正确发音（美音）| 错误发音 || --- | ----------- | ----------- | ---------- || access |   /'ækses/ |   /ˈækses/ |  ❌ /ək'ses/ || Adobe |   /ə'dəʊbi/ |   /ə'dəʊbi/ |  ❌ /əˈdub/ || admin |   /'ædmɪn/ |   /ˈædmɪn/ |  ❌ /ɜ:d'mɪn/ || adversarial |   /ˌædvəˈseəriəl/ |    /ˌædvərˈseriəl/ |  ❌ /ədˈvɜːrsəriəl/ || agile |   /'ædʒaɪl/ |   /ˈædʒl/ |  ❌ /ə'dʒaɪl/ || amazon |   /'æməzən/ |   /ˈæməzɑːn/ |  ❌ /'əmeizən/ /ə'meizən/ || analogy |   /əˈnælədʒi/ |   /əˈnælədʒi/ |  ❌ /ænə'lɒdʒi/ || Angular |   /'æŋgjʊlə/ |   /ˈæŋɡjələr/ |  ❌ /'æŋɡələ/ /'æŋdʒʌlə/ || AJAX |   /'eidʒæks/ |   /'eidʒæks/ |  ❌ /ə'dʒʌks/ || alias |   /ˈeɪliəs/ |   /ˈeɪliəs/ |  ❌ /ə'lais/ || align |   /əˈlaɪn/ |   /əˈlaɪn/ |  ❌ /ə'lidʒen/ || Apache |   /ə'pætʃɪ/ |   /əˈpætʃi/ |  ❌ /ʌpʌtʃ/ || app |   /æp/ |   /æp/ |  ❌ /eipi'pi/ || archive |   /'ɑːkaɪv/ |   /'ɑːkaɪv/ |  ❌ /'ətʃɪv/ || array |   /ə'rei/ |   /əˈreɪ/ |  ❌ /æ'rei/ || ASCII |   /'æski/ |   /ˈæski/ |  ❌ /ɑːsk/ || aspect |   /'æspekt/ |   /ˈæspekt/ |  ❌ /ə'spekt/ || async |  /əˈsɪŋk/ |  /æˈsɪŋk/ | ❌ /'æsɪŋk/ || avatar |   /'ævətɑː/ |   /ˈævətɑːr/ |  ❌ /ə'vʌtɑ/ || Azure |   /'æʒə/ |   /ˈæʒər/ |  ❌ /ˈæzʊʒə/ || bind |   /baɪnd/ |   /baɪnd/ |  ❌ /bɪnd/ || BIOS |   /ˈbaɪɒs/ | /'baɪɑs/ |  ❌ /ˈbɪɒs/ || cache |   /kæʃ/ |   /kæʃ/ |  ❌ /kætʃ/ || canal |  /kəˈnæl/ |  /kəˈnæl/ | ❌ /ˈkænl/ || chaos |  /ˈkeɪɒs/ |  /ˈkeɪɑːs/ |  ❌ /ˈtʃoʊs/ || Chrome |  /krəʊm/ |  /kroʊm/ |  ❌ /tʃɔːm/ || clang |   /klæŋ/ |   /klæŋ/ |  ❌ /sɪlæŋ/ || context |   /ˈkɒntekst/ |   / ˈkɑːntekst/ |  ❌ /kənˈtekst/ || Coq |  IPA French ['kɔkʲ] 读若拼音“goq” | | ❌ IPA English ['kʰɒk] || daemon |   /'diːmən/ |   /ˈdiːmən/ |  ❌ /dæmən/ || debt |   /det/ |   /det/ |  ❌ /de'bit/ || deny |   /dɪ'naɪ/ |   /dɪˈnaɪ/ |  ❌ /'dæni/ || deprecate |  /ˈdeprəkeɪt/ |  /ˈdeprəkeɪt/ |  || deque |   /'dek/ |   /dɛk/ |  ❌ /di'kju/ || digest |   n. /'dɑɪdʒɛst/ v. /dɑɪ'dʒɛst/ |   /daɪˈdʒest,dɪˈdʒest/ |  ❌ /'dɪgɛst/ || Dijkstra |   Dutch:/ˈdɛikstra/ English:/ˈdaɪkstrə/ |     |   || Django |   /ˈdʒæŋɡoʊ/ |   /ˈdʒæŋɡoʊ/ |  ❌ /diˈdʒæŋɡoʊ/ || doc |   /dɒk/ |   /dɒk/ |  ❌ /daʊk/ || dotnet |   /dɒtnet/ |   /dɑːtnet/ |  ❌ /daʊtnet/ || edition |   /ɪˈdɪʃ(ə)n/ |   /ɪˈdɪʃn/ |  ❌ /eˈdɪʃn/ || ephemeral |   /ɪˈfemərəl/ |   /ɪˈfemərəl/ |  ❌ /efe'merəl/ || epoch  |   /ˈiːpɒk/ |   /ˈepək/ |  ❌ /'ɛpətʃ/ || execute |  /ˈeksɪkjuːt/ |  /ˈeksɪkjuːt/ |  || executor |  /ɪɡˈzekjətə(r)/ |  /ɪɡˈzekjətər/ |  || event |   /ɪ'vent/ |   /ɪˈvent/ |  ❌ /'ɪvənt/ || exit |  /ˈeksɪt/ |  /ˈeksɪt; ˈeɡzɪt/ | ❌ /ig'zit/ || facade |   /fə'sɑːd/ |   /fəˈsɑːd/ |  ❌ /'feikeid/ || fedora |   /fɪ'dɔːrə/ |   /fɪˈdɔːrə/ |  ❌ /'fedərə/ || format |   /'fɔːmæt/ |   /ˈfɔːrmæt/ |  ❌ /fɔ'mæt/ || gauge |  /ɡeɪdʒ/ |  /ɡeɪdʒ/ |  ❌ /ɡɑudʒ/ || Git |   /ɡɪt/ |   /ɡɪt/ |  ❌ /dʒɪt/ || GNU |   /gnu:/ |   /gnuː,gnjuː/ |  || Grafana |    /grəˈfɑːnˌɑː/ |    /grəˈfɑːnˌɑː/ |  || GraphQL |   /græf kju ɛl/ |   /græf kju ɛl/ |  ❌ /dʒɪgræf kju ɛl/ || GUI |   /ˈɡu:i/ |   /ˈɡu:i/ |  || Haskell |   /ˈhæskəl/ |   /ˈhæskəl/ |  ❌ /hæˈskəl/ || height |   /haɪt/ |   /haɪt/ |  ❌ /heɪt/ || hidden |   /'hɪdn/ |   /ˈhɪdn/ |  ❌ /'haɪdn/ || image |   /'ɪmɪdʒ/ |   /ˈɪmɪdʒ/ |  ❌ /ɪ'meɪdʒ/ || implement |   /'ɪmplɪm(ə)nt/ |   /ˈɪmplɪmənt/ /ˈɪmpləˌment/ |  ❌ /ɪm'plem(ə)nt/ || integer |   /'ɪntɪdʒə/ |   /ˈɪntɪdʒər/ |  ❌ /ˈɪntaɪgə/ || issue |   /'ɪʃuː/ |   /ˈɪʃuː/ |  ❌ /ˈaɪʃuː/ || Java |   /'dʒɑːvə/ |   /ˈdʒɑːvə/ |  || jpg|   /'dʒeɪpeɡ/ |   /'dʒeɪpeɡ/ |  ❌ /ˈdʒeɪˈpi:ˈdʒiː/ || key |   /kiː/ |   /kiː/ |  ❌ /kei/ || Kubernetes* |   /kubз'netɪs/ |   /kuːbə˞'netiz/ |   || lambda |   /ˈlæmdə/ |   /ˈlæmdə/ |  ❌ /ˈlɒŋmdɑ/ || Ldap |   /el'dæp/ |   /el'dæp/ |  ❌ /el'daːp/ || legacy |   /'leɡəsi/ |   /'leɡəsi/ |  ❌ /'li:gasi/ || linear |   /'lɪnɪə/ |   /ˈlɪniər/ |  ❌ /'laɪə/ || LINQ |  /lɪŋk/ |  /lɪŋk/ |  ❌ /lɪŋkju:/ || Linux |   /'lɪnəks/ |   /ˈlaɪnəks/ /ˈlɪnəks/ |  ❌ /ˈlɪnʌks/ /ˈlɪnjuːks/ || locale |   /ləʊ'kɑːl/ |   /loʊˈkæl/ |  ❌ /ˈloʊk(ə)l/ || Lucene |   /lu'siːn/ |   /lu'siːn/ |  ❌ /'lu:sən/ || macro |   /ˈmækrəʊ/ |   /ˈmækroʊ/ | ❌ /ˈmakroʊ/                || main |   /meɪn/ |   /meɪn/ |  ❌ /mɪn/ || margin |   /'mɑːdʒɪn/ |   /ˈmɑːrdʒɪn/ |  ❌ /'mʌgɪn/ || matrix |   /ˈmeɪtrɪks/ |   /ˈmeɪtrɪks/ |  ❌ /ˈmɑ:trɪks/ || maven |   /'meɪvn/ |   /ˈmeɪvn/ |  ❌ /'maːvn/ || max |  /mæks/ |  /mæks/ |  ❌ /mɑ:ks/ || Microsoft |   /'maikrəusɔft/ |   /ˈmaɪkrəsɔːft/ |  ❌ /'mikrəusɔft/ || migrate |  /maɪˈɡreɪt/ |  /ˈmaɪɡreɪt/ |  ❌ /ˈmɪɡreɪt/ || miscellaneous |  /ˌmɪsəˈleɪniəs/ |  /ˌmɪsəˈleɪniəs/ |  || module |   /'mɒdjuːl/ |   /ˈmɑːdʒuːl/ |  ❌ /'məʊdl/ || native |  /ˈneɪtɪv/ |  /ˈneɪtɪv/ | ❌ /ˈnætɪv/ || nginx |      Engine X |    Engine X  |  || null |   /nʌl/ |   /nʌl/ |  ❌ /naʊ/ || obsolete |  /ˈɒbsəliːt/ |  /ˌɑːbsəˈliːt/ |  || OS X |    OS ten |    OS ten |  ❌ /ɔs eks/ || phantom |   /'fæntəm/ |   /ˈfæntəm/ |  ❌ /'pæntəm/ || parameter |   /pə'ræmɪtə/ |   /pəˈræmɪtər/ |  ❌ /'pærəmɪtə/ || premise |  /ˈpremɪs/ |  /ˈpremɪs/ | ❌ /prɪ'mɪs/|| privilege |   /'prɪvəlɪdʒ/ |   /ˈprɪvəlɪdʒ/ |  ❌ /'prɪvɪlɪdʒ/ || probe |  /prəʊb/ |  /proʊb/ | ❌ /proʊbi/ || Prometheus |   /prə-ˈmē-thē-əs/ |   /pro'miθɪəs/ |   || putty |   /ˈpʌti/ |   /ˈpʌti/ |  ❌ /ˈpuːti/ || Qt |   /kjuːt/ |   /kjuːt/ |  || query |   /'kwɪəri/ |   /ˈkwɪri/ |  ❌ /'kwaɪri/ || Realm |   /relm/ |   /relm/ |  ❌ /riəlm/ || reconcile |  /ˈrekənsaɪl/ |  /ˈrekənsaɪl/ |  || Redux |   /ri'dʌks/ |   /ri'dʌks/ |  ❌ /'ridju:ks/ || resume |    /rɪ'zju:m/ |   /rɪˈzuːm/ |  ❌  /rɪ'sju:m/ || resolved |   /rɪ'zɒlvd/ |   /rɪˈzɑːlvd/ |  ❌ /rɪ'səʊvd/ || resort |   /rɪˈzɔ:t/ |   /rɪˈzɔːrt/ |  ❌ /rɪˈsɔ:t/ || retina |   /'retɪnə/ |   /ˈretɪnə/ |  ❌ /ri'tina/ || route |   /ruːt/ |   /ruːt,raʊt/ |  ❌ /rəʊt/ || San Jose |   /sænhəu'zei/ |   /sænhəu'zei/ |  ❌ /sæn'ju:s/ || safari |   /sə'fɑːrɪ/ |   /səˈfɑːri/ |  ❌ /sæfərɪ/ || scheme |   /skiːm/ |   /skiːm/ |  ❌ /s'kæmə/ || scala |   /ˈskɑːlɑ/ |   /ˈskɑːlɑ/ |  ❌ /ˈskæːlɑ/ || segue |   /'sɛɡwe/ |   /ˈseɡweɪ/ |  ❌ /se'dʒ/ || SQL | /ˈsiːkwəl/ /ˈesˈkjuːˈel/ | /ˈsiːkwəl/ /ˈesˈkjuːˈel/ | ❌ /sərk(ə)l/ || sudo | /'suːduː/ | /'suːduː/ |  || suite |   /swiːt/ |   /swiːt/ |  ❌ /sjuːt/ || telemetry |  /təˈlemətri/ |  /təˈlemətri/ | ❌ /ˈtelɪmətri/ || thymeleaf |   /ˈtaɪmˌlɪːf/ |   /ˈtaɪmˌlɪːf/ |  ❌ /θiːmɪlɪːf/ || tuple |  /tjʊpəl/ |  /tuːpəl/ |  || typical |   /'tɪpɪkl/ |   /ˈtɪpɪkl/ |  ❌ /'taɪpɪkəl/ || Ubuntu |   /ʊ'bʊntʊ/ |   /ʊ'bʊntʊ/ |  ❌ /juː'bʊntʊ/ || UEFI | U-E-F-I | U-E-F-I  | ❌ /jufi/ /ɔːfi/ || Vagrant |  /ˈveɪɡrənt/ |  /ˈveɪɡrənt/ | /ˈvagɹent/ || variable |   /'veəriəbl/ |   /ˈveriəbl,ˈværiəbl/ | ❌ /və'raiəbl/ || verbose |   /vɜːˈbəʊs/ |   /vɜːrˈboʊs/ |  ❌ /'vɜːrboʊs/ || vue |   /v'ju:/ |   /v'ju:/ |  ❌ /v'ju:i/ || width |   /wɪdθ/ |   /wɪdθ,wɪtθ/ |  ❌ /waɪdθ/ || YouTube |   /'juː'tjuːb/ |   /'juː'tjuːb/ |  ❌ /'juː'tʊbɪ/ || Vite |   /vit/ |   /vit/ |  ❌ /vaɪt/ |附注相关链接说明参考资料Star History"
https://github.com/hwalsuklee/tensorflow-generative-model-collections,Collection of generative models in Tensorflow,"tensorflow-generative-model-collectionsTensorflow implementation of various GANs and VAEs.Related RepositoriesPytorch versionPytorch version of this repository is availabel at https://github.com/znxlwm/pytorch-generative-model-collections""Are GANs Created Equal? A Large-Scale Study"" Paperhttps://github.com/google/compare_gan is the code that was used in .It provides IS/FID and rich experimental results for all gan-variants.  Generative Adversarial Networks (GANs)ListsName | Paper Link | Value Function:---: | :---: | :--- |GAN |  | LSGAN|  | WGAN|  | WGAN_GP|  | DRAGAN|  | CGAN|  | infoGAN|  | ACGAN|  | EBGAN|  | BEGAN|  |   Variants of GAN structureResults for mnistNetwork architecture of generator and discriminator is the exaclty sames as in .For fair comparison of core ideas in all gan variants, all implementations for network architecture are kept same except EBGAN and BEGAN. Small modification is made for EBGAN/BEGAN, since those adopt auto-encoder strucutre for discriminator. But I tried to keep the capacity of discirminator.The following results can be reproduced with command:  python main.py --dataset mnist --gan_type <TYPE> --epoch 25 --batch_size 64
Random generationAll results are randomly sampled.Name | Epoch 2 | Epoch 10 | Epoch 25:---: | :---: | :---: | :---: |GAN |  |  | LSGAN |  |  | WGAN |  |  | WGAN_GP |  |  | DRAGAN |  |  | EBGAN |  |  | BEGAN |  |  | Conditional generationEach row has the same noise vector and each column has the same label condition.Name | Epoch 1 | Epoch 10 | Epoch 25:---: | :---: | :---: | :---: |CGAN |  |  | ACGAN |  |  | infoGAN |  |  | InfoGAN : Manipulating two continous codesResults for fashion-mnistComments on network architecture in mnist are also applied to here. is a recently proposed dataset consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. (T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot)The following results can be reproduced with command:  python main.py --dataset fashion-mnist --gan_type <TYPE> --epoch 40 --batch_size 64
Random generationAll results are randomly sampled.Name | Epoch 1 | Epoch 20 | Epoch 40:---: | :---: | :---: | :---: |GAN |  |  | LSGAN |  |  | WGAN |  |  | WGAN_GP |  |  | DRAGAN |  |  | EBGAN |  |  | BEGAN |  |  | Conditional generationEach row has the same noise vector and each column has the same label condition.Name | Epoch 1 | Epoch 20 | Epoch 40:---: | :---: | :---: | :---: |CGAN |  |  | ACGAN |  |  | infoGAN |  |  | Without hyper-parameter tuning from mnist-version, ACGAN/infoGAN does not work well as compared with CGAN.ACGAN tends to fall into mode-collapse.infoGAN tends to ignore noise-vector. It results in that various style within the same class can not be represented.InfoGAN : Manipulating two continous codesSome results for celebA(to be added)Variational Auto-Encoders (VAEs)ListsName | Paper Link | Loss Function:---: | :---: | :---VAE|  | CVAE|  | DVAE|  | (to be added)AAE|  | (to be added) Variants of VAE structureResults for mnistNetwork architecture of decoder(generator) and encoder(discriminator) is the exaclty sames as in . The number of output nodes in encoder is different. (2x z_dim for VAE, 1 for GAN)The following results can be reproduced with command:  python main.py --dataset mnist --gan_type <TYPE> --epoch 25 --batch_size 64
Random generationAll results are randomly sampled.Name | Epoch 1 | Epoch 10 | Epoch 25:---: | :---: | :---: | :---: |VAE |  |  | GAN |  |  | Results of GAN is also given to compare images generated from VAE and GAN.The main difference (VAE generates smooth and blurry images, otherwise GAN generates sharp and artifact images) is cleary observed from the results.Conditional generationEach row has the same noise vector and each column has the same label condition.Name | Epoch 1 | Epoch 10 | Epoch 25:---: | :---: | :---: | :---: |CVAE |  |  | CGAN |  |  | Results of CGAN is also given to compare images generated from CVAE and CGAN.Learned manifoldThe following results can be reproduced with command:  python main.py --dataset mnist --gan_type VAE --epoch 25 --batch_size 64 --dim_z 2
Please notice that dimension of noise-vector z is 2.Name | Epoch 1 | Epoch 10 | Epoch 25:---: | :---: | :---: | :---: |VAE |  |  | Results for fashion-mnistComments on network architecture in mnist are also applied to here. The following results can be reproduced with command:  python main.py --dataset fashion-mnist --gan_type <TYPE> --epoch 40 --batch_size 64
Random generationAll results are randomly sampled.Name | Epoch 1 | Epoch 20 | Epoch 40:---: | :---: | :---: | :---: |VAE |  |  | GAN |  |  | Results of GAN is also given to compare images generated from VAE and GAN.Conditional generationEach row has the same noise vector and each column has the same label condition.Name | Epoch 1 | Epoch 20 | Epoch 40:---: | :---: | :---: | :---: |CVAE |  |  | CGAN |  |  | Results of CGAN is also given to compare images generated from CVAE and CGAN.Learned manifoldThe following results can be reproduced with command:  python main.py --dataset fashion-mnist --gan_type VAE --epoch 25 --batch_size 64 --dim_z 2
Please notice that dimension of noise-vector z is 2.Name | Epoch 1 | Epoch 10 | Epoch 25:---: | :---: | :---: | :---: |VAE |  |  | Results for celebA(to be added)Folder structureThe following shows basic folder structure.├── main.py # gateway
├── data
│   ├── mnist # mnist data (not included in this repo)
│   |   ├── t10k-images-idx3-ubyte.gz
│   |   ├── t10k-labels-idx1-ubyte.gz
│   |   ├── train-images-idx3-ubyte.gz
│   |   └── train-labels-idx1-ubyte.gz
│   └── fashion-mnist # fashion-mnist data (not included in this repo)
│       ├── t10k-images-idx3-ubyte.gz
│       ├── t10k-labels-idx1-ubyte.gz
│       ├── train-images-idx3-ubyte.gz
│       └── train-labels-idx1-ubyte.gz
├── GAN.py # vanilla GAN
├── ops.py # some operations on layer
├── utils.py # utils
├── logs # log files for tensorboard to be saved here
└── checkpoint # model files to be saved here
AcknowledgementsThis implementation has been based on  and tested with Tensorflow over ver1.0 on Windows 10 and Ubuntu 14.04."
https://github.com/openai/gym,A toolkit for developing and comparing reinforcement learning algorithms.," Important NoticeThe team that has been maintaining Gym since 2021 has moved all future development to , a drop in replacement for Gym (import gymnasium as gym), and Gym will not be receiving any future updates. Please switch over to Gymnasium as soon as you're able to do so. If you'd like to read more about the story behind this switch, please check out .GymGym is an open source Python library for developing and comparing reinforcement learning algorithms by providing a standard API to communicate between learning algorithms and environments, as well as a standard set of environments compliant with that API. Since its release, Gym's API has become the field standard for doing this.Gym documentation website is at , and you can propose fixes and changes to it .Gym also has a discord server for development purposes that you can join here: https://discord.gg/nHg2JRN489InstallationTo install the base Gym library, use .This does not include dependencies for all families of environments (there's a massive number, and some can be problematic to install on certain systems). You can install these dependencies for one family like  or use  to install all dependencies.We support Python 3.7, 3.8, 3.9 and 3.10 on Linux and macOS. We will accept PRs related to Windows, but do not officially support it.APIThe Gym API's API models environments as simple Python  classes. Creating environment instances and interacting with them is very simple- here's an example using the ""CartPole-v1"" environment:import gym
env = gym.make(""CartPole-v1"")
observation, info = env.reset(seed=42)

for _ in range(1000):
    action = env.action_space.sample()
    observation, reward, terminated, truncated, info = env.step(action)

    if terminated or truncated:
        observation, info = env.reset()
env.close()
Notable Related LibrariesPlease note that this is an incomplete list, and just includes libraries that the maintainers most commonly point newcommers to when asked for recommendations.Environment VersioningGym keeps strict versioning for reproducibility reasons. All environments end in a suffix like ""v0"".  When changes are made to environments that might impact learning results, the number is increased by one to prevent potential confusion.MuJoCo EnvironmentsThe latest ""v4"" and future versions of the MuJoCo environments will no longer depend on . Instead  will be the required dependency for future gym MuJoCo environment versions. Old gym MuJoCo environment versions that depend on  will still be kept but unmaintained.To install the dependencies for the latest gym MuJoCo environments use . Dependencies for old MuJoCo environments can still be installed by . CitationA whitepaper from when Gym just came out is available https://arxiv.org/pdf/1606.01540, and can be cited with the following bibtex entry:@misc{1606.01540,
  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  Title = {OpenAI Gym},
  Year = {2016},
  Eprint = {arXiv:1606.01540},
}
Release NotesThere used to be release notes for all the new Gym versions here. New release notes are being moved to  on GitHub, like most other libraries do. Old notes can be viewed ."
https://github.com/allanzelener/YAD2K,YAD2K: Yet Another Darknet 2 Keras,"YAD2K: Yet Another Darknet 2 KerasWelcome to YAD2KYou only look once, but you reimplement neural nets over and over again.YAD2K is a 90% Keras/10% Tensorflow implementation of YOLO_v2.Original paper:  by Joseph Redmond and Ali Farhadi.RequirementsInstallationgit clone https://github.com/allanzelener/yad2k.git
cd yad2k

# [Option 1] To replicate the conda environment:
conda env create -f environment.yml
source activate yad2k
# [Option 2] Install everything globaly.
pip install numpy h5py pillow
pip install tensorflow-gpu  # CPU-only: conda install -c conda-forge tensorflow
pip install keras # Possibly older release: conda install keras
Quick Startwget http://pjreddie.com/media/files/yolo.weights
wget https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolo.cfg
./yad2k.py yolo.cfg yolo.weights model_data/yolo.h5
./test_yolo.py model_data/yolo.h5  # output in images/out/
See  and  for more options.More DetailsThe YAD2K converter currently only supports YOLO_v2 style models, this include the following configurations: , , , and . will produce a plot of the generated Keras model. For example see .YAD2K assumes the Keras backend is Tensorflow. In particular for YOLO_v2 models with a passthrough layer, YAD2K uses  to implement the passthrough layer. The evaluation script also directly uses Tensorflow tensors and uses  for the final output. contains two scripts for converting the Pascal VOC image dataset with XML annotations to either HDF5 or TFRecords format for easier training with Keras or Tensorflow. contains reference implementations of Darknet-19 and YOLO_v2. is a sample training script that overfits a YOLO_v2 model to a single image from the Pascal VOC dataset.Known Issues and TODOsDarknets of YoreYAD2K stands on the shoulders of giants."
https://github.com/ctf-wiki/ctf-wiki,"Come and join us, we need you!","CTF Wiki  Welcome to CTF Wiki！CTF (Capture The Flag) started from DEFCON CTF, a competitive game among computer security enthusiasts, originally hosted in 1996.CTF covers a wide range of fields. Along with the evolving security technology, the difficulty of CTF challenges is getting harder and harder. As a result, the learning curve for beginners is getting steeper. Most online information is scattered and trivial. Beginners often don't know how to systematically learn CTF, which requires a lot of work and effort.In order to let those people who are interested in CTFs start easily, in October 2016, CTF Wiki was established on Github. Along with gradually improved content over time, CTF Wiki has received lots of appreciation from security enthusiasts, many of those are guys that we think we would never meet.As a freedom site, primarily focusing on recent CTFs, CTF Wiki introduces the knowledge and techniques in all aspects of CTF to make it easier for beginners to learn CTF.Now, CTF Wiki mainly contains the basic skills for CTF, but we are working hard to improve the following contents.For the above-mentioned parts to be improved, please refer to  which details what are planned.Although now CTF Wiki mainly focus CTF, it is not strictly limited to CTF topics. In the future, CTF Wiki will includeIn addition, given the following two pointsTherefore, CTF Wiki will never publish books.Finally, originating from the community, as an independent organization, CTF Wiki advocates freedom of knowledge, will never be commercialized, and will always maintain the character of independence and freedom.How to build？CTF Wiki uses  to show its contents. And it is deployed at .It can also be deployed locally, with the following steps:# 1. clone
git clone https://github.com/ctf-wiki/ctf-wiki.git
# 2. requirements
pip install -r requirements.txt
# generate static file in site/
python3 scripts/docs.py build-all
# deploy at http://127.0.0.1:8008
python3 scripts/docs.py serve
A local instance of mkdocs is dynamically updated, for instance when a markdown file is modified, the corresponding page will be modified too.If you just want to view it statically, try Docker!docker run -d --name=ctf-wiki -p 4100:80 ctfwiki/ctf-wiki
And then access  .How to practice？Firstly, learn some basic security knowledge through online reading.Secondly, CTF Wiki has two sister projects.How to make CTF Wiki Better？We welcome to write content for the wiki and share what you have learned. Before you decide to contribute content, please read .Thank you to all the people who have already contributed to CTF Wiki.What can you get?Before reading the Wiki, we hope to give you some advice:The security circle is small and the areas of exploration is vast. Let's get started with CTF Wiki!"
https://github.com/jupyterhub/jupyterhub,Multi-user server for Jupyter notebooks,"[<marko.inline.RawText object at 0x000001592FE0AD88>] |[<marko.inline.RawText object at 0x000001592FF31588>] |[<marko.inline.RawText object at 0x000001592FF31F08>] |[<marko.inline.RawText object at 0x000001592FF31D88>] |[<marko.inline.RawText object at 0x000001592FF31D08>] |[<marko.inline.RawText object at 0x000001592FDC8108>] |[<marko.inline.RawText object at 0x000001592FDC8E88>]With  you can create amulti-user Hub that spawns, manages, and proxies multiple instances of thesingle-user server. created JupyterHub to support manyusers. The Hub can offer notebook servers to a class of students, a corporatedata science workgroup, a scientific research project, or a high-performancecomputing group.Technical overviewThree main actors make up JupyterHub:Basic principles for operation are:JupyterHub also provides afor administration of the Hub and its users.InstallationCheck prerequisitesInstall packagesUsing To install JupyterHub along with its dependencies including nodejs/npm:conda install -c conda-forge jupyterhub
If you plan to run notebook servers locally, install JupyterLab or Jupyter notebook:conda install jupyterlab
conda install notebook
Using JupyterHub can be installed with , and the proxy with :npm install -g configurable-http-proxy
python3 -m pip install jupyterhub
If you plan to run notebook servers locally, you will need to install:python3 -m pip install --upgrade jupyterlab
python3 -m pip install --upgrade notebook
Run the Hub serverTo start the Hub server, run the command:jupyterhub
Visit  in your browser, and sign in with your system username and password.Note: To allow multiple users to sign in to the server, you will need torun the  command as a privileged user, such as root.The describes how to run the server as a less privileged user, which requiresmore configuration of the system.ConfigurationThe  section of thedocumentation explains the common steps in setting up JupyterHub.The provides an in-depth video and sample configurations of JupyterHub.Create a configuration fileTo generate a default config file with settings and descriptions:jupyterhub --generate-config
Start the HubTo start the Hub on a specific url and port  with https:jupyterhub --ip 10.0.1.2 --port 443 --ssl-key my_ssl.key --ssl-cert my_ssl.cert
Authenticators| Authenticator                                                                | Description                                       || ---------------------------------------------------------------------------- | ------------------------------------------------- || PAMAuthenticator                                                             | Default, built-in authenticator                   ||                | OAuth + JupyterHub Authenticator = OAuthenticator ||          | Simple LDAP Authenticator Plugin for JupyterHub   ||  | Kerberos Authenticator Plugin for JupyterHub      |Spawners| Spawner                                                        | Description                                                                || -------------------------------------------------------------- | -------------------------------------------------------------------------- || LocalProcessSpawner                                            | Default, built-in spawner starts single-user servers as local processes    ||    | Spawn single-user servers in Docker containers                             ||        | Kubernetes spawner for JupyterHub                                          ||        | Spawn single-user servers without being root                               ||  | Spawn single-user notebook servers using systemd                           ||      | Designed for clusters using batch scheduling software                      ||        | Spawn single-user notebook servers distributed on a Hadoop cluster         ||        | WrapSpawner and ProfilesSpawner enabling runtime configuration of spawners |DockerA starter gives a baseline deployment of JupyterHub using Docker.Important: This  image contains only the Hub itself,with no configuration. In general, one needs to make a derivative image, withat least a  setting up an Authenticator and/or a Spawner.To run the single-user servers, which may be on the same system as the Hub ornot, Jupyter Notebook version 4 or greater must be installed.The JupyterHub docker image can be started with the following command:docker run -p 8000:8000 -d --name jupyterhub jupyterhub/jupyterhub jupyterhub
This command will create a container named  that you canstop and resume with .The Hub service will be listening on all interfaces at port 8000, which makesthis a good choice for testing JupyterHub on your desktop or laptop.If you want to run docker on a computer that has a public IP then you should(as in MUST) secure it with ssl by adding ssl options to your dockerconfiguration or by using an ssl enabled proxy. willallow you to store data outside the docker image (host system) so it will be persistent, even when you starta new image.The command  will spawn a root shell in your dockercontainer. You can use the root shell to create system users in the container.These accounts will be used for authentication in JupyterHub's default configuration.ContributingIf you would like to contribute to the project, please read ourand the . The  fileexplains how to set up a development installation, how to run the test suite,and how to contribute to documentation.For a high-level view of the vision and next directions of the project, see the.A note about platform supportJupyterHub is supported on Linux/Unix based systems.JupyterHub officially does not support Windows. You may be able to useJupyterHub on Windows if you use a Spawner and Authenticator that work onWindows, but the JupyterHub defaults will not. Bugs reported on Windows will notbe accepted, and the test suite will not run on Windows. Small patches that fixminor Windows compatibility issues (such as basic installation) may be accepted,however. For Windows-based systems, we would recommend running JupyterHub in adocker container or Linux VM. Tornado's documentation on Windows platform supportLicenseWe use a shared copyright model that enables all contributors to maintain thecopyright on their contributions.All code is licensed under the terms of the .Help and resourcesWe encourage you to ask questions and share ideas on the .You can also talk with us on our JupyterHub  channel.JupyterHub follows the Jupyter .[<marko.inline.RawText object at 0x000001592FD37088>] |[<marko.inline.RawText object at 0x000001592FD44688>] |[<marko.inline.RawText object at 0x000001592FDAC588>] |[<marko.inline.RawText object at 0x000001592FD3C408>] |[<marko.inline.RawText object at 0x000001592FDA8388>] |[<marko.inline.RawText object at 0x000001592FD56348>] |[<marko.inline.RawText object at 0x000001592FD561C8>]"
https://github.com/pytoolz/toolz,A functional standard library for Python.,"Toolz|Build Status| |Coverage Status| |Version Status|A set of utility functions for iterators, functions, and dictionaries.See the PyToolz documentation at https://toolz.readthedocs.ioLICENSENew BSD. See __.Install is on the Python Package Index (PyPI):::pip install toolz
Structure and Heritage is implemented in three parts:|literal itertoolz|_, for operations on iterables. Examples: ,, ,|literal functoolz|_, for higher-order functions. Examples: ,, ,|literal dicttoolz|_, for operations on dictionaries. Examples: ,, ... |literal itertoolz| replace:: .. _literal itertoolz: https://github.com/pytoolz/toolz/blob/master/toolz/itertoolz.py.. |literal functoolz| replace:: .. _literal functoolz: https://github.com/pytoolz/toolz/blob/master/toolz/functoolz.py.. |literal dicttoolz| replace:: .. _literal dicttoolz: https://github.com/pytoolz/toolz/blob/master/toolz/dicttoolz.pyThese functions come from the legacy of functional languages for listprocessing. They interoperate well to accomplish common complex tasks.Read our __ formore details.ExampleThis builds a standard wordcount function from pieces within :.. code:: python>>> def stem(word):
...     """""" Stem word to primitive form """"""
...     return word.lower().rstrip("",.!:;'-\"""").lstrip(""'\"""")

>>> from toolz import compose, frequencies
>>> from toolz.curried import map
>>> wordcount = compose(frequencies, map(stem), str.split)

>>> sentence = ""This cat jumped over this other cat!""
>>> wordcount(sentence)
{'this': 2, 'cat': 2, 'jumped': 1, 'over': 1, 'other': 1}
Dependencies supports Python 3.5+ with a common codebase.It is pure Python and requires no dependencies beyond the standardlibrary.It is, in short, a lightweight dependency.CyToolzThe  project has been reimplemented in . for moredetails.See AlsoContributions Welcome aims to be a repository for utility functions, particularlythose that come from the functional programming and list processingtraditions. We welcome contributions that fall within this scope.We also try to keep the API small to keep  manageable.  The idealcontribution is significantly different from existing functions and hasprecedent in a few other functional systems.Please take a look at our__for contribution ideas.CommunitySee our __.We're friendly... |Build Status| image:: https://github.com/pytoolz/toolz/workflows/Test/badge.svg:target: https://github.com/pytoolz/toolz/actions.. |Coverage Status| image:: https://coveralls.io/repos/pytoolz/toolz/badge.svg?branch=master:target: https://coveralls.io/r/pytoolz/toolz.. |Version Status| image:: https://badge.fury.io/py/toolz.svg:target: https://badge.fury.io/py/toolz"
https://github.com/chubin/cheat.sh,the only cheat sheet you need,"Unified access to the best community driven cheat sheets repositories of the world.Let's imagine for a moment that there is such a thing as an ideal cheat sheet.What should it look like?What features should it have?Such a thing exists! It's easy to  and there's even .Featurescheat.shContentsUsageTo get a cheat sheet for a UNIX/Linux command from a command line, query the service using  or any other HTTP/HTTPS clientspecifying the name of the command in the query:    curl cheat.sh/tar
    curl cht.sh/curl
    curl https://cheat.sh/rsync
    curl https://cht.sh/tr
As you can see, you can use both HTTPS and HTTP to access the service, and both the long (cheat.sh) and the short (cht.sh) service names.Here , , , and  are names of the UNIX/Linux commands you want to get cheat sheets for.If you don't know the name of the command you need, you can search for it using the  notation.For example, to see how you can make  of a filesystem/volume/something else:    curl cht.sh/~snapshot
The programming language cheat sheets are located in special namespaces dedicated to them.    curl cht.sh/go/Pointers
    curl cht.sh/scala/Functions
    curl cht.sh/python/lambda
To get the list of available programming language cheat sheets, use the special query :    curl cht.sh/go/:list
Almost each programming language has a special page named that describes the language basics (that's a direct mapping from the ""Learn X in Y"" project).It could be a good starting point if you've just started learning a language.If there is no cheat sheet for a programming language query (and it is almost always the case),it is generated on the fly, based on available cheat sheets and answers on StackOverflow.Of course, there is no guarantee that the returned cheat sheet will be a 100% hit, but it is almost always exactly what you are looking for.Try these (and your own) queries to get the impression of that, what the answers look like:    curl cht.sh/go/reverse+a+list
    curl cht.sh/python/random+list+elements
    curl cht.sh/js/parse+json
    curl cht.sh/lua/merge+tables
    curl cht.sh/clojure/variadic+function
If you don't like an answer for your queries, you can pick another one. For that, repeat the query with an additional parameter ,  etc. appended:    curl cht.sh/python/random+string
    curl cht.sh/python/random+string/1
    curl cht.sh/python/random+string/2
Cheat sheets are formatted as code of the queried programming language (at least we are trying our best to do so)so they can be pasted into a program in this language directly. Text comments, if there are any, are formatted according to the language syntax.    $ curl cht.sh/lua/table+keys
    -- lua: retrieve list of keys in a table

    local keyset={}
    local n=0

    for k,v in pairs(tab) do
      n=n+1
      keyset[n]=k
    end

    --[[
       [ Note that you cannot guarantee any order in keyset. If you want the
       [ keys in sorted order, then sort keyset with table.sort(keyset).
       [ 
       [ [lhf] [so/q/12674345] [cc by-sa 3.0]
       ]]

If you don't need text comments in the answer, you can eliminate themusing a special option :    $ curl cht.sh/lua/table+keys\?Q
    local keyset={}
    local n=0

    for k,v in pairs(tab) do
      n=n+1
      keyset[n]=k
    end
And if you don't need syntax highlighting, switch it off using .You can combine the options together:    curl cht.sh/go/reverse+a+list\?Q
    curl cht.sh/python/random+list+elements\?Q
    curl cht.sh/js/parse+json\?Q
    curl cht.sh/lua/merge+tables\?QT
    curl cht.sh/clojure/variadic+function\?QT
Full list of all options described below and in .Try your own queries. Follow these rules:Read more about the programming languages queries below.Command line client, cht.shThe cheat.sh service has its own command line client () thathas several useful features compared to querying the service directly with :InstallationTo install the client:PATH_DIR=""$HOME/bin""  # or another directory on your $PATH
mkdir -p ""$PATH_DIR""
curl https://cht.sh/:cht.sh > ""$PATH_DIR/cht.sh""
chmod +x ""$PATH_DIR/cht.sh""
or to install it globally (for all users):curl -s https://cht.sh/:cht.sh | sudo tee /usr/local/bin/cht.sh && sudo chmod +x /usr/local/bin/cht.sh
Note: The package ""rlwrap"" is a required dependency to run in shell mode. Install this using Client usageNow, you can use  instead of , and write your queries in more natural way,with spaces instead of :    $ cht.sh go reverse a list
    $ cht.sh python random list elements
    $ cht.sh js parse json
It is even more convenient to start the client in a special shell mode:    $ cht.sh --shell
    cht.sh> go reverse a list
If all your queries are about the same language, you can change the contextand spare repeating the programming language name:    $ cht.sh --shell
    cht.sh> cd go
    cht.sh/go> reverse a list
or even start the client in this context:    $ cht.sh --shell go
    cht.sh/go> reverse a list
    ...
    cht.sh/go> join a list
    ...
If you want to change the context, you can do it with the  command,or if you want do a single query for some other language, just prepend it with :    $ cht.sh --shell go
    ...
    cht.sh/go> /python dictionary comprehension
    ...
If you want to copy the last answer into the clipboard, you canuse the  () command, or  (, without comments).    cht.sh/python> append file
    #  python - How do you append to a file?

    with open(""test.txt"", ""a"") as myfile:
        myfile.write(""appended text"")
    cht.sh/python> C
    copy: 2 lines copied to the selection
Type  for other internal  commands.	cht.sh> help
	help    - show this help
	hush    - do not show the 'help' string at start anymore
	cd LANG - change the language context
	copy    - copy the last answer in the clipboard (aliases: yank, y, c)
	ccopy   - copy the last answer w/o comments (cut comments; aliases: cc, Y, C)
	exit    - exit the cheat shell (aliases: quit, ^D)
	id [ID] - set/show an unique session id (""reset"" to reset, ""remove"" to remove)
	stealth - stealth mode (automatic queries for selected text)
	update  - self update (only if the scriptfile is writeable)
	version - show current cht.sh version
	/:help  - service help
	QUERY   - space separated query staring (examples are below)
				  cht.sh> python zip list
				  cht.sh/python> zip list
				  cht.sh/go> /python zip list
The  client has its configuration file which is located at (location of the file can be overridden by the environment variable ).Use it to specify query options that you would use with each query.For example, to switch syntax highlighting off create the file with the followingcontent:CHTSH_QUERY_OPTIONS=""T""
Or if you want to use a special syntax highlighting theme:CHTSH_QUERY_OPTIONS=""style=native""
( to see all supported styles).Other cht.sh configuration parameters:CHTSH_CURL_OPTIONS=""-A curl""        # curl options used for cht.sh queries
CHTSH_URL=https://cht.sh            # URL of the cheat.sh server
Tab completionBash Tab completionTo activate tab completion support for , add the  script to your :    curl https://cheat.sh/:bash_completion > ~/.bash.d/cht.sh
    . ~/.bash.d/cht.sh
    # and add . ~/.bash.d/cht.sh to ~/.bashrc
ZSH Tab completionTo activate tab completion support for , add the  script to the fpath in your :    curl https://cheat.sh/:zsh > ~/.zsh.d/_cht
    echo 'fpath=(~/.zsh.d/ $fpath)' >> ~/.zshrc
    # Open a new shell to load the plugin
Stealth modeBeing used fully unnoticed is one of the most important property of any cheat sheet.cheat.sh can be used completely unnoticed too. The cheat.sh client, , hasa special mode, called stealth mode. Using that, you don't even need to touch yourkeyboard to open a cheat sheet.In this mode, as soon as you select some text with the mouse (and thus adding itinto the selection buffer of X Window System or into the clipboard) it's usedas a query string for cheat.sh, and the correspondent cheat sheet is automatically shown.Let's imagine, that you are having an online interview, where your interviewer asks yousome questions using a shared document (say Google Docs) and you are supposedto write your coding answers there (it's possible too that you'll type in the questionson your own, just to show to the interviewer that you've heard it right).When using the stealth mode of , the only thing you need to do in order to seea cheat sheet for some question, is to select the question using the mouse.If you don't want any text in the answers and the only thing you need is code,use the  option when starting the stealth mode.You: Hi!                                            | $ cht.sh --shell python
She: Hi!                                            | cht.sh/python> stealth Q
She: Are you ready for a small interview?           | stealth: you are in the stealth mode; select any text
She: Just a couple of questions                     | stealth: selections longer than 5 words are ignored
She: We will talk about python                      | stealth: query arguments: ?Q
She: Let's start from something simple.             | stealth: use ^C to leave this mode
She: Do you know how to reverse a list in python?   |
You: Sure                                           |
You: (selecting ""reverse a list"")                   | stealth: reverse a list
                                                    | reverse_lst = lst[::-1]
You: lst[::-1]?                                     |
She: Good.                                          |
She: Do you know how to chain a list of lists?      |
You: (selecting ""chain a list of lists"")            | stealth: chain a list of lists
                                                    | import itertools
                                                    | a = [[""a"",""b""], [""c""]]
                                                    | print list(itertools.chain.from_iterable(a))
You: May I use external modules?                    |
She: What module do you want to use?                |
You: itertools                                      |
She: Yes, you may use it                            |
You: Ok, then:                                      |
You: itertools.chain.from_iterable(a)               |
She: Good. Let's try something harder.              |
She: What about quicksort implementation?           |
You: (selecting ""quicksort implementation"")         | stealth: quicksort implementation
You: Let me think about it.                         | (some big and clumsy lowlevel implementation shown)
You: Well...(starting typing it in)                 | def sort(array=[12,4,5,6,7,3,1,15]):
                                                    |     less = []
She: (seeing your ugly pascal style)                |     equal = []
She: Could you write it more concise?               |     greater = []
                                                    |     if len(array) > 1:
You: What do you mean?                              |         pivot = array[0]
                                                    |         for x in array:
She: I mean,                                        |             if x < pivot: less.append(x)
She: do you really need all these ifs and fors?     |             if x == pivot: equal.append(x)
She: Could you maybe just use filter instead?       |             if x > pivot: greater.append(x)
                                                    |         return sort(less)+equal+sort(greater)
You: quicksort with filter?                         |     else:
                                                    |         return array
She: Yes                                            |
You: (selecting ""quicksort with filter"")            | stealth: quicksort with filter
You: Ok, I will try.                                | return qsort(filter(lt, L[1:]))+[pivot] \
You: Something like this?                           |     +qsort(filter(ge, L[1:]))
You: qsort(filter(lt, L[1:]))+[pivot] \             |
       + qsort(filter(ge, L[1:]))                   |
                                                    |
She: Yes! Perfect! Exactly what I wanted to see!    |
                                                    |

Of course, this is just for fun, and you should never cheat in your coding interviews,because you know what happens when you do.Windows command line clientYou can access cheat.sh from Windows command line too.Use cheat.sh command line client for that: .It supports:You can also use  command-line installer for Windows to get it:scoop install cht
Self-HostingDockerCurrently, the easiest way to get a self-hosted instance running is by usingthe  file.docker-compose up
This builds and runs the image with baked in cheatsheets and starts the appand a Redis instance to back it, making the service available athttp://localhost:8002 This is currently an early implementation and shouldprobably not be used for anything outside of internal/dev/personal use rightnow.Editors integrationYou can use cheat.sh directly from the editor(Emacs, Sublime, Vim, and Visual Studio Code are currently supported;not all features are supported by all plugins though; see below).Instead of opening your browser, googling, browsing Stack Overflowand eventually copying the code snippets you need into the clipboardand later pasting them into the editor,you can achieve the same instantly and without leaving the editor at all!Here is what it looks like in Vim:If you use some static analysis plugin such as syntastic (for Vim), you can useits warning and error messages as cheat.sh queries: place the cursor on the problem lineand press : explanation for the warning will be opened in a new buffer.Features supported by cheat.sh plugins for different editors:|Feature            |Emacs|Sublime|Vim|VSCode|IDEA|QtCreator||-------------------|-----|-------|---|------|----|---------||Command queries    |✓    |✓      |✓  |✓     |✓   |✓        ||Queries from buffer|     |       |✓  |✓     |    |✓        ||Toggle comments    |     |       |✓  |✓     |✓   |✓        ||Prev/next answer   |     |       |✓  |✓     |✓   |✓        ||Multiple answers   |     |✓      |   |      |✓   |         ||Warnings as queries|     |       |✓  |      |    |         ||Queries history    |     |       |✓  |✓     |    |         ||Session id         |     |       |✓  |      |    |         ||Configurable server|✓    |       |✓  |✓     |    |✓        |VimHere is Vim configuration example:"" some configuration above ...

let mapleader="" ""

call vundle#begin()
Bundle 'gmarik/vundle'
Bundle 'scrooloose/syntastic'
Bundle 'dbeniamine/cheat.sh-vim'
call vundle#end()

let g:syntastic_javascript_checkers = [ 'jshint' ]
let g:syntastic_ocaml_checkers = ['merlin']
let g:syntastic_python_checkers = ['pylint']
let g:syntastic_shell_checkers = ['shellcheck']

"" some configuration below ...
In this example, several Vim plugins are used:Syntastic shows warnings and errors (found by code analysis tools: , , ,  etc.),and  shows you explanations for the errors and warningsand answers on programming languages queries written in the editor.Watch a demo, where the most important features of the cheat.sh Vim plugin are shown (5 Min):Or, if you want to scroll and/or pause, the same on YouTube:EmacsVisual Studio CodeUsage: (GIF courtesy: Matthias Endler, @mre)SublimeUsage:(GIF courtesy: Gaurav Kukreja, @gauravk-in)IntelliJ IDEAUsage: (GIF courtesy: Szymon Przebierowski, @szymonprz)QtCreatorCurrent features:(GIF courtesy: Pozemka, @pozemka)Special pagesThere are several special pages that are not cheat sheets.Their names start with colon and have special meaning.Getting started:    :help               description of all special pages and options
    :intro              cheat.sh introduction, covering the most important usage questions
    :list               list all cheat sheets (can be used in a subsection too: /go/:list)
Command line client  and shells support:    :cht.sh             code of the cht.sh client
    :bash_completion    bash function for tab completion
    :bash               bash function and tab completion setup
    :fish               fish function and tab completion setup
    :zsh                zsh function and tab completion setup
Editors support:    :vim                cheat.sh support for Vim
    :emacs              cheat.sh function for Emacs
    :emacs-ivy          cheat.sh function for Emacs (uses ivy)
Other pages:    :post               how to post new cheat sheet
    :styles             list of color styles
    :styles-demo        show color styles usage examples
    :random             fetches a random page (can be used in a subsection too: /go/:random)
SearchTo search for a keyword, use the query:    /~keyword
In this case search is not recursive — it is conducted only in a page of the specified level.For example:    /~snapshot          look for snapshot in the first level cheat sheets
    /scala/~currying     look for currying in scala cheat sheets
For a recursive search in all cheat sheets, use double slash:    /~snapshot/r         look for snapshot in all cheat sheets
You can use special search options after the closing slash:    /~shot/bi           case insensitive (i), word boundaries (b)
List of search options:    i   case insensitive search
    b   word boundaries
    r   recursive search
Programming languages cheat sheetsCheat sheets related to programming languagesare organized in namespaces (subdirectories), that are named accordingto the programming language.For each supported programming languagethere are several special cheat sheets: its own sheet, ,  and .Say for lua it will look like:    lua
    lua/hello
    lua/:list
    lua/:learn
Some languages has the one-liners-cheat sheet, :    perl/1line
At the moment, cheat.sh covers the 58 following programming languages (alphabetically sorted):|Prefix     |Language  |Basics|One-liners|Weirdness|StackOverflow||-----------|----------|------|----------|---------|-------------|| |Arduino   |      |          |         |✓            |||Assembly  |      |          |         |✓            ||     |AWK       |✓     |          |         |✓            ||    |Bash      |✓     |          |         |✓            ||   |BASIC     |      |          |         |✓            ||      |Brainfuck |✓     |          |         |✓            ||       |C         |✓     |          |         |✓            ||  |Chapel    |✓     |          |         |✓            ||   |Clean     |      |          |         |✓            || |Clojure   |✓     |          |         |✓            ||  |CoffeeScript|✓   |          |         |✓            ||     |C++       |✓     |          |         |✓            ||  |C#        |✓     |          |         |✓            ||       |D         |✓     |          |         |✓            ||    |Dart      |✓     |          |         |✓            ||  |Dephi     |      |          |         |✓            ||   |Dylan     |✓     |          |         |✓            ||  |Eiffel    |      |          |         |✓            ||  |Elixir    |✓     |          |         |✓            ||   |ELisp     |✓     |          |         |✓            ||     |Elm       |✓     |          |         |✓            ||  |Erlang    |✓     |          |         |✓            ||  |Factor    |✓     |          |         |✓            || |Fortran   |✓     |          |         |✓            ||   |Forth     |✓     |          |         |✓            ||  |F#        |✓     |          |         |✓            ||      |Go        |✓     |          |         |✓            ||  |Groovy    |✓     |          |         |✓            || |Haskell   |✓     |          |         |✓            ||    |Java      |✓     |          |         |✓            ||      |JavaScript|✓     |✓         |✓        |✓            ||   |Julia     |✓     |          |         |✓            ||  |Kotlin    |✓     |          |         |✓            ||   |LaTeX     |✓     |          |         |✓            ||    |Lisp      |✓     |          |         |✓            ||     |Lua       |✓     |          |         |✓            ||  |MATLAB    |✓     |          |         |✓            ||     |Nim       |✓     |          |         |✓            ||   |OCaml     |✓     |          |         |✓            ||  |Octave    |✓     |          |         |✓            ||    |Perl      |✓     |✓         |         |✓            ||   |Perl 6    |✓     |✓         |         |✓            ||     |PHP       |✓     |          |         |✓            ||    |Pike      |      |          |         |✓            ||  |Python    |✓     |✓         |         |✓            || |Python 3  |✓     |          |         |✓            ||       |R         |✓     |          |         |✓            ||  |Racket    |✓     |          |         |✓            ||    |Ruby      |✓     |          |         |✓            ||    |Rust      |✓     |          |         |✓            ||   |Scala     |✓     |          |         |✓            ||  |Scheme    |✓     |          |         |✓            |||Solidity  |✓     |          |         |✓            ||   |Swift     |✓     |          |         |✓            ||    |Tcsh      |✓     |          |         |✓            ||     |Tcl       |✓     |          |         |✓            |||Objective-C|✓ |          |         |✓            ||      |VisualBasic|✓    |          |         |✓            ||   |VB.Net    |✓     |          |         |✓            |And several other topics, that are though related to programming,are not programming languages:|Prefix     |Topic     |Basics|StackOverflow||-----------|----------|------|-------------||   |CMake     |✓     |✓            ||  |Django    |      |✓            ||   |Flask     |      |✓            ||     |Git       |✓     |✓            |Cheat sheets sourcesInstead of creating yet another mediocre cheat sheet repository,we are concentrating our efforts on creation of a unifiedmechanism to access selected existing well developed and good maintainedcheat sheet repositories covering topics of our interest:programming and operating systems usage.cheat.sh uses selected community driven cheat sheet repositoriesand information sources, maintained by thousands of users, developers and authorsall over the world(in the Users column number of contributors/number of stars is shown):|Cheat sheets           |Repository                                                                          |C/U*                                                                                                                  |Stars                                                                                                   |Creation Date||-----------------------|------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------|-------------||UNIX/Linux, programming|                              |           |           |May 1, 2017  ||UNIX/Linux commands    |                               |                           |               |Dec 8, 2013  ||UNIX/Linux commands    |                     |          |          |Jul 28, 2013 ||Programming languages  | |||Jun 23, 2013 ||Go                     |               |       |       |Feb 9, 2014  ||Perl                   |                 |        |        |Nov 4, 2011  ||Programming languages  |                                          |                                                       |N/A                                                                                                     |Sep 15, 2008 |(*) C/U — contributors for GitHub repositories, Users for StackoverflowPie diagram reflecting cheat sheets sources distribution (by number of cheat sheets on cheat.sh originating from a repository):How to contributeHow to edit a cheat sheetIf you want to edit a cheat.sh cheat sheet, you should edit it in the upstream repository.You will find the name of the source repository in a browser when you open a cheat sheet.There are two github buttons at the bottom of the page: the second one is the buttonof the repository, which belongs the current cheat sheet.You can edit the cheat sheet directly in your browser (you need a github account for it).There is an edit button in the top right corner. If you click on it, an editor will be open.There you will change the cheat sheet (under the hood: the upstream repository is forked, your changes arecommitted in the forked repository, a pull request to the upstream repository owner is sent).How to add a cheat sheetIf you want to add a cheat sheet, you have one of the followingways:If you want to change an existing cheat sheet,you have to find the original repository (when you open a cheat sheet in a browser,you see the repository's github button in the bottom of the cheat sheet),the cheat sheet is coming from, and change it there.After some time the changes will be synchronized on cheat.sh.How to add a cheat sheet repositoryIf you want to add a cheat sheet repository to cheat.sh, please open an issue:Please specify the name of the repository, and give its short description.Installation and standalone usageYou don't need to install anything, to start using cheat.sh.There are two cases, when you want to install cheat.sh locally:Installation process in described in details here: "
https://github.com/p-christ/Deep-Reinforcement-Learning-Algorithms-with-PyTorch,PyTorch implementations of deep reinforcement learning algorithms and environments,"Deep Reinforcement Learning Algorithms with PyTorch   This repository contains PyTorch implementations of deep reinforcement learning algorithms and environments. (To help you remember things you learn about machine learning in general write them in )Algorithms ImplementedAll implementations are able to quickly solve Cart Pole (discrete actions), Mountain Car Continuous (continuous actions),Bit Flipping (discrete actions with dynamic goals) or Fetch Reach (continuous actions with dynamic goals). I plan to add more hierarchical RL algorithms soon.Environments ImplementedResults1. Cart Pole and Mountain CarBelow shows various RL algorithms successfully learning discrete action game or continuous action game . The mean result from running the algorithmswith 3 random seeds is shown with the shaded area representing plus and minus 1 standard deviation. Hyperparametersused can be found in files  and .  2. Hindsight Experience Replay (HER) ExperiementsBelow shows the performance of DQN and DDPG with and without Hindsight Experience Replay (HER) in the Bit Flipping (14 bits)and Fetch Reach environments described in the papers and . The results replicate the results found inthe papers and show how adding HER can allow an agent to solve problems that it otherwise would not be able to solve at all. Note that the same hyperparameters were used within each pair of agents and so the only differencebetween them was whether hindsight was used or not. 3. Hierarchical Reinforcement Learning ExperimentsThe results on the left below show the performance of DQN and the algorithm hierarchical-DQN from on the Long Corridor environment also explained in . The environmentrequires the agent to go to the end of a corridor before coming back in order to receive a larger reward. This delayedgratification and the aliasing of states makes it a somewhat impossible game for DQN to learn but if we introduce ameta-controller (as in h-DQN) which directs a lower-level controller how to behave we are able to make more progress. Thisaligns with the results found in the paper. The results on the right show the performance of DDQN and algorithm Stochastic NNs for Hierarchical Reinforcement Learning(SNN-HRL) from . DDQN is used as the comparison becausethe implementation of SSN-HRL uses 2 DDQN algorithms within it. Note that the first 300 episodes of trainingfor SNN-HRL were used for pre-training which is why there is no reward for those episodes. UsageThe repository's high-level structure is:├── agents                    
    ├── actor_critic_agents   
    ├── DQN_agents         
    ├── policy_gradient_agents
    └── stochastic_policy_search_agents 
├── environments   
├── results             
    └── data_and_graphs        
├── tests
├── utilities             
    └── data structures            
i) To watch the agents learn the above gamesTo watch all the different agents learn Cart Pole follow these steps:git clone https://github.com/p-christ/Deep_RL_Implementations.git
cd Deep_RL_Implementations

conda create --name myenvname
y
conda activate myenvname

pip3 install -r requirements.txt

python results/Cart_Pole.py
For other games change the last line to one of the other files in the Results folder. ii) To train the agents on another gameMost Open AI gym environments should work. All you would need to do is change the config.environment field (look at   for an example of this). You can also play with your own custom game if you create a separate class that inherits from gym.Env. See for an example of a custom environment and then see the script  to see how to have agents play the environment."
https://github.com/trailofbits/manticore,Symbolic execution tool,":warning: Project is in Maintenance Mode :warning:This project is no longer internally developed and maintained. However, we are happy to review and accept small, well-written pull requests by the community. We will only consider bug fixes and minor enhancements.Any new or currently open issues and discussions shall be answered and supported by the community.ManticoreManticore is a symbolic execution tool for the analysis of smart contracts and binaries.FeaturesManticore can analyze the following types of programs:InstallationOption 1: Installing from PyPI:pip install manticore
Option 2: Installing from PyPI, with extra dependencies needed to execute native binaries:pip install ""manticore[native]""
Option 3: Installing a nightly development build:pip install --pre ""manticore[native]""
Option 4: Installing from the  branch:git clone https://github.com/trailofbits/manticore.git
cd manticore
pip install -e "".[native]""
Option 5: Install via Docker:docker pull trailofbits/manticore
Once installed, the  CLI tool and Python API will be available.For a development installation, see our .UsageCLIManticore has a command line interface which can perform a basic symbolic analysis of a binary or smart contract.Analysis results will be placed into a workspace directory beginning with . For information about the workspace, see the .EVMManticore CLI automatically detects you are trying to test a contract if (for ex.)the contract has a  or a  extension. See a .$ manticore examples/evm/umd_example.sol 
 [9921] m.main:INFO: Registered plugins: DetectUninitializedMemory, DetectReentrancySimple, DetectExternalCallAndLeak, ...
 [9921] m.e.manticore:INFO: Starting symbolic create contract
 [9921] m.e.manticore:INFO: Starting symbolic transaction: 0
 [9921] m.e.manticore:INFO: 4 alive states, 6 terminated states
 [9921] m.e.manticore:INFO: Starting symbolic transaction: 1
 [9921] m.e.manticore:INFO: 16 alive states, 22 terminated states
[13761] m.c.manticore:INFO: Generated testcase No. 0 - STOP(3 txs)
[13754] m.c.manticore:INFO: Generated testcase No. 1 - STOP(3 txs)
...
[13743] m.c.manticore:INFO: Generated testcase No. 36 - THROW(3 txs)
[13740] m.c.manticore:INFO: Generated testcase No. 37 - THROW(3 txs)
[9921] m.c.manticore:INFO: Results in ~/manticore/mcore_gsncmlgx
Manticore-verifierAn alternative CLI tool is provided that simplifies contract testing andallows writing properties methods in the same high-level language the contract uses.Checkout manticore-verifier .See a Native$ manticore examples/linux/basic
[9507] m.n.manticore:INFO: Loading program examples/linux/basic
[9507] m.c.manticore:INFO: Generated testcase No. 0 - Program finished with exit status: 0
[9507] m.c.manticore:INFO: Generated testcase No. 1 - Program finished with exit status: 0
[9507] m.c.manticore:INFO: Results in ~/manticore/mcore_7u7hgfay
[9507] m.n.manticore:INFO: Total time: 2.8029580116271973
APIManticore provides a Python programming interface which can be used to implement powerful custom analyses.EVMFor Ethereum smart contracts, the API can be used for detailed verification of arbitrary contract properties. Users can set the starting conditions,execute symbolic transactions, and then review discovered states to ensure invariants for a contract hold.from manticore.ethereum import ManticoreEVM
contract_src=""""""
contract Adder {
    function incremented(uint value) public returns (uint){
        if (value == 1)
            revert();
        return value + 1;
    }
}
""""""
m = ManticoreEVM()

user_account = m.create_account(balance=10000000)
contract_account = m.solidity_create_contract(contract_src,
                                              owner=user_account,
                                              balance=0)
value = m.make_symbolic_value()

contract_account.incremented(value)

for state in m.ready_states:
    print(""can value be 1? {}"".format(state.can_be_true(value == 1)))
    print(""can value be 200? {}"".format(state.can_be_true(value == 200)))
NativeIt is also possible to use the API to create custom analysis tools for Linux binaries. Tailoring the initial state helps avoid state explosionproblems that commonly occur when using the CLI. # example Manticore script
from manticore.native import Manticore

m = Manticore.linux('./example')

@m.hook(0x400ca0)
def hook(state):
  cpu = state.cpu
  print('eax', cpu.EAX)
  print(cpu.read_int(cpu.ESP))

  m.kill()  # tell Manticore to stop

m.run()
WASMManticore can also evaluate WebAssembly functions over symbolic inputs for property validation or general analysis. from manticore.wasm import ManticoreWASM

m = ManticoreWASM(""collatz.wasm"")

def arg_gen(state):
    # Generate a symbolic argument to pass to the collatz function.
    # Possible values: 4, 6, 8
    arg = state.new_symbolic_value(32, ""collatz_arg"")
    state.constrain(arg > 3)
    state.constrain(arg < 9)
    state.constrain(arg % 2 == 0)
    return [arg]


# Run the collatz function with the given argument generator.
m.collatz(arg_gen)

# Manually collect return values
# Prints 2, 3, 8
for idx, val_list in enumerate(m.collect_returns()):
    print(""State"", idx, ""::"", val_list[0])
RequirementsCompiling Smart ContractsUsing a different solver (Yices, Z3, CVC4)Manticore relies on an external solver supporting smtlib2. Currently Z3, Yices and CVC4 are supported and can be selected via command-line or configuration settings.If Yices is available, Manticore will use it by default. If not, it will fall back to Z3 or CVC4. If you want to manually choose which solver to use, you can do so like this:Installing CVC4For more details go to https://cvc4.github.io/. Otherwise, just get the binary and use it.    sudo wget -O /usr/bin/cvc4 https://github.com/CVC4/CVC4/releases/download/1.7/cvc4-1.7-x86_64-linux-opt
    sudo chmod +x /usr/bin/cvc4
Installing YicesYices is incredibly fast. More details here https://yices.csl.sri.com/    sudo add-apt-repository ppa:sri-csl/formal-methods
    sudo apt-get update
    sudo apt-get install yices2
Getting HelpFeel free to stop by our #manticore slack channel in  for help using or extending Manticore.Documentation is available in several places:If you'd like to file a bug report or feature request, please use our  page. For questions and clarifications, please visit the  page.LicenseManticore is licensed and distributed under the AGPLv3 license.  if you're looking for an exception to the terms.PublicationsIf you are using Manticore in academic work, consider applying to the .Demo Video from ASE 2019Tool Integrations"
https://github.com/openai/glow,"Code for reproducing results in ""Glow: Generative Flow with Invertible 1x1 Convolutions""","Status: Archive (code is provided as-is, no updates expected)GlowCode for reproducing results in To use pretrained CelebA-HQ model, make your own manipulation vectors and run our interactive demo, check  folder.RequirementsRunpip install -r requirements.txt
To setup (Open)MPI, check instructions on Horovod github .Download datasetsFor small scale experiments, use MNIST/CIFAR-10 (directly downloaded by  using keras)For larger scale experiments, the datasets used are in the Google Cloud locations . The dataset_names are below, we mention the exact preprocessing / downsampling method for a correct comparison of likelihood.Quantitative resultsQualitative resultsTo download and extract celeb for example, runwget https://openaipublic.azureedge.net/glow-demo/data/celeba-tfr.tar
tar -xvf celeb-tfr.tar
Change  in train.py file to point to the above folder (or use the  flag when you run train.py)For , since download can be quite big, you can instead follow the instructions in  to generate the tfr file directly from LSUN images.  will be the smallest category.Simple Train with 1 GPURun wtih small depth to testCUDA_VISIBLE_DEVICES=0 python train.py --depth 1
Train with multiple GPUs using MPI and HorovodRun default training script with 8 GPUs:mpiexec -n 8 python train.py
Ablation experimentsmpiexec -n 8 python train.py --problem cifar10 --image_size 32 --n_level 3 --depth 32 --flow_permutation [0/1/2] --flow_coupling [0/1] --seed [0/1/2] --learntop --lr 0.001
Pretrained models, logs and sampleswget https://openaipublic.azureedge.net/glow-demo/logs/abl-[reverse/shuffle/1x1]-[add/aff].tar
CIFAR-10 Quantitative resultmpiexec -n 8 python train.py --problem cifar10 --image_size 32 --n_level 3 --depth 32 --flow_permutation 2 --flow_coupling 1 --seed 0 --learntop --lr 0.001 --n_bits_x 8
ImageNet 32x32 Quantitative resultmpiexec -n 8 python train.py --problem imagenet-oord --image_size 32 --n_level 3 --depth 48 --flow_permutation 2 --flow_coupling 1 --seed 0 --learntop --lr 0.001 --n_bits_x 8
ImageNet 64x64 Quantitative resultmpiexec -n 8 python train.py --problem imagenet-oord --image_size 64 --n_level 4 --depth 48 --flow_permutation 2 --flow_coupling 1 --seed 0 --learntop --lr 0.001 --n_bits_x 8
LSUN 64x64 Quantitative resultmpiexec -n 8 python train.py --problem lsun_realnvp --category [bedroom/church_outdoor/tower] --image_size 64 --n_level 3 --depth 48 --flow_permutation 2 --flow_coupling 1 --seed 0 --learntop --lr 0.001 --n_bits_x 8
Pretrained models, logs and sampleswget https://openaipublic.azureedge.net/glow-demo/logs/lsun-rnvp-[bdr/crh/twr].tar
CelebA-HQ 256x256 Qualitative resultmpiexec -n 40 python train.py --problem celeba --image_size 256 --n_level 6 --depth 32 --flow_permutation 2 --flow_coupling 0 --seed 0 --learntop --lr 0.001 --n_bits_x 5
LSUN 96x96 and 128x128 Qualitative resultmpiexec -n 40 python train.py --problem lsun --category [bedroom/church_outdoor/tower] --image_size [96/128] --n_level 5 --depth 64 --flow_permutation 2 --flow_coupling 0 --seed 0 --learntop --lr 0.001 --n_bits_x 5
Logs and sampleswget https://openaipublic.azureedge.net/glow-demo/logs/lsun-bdr-[96/128].tar
Conditional CIFAR-10 Qualitative resultmpiexec -n 8 python train.py --problem cifar10 --image_size 32 --n_level 3 --depth 32 --flow_permutation 2 --flow_coupling 0 --seed 0 --learntop --lr 0.001 --n_bits_x 5 --ycond --weight_y=0.01
Conditional ImageNet 32x32 Qualitative resultmpiexec -n 8 python train.py --problem imagenet --image_size 32 --n_level 3 --depth 48 --flow_permutation 2 --flow_coupling 0 --seed 0 --learntop --lr 0.001 --n_bits_x 5 --ycond --weight_y=0.01
"
https://github.com/sherlock-project/sherlock,🔎 Hunt down social media accounts by username across social networks,"Installation# clone the repo
$ git clone https://github.com/sherlock-project/sherlock.git

# change the working directory to sherlock
$ cd sherlock

# install the requirements
$ python3 -m pip install -r requirements.txt
Usage$ python3 sherlock --help
usage: sherlock [-h] [--version] [--verbose] [--folderoutput FOLDEROUTPUT]
                [--output OUTPUT] [--tor] [--unique-tor] [--csv]
                [--site SITE_NAME] [--proxy PROXY_URL] [--json JSON_FILE]
                [--timeout TIMEOUT] [--print-all] [--print-found] [--no-color]
                [--browse] [--local] [--nsfw]
                USERNAMES [USERNAMES ...]

Sherlock: Find Usernames Across Social Networks (Version 0.14.2)

positional arguments:
  USERNAMES             One or more usernames to check with social networks.
                        Check similar usernames using {%} (replace to '_', '-', '.').

optional arguments:
  -h, --help            show this help message and exit
  --version             Display version information and dependencies.
  --verbose, -v, -d, --debug
                        Display extra debugging information and metrics.
  --folderoutput FOLDEROUTPUT, -fo FOLDEROUTPUT
                        If using multiple usernames, the output of the results will be
                        saved to this folder.
  --output OUTPUT, -o OUTPUT
                        If using single username, the output of the result will be saved
                        to this file.
  --tor, -t             Make requests over Tor; increases runtime; requires Tor to be
                        installed and in system path.
  --unique-tor, -u      Make requests over Tor with new Tor circuit after each request;
                        increases runtime; requires Tor to be installed and in system
                        path.
  --csv                 Create Comma-Separated Values (CSV) File.
  --xlsx                Create the standard file for the modern Microsoft Excel
                        spreadsheet (xslx).
  --site SITE_NAME      Limit analysis to just the listed sites. Add multiple options to
                        specify more than one site.
  --proxy PROXY_URL, -p PROXY_URL
                        Make requests over a proxy. e.g. socks5://127.0.0.1:1080
  --json JSON_FILE, -j JSON_FILE
                        Load data from a JSON file or an online, valid, JSON file.
  --timeout TIMEOUT     Time (in seconds) to wait for response to requests (Default: 60)
  --print-all           Output sites where the username was not found.
  --print-found         Output sites where the username was found.
  --no-color            Don't color terminal output
  --browse, -b          Browse to all results on default browser.
  --local, -l           Force the use of the local data.json file.
  --nsfw                Include checking of NSFW sites from default list.
To search for only one user:python3 sherlock user123
To search for more than one user:python3 sherlock user1 user2 user3
Accounts found will be stored in an individual text file with the corresponding username (e.g ).Anaconda (Windows) NotesIf you are using Anaconda in Windows, using  might not work. Use  instead.Docker NotesIf docker is installed you can build an image and run this as a container.docker build -t mysherlock-image .
Once the image is built, sherlock can be invoked by running the following:docker run --rm -t mysherlock-image user123
Use the following command to access the saved results:docker run --rm -t -v ""$PWD/results:/opt/sherlock/results"" mysherlock-image -o /opt/sherlock/results/text.txt user123
Docker is instructed to create (or use) the folder  in the current working directory and to mount it at  on the docker container by using the  options.  is instructed to export the result using the  option.Using You can use the  file from the repository and use this command:docker-compose run sherlock -o /opt/sherlock/results/text.txt user123
ContributingWe would love to have you help us with the development of Sherlock. Each and every contribution is greatly valued!Here are some things we would appreciate your help on:[1] Please look at the Wiki entry on to understand the issues.TestsThank you for contributing to Sherlock!Before creating a pull request with new development, please run the teststo ensure that everything is working great.  It would also be a good idea to run the testsbefore starting development to distinguish problems between yourenvironment and the Sherlock software.The following is an example of the command line to run all the tests forSherlock.  This invocation hides the progress text that Sherlock normallyoutputs, and instead shows the verbose output of the tests.$ cd sherlock/sherlock
$ python3 -m unittest tests.all --verbose
Note that we do currently have 100% test coverage.  Unfortunately, some ofthe sites that Sherlock checks are not always reliable, so it is commonto get response problems.  Any problems in connection will show up aswarnings in the tests instead of true errors.If some sites are failing due to connection problems (site is down, in maintenance, etc)you can exclude them from tests by creating a  file with alist of sites to ignore (one site name per line).Stargazers over timeLicenseMIT © Sherlock ProjectOriginal Creator - "
https://github.com/s0md3v/Photon,Incredibly fast crawler designed for OSINT.,"Key FeaturesData ExtractionPhoton can extract the following data while crawling:The extracted information is saved in an organized manner or can be .FlexibleControl timeout, delay, add seeds, exclude URLs matching a regex pattern and other cool stuff.The extensive range of  provided by Photon lets you crawl the web exactly the way you want.GeniusPhoton's smart thread management & refined logic gives you top notch performance.Still, crawling can be resource intensive but Photon has some tricks up it's sleeves. You can fetch URLs archived by  to be used as seeds by using  option.PluginsDockerPhoton can be launched using a lightweight Python-Alpine (103 MB) Docker image.$ git clone https://github.com/s0md3v/Photon.git
$ cd Photon
$ docker build -t photon .
$ docker run -it --name photon photon:latest -u google.com
To view results, you can either head over to the local docker volume, which you can find by running  or by mounting the target loot folder:$ docker run -it --name photon -v ""$PWD:/Photon/google.com"" photon:latest -u google.com
Frequent & Seamless UpdatesPhoton is under heavy development and updates for fixing bugs. optimizing performance & new features are being rolled regularly.If you would like to see features and issues that are being worked on, you can do that on  project board.Updates can be installed & checked for with the  option. Photon has seamless update capabilities which means you can update Photon without losing any of your saved data.Contribution & LicenseYou can contribute in following ways:Please read the  before submitting a pull request or issue.Do you want to have a conversation in private? Hit me up on my , inbox is open :)Photon is licensed under "
https://github.com/ultralytics/yolov3,YOLOv3 in PyTorch > ONNX > CoreML > TFLite," | YOLOv3 🚀 is the world's most loved vision AI, representing Ultralytics open-source research into future vision AI methods, incorporating lessons learned and best practices evolved over thousands of hours of research and development.We hope that the resources here will help you get the most out of YOLOv3. Please browse the YOLOv3 Docs for details, raise an issue on GitHub for support, and join our Discord community for questions and discussions!To request an Enterprise License please complete the form at .YOLOv8 🚀 NEWWe are thrilled to announce the launch of Ultralytics YOLOv8 🚀, our NEW cutting-edge, state-of-the-art (SOTA) model released at [<marko.inline.RawText object at 0x000001593003C408>]. YOLOv8 is designed to be fast, accurate, and easy to use, making it an excellent choice for a wide range of object detection, image segmentation and image classification tasks.See the  for details and get started with: pip install ultralytics
DocumentationSee the  for full documentation on training, testing and deployment. See below for quickstart examples.Clone repo and install  in a environment, including.git clone https://github.com/ultralytics/yolov3  # clone
cd yolov3
pip install -r requirements.txt  # install
YOLOv3  inference.  download automatically from the latest YOLOv3 .import torch

# Model
model = torch.hub.load(""ultralytics/yolov3"", ""yolov3"")  # or yolov5n - yolov5x6, custom

# Images
img = ""https://ultralytics.com/images/zidane.jpg""  # or file, Path, PIL, OpenCV, numpy, list

# Inference
results = model(img)

# Results
results.print()  # or .show(), .save(), .crop(), .pandas(), etc.
 runs inference on a variety of sources, downloading  automatically from the latest YOLOv3  and saving results to .python detect.py --weights yolov5s.pt --source 0                               # webcam
                                               img.jpg                         # image
                                               vid.mp4                         # video
                                               screen                          # screenshot
                                               path/                           # directory
                                               list.txt                        # list of images
                                               list.streams                    # list of streams
                                               'path/*.jpg'                    # glob
                                               'https://youtu.be/LNwODJXcvt4'  # YouTube
                                               'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream
The commands below reproduce YOLOv3 results. and  download automatically from the latest YOLOv3 . Training times for YOLOv5n/s/m/l/x are 1/2/4/6/8 days on a V100 GPU ( times faster). Use the largest  possible, or pass  for YOLOv3 . Batch sizes shown for V100-16GB.python train.py --data coco.yaml --epochs 300 --weights '' --cfg yolov5n.yaml  --batch-size 128
                                                                 yolov5s                    64
                                                                 yolov5m                    40
                                                                 yolov5l                    24
                                                                 yolov5x                    16
Integrations|                                                           Roboflow                                                           |                                                            ClearML ⭐ NEW                                                            |                                                                        Comet ⭐ NEW                                                                         |                                           Neural Magic ⭐ NEW                                           || :--------------------------------------------------------------------------------------------------------------------------: | :---------------------------------------------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------------------------------------------------------------------: | :----------------------------------------------------------------------------------------------------: || Label and export your custom datasets directly to YOLOv3 for training with  | Automatically track, visualize and even remotely train YOLOv3 using  (open-source!) | Free forever,  lets you save YOLOv3 models, resume training, and interactively visualise and debug predictions | Run YOLOv3 inference up to 6x faster with  |Ultralytics HUBExperience seamless AI with  ⭐, the all-in-one solution for data visualization, YOLO 🚀 model training and deployment, without any coding. Transform images into actionable insights and bring your AI visions to life with ease using our cutting-edge platform and user-friendly . Start your journey for Free now!Why YOLOv3YOLOv3 has been designed to be super easy to get started and simple to learn. We prioritize real-world results.Pretrained Checkpoints| Model                                                                                           | size(pixels) | mAPval50-95 | mAPval50 | SpeedCPU b1(ms) | SpeedV100 b1(ms) | SpeedV100 b32(ms) | params(M) | FLOPs@640 (B) || ----------------------------------------------------------------------------------------------- | --------------------- | -------------------- | ----------------- | ---------------------------- | ----------------------------- | ------------------------------ | ------------------ | ---------------------- ||               | 640                   | 28.0                 | 45.7              | 45                       | 6.3                       | 0.6                        | 1.9            | 4.5                ||               | 640                   | 37.4                 | 56.8              | 98                           | 6.4                           | 0.9                            | 7.2                | 16.5                   ||               | 640                   | 45.4                 | 64.1              | 224                          | 8.2                           | 1.7                            | 21.2               | 49.0                   ||               | 640                   | 49.0                 | 67.3              | 430                          | 10.1                          | 2.7                            | 46.5               | 109.1                  ||               | 640                   | 50.7                 | 68.9              | 766                          | 12.1                          | 4.8                            | 86.7               | 205.7                  ||                                                                                                 |                       |                      |                   |                              |                               |                                |                    |                        ||             | 1280                  | 36.0                 | 54.4              | 153                          | 8.1                           | 2.1                            | 3.2                | 4.6                    ||             | 1280                  | 44.8                 | 63.7              | 385                          | 8.2                           | 3.6                            | 12.6               | 16.8                   ||             | 1280                  | 51.3                 | 69.3              | 887                          | 11.1                          | 6.8                            | 35.7               | 50.0                   ||             | 1280                  | 53.7                 | 71.3              | 1784                         | 15.8                          | 10.5                           | 76.8               | 111.4                  || +  | 12801536          | 55.055.8     | 72.772.7  | 3136-                    | 26.2-                     | 19.4-                      | 140.7-         | 209.8-             |SegmentationOur new YOLOv5  instance segmentation models are the fastest and most accurate in the world, beating all current . We've made them super simple to train, validate and deploy. See full details in our  and visit our  for quickstart tutorials.We trained YOLOv5 segmentations models on COCO for 300 epochs at image size 640 using A100 GPUs. We exported all models to ONNX FP32 for CPU speed tests and to TensorRT FP16 for GPU speed tests. We ran all speed tests on Google  notebooks for easy reproducibility.| Model                                                                                      | size(pixels) | mAPbox50-95 | mAPmask50-95 | Train time300 epochsA100 (hours) | SpeedONNX CPU(ms) | SpeedTRT A100(ms) | params(M) | FLOPs@640 (B) || ------------------------------------------------------------------------------------------ | --------------------- | -------------------- | --------------------- | --------------------------------------------- | ------------------------------ | ------------------------------ | ------------------ | ---------------------- ||  | 640                   | 27.6                 | 23.4                  | 80:17                                         | 62.7                       | 1.2                        | 2.0            | 7.1                ||  | 640                   | 37.6                 | 31.7                  | 88:16                                         | 173.3                          | 1.4                            | 7.6                | 26.4                   ||  | 640                   | 45.0                 | 37.1                  | 108:36                                        | 427.0                          | 2.2                            | 22.0               | 70.8                   ||  | 640                   | 49.0                 | 39.9                  | 66:43 (2x)                                    | 857.4                          | 2.9                            | 47.9               | 147.7                  ||  | 640                   | 50.7             | 41.4              | 62:56 (3x)                                    | 1579.2                         | 4.5                            | 88.8               | 265.7                  |TrainYOLOv5 segmentation training supports auto-download COCO128-seg segmentation dataset with  argument and manual download of COCO-segments dataset with  and then .# Single-GPU
python segment/train.py --data coco128-seg.yaml --weights yolov5s-seg.pt --img 640

# Multi-GPU DDP
python -m torch.distributed.run --nproc_per_node 4 --master_port 1 segment/train.py --data coco128-seg.yaml --weights yolov5s-seg.pt --img 640 --device 0,1,2,3
ValValidate YOLOv5s-seg mask mAP on COCO dataset:bash data/scripts/get_coco.sh --val --segments  # download COCO val segments split (780MB, 5000 images)
python segment/val.py --weights yolov5s-seg.pt --data coco.yaml --img 640  # validate
PredictUse pretrained YOLOv5m-seg.pt to predict bus.jpg:python segment/predict.py --weights yolov5m-seg.pt --data data/images/bus.jpg
model = torch.hub.load(
    ""ultralytics/yolov5"", ""custom"", ""yolov5m-seg.pt""
)  # load from PyTorch Hub (WARNING: inference not yet supported)
|  |  || ---------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------- |ExportExport YOLOv5s-seg model to ONNX and TensorRT:python export.py --weights yolov5s-seg.pt --include onnx engine --img 640 --device 0
ClassificationYOLOv5  brings support for classification model training, validation and deployment! See full details in our  and visit our  for quickstart tutorials.We trained YOLOv5-cls classification models on ImageNet for 90 epochs using a 4xA100 instance, and we trained ResNet and EfficientNet models alongside with the same default training settings to compare. We exported all models to ONNX FP32 for CPU speed tests and to TensorRT FP16 for GPU speed tests. We ran all speed tests on Google  for easy reproducibility.| Model                                                                                              | size(pixels) | acctop1 | acctop5 | Training90 epochs4xA100 (hours) | SpeedONNX CPU(ms) | SpeedTensorRT V100(ms) | params(M) | FLOPs@224 (B) || -------------------------------------------------------------------------------------------------- | --------------------- | ---------------- | ---------------- | -------------------------------------------- | ------------------------------ | ----------------------------------- | ------------------ | ---------------------- ||          | 224                   | 64.6             | 85.4             | 7:59                                         | 3.3                        | 0.5                             | 2.5            | 0.5                ||          | 224                   | 71.5             | 90.2             | 8:09                                         | 6.6                            | 0.6                                 | 5.4                | 1.4                    ||          | 224                   | 75.9             | 92.9             | 10:06                                        | 15.5                           | 0.9                                 | 12.9               | 3.9                    ||          | 224                   | 78.0             | 94.0             | 11:56                                        | 26.9                           | 1.4                                 | 26.5               | 8.5                    ||          | 224                   | 79.0         | 94.4         | 15:04                                        | 54.3                           | 1.8                                 | 48.1               | 15.9                   ||                                                                                                    |                       |                  |                  |                                              |                                |                                     |                    |                        ||                | 224                   | 70.3             | 89.5             | 6:47                                     | 11.2                           | 0.5                                 | 11.7               | 3.7                    ||                | 224                   | 73.9             | 91.8             | 8:33                                         | 20.6                           | 0.9                                 | 21.8               | 7.4                    ||                | 224                   | 76.8             | 93.4             | 11:10                                        | 23.4                           | 1.0                                 | 25.6               | 8.5                    ||              | 224                   | 78.5             | 94.3             | 17:10                                        | 42.1                           | 1.9                                 | 44.5               | 15.9                   ||                                                                                                    |                       |                  |                  |                                              |                                |                                     |                    |                        ||  | 224                   | 75.1             | 92.4             | 13:03                                        | 12.5                           | 1.3                                 | 5.3                | 1.0                    ||  | 224                   | 76.4             | 93.2             | 17:04                                        | 14.9                           | 1.6                                 | 7.8                | 1.5                    ||  | 224                   | 76.6             | 93.4             | 17:10                                        | 15.9                           | 1.6                                 | 9.1                | 1.7                    ||  | 224                   | 77.7             | 94.0             | 19:19                                        | 18.9                           | 1.9                                 | 12.2               | 2.4                    |TrainYOLOv5 classification training supports auto-download of MNIST, Fashion-MNIST, CIFAR10, CIFAR100, Imagenette, Imagewoof, and ImageNet datasets with the  argument. To start training on MNIST for example use .# Single-GPU
python classify/train.py --model yolov5s-cls.pt --data cifar100 --epochs 5 --img 224 --batch 128

# Multi-GPU DDP
python -m torch.distributed.run --nproc_per_node 4 --master_port 1 classify/train.py --model yolov5s-cls.pt --data imagenet --epochs 5 --img 224 --device 0,1,2,3
ValValidate YOLOv5m-cls accuracy on ImageNet-1k dataset:bash data/scripts/get_imagenet.sh --val  # download ImageNet val split (6.3G, 50000 images)
python classify/val.py --weights yolov5m-cls.pt --data ../datasets/imagenet --img 224  # validate
PredictUse pretrained YOLOv5s-cls.pt to predict bus.jpg:python classify/predict.py --weights yolov5s-cls.pt --data data/images/bus.jpg
model = torch.hub.load(
    ""ultralytics/yolov5"", ""custom"", ""yolov5s-cls.pt""
)  # load from PyTorch Hub
ExportExport a group of trained YOLOv5s-cls, ResNet and EfficientNet models to ONNX and TensorRT:python export.py --weights yolov5s-cls.pt resnet50.pt efficientnet_b0.pt --include onnx engine --img 224
EnvironmentsGet started in seconds with our verified environments. Click each icon below for details.ContributeWe love your input! We want to make contributing to YOLOv3 as easy and transparent as possible. Please see our  to get started, and fill out the  to send us feedback on your experiences. Thank you to all our contributors!LicenseUltralytics offers two licensing options to accommodate diverse use cases:ContactFor YOLOv3 bug reports and feature requests please visit , and join our  community for questions and discussions!"
https://github.com/cloud-custodian/cloud-custodian,"Rules engine for cloud security, cost optimization, and governance, DSL in yaml for policies to query, filter, and take actions on resources","Cloud Custodian (c7n)Cloud Custodian, also known as c7n, is a rules engine for managingpublic cloud accounts and resources. It allows users to definepolicies to enable a well managed cloud infrastructure, thats bothsecure and cost optimized. It consolidates many of the adhoc scriptsorganizations have into a lightweight and flexible tool, with unifiedmetrics and reporting.Custodian can be used to manage AWS, Azure, and GCP environments byensuring real time compliance to security policies (like encryption andaccess requirements), tag policies, and cost management via garbagecollection of unused resources and off-hours resource management.Custodian also supports running policies on infrastructure as code assetsto provide feedback directly on developer workstations or within CI pipelines.Custodian policies are written in simple YAML configuration files thatenable users to specify policies on a resource type (EC2, ASG, Redshift,CosmosDB, PubSub Topic) and are constructed from a vocabulary of filtersand actions.It integrates with the cloud native serverless capabilities of eachprovider to provide for real time enforcement of policies with builtinprovisioning. Or it can be run as a simple cron job on a server toexecute against large existing fleets.Cloud Custodian is a CNCF Incubating project, lead by a community of hundredsof contributors.FeaturesLinksQuick InstallCustodian is published on pypi as a series of packages with the prefix, its also available as a docker image.$ python3 -m venv custodian
$ source custodian/bin/activate
(custodian) $ pip install c7n
UsageThe first step to using Cloud Custodian (c7n) is writing a YAML filecontaining the policies that you want to run. Each policy specifiesthe resource type that the policy will run on, a set of filters whichcontrol resources will be affected by this policy, actions which the policywith take on the matched resources, and a mode which controls whichhow the policy will execute.The best getting started guides are the cloud provider specific tutorials.As a quick walk through, below are some sample policies for AWS resources.policies:
 - name: s3-cross-account
   description: |
     Checks S3 for buckets with cross-account access and
     removes the cross-account access.
   resource: aws.s3
   region: us-east-1
   filters:
     - type: cross-account
   actions:
     - type: remove-statements
       statement_ids: matched

 - name: ec2-require-non-public-and-encrypted-volumes
   resource: aws.ec2
   description: |
    Provision a lambda and cloud watch event target
    that looks at all new instances and terminates those with
    unencrypted volumes.
   mode:
    type: cloudtrail
    role: CloudCustodian-QuickStart
    events:
      - RunInstances
   filters:
    - type: ebs
      key: Encrypted
      value: false
   actions:
    - terminate

 - name: tag-compliance
   resource: aws.ec2
   description: |
     Schedule a resource that does not meet tag compliance policies to be stopped in four days. Note a separate policy using the`marked-for-op` filter is required to actually stop the instances after four days.
   filters:
    - State.Name: running
    - ""tag:Environment"": absent
    - ""tag:AppId"": absent
    - or:
      - ""tag:OwnerContact"": absent
      - ""tag:DeptID"": absent
   actions:
    - type: mark-for-op
      op: stop
      days: 4
You can validate, test, and run Cloud Custodian with the example policy with these commands:# Validate the configuration (note this happens by default on run)
$ custodian validate policy.yml

# Dryrun on the policies (no actions executed) to see what resources
# match each policy.
$ custodian run --dryrun -s out policy.yml

# Run the policy
$ custodian run -s out policy.yml
You can run Cloud Custodian via Docker as well:# Download the image
$ docker pull cloudcustodian/c7n
$ mkdir output

# Run the policy
#
# This will run the policy using only the environment variables for authentication
$ docker run -it \
  -v $(pwd)/output:/home/custodian/output \
  -v $(pwd)/policy.yml:/home/custodian/policy.yml \
  --env-file <(env | grep ""^AWS\|^AZURE\|^GOOGLE"") \
  cloudcustodian/c7n run -v -s /home/custodian/output /home/custodian/policy.yml

# Run the policy (using AWS's generated credentials from STS)
#
# NOTE: We mount the ``.aws/credentials`` and ``.aws/config`` directories to
# the docker container to support authentication to AWS using the same credentials
# credentials that are available to the local user if authenticating with STS.

$ docker run -it \
  -v $(pwd)/output:/home/custodian/output \
  -v $(pwd)/policy.yml:/home/custodian/policy.yml \
  -v $(cd ~ && pwd)/.aws/credentials:/home/custodian/.aws/credentials \
  -v $(cd ~ && pwd)/.aws/config:/home/custodian/.aws/config \
  --env-file <(env | grep ""^AWS"") \
  cloudcustodian/c7n run -v -s /home/custodian/output /home/custodian/policy.yml
The  is a go binarythat provides a transparent front end to docker that mirors the regularcustodian cli, but automatically takes care of mounting volumes.Consult the documentation for additional information, or reach out on gitter.Cloud Provider Specific HelpFor specific instructions for AWS, Azure, and GCP, visit the relevant getting started page.Get InvolvedCommunity ResourcesWe have a regular community meeting that is open to all users and developers of every skill level.Joining the  will automatically send you a meeting invite.See the notes below for more technical information on joining the meeting. Additional ToolsThe Custodian project also develops and maintains a suite of additionaltools here:ContributingSee SecurityIf you've found a security related issue, a vulnerability, or apotential vulnerability in Cloud Custodian please let the Cloud know withthe details of the vulnerability. We'll send a confirmation email toacknowledge your report, and we'll send an additional email when we'veidentified the issue positively or negatively.Code of ConductThis project adheres to the By participating, you are expected to honor this code."
https://github.com/scikit-learn-contrib/imbalanced-learn, A Python Package to Tackle the Curse of Imbalanced Datasets in Machine Learning,".. -- mode: rst --.. _scikit-learn: http://scikit-learn.org/stable/.. _scikit-learn-contrib: https://github.com/scikit-learn-contrib|Azure|_ |Codecov|_ |CircleCI|_ |PythonVersion|_ |Pypi|_ |Gitter|_ |Black|_.. |Azure| image:: https://dev.azure.com/imbalanced-learn/imbalanced-learn/_apis/build/status/scikit-learn-contrib.imbalanced-learn?branchName=master.. _Azure: https://dev.azure.com/imbalanced-learn/imbalanced-learn/_build.. |Codecov| image:: https://codecov.io/gh/scikit-learn-contrib/imbalanced-learn/branch/master/graph/badge.svg.. _Codecov: https://codecov.io/gh/scikit-learn-contrib/imbalanced-learn.. |CircleCI| image:: https://circleci.com/gh/scikit-learn-contrib/imbalanced-learn.svg?style=shield.. _CircleCI: https://circleci.com/gh/scikit-learn-contrib/imbalanced-learn/tree/master.. |PythonVersion| image:: https://img.shields.io/pypi/pyversions/imbalanced-learn.svg.. _PythonVersion: https://img.shields.io/pypi/pyversions/imbalanced-learn.svg.. |Pypi| image:: https://badge.fury.io/py/imbalanced-learn.svg.. _Pypi: https://badge.fury.io/py/imbalanced-learn.. |Gitter| image:: https://badges.gitter.im/scikit-learn-contrib/imbalanced-learn.svg.. _Gitter: https://gitter.im/scikit-learn-contrib/imbalanced-learn?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge.. |Black| image:: https://img.shields.io/badge/code%20style-black-000000.svg.. _Black: :target: https://github.com/psf/black.. |PythonMinVersion| replace:: 3.8.. |NumPyMinVersion| replace:: 1.17.3.. |SciPyMinVersion| replace:: 1.5.0.. |ScikitLearnMinVersion| replace:: 1.0.2.. |MatplotlibMinVersion| replace:: 3.1.2.. |PandasMinVersion| replace:: 1.0.5.. |TensorflowMinVersion| replace:: 2.4.3.. |KerasMinVersion| replace:: 2.4.3.. |SeabornMinVersion| replace:: 0.9.0.. |PytestMinVersion| replace:: 5.0.1imbalanced-learnimbalanced-learn is a python package offering a number of re-sampling techniquescommonly used in datasets showing strong between-class imbalance.It is compatible with scikit-learn_ and is part of scikit-learn-contrib_projects.DocumentationInstallation documentation, API documentation, and examples can be found on thedocumentation_... _documentation: https://imbalanced-learn.org/stable/InstallationDependencies
`imbalanced-learn` requires the following dependencies:

- Python (>= |PythonMinVersion|)
- NumPy (>= |NumPyMinVersion|)
- SciPy (>= |SciPyMinVersion|)
- Scikit-learn (>= |ScikitLearnMinVersion|)

Additionally, `imbalanced-learn` requires the following optional dependencies:

- Pandas (>= |PandasMinVersion|) for dealing with dataframes
- Tensorflow (>= |TensorflowMinVersion|) for dealing with TensorFlow models
- Keras (>= |KerasMinVersion|) for dealing with Keras models

The examples will requires the following additional dependencies:

- Matplotlib (>= |MatplotlibMinVersion|)
- Seaborn (>= |SeabornMinVersion|)

Installation
From PyPi or conda-forge repositories.....................................imbalanced-learn is currently available on the PyPi's repositories and you caninstall it via ::pip install -U imbalanced-learnThe package is release also in Anaconda Cloud platform::conda install -c conda-forge imbalanced-learnFrom source available on GitHub...............................If you prefer, you can clone it and run the setup.py file. Use the followingcommands to get a copy from Github and install all dependencies::git clone https://github.com/scikit-learn-contrib/imbalanced-learn.gitcd imbalanced-learnpip install .Be aware that you can install in developer mode with::pip install --no-build-isolation --editable .If you wish to make pull-requests on GitHub, we advise you to installpre-commit::pip install pre-commitpre-commit installTesting
After installation, you can use `pytest` to run the test suite::

  make coverage

Development
-----------

The development of this scikit-learn-contrib is in line with the one
of the scikit-learn community. Therefore, you can refer to their
`Development Guide
<http://scikit-learn.org/stable/developers>`_.

About
-----

If you use imbalanced-learn in a scientific publication, we would appreciate
citations to the following paper::

  @article{JMLR:v18:16-365,
  author  = {Guillaume  Lema{{\^i}}tre and Fernando Nogueira and Christos K. Aridas},
  title   = {Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning},
  journal = {Journal of Machine Learning Research},
  year    = {2017},
  volume  = {18},
  number  = {17},
  pages   = {1-5},
  url     = {http://jmlr.org/papers/v18/16-365}
  }

Most classification algorithms will only perform optimally when the number of
samples of each class is roughly the same. Highly skewed datasets, where the
minority is heavily outnumbered by one or more classes, have proven to be a
challenge while at the same time becoming more and more common.

One way of addressing this issue is by re-sampling the dataset as to offset this
imbalance with the hope of arriving at a more robust and fair decision boundary
than you would otherwise.

You can refer to the `imbalanced-learn`_ documentation to find details about
the implemented algorithms.

.. _imbalanced-learn: https://imbalanced-learn.org/stable/user_guide.html
"
https://github.com/jarun/googler,:mag: Google from the terminal," is a power tool to Google (web, news, videos and site search) from the command-line. It shows the title, URL and abstract for each result, which can be directly opened in a browser from the terminal. Results are fetched in pages (with page navigation). Supports sequential searches in a single  instance. was initially written to cater to headless servers without X. You can integrate it with a text-based browser. However, it has grown into a very handy and flexible utility that delivers much more. For example, fetch any number of results or start anywhere, limit search by any duration, define aliases to google search any number of websites, switch domains easily... all of this in a very clean interface without ads or stray URLs. The shell completion scripts make sure you don't need to remember any options. isn't affiliated to Google in any way.Here are some usage examples:More fun stuff you can try with :Table of contentsFeaturesInstallationDependencies requires Python 3.6 or later. Only the latest patch release of each minor version is supported.To copy url to clipboard at the omniprompt,  looks for  or  or  (in the same order) on Linux,  (default installed) on macOS and  (default installed) on Windows. It also supports GNU Screen and tmux copy-paste buffers in the absence of X11.From a package managerInstall  from your package manager. If the version available is dated try an alternative installation method.Tips for packagers v2.7 and later ships with an in-place self-upgrade mechanism which you may want to disable. To do this, run$ make disable-self-upgrade
before installation.Release packagesPackages for Arch Linux, CentOS, Debian, Fedora, openSUSE and Ubuntu are available with the .From sourceIf you have git installed, clone this repository. Otherwise download the  or .To install to the default location ():$ sudo make install
To remove  and associated docs, run$ sudo make uninstall
 is supported, in case you want to install to a different location.Running standalone is a standalone executable (and can run even on environments like Termux). From the containing directory:$ ./googler
Downloading a single file is a single standalone script, so you could download just a single file if you'd like to.To install the latest stable version, run$ sudo curl -o /usr/local/bin/googler https://raw.githubusercontent.com/jarun/googler/v4.3.2/googler && sudo chmod +x /usr/local/bin/googler
You could then let googler upgrade itself by running$ sudo googler -u
Similarly, if you want to install from git master (risky), run$ sudo curl -o /usr/local/bin/googler https://raw.githubusercontent.com/jarun/googler/master/googler && sudo chmod +x /usr/local/bin/googler
and upgrade by running$ sudo googler -u --include-git
Shell completionSearch keyword and option completion scripts for Bash, Fish and Zsh can be found in respective subdirectories of . Please refer to your shell's manual for installation instructions.UsageCmdline optionsusage: googler [-h] [-s N] [-n N] [-N] [-V] [-c TLD] [-l LANG] [-g CC] [-x]
               [--colorize [{auto,always,never}]] [-C] [--colors COLORS] [-j] [-t dN] [--from FROM]
               [--to TO] [-w SITE] [-e SITE] [--unfilter] [-p PROXY] [--notweak] [--json]
               [--url-handler UTIL] [--show-browser-logs] [--np] [-4] [-6] [-u] [--include-git] [-v] [-d]
               [KEYWORD [KEYWORD ...]]

Google from the command-line.

positional arguments:
  KEYWORD               search keywords

optional arguments:
  -h, --help            show this help message and exit
  -s N, --start N       start at the Nth result
  -n N, --count N       show N results (default 10)
  -N, --news            show results from news section
  -V, --videos          show results from videos section
  -c TLD, --tld TLD     country-specific search with top-level domain .TLD, e.g., 'in' for India
  -l LANG, --lang LANG  display in language LANG
  -g CC, --geoloc CC    country-specific geolocation search with country code CC, e.g. 'in' for India.
                        Country codes are the same as top-level domains
  -x, --exact           disable automatic spelling correction
  --colorize [{auto,always,never}]
                        whether to colorize output; defaults to 'auto', which enables color when stdout
                        is a tty device; using --colorize without an argument is equivalent to
                        --colorize=always
  -C, --nocolor         equivalent to --colorize=never
  --colors COLORS       set output colors (see man page for details)
  -j, --first, --lucky  open the first result in web browser and exit
  -t dN, --time dN      time limit search [h5 (5 hrs), d5 (5 days), w5 (5 weeks), m5 (5 months), y5 (5
                        years)]
  --from FROM           starting date/month/year of date range; must use American date format with
                        slashes, e.g., 2/24/2020, 2/2020, 2020; can be used in conjunction with --to,
                        and overrides -t, --time
  --to TO               ending date/month/year of date range; see --from
  -w SITE, --site SITE  search a site using Google
  -e SITE, --exclude SITE
                        exclude site from results
  --unfilter            do not omit similar results
  -p PROXY, --proxy PROXY
                        tunnel traffic through an HTTP proxy; PROXY is of the form
                        [http://][user:password@]proxyhost[:port]
  --notweak             disable TCP optimizations and forced TLS 1.2
  --json                output in JSON format; implies --noprompt
  --url-handler UTIL    custom script or cli utility to open results
  --show-browser-logs   do not suppress browser output (stdout and stderr)
  --np, --noprompt      search and exit, do not prompt
  -4, --ipv4            only connect over IPv4 (by default, IPv4 is preferred but IPv6 is used as a
                        fallback)
  -6, --ipv6            only connect over IPv6
  -u, --upgrade         perform in-place self-upgrade
  --include-git         when used with --upgrade, get latest git master
  -v, --version         show program's version number and exit
  -d, --debug           enable debugging

omniprompt keys:
  n, p                  fetch the next or previous set of search results
  index                 open the result corresponding to index in browser
  f                     jump to the first page
  o [index|range|a ...] open space-separated result indices, numeric ranges
                        (sitelinks unsupported in ranges), or all, in browser
                        open the current search in browser, if no arguments
  O [index|range|a ...] like key 'o', but try to open in a GUI browser
  g keywords            new Google search for 'keywords' with original options
                        should be used to search omniprompt keys and indices
  c index               copy url to clipboard
  u                     toggle url expansion
  q, ^D, double Enter   exit googler
  ?                     show omniprompt help
  *                     other inputs issue a new search with original options
Configuration file doesn't have any! This is to retain the speed of the utility and avoid OS-specific differences. Users can enjoy the advantages of config files using aliases (with the exception of the color scheme, which can be additionally customized through an environment variable; see ). There's no need to memorize options.For example, the following alias for bash/zsh/ksh/etc.alias g='googler -n 7 -c ru -l ru'
fetches 7 results from the Google Russia server, with preference towards results in Russian.The alias serves both the purposes of using config files:googler @t is a convenient add-on to Google Site Search with unique keywords. While  has an integrated option to search a site, we simplified it further with aliases. The file  contains a list of website search aliases. To source it, run:$ source googler_at
or,$ . googler_at
With , here's how you search Wikipedia for :$ @w hexspeak
Oh yes! You can combine other  options too! To make life easier, you can also configure your shell to source the file when it starts.All the aliases start with the  symbol (hence the name ) and there is minimum chance they will conflict with any shell commands. Feel free to add your own aliases to the file and contribute back the interesting ones.Text-based browser integration works out of the box with several text-based browsers if the  environment variable is set. For instance,$ export BROWSER=w3m
or for one-time use,$ BROWSER=w3m googler query
Due to certain graphical browsers spewing messages to the console,  suppresses browser output by default unless  is set to one of the known text-based browsers: currently , , ,  or . If you use a different text-based browser, you will need to explicitly enable browser output with the  option. If you believe your browser is popular enough, please submit an issue or pull request and we will consider whitelisting it. See the man page for more details on .If you need to use a GUI browser with  set, use the omniprompt key .  will try to ignore text-based browsers and invoke a GUI browser. Browser logs are always suppressed with .Colors allows you to customize the color scheme via a six-letter string, reminiscent of BSD . The six letters represent the colors ofrespectively. The six-letter string is passed in either as the argument to the  option, or as the value of the environment variable .We offer the following colors/styles:Letter | Color/Style------ | -----------a      | blackb      | redc      | greend      | yellowe      | bluef      | magentag      | cyanh      | whitei      | bright blackj      | bright redk      | bright greenl      | bright yellowm      | bright bluen      | bright magentao      | bright cyanp      | bright whiteA-H    | bold version of the lowercase-letter colorI-P    | bold version of the lowercase-letter bright colorx      | normalX      | boldy      | reverse videoY      | bold reverse videoThe default colors string is , which stands forNote thatPlease consult the manual of your terminal emulator as well as the  on ANSI escape sequences.Domain-only URLTo show the domain names in search results instead of the expanded URL (and use lesser space), set the environment variable .Windows Subsystem for Linux (WSL)On WSL, GUI browsers on the Windows side cannot be detected by default. You need to explicitly set the  environment variable to the path of a Windows executable. For instance, you can put the following in your shell's rc:$ export BROWSER='/mnt/c/Program Files (x86)/Google/Chrome/Application/chrome.exe'
TroubleshootingNotesContributionsPull requests are welcome. Please visit  for a list of TODOs.DevelopersSpecial thanks to  and  for their contributions.LogoLogo copyright © 2017 Zhiming Wang.You may freely redistribute it alongside the code, or use it when describing or linking to this project. You should NOT create modified versions of it, make it the logo or icon of your project (except personal forks and/or forks with the goal of upstreaming), or otherwise use it without written permission."
https://github.com/jhao104/proxy_pool,Python ProxyPool for web spider,"ProxyPool 爬虫代理IP池______                        ______             _
| ___ \_                      | ___ \           | |
| |_/ / \__ __   __  _ __   _ | |_/ /___   ___  | |
|  __/|  _// _ \ \ \/ /| | | ||  __// _ \ / _ \ | |
| |   | | | (_) | >  < \ |_| || |  | (_) | (_) || |___
\_|   |_|  \___/ /_/\_\ \__  |\_|   \___/ \___/ \_____\
                       __ / /
                      /___ /
ProxyPool爬虫代理IP池项目,主要功能为定时采集网上发布的免费代理验证入库，定时验证入库的代理保证代理的可用性，提供API和CLI两种使用方式。同时你也可以扩展代理源以增加代理池IP的质量和数量。运行项目下载代码:git clone git@github.com:jhao104/proxy_pool.git
https://github.com/jhao104/proxy_pool/releases 下载对应zip文件
安装依赖:pip install -r requirements.txt
更新配置:# setting.py 为项目配置文件

# 配置API服务

HOST = ""0.0.0.0""               # IP
PORT = 5000                    # 监听端口


# 配置数据库

DB_CONN = 'redis://:pwd@127.0.0.1:8888/0'


# 配置 ProxyFetcher

PROXY_FETCHER = [
    ""freeProxy01"",      # 这里是启用的代理抓取方法名，所有fetch方法位于fetcher/proxyFetcher.py
    ""freeProxy02"",
    # ....
]
启动项目:# 如果已经具备运行条件, 可用通过proxyPool.py启动。
# 程序分为: schedule 调度程序 和 server Api服务

# 启动调度程序
python proxyPool.py schedule

# 启动webApi服务
python proxyPool.py server

Docker Imagedocker pull jhao104/proxy_pool

docker run --env DB_CONN=redis://:password@ip:port/0 -p 5010:5010 jhao104/proxy_pool:latest
docker-compose项目目录下运行: docker-compose up -d
使用启动web服务后, 默认配置下会开启 http://127.0.0.1:5010 的api接口服务:| api | method | Description | params|| ----| ---- | ---- | ----|| / | GET | api介绍 | None || /get | GET | 随机获取一个代理| 可选参数:  过滤支持https的代理|| /pop | GET | 获取并删除一个代理| 可选参数:  过滤支持https的代理|| /all | GET | 获取所有代理 |可选参数:  过滤支持https的代理|| /count | GET | 查看代理数量 |None|| /delete | GET | 删除代理  ||如果要在爬虫代码中使用的话， 可以将此api封装成函数直接使用，例如：import requests

def get_proxy():
    return requests.get(""http://127.0.0.1:5010/get/"").json()

def delete_proxy(proxy):
    requests.get(""http://127.0.0.1:5010/delete/?proxy={}"".format(proxy))

# your spider code

def getHtml():
    # ....
    retry_count = 5
    proxy = get_proxy().get(""proxy"")
    while retry_count > 0:
        try:
            html = requests.get('http://www.example.com', proxies={""http"": ""http://{}"".format(proxy)})
            # 使用代理访问
            return html
        except Exception:
            retry_count -= 1
    # 删除代理池中代理
    delete_proxy(proxy)
    return None
扩展代理项目默认包含几个免费的代理获取源，但是免费的毕竟质量有限，所以如果直接运行可能拿到的代理质量不理想。所以，提供了代理获取的扩展方法。添加一个新的代理源方法如下:
class ProxyFetcher(object):
    # ....

    # 自定义代理源获取方法
    @staticmethod
    def freeProxyCustom1():  # 命名不和已有重复即可

        # 通过某网站或者某接口或某数据库获取代理
        # 假设你已经拿到了一个代理列表
        proxies = [""x.x.x.x:3128"", ""x.x.x.x:80""]
        for proxy in proxies:
            yield proxy
        # 确保每个proxy都是 host:ip正确的格式返回
在下添加自定义方法的名字:PROXY_FETCHER = [
    ""freeProxy01"",    
    ""freeProxy02"",
    # ....
    ""freeProxyCustom1""  #  # 确保名字和你添加方法名字一致
]
 进程会每隔一段时间抓取一次代理，下次抓取时会自动识别调用你定义的方法。免费代理源目前实现的采集免费代理网站有(排名不分先后, 下面仅是对其发布的免费代理情况, 付费代理测评可以参考): |   代理名称   |  状态  |  更新速度 |  可用率  |  地址 | 代码                                             || ---------   |  ---- | --------  | ------  | ----- |------------------------------------------------|| 站大爷     |  ✔    |     ★     |   **     |     |   || 66代理     |  ✔    |     ★     |   *     |          |   || 开心代理     |   ✔   |     ★     |   *     |      |   || FreeProxyList |   ✔  |    ★     |   *    |  |   || 快代理       |  ✔    |     ★     |   *     |   |   || FateZero   |  ✔    |    ★★    |   *     |  |  || 云代理       |  ✔    |    ★     |   *     |       |  || 小幻代理     |  ✔    |    ★★    |    *    |         |  || 免费代理库   |  ✔    |     ☆     |    *    |    |  || 89代理      |  ✔    |     ☆     |   *     |          |  || 稻壳代理     |  ✔    |     ★★    |   ***   |          |  |如果还有其他好的免费代理网站, 可以在提交在, 下次更新时会考虑在项目中支持。问题反馈任何问题欢迎在 中反馈，同时也可以到我的中留言。你的反馈会让此项目变得更加完美。贡献代码本项目仅作为基本的通用的代理池架构，不接收特有功能(当然,不限于特别好的idea)。本项目依然不够完善，如果发现bug或有新的功能添加，请在中提交bug(或新功能)描述，我会尽力改进，使她更加完美。这里感谢以下contributor的无私奉献： |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | Release Notes"
https://github.com/HumanSignal/label-studio,Label Studio is a multi-type data labeling and annotation tool with standardized output format,"   •  •  • What is Label Studio?Label Studio is an open source data labeling tool. It lets you label data types like audio, text, images, videos, and time series with a simple and straightforward UI and export to various model formats. It can be used to prepare raw data or improve existing training data to get more accurate ML models.Have a custom dataset? You can customize Label Studio to fit your needs. Read an  to learn more. Try out Label StudioInstall Label Studio locally, or deploy it in a cloud instance. .Install locally with DockerOfficial Label Studio docker image is  and it can be downloaded with .Run Label Studio in a Docker container and access it at .docker pull heartexlabs/label-studio:latest
docker run -it -p 8080:8080 -v $(pwd)/mydata:/label-studio/data heartexlabs/label-studio:latest
You can find all the generated assets, including SQLite3 database storage  and uploaded files, in the  directory.Override default Docker installYou can override the default launch command by appending the new arguments:docker run -it -p 8080:8080 -v $(pwd)/mydata:/label-studio/data heartexlabs/label-studio:latest label-studio --log-level DEBUG
Build a local image with DockerIf you want to build a local image, run:docker build -t heartexlabs/label-studio:latest .
Run with Docker ComposeDocker Compose script provides production-ready stack consisting of the following components:To start using the app from  run this command:docker-compose up
Run with Docker Compose + MinIOYou can also run it with an additional MinIO server for local S3 storage. This is particularly useful when you want totest the behavior with S3 storage on your local system. To start Label Studio in this way, you need to run the following command:# Add sudo on Linux if you are not a member of the docker group
docker compose -f docker-compose.yml -f docker-compose.minio.yml up -d
If you do not have a static IP address, you must create an entry in your hosts file so that both Label Studio and yourbrowser can access the MinIO server. For more detailed instructions, please refer to .Install locally with pip# Requires Python >=3.8
pip install label-studio

# Start the server at http://localhost:8080
label-studio
Install locally with Anacondaconda create --name label-studio
conda activate label-studio
conda install psycopg2
pip install label-studio
Install for local developmentYou can run the latest Label Studio version locally without installing the package with pip. # Install all package dependencies
pip install -e .
# Run database migrations
python label_studio/manage.py migrate
python label_studio/manage.py collectstatic
# Start the server in development mode at http://localhost:8080
python label_studio/manage.py runserver
Deploy in a cloud instanceYou can deploy Label Studio with one click in Heroku, Microsoft Azure, or Google Cloud Platform: Apply frontend changesThe frontend part of Label Studio app lies in the  folder and written in React JSX. In case you've made some changes there, the following commands should be run before building / starting the instance:cd label_studio/frontend/
yarn install --frozen-lockfile
npx webpack
cd ../..
python label_studio/manage.py collectstatic --no-input
Troubleshoot installationIf you see any errors during installation, try to rerun the installationpip install --ignore-installed label-studio
Install dependencies on WindowsTo run Label Studio on Windows, download and install the following wheel packages from  to ensure you're using the correct version of Python:# Upgrade pip 
pip install -U pip

# If you're running Win64 with Python 3.8, install the packages downloaded from Gohlke:
pip install lxml‑4.5.0‑cp38‑cp38‑win_amd64.whl

# Install label studio
pip install label-studio
Run test suiteTo add the tests' dependencies to your local install:pip install -r deploy/requirements-test.txt
Alternatively, it is possible to run the unit tests from a Docker container in which the test dependencies are installed:make build-testing-image
make docker-testing-shell
In either case, to run the unit tests:cd label_studio

# sqlite3
DJANGO_DB=sqlite DJANGO_SETTINGS_MODULE=core.settings.label_studio pytest -vv

# postgres (assumes default postgres user,db,pass. Will not work in Docker
# testing container without additional configuration)
DJANGO_DB=default DJANGO_SETTINGS_MODULE=core.settings.label_studio pytest -vv
What you get from Label StudioIncluded templates for labeling data in Label StudioLabel Studio includes a variety of templates to help you label your data, or you can create your own using specifically designed configuration language. The most common templates and use cases for labeling include the following cases:Set up machine learning models with Label StudioConnect your favorite machine learning model using the Label Studio Machine Learning SDK. Follow these steps:This lets you:Integrate Label Studio with your existing toolsYou can use Label Studio as an independent part of your machine learning workflow or integrate the frontend or backend into your existing tools.  Ecosystem| Project | Description ||-|-|| label-studio | Server, distributed as a pip package ||  | React and JavaScript frontend and can run standalone in a web browser or be embedded into your application. ||  | React and JavaScript frontend for managing data. Includes the Label Studio Frontend. Relies on the label-studio server or a custom backend with the expected API methods. ||  | Encode labels in the format of your favorite machine learning library ||  | Transformers library connected and configured for use with Label Studio |RoadmapWant to use The Coolest Feature X but Label Studio doesn't support it? Check out !Citation@misc{Label Studio,
  title={{Label Studio}: Data labeling software},
  url={https://github.com/heartexlabs/label-studio},
  note={Open source software available from https://github.com/heartexlabs/label-studio},
  author={
    Maxim Tkachenko and
    Mikhail Malyuk and
    Andrey Holmanyuk and
    Nikolai Liubimov},
  year={2020-2022},
}
LicenseThis software is licensed under the  © . 2020-2022"
https://github.com/catalyst-team/catalyst,Accelerated deep learning R&D,"Accelerated Deep Learning R&DCatalyst is a PyTorch framework for Deep Learning Research and Development.It focuses on reproducibility, rapid experimentation, and codebase reuseso you can create something new rather than write yet another train loop. Break the cycle – use the Catalyst!Getting startedpip install -U catalyst
import os
from torch import nn, optim
from torch.utils.data import DataLoader
from catalyst import dl, utils
from catalyst.contrib.datasets import MNIST

model = nn.Sequential(nn.Flatten(), nn.Linear(28 * 28, 10))
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.02)
loaders = {
    ""train"": DataLoader(MNIST(os.getcwd(), train=True), batch_size=32),
    ""valid"": DataLoader(MNIST(os.getcwd(), train=False), batch_size=32),
}

runner = dl.SupervisedRunner(
    input_key=""features"", output_key=""logits"", target_key=""targets"", loss_key=""loss""
)

# model training
runner.train(
    model=model,
    criterion=criterion,
    optimizer=optimizer,
    loaders=loaders,
    num_epochs=1,
    callbacks=[
        dl.AccuracyCallback(input_key=""logits"", target_key=""targets"", topk=(1, 3, 5)),
        dl.PrecisionRecallF1SupportCallback(input_key=""logits"", target_key=""targets""),
    ],
    logdir=""./logs"",
    valid_loader=""valid"",
    valid_metric=""loss"",
    minimize_valid_metric=True,
    verbose=True,
)

# model evaluation
metrics = runner.evaluate_loader(
    loader=loaders[""valid""],
    callbacks=[dl.AccuracyCallback(input_key=""logits"", target_key=""targets"", topk=(1, 3, 5))],
)

# model inference
for prediction in runner.predict_loader(loader=loaders[""valid""]):
    assert prediction[""logits""].detach().cpu().numpy().shape[-1] == 10

# model post-processing
model = runner.model.cpu()
batch = next(iter(loaders[""valid""]))[0]
utils.trace_model(model=model, batch=batch)
utils.quantize_model(model=model)
utils.prune_model(model=model, pruning_fn=""l1_unstructured"", amount=0.8)
utils.onnx_export(model=model, batch=batch, file=""./logs/mnist.onnx"", verbose=True)
Step-by-step GuideTable of ContentsOverviewCatalyst helps you implement compactbut full-featured Deep Learning pipelines with just a few lines of code.You get a training loop with metrics, early-stopping, model checkpointing,and other features without the boilerplate.InstallationGeneric installation:pip install -U catalyst
pip install catalyst[ml]         # installs ML-based Catalyst
pip install catalyst[cv]         # installs CV-based Catalyst
# master version installation
pip install git+https://github.com/catalyst-team/catalyst@master --upgrade
# all available extensions are listed here:
# https://github.com/catalyst-team/catalyst/blob/master/setup.py
Catalyst is compatible with: Python 3.7+. PyTorch 1.4+. Tested on Ubuntu 16.04/18.04/20.04, macOS 10.15, Windows 10, and Windows Subsystem for Linux.DocumentationMinimal Examplesimport os
from torch import nn, optim
from torch.nn import functional as F
from torch.utils.data import DataLoader
from catalyst import dl, metrics
from catalyst.contrib.datasets import MNIST

model = nn.Sequential(nn.Flatten(), nn.Linear(28 * 28, 10))
optimizer = optim.Adam(model.parameters(), lr=0.02)

train_data = MNIST(os.getcwd(), train=True)
valid_data = MNIST(os.getcwd(), train=False)
loaders = {
    ""train"": DataLoader(train_data, batch_size=32),
    ""valid"": DataLoader(valid_data, batch_size=32),
}

class CustomRunner(dl.Runner):
    def predict_batch(self, batch):
        # model inference step
        return self.model(batch[0].to(self.engine.device))

    def on_loader_start(self, runner):
        super().on_loader_start(runner)
        self.meters = {
            key: metrics.AdditiveMetric(compute_on_call=False)
            for key in [""loss"", ""accuracy01"", ""accuracy03""]
        }

    def handle_batch(self, batch):
        # model train/valid step
        # unpack the batch
        x, y = batch
        # run model forward pass
        logits = self.model(x)
        # compute the loss
        loss = F.cross_entropy(logits, y)
        # compute the metrics
        accuracy01, accuracy03 = metrics.accuracy(logits, y, topk=(1, 3))
        # log metrics
        self.batch_metrics.update(
            {""loss"": loss, ""accuracy01"": accuracy01, ""accuracy03"": accuracy03}
        )
        for key in [""loss"", ""accuracy01"", ""accuracy03""]:
            self.meters[key].update(self.batch_metrics[key].item(), self.batch_size)
        # run model backward pass
        if self.is_train_loader:
            self.engine.backward(loss)
            self.optimizer.step()
            self.optimizer.zero_grad()

    def on_loader_end(self, runner):
        for key in [""loss"", ""accuracy01"", ""accuracy03""]:
            self.loader_metrics[key] = self.meters[key].compute()[0]
        super().on_loader_end(runner)

runner = CustomRunner()
# model training
runner.train(
    model=model,
    optimizer=optimizer,
    loaders=loaders,
    logdir=""./logs"",
    num_epochs=5,
    verbose=True,
    valid_loader=""valid"",
    valid_metric=""loss"",
    minimize_valid_metric=True,
)
# model inference
for logits in runner.predict_loader(loader=loaders[""valid""]):
    assert logits.detach().cpu().numpy().shape[-1] == 10
import torch
from torch.utils.data import DataLoader, TensorDataset
from catalyst import dl

# data
num_samples, num_features = int(1e4), int(1e1)
X, y = torch.rand(num_samples, num_features), torch.rand(num_samples)
dataset = TensorDataset(X, y)
loader = DataLoader(dataset, batch_size=32, num_workers=1)
loaders = {""train"": loader, ""valid"": loader}

# model, criterion, optimizer, scheduler
model = torch.nn.Linear(num_features, 1)
criterion = torch.nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters())
scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [3, 6])

# model training
runner = dl.SupervisedRunner()
runner.train(
    model=model,
    criterion=criterion,
    optimizer=optimizer,
    scheduler=scheduler,
    loaders=loaders,
    logdir=""./logdir"",
    valid_loader=""valid"",
    valid_metric=""loss"",
    minimize_valid_metric=True,
    num_epochs=8,
    verbose=True,
)
import torch
from torch.utils.data import DataLoader, TensorDataset
from catalyst import dl

# sample data
num_samples, num_features, num_classes = int(1e4), int(1e1), 4
X = torch.rand(num_samples, num_features)
y = (torch.rand(num_samples,) * num_classes).to(torch.int64)

# pytorch loaders
dataset = TensorDataset(X, y)
loader = DataLoader(dataset, batch_size=32, num_workers=1)
loaders = {""train"": loader, ""valid"": loader}

# model, criterion, optimizer, scheduler
model = torch.nn.Linear(num_features, num_classes)
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters())
scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [2])

# model training
runner = dl.SupervisedRunner(
    input_key=""features"", output_key=""logits"", target_key=""targets"", loss_key=""loss""
)
runner.train(
    model=model,
    criterion=criterion,
    optimizer=optimizer,
    scheduler=scheduler,
    loaders=loaders,
    logdir=""./logdir"",
    num_epochs=3,
    valid_loader=""valid"",
    valid_metric=""accuracy03"",
    minimize_valid_metric=False,
    verbose=True,
    callbacks=[
        dl.AccuracyCallback(input_key=""logits"", target_key=""targets"", num_classes=num_classes),
        # uncomment for extra metrics:
        # dl.PrecisionRecallF1SupportCallback(
        #     input_key=""logits"", target_key=""targets"", num_classes=num_classes
        # ),
        # dl.AUCCallback(input_key=""logits"", target_key=""targets""),
        # catalyst[ml] required ``pip install catalyst[ml]``
        # dl.ConfusionMatrixCallback(
        #     input_key=""logits"", target_key=""targets"", num_classes=num_classes
        # ),
    ],
)
import torch
from torch.utils.data import DataLoader, TensorDataset
from catalyst import dl

# sample data
num_samples, num_features, num_classes = int(1e4), int(1e1), 4
X = torch.rand(num_samples, num_features)
y = (torch.rand(num_samples, num_classes) > 0.5).to(torch.float32)

# pytorch loaders
dataset = TensorDataset(X, y)
loader = DataLoader(dataset, batch_size=32, num_workers=1)
loaders = {""train"": loader, ""valid"": loader}

# model, criterion, optimizer, scheduler
model = torch.nn.Linear(num_features, num_classes)
criterion = torch.nn.BCEWithLogitsLoss()
optimizer = torch.optim.Adam(model.parameters())
scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [2])

# model training
runner = dl.SupervisedRunner(
    input_key=""features"", output_key=""logits"", target_key=""targets"", loss_key=""loss""
)
runner.train(
    model=model,
    criterion=criterion,
    optimizer=optimizer,
    scheduler=scheduler,
    loaders=loaders,
    logdir=""./logdir"",
    num_epochs=3,
    valid_loader=""valid"",
    valid_metric=""accuracy01"",
    minimize_valid_metric=False,
    verbose=True,
    callbacks=[
        dl.BatchTransformCallback(
            transform=torch.sigmoid,
            scope=""on_batch_end"",
            input_key=""logits"",
            output_key=""scores""
        ),
        dl.AUCCallback(input_key=""scores"", target_key=""targets""),
        # uncomment for extra metrics:
        # dl.MultilabelAccuracyCallback(input_key=""scores"", target_key=""targets"", threshold=0.5),
        # dl.MultilabelPrecisionRecallF1SupportCallback(
        #     input_key=""scores"", target_key=""targets"", threshold=0.5
        # ),
    ]
)
import torch
from torch import nn, optim
from torch.utils.data import DataLoader, TensorDataset
from catalyst import dl

# sample data
num_samples, num_features, num_classes1, num_classes2 = int(1e4), int(1e1), 4, 10
X = torch.rand(num_samples, num_features)
y1 = (torch.rand(num_samples,) * num_classes1).to(torch.int64)
y2 = (torch.rand(num_samples,) * num_classes2).to(torch.int64)

# pytorch loaders
dataset = TensorDataset(X, y1, y2)
loader = DataLoader(dataset, batch_size=32, num_workers=1)
loaders = {""train"": loader, ""valid"": loader}

class CustomModule(nn.Module):
    def __init__(self, in_features: int, out_features1: int, out_features2: int):
        super().__init__()
        self.shared = nn.Linear(in_features, 128)
        self.head1 = nn.Linear(128, out_features1)
        self.head2 = nn.Linear(128, out_features2)

    def forward(self, x):
        x = self.shared(x)
        y1 = self.head1(x)
        y2 = self.head2(x)
        return y1, y2

# model, criterion, optimizer, scheduler
model = CustomModule(num_features, num_classes1, num_classes2)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters())
scheduler = optim.lr_scheduler.MultiStepLR(optimizer, [2])

class CustomRunner(dl.Runner):
    def handle_batch(self, batch):
        x, y1, y2 = batch
        y1_hat, y2_hat = self.model(x)
        self.batch = {
            ""features"": x,
            ""logits1"": y1_hat,
            ""logits2"": y2_hat,
            ""targets1"": y1,
            ""targets2"": y2,
        }

# model training
runner = CustomRunner()
runner.train(
    model=model,
    criterion=criterion,
    optimizer=optimizer,
    scheduler=scheduler,
    loaders=loaders,
    num_epochs=3,
    verbose=True,
    callbacks=[
        dl.CriterionCallback(metric_key=""loss1"", input_key=""logits1"", target_key=""targets1""),
        dl.CriterionCallback(metric_key=""loss2"", input_key=""logits2"", target_key=""targets2""),
        dl.MetricAggregationCallback(metric_key=""loss"", metrics=[""loss1"", ""loss2""], mode=""mean""),
        dl.BackwardCallback(metric_key=""loss""),
        dl.OptimizerCallback(metric_key=""loss""),
        dl.SchedulerCallback(),
        dl.AccuracyCallback(
            input_key=""logits1"", target_key=""targets1"", num_classes=num_classes1, prefix=""one_""
        ),
        dl.AccuracyCallback(
            input_key=""logits2"", target_key=""targets2"", num_classes=num_classes2, prefix=""two_""
        ),
        # catalyst[ml] required ``pip install catalyst[ml]``
        # dl.ConfusionMatrixCallback(
        #     input_key=""logits1"", target_key=""targets1"", num_classes=num_classes1, prefix=""one_cm""
        # ),
        # dl.ConfusionMatrixCallback(
        #     input_key=""logits2"", target_key=""targets2"", num_classes=num_classes2, prefix=""two_cm""
        # ),
        dl.CheckpointCallback(
            logdir=""./logs/one"",
            loader_key=""valid"", metric_key=""one_accuracy01"", minimize=False, topk=1
        ),
        dl.CheckpointCallback(
            logdir=""./logs/two"",
            loader_key=""valid"", metric_key=""two_accuracy03"", minimize=False, topk=3
        ),
    ],
    loggers={""console"": dl.ConsoleLogger(), ""tb"": dl.TensorboardLogger(""./logs/tb"")},
)
import torch
from torch.utils.data import DataLoader, TensorDataset
from catalyst import dl

# sample data
num_users, num_features, num_items = int(1e4), int(1e1), 10
X = torch.rand(num_users, num_features)
y = (torch.rand(num_users, num_items) > 0.5).to(torch.float32)

# pytorch loaders
dataset = TensorDataset(X, y)
loader = DataLoader(dataset, batch_size=32, num_workers=1)
loaders = {""train"": loader, ""valid"": loader}

# model, criterion, optimizer, scheduler
model = torch.nn.Linear(num_features, num_items)
criterion = torch.nn.BCEWithLogitsLoss()
optimizer = torch.optim.Adam(model.parameters())
scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [2])

# model training
runner = dl.SupervisedRunner(
    input_key=""features"", output_key=""logits"", target_key=""targets"", loss_key=""loss""
)
runner.train(
    model=model,
    criterion=criterion,
    optimizer=optimizer,
    scheduler=scheduler,
    loaders=loaders,
    num_epochs=3,
    verbose=True,
    callbacks=[
        dl.BatchTransformCallback(
            transform=torch.sigmoid,
            scope=""on_batch_end"",
            input_key=""logits"",
            output_key=""scores""
        ),
        dl.CriterionCallback(input_key=""logits"", target_key=""targets"", metric_key=""loss""),
        # uncomment for extra metrics:
        # dl.AUCCallback(input_key=""scores"", target_key=""targets""),
        # dl.HitrateCallback(input_key=""scores"", target_key=""targets"", topk=(1, 3, 5)),
        # dl.MRRCallback(input_key=""scores"", target_key=""targets"", topk=(1, 3, 5)),
        # dl.MAPCallback(input_key=""scores"", target_key=""targets"", topk=(1, 3, 5)),
        # dl.NDCGCallback(input_key=""scores"", target_key=""targets"", topk=(1, 3, 5)),
        dl.BackwardCallback(metric_key=""loss""),
        dl.OptimizerCallback(metric_key=""loss""),
        dl.SchedulerCallback(),
        dl.CheckpointCallback(
            logdir=""./logs"", loader_key=""valid"", metric_key=""loss"", minimize=True
        ),
    ]
)
import os
from torch import nn, optim
from torch.utils.data import DataLoader
from catalyst import dl
from catalyst.contrib.datasets import MNIST

model = nn.Sequential(nn.Flatten(), nn.Linear(28 * 28, 10))
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.02)

train_data = MNIST(os.getcwd(), train=True)
valid_data = MNIST(os.getcwd(), train=False)
loaders = {
    ""train"": DataLoader(train_data, batch_size=32),
    ""valid"": DataLoader(valid_data, batch_size=32),
}

runner = dl.SupervisedRunner()
# model training
runner.train(
    model=model,
    criterion=criterion,
    optimizer=optimizer,
    loaders=loaders,
    num_epochs=1,
    logdir=""./logs"",
    valid_loader=""valid"",
    valid_metric=""loss"",
    minimize_valid_metric=True,
    verbose=True,
# uncomment for extra metrics:
#     callbacks=[
#         dl.AccuracyCallback(input_key=""logits"", target_key=""targets"", num_classes=10),
#         dl.PrecisionRecallF1SupportCallback(
#             input_key=""logits"", target_key=""targets"", num_classes=10
#         ),
#         dl.AUCCallback(input_key=""logits"", target_key=""targets""),
#         # catalyst[ml] required ``pip install catalyst[ml]``
#         dl.ConfusionMatrixCallback(
#             input_key=""logits"", target_key=""targets"", num_classes=num_classes
#         ),
#     ]
)
import os
import torch
from torch import nn
from torch.utils.data import DataLoader
from catalyst import dl
from catalyst.contrib.datasets import MNIST
from catalyst.contrib.losses import IoULoss


model = nn.Sequential(
    nn.Conv2d(1, 1, 3, 1, 1), nn.ReLU(),
    nn.Conv2d(1, 1, 3, 1, 1), nn.Sigmoid(),
)
criterion = IoULoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.02)

train_data = MNIST(os.getcwd(), train=True)
valid_data = MNIST(os.getcwd(), train=False)
loaders = {
    ""train"": DataLoader(train_data, batch_size=32),
    ""valid"": DataLoader(valid_data, batch_size=32),
}

class CustomRunner(dl.SupervisedRunner):
    def handle_batch(self, batch):
        x = batch[self._input_key]
        x_noise = (x + torch.rand_like(x)).clamp_(0, 1)
        x_ = self.model(x_noise)
        self.batch = {self._input_key: x, self._output_key: x_, self._target_key: x}

runner = CustomRunner(
    input_key=""features"", output_key=""scores"", target_key=""targets"", loss_key=""loss""
)
# model training
runner.train(
    model=model,
    criterion=criterion,
    optimizer=optimizer,
    loaders=loaders,
    num_epochs=1,
    callbacks=[
        dl.IOUCallback(input_key=""scores"", target_key=""targets""),
        dl.DiceCallback(input_key=""scores"", target_key=""targets""),
        dl.TrevskyCallback(input_key=""scores"", target_key=""targets"", alpha=0.2),
    ],
    logdir=""./logdir"",
    valid_loader=""valid"",
    valid_metric=""loss"",
    minimize_valid_metric=True,
    verbose=True,
)
import os
from torch.optim import Adam
from torch.utils.data import DataLoader
from catalyst import dl
from catalyst.contrib.data import HardTripletsSampler
from catalyst.contrib.datasets import MnistMLDataset, MnistQGDataset
from catalyst.contrib.losses import TripletMarginLossWithSampler
from catalyst.contrib.models import MnistSimpleNet
from catalyst.data.sampler import BatchBalanceClassSampler


# 1. train and valid loaders
train_dataset = MnistMLDataset(root=os.getcwd())
sampler = BatchBalanceClassSampler(
    labels=train_dataset.get_labels(), num_classes=5, num_samples=10, num_batches=10
)
train_loader = DataLoader(dataset=train_dataset, batch_sampler=sampler)

valid_dataset = MnistQGDataset(root=os.getcwd(), gallery_fraq=0.2)
valid_loader = DataLoader(dataset=valid_dataset, batch_size=1024)

# 2. model and optimizer
model = MnistSimpleNet(out_features=16)
optimizer = Adam(model.parameters(), lr=0.001)

# 3. criterion with triplets sampling
sampler_inbatch = HardTripletsSampler(norm_required=False)
criterion = TripletMarginLossWithSampler(margin=0.5, sampler_inbatch=sampler_inbatch)

# 4. training with catalyst Runner
class CustomRunner(dl.SupervisedRunner):
    def handle_batch(self, batch) -> None:
        if self.is_train_loader:
            images, targets = batch[""features""].float(), batch[""targets""].long()
            features = self.model(images)
            self.batch = {""embeddings"": features, ""targets"": targets,}
        else:
            images, targets, is_query = \
                batch[""features""].float(), batch[""targets""].long(), batch[""is_query""].bool()
            features = self.model(images)
            self.batch = {""embeddings"": features, ""targets"": targets, ""is_query"": is_query}

callbacks = [
    dl.ControlFlowCallbackWrapper(
        dl.CriterionCallback(input_key=""embeddings"", target_key=""targets"", metric_key=""loss""),
        loaders=""train"",
    ),
    dl.ControlFlowCallbackWrapper(
        dl.CMCScoreCallback(
            embeddings_key=""embeddings"",
            labels_key=""targets"",
            is_query_key=""is_query"",
            topk=[1],
        ),
        loaders=""valid"",
    ),
    dl.PeriodicLoaderCallback(
        valid_loader_key=""valid"", valid_metric_key=""cmc01"", minimize=False, valid=2
    ),
]

runner = CustomRunner(input_key=""features"", output_key=""embeddings"")
runner.train(
    model=model,
    criterion=criterion,
    optimizer=optimizer,
    callbacks=callbacks,
    loaders={""train"": train_loader, ""valid"": valid_loader},
    verbose=False,
    logdir=""./logs"",
    valid_loader=""valid"",
    valid_metric=""cmc01"",
    minimize_valid_metric=False,
    num_epochs=10,
)
import os
import torch
from torch import nn
from torch.utils.data import DataLoader
from catalyst import dl
from catalyst.contrib.datasets import MNIST
from catalyst.contrib.layers import GlobalMaxPool2d, Lambda

latent_dim = 128
generator = nn.Sequential(
    # We want to generate 128 coefficients to reshape into a 7x7x128 map
    nn.Linear(128, 128 * 7 * 7),
    nn.LeakyReLU(0.2, inplace=True),
    Lambda(lambda x: x.view(x.size(0), 128, 7, 7)),
    nn.ConvTranspose2d(128, 128, (4, 4), stride=(2, 2), padding=1),
    nn.LeakyReLU(0.2, inplace=True),
    nn.ConvTranspose2d(128, 128, (4, 4), stride=(2, 2), padding=1),
    nn.LeakyReLU(0.2, inplace=True),
    nn.Conv2d(128, 1, (7, 7), padding=3),
    nn.Sigmoid(),
)
discriminator = nn.Sequential(
    nn.Conv2d(1, 64, (3, 3), stride=(2, 2), padding=1),
    nn.LeakyReLU(0.2, inplace=True),
    nn.Conv2d(64, 128, (3, 3), stride=(2, 2), padding=1),
    nn.LeakyReLU(0.2, inplace=True),
    GlobalMaxPool2d(),
    nn.Flatten(),
    nn.Linear(128, 1),
)

model = nn.ModuleDict({""generator"": generator, ""discriminator"": discriminator})
criterion = {""generator"": nn.BCEWithLogitsLoss(), ""discriminator"": nn.BCEWithLogitsLoss()}
optimizer = {
    ""generator"": torch.optim.Adam(generator.parameters(), lr=0.0003, betas=(0.5, 0.999)),
    ""discriminator"": torch.optim.Adam(discriminator.parameters(), lr=0.0003, betas=(0.5, 0.999)),
}
train_data = MNIST(os.getcwd(), train=False)
loaders = {""train"": DataLoader(train_data, batch_size=32)}

class CustomRunner(dl.Runner):
    def predict_batch(self, batch):
        batch_size = 1
        # Sample random points in the latent space
        random_latent_vectors = torch.randn(batch_size, latent_dim).to(self.engine.device)
        # Decode them to fake images
        generated_images = self.model[""generator""](random_latent_vectors).detach()
        return generated_images

    def handle_batch(self, batch):
        real_images, _ = batch
        batch_size = real_images.shape[0]

        # Sample random points in the latent space
        random_latent_vectors = torch.randn(batch_size, latent_dim).to(self.engine.device)

        # Decode them to fake images
        generated_images = self.model[""generator""](random_latent_vectors).detach()
        # Combine them with real images
        combined_images = torch.cat([generated_images, real_images])

        # Assemble labels discriminating real from fake images
        labels = \
            torch.cat([torch.ones((batch_size, 1)), torch.zeros((batch_size, 1))]).to(self.engine.device)
        # Add random noise to the labels - important trick!
        labels += 0.05 * torch.rand(labels.shape).to(self.engine.device)

        # Discriminator forward
        combined_predictions = self.model[""discriminator""](combined_images)

        # Sample random points in the latent space
        random_latent_vectors = torch.randn(batch_size, latent_dim).to(self.engine.device)
        # Assemble labels that say ""all real images""
        misleading_labels = torch.zeros((batch_size, 1)).to(self.engine.device)

        # Generator forward
        generated_images = self.model[""generator""](random_latent_vectors)
        generated_predictions = self.model[""discriminator""](generated_images)

        self.batch = {
            ""combined_predictions"": combined_predictions,
            ""labels"": labels,
            ""generated_predictions"": generated_predictions,
            ""misleading_labels"": misleading_labels,
        }


runner = CustomRunner()
runner.train(
    model=model,
    criterion=criterion,
    optimizer=optimizer,
    loaders=loaders,
    callbacks=[
        dl.CriterionCallback(
            input_key=""combined_predictions"",
            target_key=""labels"",
            metric_key=""loss_discriminator"",
            criterion_key=""discriminator"",
        ),
        dl.BackwardCallback(metric_key=""loss_discriminator""),
        dl.OptimizerCallback(
            optimizer_key=""discriminator"",
            metric_key=""loss_discriminator"",
        ),
        dl.CriterionCallback(
            input_key=""generated_predictions"",
            target_key=""misleading_labels"",
            metric_key=""loss_generator"",
            criterion_key=""generator"",
        ),
        dl.BackwardCallback(metric_key=""loss_generator""),
        dl.OptimizerCallback(
            optimizer_key=""generator"",
            metric_key=""loss_generator"",
        ),
    ],
    valid_loader=""train"",
    valid_metric=""loss_generator"",
    minimize_valid_metric=True,
    num_epochs=20,
    verbose=True,
    logdir=""./logs_gan"",
)

# visualization (matplotlib required):
# import matplotlib.pyplot as plt
# %matplotlib inline
# plt.imshow(runner.predict_batch(None)[0, 0].cpu().numpy())
import os
import torch
from torch import nn, optim
from torch.nn import functional as F
from torch.utils.data import DataLoader
from catalyst import dl, metrics
from catalyst.contrib.datasets import MNIST

LOG_SCALE_MAX = 2
LOG_SCALE_MIN = -10

def normal_sample(loc, log_scale):
    scale = torch.exp(0.5 * log_scale)
    return loc + scale * torch.randn_like(scale)

class VAE(nn.Module):
    def __init__(self, in_features, hid_features):
        super().__init__()
        self.hid_features = hid_features
        self.encoder = nn.Linear(in_features, hid_features * 2)
        self.decoder = nn.Sequential(nn.Linear(hid_features, in_features), nn.Sigmoid())

    def forward(self, x, deterministic=False):
        z = self.encoder(x)
        bs, z_dim = z.shape

        loc, log_scale = z[:, : z_dim // 2], z[:, z_dim // 2 :]
        log_scale = torch.clamp(log_scale, LOG_SCALE_MIN, LOG_SCALE_MAX)

        z_ = loc if deterministic else normal_sample(loc, log_scale)
        z_ = z_.view(bs, -1)
        x_ = self.decoder(z_)

        return x_, loc, log_scale

class CustomRunner(dl.IRunner):
    def __init__(self, hid_features, logdir, engine):
        super().__init__()
        self.hid_features = hid_features
        self._logdir = logdir
        self._engine = engine

    def get_engine(self):
        return self._engine

    def get_loggers(self):
        return {
            ""console"": dl.ConsoleLogger(),
            ""csv"": dl.CSVLogger(logdir=self._logdir),
            ""tensorboard"": dl.TensorboardLogger(logdir=self._logdir),
        }

    @property
    def num_epochs(self) -> int:
        return 1

    def get_loaders(self):
        loaders = {
            ""train"": DataLoader(MNIST(os.getcwd(), train=False), batch_size=32),
            ""valid"": DataLoader(MNIST(os.getcwd(), train=False), batch_size=32),
        }
        return loaders

    def get_model(self):
        model = self.model if self.model is not None else VAE(28 * 28, self.hid_features)
        return model

    def get_optimizer(self, model):
        return optim.Adam(model.parameters(), lr=0.02)

    def get_callbacks(self):
        return {
            ""backward"": dl.BackwardCallback(metric_key=""loss""),
            ""optimizer"": dl.OptimizerCallback(metric_key=""loss""),
            ""checkpoint"": dl.CheckpointCallback(
                self._logdir,
                loader_key=""valid"",
                metric_key=""loss"",
                minimize=True,
                topk=3,
            ),
        }

    def on_loader_start(self, runner):
        super().on_loader_start(runner)
        self.meters = {
            key: metrics.AdditiveMetric(compute_on_call=False)
            for key in [""loss_ae"", ""loss_kld"", ""loss""]
        }

    def handle_batch(self, batch):
        x, _ = batch
        x = x.view(x.size(0), -1)
        x_, loc, log_scale = self.model(x, deterministic=not self.is_train_loader)

        loss_ae = F.mse_loss(x_, x)
        loss_kld = (
            -0.5 * torch.sum(1 + log_scale - loc.pow(2) - log_scale.exp(), dim=1)
        ).mean()
        loss = loss_ae + loss_kld * 0.01

        self.batch_metrics = {""loss_ae"": loss_ae, ""loss_kld"": loss_kld, ""loss"": loss}
        for key in [""loss_ae"", ""loss_kld"", ""loss""]:
            self.meters[key].update(self.batch_metrics[key].item(), self.batch_size)

    def on_loader_end(self, runner):
        for key in [""loss_ae"", ""loss_kld"", ""loss""]:
            self.loader_metrics[key] = self.meters[key].compute()[0]
        super().on_loader_end(runner)

    def predict_batch(self, batch):
        random_latent_vectors = torch.randn(1, self.hid_features).to(self.engine.device)
        generated_images = self.model.decoder(random_latent_vectors).detach()
        return generated_images

runner = CustomRunner(128, ""./logs"", dl.CPUEngine())
runner.run()
# visualization (matplotlib required):
# import matplotlib.pyplot as plt
# %matplotlib inline
# plt.imshow(runner.predict_batch(None)[0].cpu().numpy().reshape(28, 28))
import os
import optuna
import torch
from torch import nn
from torch.utils.data import DataLoader
from catalyst import dl
from catalyst.contrib.datasets import MNIST


def objective(trial):
    lr = trial.suggest_loguniform(""lr"", 1e-3, 1e-1)
    num_hidden = int(trial.suggest_loguniform(""num_hidden"", 32, 128))

    train_data = MNIST(os.getcwd(), train=True)
    valid_data = MNIST(os.getcwd(), train=False)
    loaders = {
        ""train"": DataLoader(train_data, batch_size=32),
        ""valid"": DataLoader(valid_data, batch_size=32),
    }
    model = nn.Sequential(
        nn.Flatten(), nn.Linear(784, num_hidden), nn.ReLU(), nn.Linear(num_hidden, 10)
    )
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss()

    runner = dl.SupervisedRunner(input_key=""features"", output_key=""logits"", target_key=""targets"")
    runner.train(
        model=model,
        criterion=criterion,
        optimizer=optimizer,
        loaders=loaders,
        callbacks={
            ""accuracy"": dl.AccuracyCallback(
                input_key=""logits"", target_key=""targets"", num_classes=10
            ),
            # catalyst[optuna] required ``pip install catalyst[optuna]``
            ""optuna"": dl.OptunaPruningCallback(
                loader_key=""valid"", metric_key=""accuracy01"", minimize=False, trial=trial
            ),
        },
        num_epochs=3,
    )
    score = trial.best_score
    return score

study = optuna.create_study(
    direction=""maximize"",
    pruner=optuna.pruners.MedianPruner(
        n_startup_trials=1, n_warmup_steps=0, interval_steps=1
    ),
)
study.optimize(objective, n_trials=3, timeout=300)
print(study.best_value, study.best_params)
runner:
  _target_: catalyst.runners.SupervisedRunner
  model:
    _var_: model
    _target_: torch.nn.Sequential
    args:
      - _target_: torch.nn.Flatten
      - _target_: torch.nn.Linear
        in_features: 784  # 28 * 28
        out_features: 10
  input_key: features
  output_key: &output_key logits
  target_key: &target_key targets
  loss_key: &loss_key loss

run:
  # ≈ stage 1
  - _call_: train  # runner.train(...)

    criterion:
      _target_: torch.nn.CrossEntropyLoss

    optimizer:
      _target_: torch.optim.Adam
      params:  # model.parameters()
        _var_: model.parameters
      lr: 0.02

    loaders:
      train:
        _target_: torch.utils.data.DataLoader
        dataset:
          _target_: catalyst.contrib.datasets.MNIST
          root: data
          train: y
        batch_size: 32

      &valid_loader_key valid:
        &valid_loader
        _target_: torch.utils.data.DataLoader
        dataset:
          _target_: catalyst.contrib.datasets.MNIST
          root: data
          train: n
        batch_size: 32

    callbacks:
      - &accuracy_metric
        _target_: catalyst.callbacks.AccuracyCallback
        input_key: *output_key
        target_key: *target_key
        topk: [1,3,5]
      - _target_: catalyst.callbacks.PrecisionRecallF1SupportCallback
        input_key: *output_key
        target_key: *target_key

    num_epochs: 1
    logdir: logs
    valid_loader: *valid_loader_key
    valid_metric: *loss_key
    minimize_valid_metric: y
    verbose: y

  # ≈ stage 2
  - _call_: evaluate_loader  # runner.evaluate_loader(...)
    loader: *valid_loader
    callbacks:
      - *accuracy_metric

catalyst-run --config example.yaml
TestsAll Catalyst code, features, and pipelines .We also have our own  and a corresponding pre-commit hook.During testing, we train a variety of different models: image classification,image segmentation, text classification, GANs, and much more.We then compare their convergence metrics in order to verifythe correctness of the training procedure and its reproducibility.As a result, Catalyst provides fully tested and reproduciblebest practices for your deep learning research and development.CommunityAccelerated with CatalystSee other projects at .If your project implements a paper,a notable use-case/tutorial, or a Kaggle competition solution, orif your code simply presents interesting results and uses Catalyst,we would be happy to add your project to the list above!Do not hesitate to send us a PR with a brief description of the project similar to the above.Contribution GuideWe appreciate all contributions.If you are planning to contribute back bug-fixes, there is no need to run that by us; just send a PR.If you plan to contribute new features, new utility functions, or extensions,please open an issue first and discuss it with us.User FeedbackWe've created  as an additional channel for user feedback.We appreciate any type of feedback. Thank you!AcknowledgmentsSince the beginning of the Сatalyst development, a lot of people have influenced it in a lot of different ways.Catalyst.TeamCatalyst.ContributorsTrusted byCitationPlease use this bibtex if you want to cite this repository in your publications:@misc{catalyst,
    author = {Kolesnikov, Sergey},
    title = {Catalyst - Accelerated deep learning R&D},
    year = {2018},
    publisher = {GitHub},
    journal = {GitHub repository},
    howpublished = {\url{https://github.com/catalyst-team/catalyst}},
}
"
https://github.com/albumentations-team/albumentations,Fast image augmentation library and an easy-to-use wrapper around other libraries. Documentation:  https://albumentations.ai/docs/ Paper about the library: https://www.mdpi.com/2078-2489/11/2/125,"AlbumentationsAlbumentations is a Python library for image augmentation. Image augmentation is used in deep learning and computer vision tasks to increase the quality of trained models. The purpose of image augmentation is to create new training samples from the existing data.Here is an example of how you can apply some  augmentations from Albumentations to create new images from the original one:Why AlbumentationsTable of contentsAuthors |  |  |  |  | InstallationAlbumentations requires Python 3.7 or higher. To install the latest version from PyPI:pip install -U albumentations
Other installation options are described in the .DocumentationThe full documentation is available at [<marko.inline.RawText object at 0x000001592FD399C8>].A simple exampleimport albumentations as A
import cv2

# Declare an augmentation pipeline
transform = A.Compose([
    A.RandomCrop(width=256, height=256),
    A.HorizontalFlip(p=0.5),
    A.RandomBrightnessContrast(p=0.2),
])

# Read an image with OpenCV and convert it to the RGB colorspace
image = cv2.imread(""image.jpg"")
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# Augment an image
transformed = transform(image=image)
transformed_image = transformed[""image""]
Getting startedI am new to image augmentationPlease start with the  about why image augmentation is important and how it helps to build better models.I want to use Albumentations for the specific task such as classification or segmentationIf you want to use Albumentations for a specific task such as classification, segmentation, or object detection, refer to the  that has an in-depth description of this task. We also have a  on applying Albumentations for different use cases.I want to know how to use Albumentations with deep learning frameworksWe have  along with PyTorch and TensorFlow.I want to explore augmentations and see Albumentations in actionCheck the . With it, you can apply augmentations to different images and see the result. Also, we have a .Who is using AlbumentationsSee also:List of augmentationsPixel-level transformsPixel-level transforms will change just an input image and will leave any additional targets such as masks, bounding boxes, and keypoints unchanged. The list of pixel-level transforms:Spatial-level transformsSpatial-level transforms will simultaneously change both an input image as well as additional targets such as masks, bounding boxes, and keypoints. The following table shows which additional targets are supported by each transform.| Transform                                                                                                                                                                       | Image | Masks | BBoxes | Keypoints || ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---: | :---: | :----: | :-------: ||                              | ✓     | ✓     | ✓      | ✓         ||              | ✓     | ✓     | ✓      |           ||                              | ✓     | ✓     | ✓      | ✓         ||            | ✓     | ✓     |        | ✓         ||                                          | ✓     | ✓     | ✓      | ✓         ||                              | ✓     | ✓     | ✓      | ✓         ||  | ✓     | ✓     | ✓      | ✓         ||          | ✓     | ✓     | ✓      |           ||                                  | ✓     | ✓     | ✓      | ✓         ||              | ✓     | ✓     | ✓      |           ||                    | ✓     | ✓     |        |           ||              | ✓     | ✓     | ✓      | ✓         ||                                                  | ✓     | ✓     | ✓      | ✓         ||                      | ✓     | ✓     | ✓      | ✓         ||                    | ✓     | ✓     |        |           ||                                                    | ✓     | ✓     | ✓      | ✓         ||        | ✓     | ✓     | ✓      |           ||                    | ✓     | ✓     | ✓      | ✓         ||                    | ✓     | ✓     | ✓      | ✓         ||            | ✓     | ✓     | ✓      | ✓         ||                                      | ✓     | ✓     | ✓      | ✓         ||                              | ✓     | ✓     | ✓      | ✓         ||        | ✓     | ✓     | ✓      | ✓         ||              | ✓     | ✓     | ✓      | ✓         ||                            | ✓     | ✓     |        | ✓         ||                | ✓     | ✓     | ✓      | ✓         ||                      | ✓     | ✓     | ✓      | ✓         ||                            | ✓     | ✓     | ✓      | ✓         ||    | ✓     | ✓     | ✓      |           ||                    | ✓     | ✓     | ✓      | ✓         ||                                      | ✓     | ✓     | ✓      | ✓         ||                                      | ✓     | ✓     | ✓      | ✓         ||                              | ✓     | ✓     | ✓      | ✓         ||          | ✓     | ✓     | ✓      | ✓         ||                    | ✓     | ✓     | ✓      | ✓         ||                        | ✓     | ✓     | ✓      | ✓         ||                  | ✓     | ✓     | ✓      | ✓         |A few more examples of augmentationsSemantic segmentation on the Inria datasetMedical imagingObject detection and semantic segmentation on the Mapillary Vistas datasetKeypoints augmentationBenchmarking resultsTo run the benchmark yourself, follow the instructions in Results for running the benchmark on the first 2000 images from the ImageNet validation set using an Intel(R) Xeon(R) Gold 6140 CPU.All outputs are converted to a contiguous NumPy array with the np.uint8 data type.The table shows how many images per second can be processed on a single core; higher is better.|                      |albumentations1.1.0|imgaug0.4.0|torchvision (Pillow-SIMD backend)0.10.1|keras2.6.0|augmentor0.2.8|solt0.1.9||----------------------|:------------------------------------:|:----------------------------:|:--------------------------------------------------------:|:---------------------------:|:-------------------------------:|:--------------------------:||HorizontalFlip        |              10220               |             2702             |                           2517                           |             876             |              2528               |            6798            ||VerticalFlip          |               4438               |             2141             |                           2151                           |            4381             |              2155               |            3659            ||Rotate                |               389                |             283              |                           165                            |             28              |               60                |            367             ||ShiftScaleRotate      |               669                |             425              |                           146                            |             29              |                -                |             -              ||Brightness            |               2765               |             1124             |                           411                            |             229             |               408               |            2335            ||Contrast              |               2767               |             1137             |                           349                            |              -              |               346               |            2341            ||BrightnessContrast    |               2746               |             629              |                           190                            |              -              |               189               |            1196            ||ShiftRGB              |               2758               |             1093             |                            -                             |             360             |                -                |             -              ||ShiftHSV              |               598                |             259              |                            59                            |              -              |                -                |            144             ||Gamma                 |               2849               |              -               |                           388                            |              -              |                -                |            933             ||Grayscale             |               5219               |             393              |                           723                            |              -              |              1082               |            1309            ||RandomCrop64          |              163550              |             2562             |                          50159                           |              -              |              42842              |           22260            ||PadToSize512          |               3609               |              -               |                           602                            |              -              |                -                |            3097            ||Resize512             |                 1049                 |             611              |                         1066                         |              -              |              1041               |            1017            ||RandomSizedCrop_64_512|               3224               |             858              |                           1660                           |              -              |              1598               |            2675            ||Posterize             |               2789               |              -               |                            -                             |              -              |                -                |             -              ||Solarize              |               2761               |              -               |                            -                             |              -              |                -                |             -              ||Equalize              |                 647                  |             385              |                            -                             |              -              |             765             |             -              ||Multiply              |               2659               |             1129             |                            -                             |              -              |                -                |             -              ||MultiplyElementwise   |                 111                  |           200            |                            -                             |              -              |                -                |             -              ||ColorJitter           |               351                |              78              |                            57                            |              -              |                -                |             -              |Python and library versions: Python 3.9.5 (default, Jun 23 2021, 15:01:51) [GCC 8.3.0], numpy 1.19.5, pillow-simd 7.0.0.post3, opencv-python 4.5.3.56, scikit-image 0.18.3, scipy 1.7.1.ContributingTo create a pull request to the repository, follow the documentation at CommentsIn some systems, in the multiple GPU regime, PyTorch may deadlock the DataLoader if OpenCV was compiled with OpenCL optimizations. Adding the following two lines before the library import may help. For more details cv2.setNumThreads(0)
cv2.ocl.setUseOpenCL(False)
CitingIf you find this library useful for your research, please consider citing :@Article{info11020125,
    AUTHOR = {Buslaev, Alexander and Iglovikov, Vladimir I. and Khvedchenya, Eugene and Parinov, Alex and Druzhinin, Mikhail and Kalinin, Alexandr A.},
    TITLE = {Albumentations: Fast and Flexible Image Augmentations},
    JOURNAL = {Information},
    VOLUME = {11},
    YEAR = {2020},
    NUMBER = {2},
    ARTICLE-NUMBER = {125},
    URL = {https://www.mdpi.com/2078-2489/11/2/125},
    ISSN = {2078-2489},
    DOI = {10.3390/info11020125}
}
"
https://github.com/google/trax,Trax — Deep Learning with Clear Code and Speed,"Trax &mdash; Deep Learning with Clear Code and Speed is an end-to-end library for deep learning that focuses on clear code and speed. It is actively used and maintained in the . This notebook () shows how to use Trax and where you can find more information.We welcome contributions to Trax! We welcome PRs with code for new models and layers as well as improvements to our code and documentation. We especially love notebooks that explain how models work and show how to use them to solve problems!Here are a few example notebooks:-General SetupExecute the following cell (once) before running any of the code samples.import os
import numpy as np

!pip install -q -U trax
import trax
1. Run a pre-trained TransformerHere is how you create an English-German translator in a few lines of code:# Create a Transformer model.
# Pre-trained model config in gs://trax-ml/models/translation/ende_wmt32k.gin
model = trax.models.Transformer(
    input_vocab_size=33300,
    d_model=512, d_ff=2048,
    n_heads=8, n_encoder_layers=6, n_decoder_layers=6,
    max_len=2048, mode='predict')

# Initialize using pre-trained weights.
model.init_from_file('gs://trax-ml/models/translation/ende_wmt32k.pkl.gz',
                     weights_only=True)

# Tokenize a sentence.
sentence = 'It is nice to learn new things today!'
tokenized = list(trax.data.tokenize(iter([sentence]),  # Operates on streams.
                                    vocab_dir='gs://trax-ml/vocabs/',
                                    vocab_file='ende_32k.subword'))[0]

# Decode from the Transformer.
tokenized = tokenized[None, :]  # Add batch dimension.
tokenized_translation = trax.supervised.decoding.autoregressive_sample(
    model, tokenized, temperature=0.0)  # Higher temperature: more diverse results.

# De-tokenize,
tokenized_translation = tokenized_translation[0][:-1]  # Remove batch and EOS.
translation = trax.data.detokenize(tokenized_translation,
                                   vocab_dir='gs://trax-ml/vocabs/',
                                   vocab_file='ende_32k.subword')
print(translation)
Es ist schön, heute neue Dinge zu lernen!
2. Features and resourcesTrax includes basic models (like , , ) and RL algorithms(like , , ). It is also actively used for research and includesnew models like the  and new RL algorithms like . Trax has bindings to a large number of deep learning datasets, including and .You can use Trax either as a library from your own python scripts and notebooksor as a binary from the shell, which can be more convenient for training large models.It runs without any changes on CPUs, GPUs and TPUs.3. WalkthroughYou can learn here how Trax works, how to create new models and how to train them on your own data.Tensors and Fast MathThe basic units flowing through Trax models are tensors - multi-dimensional arrays, sometimes also known as numpy arrays, due to the most widely used package for tensor operations -- . You should take a look at the  if you don't know how to operate on tensors: Trax also uses the numpy API for that.In Trax we want numpy operations to run very fast, making use of GPUs and TPUs to accelerate them. We also want to automatically compute gradients of functions on tensors. This is done in the  package thanks to its backends --  and .from trax.fastmath import numpy as fastnp
trax.fastmath.use_backend('jax')  # Can be 'jax' or 'tensorflow-numpy'.

matrix  = fastnp.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
print(f'matrix = \n{matrix}')
vector = fastnp.ones(3)
print(f'vector = {vector}')
product = fastnp.dot(vector, matrix)
print(f'product = {product}')
tanh = fastnp.tanh(product)
print(f'tanh(product) = {tanh}')
matrix = 
[[1 2 3]
 [4 5 6]
 [7 8 9]]
vector = [1. 1. 1.]
product = [12. 15. 18.]
tanh(product) = [0.99999994 0.99999994 0.99999994]
Gradients can be calculated using .def f(x):
  return 2.0 * x * x

grad_f = trax.fastmath.grad(f)

print(f'grad(2x^2) at 1 = {grad_f(1.0)}')
grad(2x^2) at 1 = 4.0
LayersLayers are basic building blocks of Trax models. You will learn all about them in the  but for now, just take a look at the implementation of one core Trax layer, :class Embedding(base.Layer):
  """"""Trainable layer that maps discrete tokens/IDs to vectors.""""""

  def __init__(self,
               vocab_size,
               d_feature,
               kernel_initializer=init.RandomNormalInitializer(1.0)):
    """"""Returns an embedding layer with given vocabulary size and vector size.

    Args:
      vocab_size: Size of the input vocabulary. The layer will assign a unique
          vector to each ID in `range(vocab_size)`.
      d_feature: Dimensionality/depth of the output vectors.
      kernel_initializer: Function that creates (random) initial vectors for
          the embedding.
    """"""
    super().__init__(name=f'Embedding_{vocab_size}_{d_feature}')
    self._d_feature = d_feature  # feature dimensionality
    self._vocab_size = vocab_size
    self._kernel_initializer = kernel_initializer

  def forward(self, x):
    """"""Returns embedding vectors corresponding to input token IDs.

    Args:
      x: Tensor of token IDs.

    Returns:
      Tensor of embedding vectors.
    """"""
    return jnp.take(self.weights, x, axis=0, mode='clip')

  def init_weights_and_state(self, input_signature):
    """"""Returns tensor of newly initialized embedding vectors.""""""
    del input_signature
    shape_w = (self._vocab_size, self._d_feature)
    w = self._kernel_initializer(shape_w, self.rng)
    self.weights = w
Layers with trainable weights like  need to be initialized with the signature (shape and dtype) of the input, and then can be run by calling them.from trax import layers as tl

# Create an input tensor x.
x = np.arange(15)
print(f'x = {x}')

# Create the embedding layer.
embedding = tl.Embedding(vocab_size=20, d_feature=32)
embedding.init(trax.shapes.signature(x))

# Run the layer -- y = embedding(x).
y = embedding(x)
print(f'shape of y = {y.shape}')
x = [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]
shape of y = (15, 32)
ModelsModels in Trax are built from layers most often using the  and  combinators. You can read more about those combinators in the  andsee the code for many models in , e.g., this is how the  is implemented. Below is an example of how to build a sentiment classification model.model = tl.Serial(
    tl.Embedding(vocab_size=8192, d_feature=256),
    tl.Mean(axis=1),  # Average on axis 1 (length of sentence).
    tl.Dense(2),      # Classify 2 classes.
    tl.LogSoftmax()   # Produce log-probabilities.
)

# You can print model structure.
print(model)
Serial[
  Embedding_8192_256
  Mean
  Dense_2
  LogSoftmax
]
DataTo train your model, you need data. In Trax, data streams are represented as python iterators, so you can call  and get a tuple, e.g., . Trax allows you to use  easily and you can also get an iterator from your own text file using the standard .train_stream = trax.data.TFDS('imdb_reviews', keys=('text', 'label'), train=True)()
eval_stream = trax.data.TFDS('imdb_reviews', keys=('text', 'label'), train=False)()
print(next(train_stream))  # See one example.
(b""This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it."", 0)
Using the  module you can create input processing pipelines, e.g., to tokenize and shuffle your data. You create data pipelines using  and they are functions that you apply to streams to create processed streams.data_pipeline = trax.data.Serial(
    trax.data.Tokenize(vocab_file='en_8k.subword', keys=[0]),
    trax.data.Shuffle(),
    trax.data.FilterByLength(max_length=2048, length_keys=[0]),
    trax.data.BucketByLength(boundaries=[  32, 128, 512, 2048],
                             batch_sizes=[256,  64,  16,    4, 1],
                             length_keys=[0]),
    trax.data.AddLossWeights()
  )
train_batches_stream = data_pipeline(train_stream)
eval_batches_stream = data_pipeline(eval_stream)
example_batch = next(train_batches_stream)
print(f'shapes = {[x.shape for x in example_batch]}')  # Check the shapes.
shapes = [(4, 1024), (4,), (4,)]
Supervised trainingWhen you have the model and the data, use  to define training and eval tasks and create a training loop. The Trax training loop optimizes training and will create TensorBoard logs and model checkpoints for you.from trax.supervised import training

# Training task.
train_task = training.TrainTask(
    labeled_data=train_batches_stream,
    loss_layer=tl.WeightedCategoryCrossEntropy(),
    optimizer=trax.optimizers.Adam(0.01),
    n_steps_per_checkpoint=500,
)

# Evaluaton task.
eval_task = training.EvalTask(
    labeled_data=eval_batches_stream,
    metrics=[tl.WeightedCategoryCrossEntropy(), tl.WeightedCategoryAccuracy()],
    n_eval_batches=20  # For less variance in eval numbers.
)

# Training loop saves checkpoints to output_dir.
output_dir = os.path.expanduser('~/output_dir/')
!rm -rf {output_dir}
training_loop = training.Loop(model,
                              train_task,
                              eval_tasks=[eval_task],
                              output_dir=output_dir)

# Run 2000 steps (batches).
training_loop.run(2000)
Step      1: Ran 1 train steps in 0.78 secs
Step      1: train WeightedCategoryCrossEntropy |  1.33800304
Step      1: eval  WeightedCategoryCrossEntropy |  0.71843582
Step      1: eval      WeightedCategoryAccuracy |  0.56562500

Step    500: Ran 499 train steps in 5.77 secs
Step    500: train WeightedCategoryCrossEntropy |  0.62914723
Step    500: eval  WeightedCategoryCrossEntropy |  0.49253047
Step    500: eval      WeightedCategoryAccuracy |  0.74062500

Step   1000: Ran 500 train steps in 5.03 secs
Step   1000: train WeightedCategoryCrossEntropy |  0.42949259
Step   1000: eval  WeightedCategoryCrossEntropy |  0.35451687
Step   1000: eval      WeightedCategoryAccuracy |  0.83750000

Step   1500: Ran 500 train steps in 4.80 secs
Step   1500: train WeightedCategoryCrossEntropy |  0.41843575
Step   1500: eval  WeightedCategoryCrossEntropy |  0.35207348
Step   1500: eval      WeightedCategoryAccuracy |  0.82109375

Step   2000: Ran 500 train steps in 5.35 secs
Step   2000: train WeightedCategoryCrossEntropy |  0.38129005
Step   2000: eval  WeightedCategoryCrossEntropy |  0.33760912
Step   2000: eval      WeightedCategoryAccuracy |  0.85312500
After training the model, run it like any layer to get results.example_input = next(eval_batches_stream)[0][0]
example_input_str = trax.data.detokenize(example_input, vocab_file='en_8k.subword')
print(f'example input_str: {example_input_str}')
sentiment_log_probs = model(example_input[None, :])  # Add batch dimension.
print(f'Model returned sentiment probabilities: {np.exp(sentiment_log_probs)}')
example input_str: I first saw this when I was a teen in my last year of Junior High. I was riveted to it! I loved the special effects, the fantastic places and the trial-aspect and flashback method of telling the story.<br /><br />Several years later I read the book and while it was interesting and I could definitely see what Swift was trying to say, I think that while it's not as perfect as the book for social commentary, as a story the movie is better. It makes more sense to have it be one long adventure than having Gulliver return after each voyage and making a profit by selling the tiny Lilliput sheep or whatever.<br /><br />It's much more arresting when everyone thinks he's crazy and the sheep DO make a cameo anyway. As a side note, when I saw Laputa I was stunned. It looks very much like the Kingdom of Zeal from the Chrono Trigger video game (1995) that also made me like this mini-series even more.<br /><br />I saw it again about 4 years ago, and realized that I still enjoyed it just as much. Really high quality stuff and began an excellent run of Sweeps mini-series for NBC who followed it up with the solid Merlin and interesting Alice in Wonderland.<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>
Model returned sentiment probabilities: [[3.984500e-04 9.996014e-01]]
"
https://github.com/pytorch/ignite,High-level library to help with training and evaluating neural networks in PyTorch flexibly and transparently.,"|      ||:---|   ・   ・  ||   ||     ||  |TL;DRIgnite is a high-level library to help with training and evaluating neural networks in PyTorch flexibly and transparently.Click on the image to see complete codeFeaturesTable of ContentsWhy Ignite?Ignite is a library that provides three high-level features:Simplified training and validation loopNo more coding  loops on epochs and iterations. Users instantiate engines and run them.from ignite.engine import Engine, Events, create_supervised_evaluator
from ignite.metrics import Accuracy


# Setup training engine:
def train_step(engine, batch):
    # Users can do whatever they need on a single iteration
    # Eg. forward/backward pass for any number of models, optimizers, etc
    # ...

trainer = Engine(train_step)

# Setup single model evaluation engine
evaluator = create_supervised_evaluator(model, metrics={""accuracy"": Accuracy()})

def validation():
    state = evaluator.run(validation_data_loader)
    # print computed metrics
    print(trainer.state.epoch, state.metrics)

# Run model's validation at the end of each epoch
trainer.add_event_handler(Events.EPOCH_COMPLETED, validation)

# Start the training
trainer.run(training_data_loader, max_epochs=100)
Power of Events & HandlersThe cool thing with handlers is that they offer unparalleled flexibility (compared to, for example, callbacks). Handlers can be any function: e.g. lambda, simple function, class method, etc. Thus, we do not require to inherit from an interface and override its abstract methods which could unnecessarily bulk up your code and its complexity.Execute any number of functions whenever you wishtrainer.add_event_handler(Events.STARTED, lambda _: print(""Start training""))

# attach handler with args, kwargs
mydata = [1, 2, 3, 4]
logger = ...

def on_training_ended(data):
    print(f""Training is ended. mydata={data}"")
    # User can use variables from another scope
    logger.info(""Training is ended"")


trainer.add_event_handler(Events.COMPLETED, on_training_ended, mydata)
# call any number of functions on a single event
trainer.add_event_handler(Events.COMPLETED, lambda engine: print(engine.state.times))

@trainer.on(Events.ITERATION_COMPLETED)
def log_something(engine):
    print(engine.state.output)
Built-in events filtering# run the validation every 5 epochs
@trainer.on(Events.EPOCH_COMPLETED(every=5))
def run_validation():
    # run validation

# change some training variable once on 20th epoch
@trainer.on(Events.EPOCH_STARTED(once=20))
def change_training_variable():
    # ...

# Trigger handler with customly defined frequency
@trainer.on(Events.ITERATION_COMPLETED(event_filter=first_x_iters))
def log_gradients():
    # ...
Stack events to share some actionsEvents can be stacked together to enable multiple calls:@trainer.on(Events.COMPLETED | Events.EPOCH_COMPLETED(every=10))
def run_validation():
    # ...
Custom events to go beyond standard eventsCustom events related to backward and optimizer step calls:from ignite.engine import EventEnum


class BackpropEvents(EventEnum):
    BACKWARD_STARTED = 'backward_started'
    BACKWARD_COMPLETED = 'backward_completed'
    OPTIM_STEP_COMPLETED = 'optim_step_completed'

def update(engine, batch):
    # ...
    loss = criterion(y_pred, y)
    engine.fire_event(BackpropEvents.BACKWARD_STARTED)
    loss.backward()
    engine.fire_event(BackpropEvents.BACKWARD_COMPLETED)
    optimizer.step()
    engine.fire_event(BackpropEvents.OPTIM_STEP_COMPLETED)
    # ...

trainer = Engine(update)
trainer.register_events(*BackpropEvents)

@trainer.on(BackpropEvents.BACKWARD_STARTED)
def function_before_backprop(engine):
    # ...
Out-of-the-box metricsprecision = Precision(average=False)
recall = Recall(average=False)
F1_per_class = (precision * recall * 2 / (precision + recall))
F1_mean = F1_per_class.mean()  # torch mean method
F1_mean.attach(engine, ""F1"")
InstallationFrom :pip install pytorch-ignite
From :conda install ignite -c pytorch
From source:pip install git+https://github.com/pytorch/ignite
Nightly releasesFrom pip:pip install --pre pytorch-ignite
From conda (this suggests to install  instead of stableversion as dependency):conda install ignite -c pytorch-nightly
Docker ImagesUsing pre-built imagesPull a pre-built docker image from  and run it with docker v19.03+.docker run --gpus all -it -v $PWD:/workspace/project --network=host --shm-size 16G pytorchignite/base:latest /bin/bash
BaseVision:NLP:For more details, see .Getting StartedFew pointers to get you started:DocumentationAdditional MaterialsExamplesTutorialsReproducible Training ExamplesInspired by ,we provide several reproducible baselines for vision tasks:Features:Code-Generator applicationThe easiest way to create your training scripts with PyTorch-Ignite:CommunicationUser feedbackWe have created a form for . Weappreciate any type of feedback, and this is how we would like to see ourcommunity:Thank you!ContributingPlease see the  for more information.As always, PRs are welcome :)Projects using IgniteSee other projects at If your project implements a paper, represents other use-cases notcovered in our official tutorials, Kaggle competition's code, or justyour code presents interesting results and uses Ignite. We would like toadd your project to this list, so please send a PR with briefdescription of the project.Citing IgniteIf you use PyTorch-Ignite in a scientific publication, we would appreciate citations to our project.@misc{pytorch-ignite,
  author = {V. Fomin and J. Anmol and S. Desroziers and J. Kriss and A. Tejani},
  title = {High-level library to help with training neural networks in PyTorch},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/pytorch/ignite}},
}
About the team & DisclaimerPyTorch-Ignite is a , operated and maintained by volunteers in the PyTorch community in their capacities as individuals(and not as representatives of their employers). See the page for a list of core contributors. For usage questions and issues, please see the various channels. For all other questions and inquiries, please send an emailto contact@pytorch-ignite.ai."
https://github.com/JunMa11/SegLoss,A collection of loss functions for medical image segmentation,"Loss functions for image segmentation@article{LossOdyssey,
title = {Loss Odyssey in Medical Image Segmentation},
journal = {Medical Image Analysis},
volume = {71},
pages = {102035},
year = {2021},
author = {Jun Ma and Jianan Chen and Matthew Ng and Rui Huang and Yu Li and Chen Li and Xiaoping Yang and Anne L. Martel}
doi = {https://doi.org/10.1016/j.media.2021.102035},
url = {https://www.sciencedirect.com/science/article/pii/S1361841521000815}
}
Take-home message: compound loss functions are the most robust losses, especially for the highly imbalanced segmentation tasks.|Date|First Author|Title|Conference/Journal||---|---|---|---||2023 MICCAI||Robust T-Loss for Medical Image Segmentation |||2023 MICCAI||Dice Semimetric Losses: Optimizing the Dice Score with Soft Labels  |||2023 MICCAI|Fan Sun|Boundary Difference Over Union Loss For Medical Image Segmentation  |||20220517||blob loss: instance imbalance aware loss functions for semantic segmentation  |||20220426|Zhaoqi Len|PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions |||20211109|Litao Yu|Distribution-Aware Margin Calibration for Semantic Segmentation in Images  | ||20211013|Pei Wang|Relax and Focus on Brain Tumor Segmentation | ||20210418|Bingyuan Liu|The hidden label-marginal biases of segmentation losses  | ||20210330|Suprosanna Shit and Johannes C. Paetzold|clDice - a Novel Topology-Preserving Loss Function for Tubular Structure Segmentation |||20210325|Attila Szabo, Hadi Jamali-Rad|Tilted Cross Entropy (TCE): Promoting Fairness in Semantic Segmentation|||20210318|Xiaoling Hu|Topology-Aware Segmentation Using Discrete Morse Theory |||20210211|Hoel Kervadec|Beyond pixel-wise supervision: semantic segmentation with higher-order shape descriptors|||20210210|Rosana EL Jurdi|A Surprisingly Effective Perimeter-based Loss for Medical Image Segmentation|||20201222|Zeju Li|Analyzing Overfitting Under Class Imbalance in Neural Networks for Image Segmentation|||20210129|Nick Byrne|A Persistent Homology-Based Topological Loss Function for Multi-class CNN Segmentation of Cardiac MRI | ||20201019|Hyunseok Seo|Closing the Gap Between Deep Neural Network Modeling and Biomedical Decision-Making Metrics in Segmentation via Adaptive Loss Functions|||20200929|Stefan Gerl|A Distance-Based Loss for Smooth and Continuous Skin Layer Segmentation in Optoacoustic Images|||20200821|Nick Byrne|A persistent homology-based topological loss function for multi-class CNN segmentation of cardiac MRI |STACOM||20200720|Boris Shirokikh|Universal Loss Reweighting to Balance Lesion Size Inequality in 3D Medical Image Segmentation  |MICCAI 2020||20200708|Gonglei Shi|Marginal loss and exclusion loss for partially supervised multi-organ segmentation |MedIA||20200706|Yuan Lan|An Elastic Interaction-Based Loss Function for Medical Image Segmentation  |MICCAI 2020||20200615|Tom Eelbode|Optimization for Medical Image Segmentation: Theory and Practice when evaluating with Dice Score or Jaccard Index|||20200605|Guotai Wang|Noise-robust Dice loss: A Noise-robust Framework for Automatic Segmentation of COVID-19 Pneumonia Lesions from CT Images |||202004|J. H. Moltz|Contour Dice coefficient (CDC) Loss: Learning a Loss Function for Segmentation: A Feasibility Study|||201912|Yuan Xue|Shape-Aware Organ Segmentation by Predicting Signed Distance Maps  |AAAI 2020||201912|Xiaoling Hu|Topology-Preserving Deep Image Segmentation  |||201910|Shuai Zhao|Region Mutual Information Loss for Semantic Segmentation  |||201910|Shuai Zhao|Correlation Maximized Structural Similarity Loss for Semantic Segmentation |arxiv||201908|Pierre-AntoineGanaye|Removing Segmentation Inconsistencies with Semi-Supervised Non-Adjacency Constraint  |||201906|Xu Chen|Learning Active Contour Models for Medical Image Segmentation  |CVPR 2019||20190422|Davood Karimi|Reducing the Hausdorff Distance in Medical Image Segmentation with Convolutional Neural Networks |||20190417|Francesco Caliva|Distance Map Loss Penalty Term for Semantic Segmentation |||20190411|Su Yang|Major Vessel Segmentation on X-ray Coronary Angiography using Deep Networks with a Novel Penalty Loss Function |||20190405||Mumford–Shah Loss Functional for Image Segmentation With Deep Learning |||201901||Asymmetric Loss Functions and Deep Densely Connected Networks for Highly Imbalanced Medical Image Segmentation: Application to Multiple Sclerosis Lesion Detection |IEEE Access||201812||Boundary loss for highly unbalanced segmentation , |||201810||A Novel Focal Tversky loss function with improved Attention U-Net for lesion segmentation  |||201809||CE+Dice: nnU-Net: Self-adapting Framework for U-Net-Based Medical Image Segmentation |||20180831||3D Segmentation with Exponential Logarithmic Loss for Highly Unbalanced Object Sizes |MICCAI 2018||20180815||Dice+Focal: AnatomyNet: Deep Learning for Fast and Fully Automated Whole-volume Segmentation of Head and Neck Anatomy  |||201806||Weighted Hausdorff Distance: Locating Objects Without Bounding Boxes , |CVPR 2019||201805|Saeid Asgari Taghanaki|Combo Loss: Handling Input and Output Imbalance in Multi-Organ Segmentation  |||201709||Shape-aware deep convolutional neural network for vertebrae segmentation |||201708||Focal Loss for Dense Object Detection , |ICCV, TPAMI||20170711||Generalised Dice overlap as a deep learning loss function for highly unbalanced segmentations |DLMIA 2017||20170703||Generalised Wasserstein Dice Score for Imbalanced Multi-class Segmentation using Holistic Convolutional Networks |MICCAI 2017 BrainLes||201705||The Lovász-Softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks , |CVPR 2018||201701||Tversky loss function for image segmentation using 3D fully convolutional deep networks |MICCAI 2017 MLMI||201612||Optimizing Intersection-Over-Union in Deep Neural Networks for Image Segmentation |||201608||""Dice Loss (without square)"" The Importance of Skip Connections in Biomedical Image Segmentation |||201606||""Dice Loss (with square)"" V-net: Fully convolutional neural networks for volumetric medical image segmentation , |International Conference on 3D Vision||201605|Zifeng Wu|TopK loss Bridging Category-level and Instance-level Semantic Image Segmentation |arxiv||201511||""Sensitivity-Specifity loss"" Deep Convolutional Encoder Networks for Multiple Sclerosis Lesion Segmentation |||201505||""Weighted cross entropy"" U-Net: Convolutional Networks for Biomedical Image Segmentation |||201309||What is a good evaluation measure for semantic segmentation? |BMVA 2013|"
https://github.com/aws/chalice,Python Serverless Microframework for AWS,"===========AWS Chalice.. image:: https://badges.gitter.im/awslabs/chalice.svg:target: https://gitter.im/awslabs/chalice?utm_source=badge&utm_medium=badge:alt: Gitter.. image:: https://readthedocs.org/projects/chalice/badge/?version=latest:target: http://aws.github.io/chalice/?badge=latest:alt: Documentation Status.. image:: https://aws.github.io/chalice/_images/chalice-logo-whitespace.png:target: https://aws.github.io/chalice/:alt: Chalice LogoChalice is a framework for writing serverless apps in python. It allowsyou to quickly create and deploy applications that use AWS Lambda.  It provides:You can create Rest APIs:.. code-block:: pythonfrom chalice import Chalice

app = Chalice(app_name=""helloworld"")

@app.route(""/"")
def index():
    return {""hello"": ""world""}
Tasks that run on a periodic basis:.. code-block:: pythonfrom chalice import Chalice, Rate

app = Chalice(app_name=""helloworld"")

# Automatically runs every 5 minutes
@app.schedule(Rate(5, unit=Rate.MINUTES))
def periodic_task(event):
    return {""hello"": ""world""}
You can connect a lambda function to an S3 event:.. code-block:: pythonfrom chalice import Chalice

app = Chalice(app_name=""helloworld"")

# Whenever an object is uploaded to 'mybucket'
# this lambda function will be invoked.

@app.on_s3_event(bucket='mybucket')
def handler(event):
    print(""Object uploaded for bucket: %s, key: %s""
          % (event.bucket, event.key))
As well as an SQS queue:.. code-block:: pythonfrom chalice import Chalice

app = Chalice(app_name=""helloworld"")

# Invoke this lambda function whenever a message
# is sent to the ``my-queue-name`` SQS queue.

@app.on_sqs_message(queue='my-queue-name')
def handler(event):
    for record in event:
        print(""Message body: %s"" % record.body)
And several other AWS resources.Once you've written your code, you just run and Chalice takes care of deploying your app.::$ chalice deploy
...
https://endpoint/dev

$ curl https://endpoint/api
{""hello"": ""world""}
Up and running in less than 30 seconds.Give this project a try and share your feedback with us here on Github.The documentation is available__.Quickstart.. quick-start-beginIn this tutorial, you'll use the  command line utilityto create and deploy a basic REST API.  This quickstart uses Python 3.7,but AWS Chalice supports all versions of python supported by AWS Lambda,which includes python3.6, python3.7, python3.8, python3.9, python3.10.You can find the latest versions of python on the_.To install Chalice, we'll first create and activate a virtual environmentin python3.7::$ python3 --version
Python 3.7.3
$ python3 -m venv venv37
$ . venv37/bin/activate
Next we'll install Chalice using ::$ python3 -m pip install chalice
You can verify you have chalice installed by running::$ chalice --help
Usage: chalice [OPTIONS] COMMAND [ARGS]...
...
CredentialsBefore you can deploy an application, be sure you havecredentials configured.  If you have previously configured yourmachine to run boto3 (the AWS SDK for Python) or the AWS CLI thenyou can skip this section.If this is your first time configuring credentials for AWS youcan follow these steps to quickly get started::$ mkdir ~/.aws
$ cat >> ~/.aws/config
[default]
aws_access_key_id=YOUR_ACCESS_KEY_HERE
aws_secret_access_key=YOUR_SECRET_ACCESS_KEY
region=YOUR_REGION (such as us-west-2, us-west-1, etc)
If you want more information on all the supported methods forconfiguring credentials, see the__.Creating Your ProjectThe next thing we'll do is use the  command to create a newproject::$ chalice new-project helloworld
This will create a  directory.  Cd into thisdirectory.  You'll see several files have been created for you::$ cd helloworld
$ ls -la
drwxr-xr-x   .chalice
-rw-r--r--   app.py
-rw-r--r--   requirements.txt
You can ignore the  directory for now, the two main fileswe'll focus on is  and .Let's take a look at the  file:.. code-block:: pythonfrom chalice import Chalice

app = Chalice(app_name='helloworld')


@app.route('/')
def index():
    return {'hello': 'world'}
The  command created a sample app that defines asingle view, , that when called will return the JSON body.DeployingLet's deploy this app.  Make sure you're in the directory and run ::$ chalice deploy
Creating deployment package.
Creating IAM role: helloworld-dev
Creating lambda function: helloworld-dev
Creating Rest API
Resources deployed:
  - Lambda ARN: arn:aws:lambda:us-west-2:12345:function:helloworld-dev
  - Rest API URL: https://abcd.execute-api.us-west-2.amazonaws.com/api/
You now have an API up and running using API Gateway and Lambda::$ curl https://qxea58oupc.execute-api.us-west-2.amazonaws.com/api/
{""hello"": ""world""}
Try making a change to the returned dictionary from the function.  You can then redeploy your changes by running ... quick-start-endNext StepsYou've now created your first app using .  You can makemodifications to your  file and rerun  toredeploy your changes.At this point, there are several next steps you can take.If you're done experimenting with Chalice and you'd like to cleanup, you canuse the  command, and Chalice will delete all the resourcesit created when running the  command.::$ chalice delete
Deleting Rest API: abcd4kwyl4
Deleting function aws:arn:lambda:region:123456789:helloworld-dev
Deleting IAM Role helloworld-dev
FeedbackWe'd also love to hear from you.  Please create any Github issues foradditional features you'd like to see over athttps://github.com/aws/chalice/issues.  You can also chat with uson gitter: https://gitter.im/awslabs/chalice"
https://github.com/Tencent/FaceDetection-DSFD,腾讯优图高精度双分支人脸检测器,"UpdateIntroductionIn this repo, we propose a novel face detection network, named DSFD, with superior performance over the state-of-the-art face detectors. You can use the code to evaluate our DSFD for face detection. For more details, please refer to our paper ! or poster !Our DSFD face detector achieves state-of-the-art performance on  and  benchmark.WIDER FACEFDDBRequirementsGetting StartedInstallationClone the github repository. We will call the cloned directory as .git clone xxxxxx/FaceDetection-DSFD.git
cd FaceDetection-DSFD
export CUDA_VISIBLE_DEVICES=0
Evaluationpython demo.py [--trained_model [TRAINED_MODEL]] [--img_root  [IMG_ROOT]] 
               [--save_folder [SAVE_FOLDER]] [--visual_threshold [VISUAL_THRESHOLD]] 
    --trained_model      Path to the saved model
    --img_root           Path of test images
    --save_folder        Path of output detection resutls
    --visual_threshold   Confidence thresh
python widerface_val.py [--trained_model [TRAINED_MODEL]] [--save_folder [SAVE_FOLDER]] 
                         [--widerface_root [WIDERFACE_ROOT]]
    --trained_model      Path to the saved model
    --save_folder        Path of output widerface resutls
    --widerface_root     Path of widerface dataset
python widerface_test.py [--trained_model [TRAINED_MODEL]] [--split_dir [SPLIT_DIR]] 
                         [--data_dir [DATA_DIR]] [--det_dir [DET_DIR]]
    --trained_model      Path of the saved model
    --split_dir          Path of fddb folds
    --data_dir           Path of fddb all images
    --det_dir            Path to save fddb results
Qualitative ResultsCitationIf you find DSFD useful in your research, please consider citing: @inproceedings{li2018dsfd,
  title={DSFD: Dual Shot Face Detector},
  author={Li, Jian and Wang, Yabiao and Wang, Changan and Tai, Ying and Qian, Jianjun and Yang, Jian and Wang, Chengjie and Li, Jilin and Huang, Feiyue},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year={2019}
}
ContactFor any question, please file an issue or contactJian Li: swordli@tencent.com
"
https://github.com/tiangolo/fastapi,"FastAPI framework, high performance, easy to learn, fast to code, ready for production","Documentation: https://fastapi.tiangolo.comSource Code: https://github.com/tiangolo/fastapiFastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.8+ based on standard Python type hints.The key features are:* estimation based on tests on an internal development team, building production applications.SponsorsOther sponsorsOpinions""[...] I'm using """"We adopted the """"[<marko.inline.RawText object at 0x000001592FEFEE48>]""""I’m over the moon excited about """"Honestly, what you've built looks super solid and polished. In many ways, it's what I wanted """"If you're looking to learn one """"We've switched over to """"If anyone is looking to build a production Python API, I would highly recommend ""Typer, the FastAPI of CLIsIf you are building a CLI app to be used in the terminal instead of a web API, check out Typer.Typer is FastAPI's little sibling. And it's intended to be the FastAPI of CLIs. ⌨️ 🚀RequirementsPython 3.8+FastAPI stands on the shoulders of giants:Installation$ pip install fastapi

---> 100%
You will also need an ASGI server, for production such as Uvicorn or Hypercorn.$ pip install ""uvicorn[standard]""

---> 100%
ExampleCreate itfrom typing import Union

from fastapi import FastAPI

app = FastAPI()


@app.get(""/"")
def read_root():
    return {""Hello"": ""World""}


@app.get(""/items/{item_id}"")
def read_item(item_id: int, q: Union[str, None] = None):
    return {""item_id"": item_id, ""q"": q}
If your code uses  / , use :from typing import Union

from fastapi import FastAPI

app = FastAPI()


@app.get(""/"")
async def read_root():
    return {""Hello"": ""World""}


@app.get(""/items/{item_id}"")
async def read_item(item_id: int, q: Union[str, None] = None):
    return {""item_id"": item_id, ""q"": q}
Note:If you don't know, check the ""In a hurry?"" section about  and  in the docs.Run itRun the server with:$ uvicorn main:app --reload

INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
INFO:     Started reloader process [28720]
INFO:     Started server process [28722]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
The command  refers to:Check itOpen your browser at http://127.0.0.1:8000/items/5?q=somequery.You will see the JSON response as:{""item_id"": 5, ""q"": ""somequery""}
You already created an API that:Interactive API docsNow go to http://127.0.0.1:8000/docs.You will see the automatic interactive API documentation (provided by Swagger UI):Alternative API docsAnd now, go to http://127.0.0.1:8000/redoc.You will see the alternative automatic documentation (provided by ReDoc):Example upgradeNow modify the file  to receive a body from a  request.Declare the body using standard Python types, thanks to Pydantic.from typing import Union

from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()


class Item(BaseModel):
    name: str
    price: float
    is_offer: Union[bool, None] = None


@app.get(""/"")
def read_root():
    return {""Hello"": ""World""}


@app.get(""/items/{item_id}"")
def read_item(item_id: int, q: Union[str, None] = None):
    return {""item_id"": item_id, ""q"": q}


@app.put(""/items/{item_id}"")
def update_item(item_id: int, item: Item):
    return {""item_name"": item.name, ""item_id"": item_id}
The server should reload automatically (because you added  to the  command above).Interactive API docs upgradeNow go to http://127.0.0.1:8000/docs.Alternative API docs upgradeAnd now, go to http://127.0.0.1:8000/redoc.RecapIn summary, you declare once the types of parameters, body, etc. as function parameters.You do that with standard modern Python types.You don't have to learn a new syntax, the methods or classes of a specific library, etc.Just standard Python 3.8+.For example, for an :item_id: int
or for a more complex  model:item: Item
...and with that single declaration you get:Coming back to the previous code example, FastAPI will:We just scratched the surface, but you already get the idea of how it all works.Try changing the line with:    return {""item_name"": item.name, ""item_id"": item_id}
...from:        ... ""item_name"": item.name ...
...to:        ... ""item_price"": item.price ...
...and see how your editor will auto-complete the attributes and know their types:For a more complete example including more features, see the Tutorial - User Guide.Spoiler alert: the tutorial - user guide includes:PerformanceIndependent TechEmpower benchmarks show FastAPI applications running under Uvicorn as one of the fastest Python frameworks available, only below Starlette and Uvicorn themselves (used internally by FastAPI). (*)To understand more about it, see the section Benchmarks.Optional DependenciesUsed by Pydantic:Used by Starlette:Used by FastAPI / Starlette:You can install all of these with .LicenseThis project is licensed under the terms of the MIT license."
https://github.com/PokeAPI/pokeapi,The Pokémon API,"A RESTful API for Pokémon - Setup &nbsp; Database setupmake build-db
Visit  to see the running API!Each time the build script is run, it will iterate over each table in the database, wipe it, and rewrite each row using the data found in data/v2/csv.The option to build individual portions of the database was removed in order to increase performance of the build script.If you ever need to wipe the database use this command:make wipe_db
Docker and Compose &nbsp; There is also a multi-container setup, managed by . This setup allows you to deploy a production-like environment, with separate containers for each services and is recommended if you need to simply spin up PokéAPI.Start everything bymake docker-setup
If you don't have  on your machine you can use the following commandsdocker-compose up -d
docker-compose exec -T app python manage.py migrate --settings=config.docker-compose
docker-compose exec -T app sh -c 'echo ""from data.v2.build import build_all; build_all()"" | python manage.py shell --settings=config.docker-compose'
Browse  or  on port .GraphQL &nbsp; When you start PokéAPI with the above docker-compose setup, an  server is started as well. It's possible to track all the PokeAPI tables and foreign keys by simply# hasura cli needs to be installed and available in your $PATH: https://hasura.io/docs/latest/graphql/core/hasura-cli/install-hasura-cli.html
# hasura cli's version has to be v2.0.8
make hasura-apply
When finished browse http://localhost:8080 and you will find the admin console. The GraphQL endpoint will be hosted at http://localhost:8080/v1/graphql.A free public GraphiQL console is browsable at the address https://beta.pokeapi.co/graphql/console/. The relative GraphQL endpoint is accessible at https://beta.pokeapi.co/graphql/v1betaA set of examples are provided in the directory  of this repository.Kubernetes &nbsp;  files are provided in the folder https://github.com/PokeAPI/pokeapi/tree/master/Resources/k8s/kustomize/base/. Create and change your secrets:cp Resources/k8s/kustomize/base/secrets/postgres.env.sample Resources/k8s/kustomize/base/secrets/postgres.env
cp Resources/k8s/kustomize/base/secrets/graphql.env.sample Resources/k8s/kustomize/base/secrets/graphql.env
cp Resources/k8s/kustomize/base/config/pokeapi.env.sample Resources/k8s/kustomize/base/config/pokeapi.env
# Edit the newly created files
Configure  to point to a cluster and then run the following commands to start a PokéAPI service.kubectl apply -k Resources/k8s/kustomize/base/
kubectl config set-context --current --namespace pokeapi # (Optional) Set pokeapi ns as the working ns
# Wait for the cluster to spin up
kubectl exec --namespace pokeapi deployment/pokeapi -- python manage.py migrate --settings=config.docker-compose # Migrate the DB
kubectl exec --namespace pokeapi deployment/pokeapi -- sh -c 'echo ""from data.v2.build import build_all; build_all()"" | python manage.py shell --settings=config.docker-compose' # Build the db
kubectl wait --namespace pokeapi --timeout=120s --for=condition=complete job/load-graphql # Wait for Graphql configuration job to finish
This k8s setup creates all k8s resources inside the Namespace , run  to delete them. It also creates a Service of type  which is exposed on port  and . Data is persisted on  of  volumes.Wrappers| Official wrapper | Repository | Features || --- | --- | --- || Node server-side |  | Auto caching || Browser client-side |  | Auto caching, Image caching || Java/Kotlin |  | || Python 2/3 |  | Auto caching || Python 3 |  | Auto caching, Image caching || Wrapper | Repository | Features || --- | --- | --- || PHP |  | Auto caching, lazy loading || Ruby |  | || .Net Standard |  | Auto caching || Go |  | Auto caching || Dart |  | || Rust |  | Auto caching || Spring Boot |  | Auto caching || Swift |  | || Typescript server-side/client-side |  | Auto caching || Python |  | Auto caching, asynchronous| Scala |  | Auto caching |DonationsHelp to keep PokéAPI running! If you're using PokéAPI as a teaching resource or for a project, consider sending us a $10 donation to help keep the service up. We get 330 million requests a month!Thank you to all our backers! Join Us On Slack!Have a question or just want to discuss new ideas and improvements? Hit us up on Slack. ~~Consider talking with us here before creating a new issue.~~This way we can keep issues here a bit more organized and helpful in the long run. Be excellent to each other :smile: easily!Once you've signed up visit ContributingThis project exists thanks to all the people who All contributions are welcome: bug fixes, data contributions, recommendations.Please see the  before you submit a pull request or raise an issue, someone else might have beat you to it.To contribute to this repository:Simple!DeprecationAs of October 2018, the v1 API has been removed from PokéAPI. For more information, see ."
https://github.com/vishnubob/wait-for-it,Pure bash script to test and wait on the availability of a TCP host and port,"wait-for-it is a pure bash script that will wait on the availability of ahost and TCP port.  It is useful for synchronizing the spin-up ofinterdependent services, such as linked docker containers.  Since it is a purebash script, it does not have any external dependencies.Usagewait-for-it.sh host:port [-s] [-t timeout] [-- command args]
-h HOST | --host=HOST       Host or IP under test
-p PORT | --port=PORT       TCP port under test
                            Alternatively, you specify the host and port as host:port
-s | --strict               Only execute subcommand if the test succeeds
-q | --quiet                Don't output any status messages
-t TIMEOUT | --timeout=TIMEOUT
                            Timeout in seconds, zero for no timeout
-- COMMAND ARGS             Execute command with args after the test finishes
ExamplesFor example, let's test to see if we can access port 80 on ,and if it is available, echo the message .$ ./wait-for-it.sh www.google.com:80 -- echo ""google is up""
wait-for-it.sh: waiting 15 seconds for www.google.com:80
wait-for-it.sh: www.google.com:80 is available after 0 seconds
google is up
You can set your own timeout with the  or  option.  Settingthe timeout value to 0 will disable the timeout:$ ./wait-for-it.sh -t 0 www.google.com:80 -- echo ""google is up""
wait-for-it.sh: waiting for www.google.com:80 without a timeout
wait-for-it.sh: www.google.com:80 is available after 0 seconds
google is up
The subcommand will be executed regardless if the service is up or not.  If youwish to execute the subcommand only if the service is up, add the argument. In this example, we will test port 81 on  which willfail:$ ./wait-for-it.sh www.google.com:81 --timeout=1 --strict -- echo ""google is up""
wait-for-it.sh: waiting 1 seconds for www.google.com:81
wait-for-it.sh: timeout occurred after waiting 1 seconds for www.google.com:81
wait-for-it.sh: strict mode, refusing to execute subprocess
If you don't want to execute a subcommand, leave off the  argument.  Thisway, you can test the exit condition of  in your own scripts,and determine how to proceed:$ ./wait-for-it.sh www.google.com:80
wait-for-it.sh: waiting 15 seconds for www.google.com:80
wait-for-it.sh: www.google.com:80 is available after 0 seconds
$ echo $?
0
$ ./wait-for-it.sh www.google.com:81
wait-for-it.sh: waiting 15 seconds for www.google.com:81
wait-for-it.sh: timeout occurred after waiting 15 seconds for www.google.com:81
$ echo $?
124
CommunityDebian: There is a ."
https://github.com/ActivityWatch/activitywatch,"The best free and open-source automated time tracker. Cross-platform, extensible, privacy-focused.","Do you want to receive email updates on major announcements?[<marko.inline.Link object at 0x000001592FD37948>, <marko.inline.RawText object at 0x000001592FD37AC8>]AboutThe goal of ActivityWatch is simple: Enable the collection of as much valuable lifedata as possible without compromising user privacy.We've worked towards this goal by creating an application for safe storage of the data on the user's local machine and as well as a set of watchers which record data such as:It is up to you as user to collect as much as you want, or as little as you want (and we hope some of you will help write watchers so we can collect more).ScreenshotsYou can find more (and newer) screenshots on .Installation & UsageDownloads are available on the .For instructions on how to get started, please see the .Interested in building from source? .Is this yet another time tracker?Yes, but we found that most time trackers lack one or more important features.Common dealbreakers:To sum it up:We have a plan to address all of these and we're well on our way. See the table below for our progress.Feature comparisonBasics|               | User owns data     | GUI                | Sync                       | Open Source        || ------------- |:------------------:|:------------------:|:--------------------------:|:------------------:|| ActivityWatch | :white_check_mark: | :white_check_mark: | , decentralized | :white_check_mark: ||        | :white_check_mark: | :x:                | :x:                        | :white_check_mark: ||         | :white_check_mark: | :white_check_mark: | :x:                        | :white_check_mark: ||     | :x:                | :white_check_mark: | Centralized                | :x:                ||       | :x:                | :white_check_mark: | Centralized                | Clients            |Platforms|               | Windows            | macOS              | Linux              | Android            | iOS                 || ------------- |:------------------:|:------------------:|:------------------:|:------------------:|:-------------------:|| ActivityWatch | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |:x:                  || Selfspy       | :white_check_mark: | :white_check_mark: | :white_check_mark: | :x:                |:x:                  || ulogme        | :x:                | :white_check_mark: | :white_check_mark: | :x:                |:x:                  || RescueTime    | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: |Limited functionality|Tracking|               | App & Window Title | AFK                | Browser Extensions | Editor Plugins     | Extensible            || ------------- |:------------------:|:------------------:|:------------------:|:------------------:|:---------------------:|| ActivityWatch | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark: | :white_check_mark:    || Selfspy       | :white_check_mark: | :white_check_mark: | :x:                | :x:                | :x:                   || ulogme        | :white_check_mark: | :white_check_mark: | :x:                | :x:                | :x:                   || RescueTime    | :white_check_mark: | :white_check_mark: | :white_check_mark: | :x:                | :x:                   || WakaTime      | :x:                | :white_check_mark: | :white_check_mark: | :white_check_mark: | Only for text editors |For a complete list of the things ActivityWatch can track, .About this repositoryThis repo is a bundle of the core components and official modules of ActivityWatch (managed with ). Its primary use is as a meta-package providing all the components in one repo; enabling easier packaging and installation. It is also where releases of the full suite are published (see ).Server is the official implementation of the core service which the other ActivityWatch services interact with. It provides a REST API to a datastore and query engine. It also serves the web interface developed in the  project (which provides the frontend part of the webapp).The REST API includes:The webapp includes:WatchersActivityWatch comes pre-installed with two watchers:There are lots of other watchers for ActivityWatch which can track more types of activity. Like  which tracks time spent on websites, multiple editor watchers which track spent time coding, and many more! A full list of watchers can be found in .LibrariesFolder structureContributingWant to help? Great! Check out the !Questions and supportHave a question, suggestion, problem, or just want to say hi? Post on !"
https://github.com/jorgebastida/awslogs,AWS CloudWatch logs for Humans™,"awslogs.. image:: https://badge.fury.io/py/awslogs.png:target: http://badge.fury.io/py/awslogs.. image:: https://travis-ci.org/jorgebastida/awslogs.png?branch=master:target: https://travis-ci.org/jorgebastida/awslogs.. image:: https://coveralls.io/repos/jorgebastida/awslogs/badge.svg:target: https://coveralls.io/r/jorgebastida/awslogs is a simple command line tool for querying groups, streams and events from _ logs.One of the most powerful features is to query events from several streams and consume them (ordered) in pseudo-realtime using your favourite tools such as ::$ awslogs get /var/log/syslog ip-10-1.* --start='2h ago' | grep ERROR
FeaturesExampleRunning:  will return you events from any  in the  group generated in the last day... image:: https://github.com/jorgebastida/awslogs/raw/master/media/screenshot.pngInstallationYou can easily install  using ::$ pip install awslogsIf you are on OSX El Capitan, use the following (Why? Check Donald Stufft's comment _) ::$ pip install awslogs --ignore-installed sixYou can also install it with _::$ brew install awslogsOptionsNote: You need to provide to all these options a valid AWS region using  or  env variable.Time optionsWhile querying for logs you can filter events by   and   date.Filter optionsYou can use  if you want to only retrieve logs which match one CloudWatch Logs Filter pattern.This is helpful if you know precisely what you are looking for, and don't want to download the entire stream.For example, if you only want to download only the report events from a Lambda stream you can run::$ awslogs get my_lambda_group --filter-pattern=""[r=REPORT,...]""Full documentation of how to write patterns: http://docs.aws.amazon.com/AmazonCloudWatch/latest/DeveloperGuide/FilterAndPatternSyntax.htmlJSON logsIn a similar way than the _ command, you can use   tofilter each of your json log lines and extract certain fields::$ awslogs get my_lambda_group --query=messageThis will only display the  field for each of the json log lines.Using third-party endpointsIf you use tools like localstack, fakes3 or other, consider to change boto3 endpoint using  or  env variable.AWS IAM PermissionsThe required permissions to run  are contained within the _ AWS managed permissions.As of 2020-01-13, these are the permissions::{
    ""Version"": ""2012-10-17"",
    ""Statement"": [
        {
            ""Action"": [
                ""logs:Describe*"",
                ""logs:Get*"",
                ""logs:List*"",
                ""logs:StartQuery"",
                ""logs:StopQuery"",
                ""logs:TestMetricFilter"",
                ""logs:FilterLogEvents""
            ],
            ""Effect"": ""Allow"",
            ""Resource"": ""*""
        }
    ]
}
ContributeFor more instructions see .Helpful LinksHow to provide AWS credentials to awslogsAlthough, the most straightforward thing to do might be use  and , this will eventually become a pain in the ass."
https://github.com/oreilly-japan/deep-learning-from-scratch,"『ゼロから作る Deep Learning』(O'Reilly Japan, 2016)","ゼロから作る Deep Learning書籍『』(オライリー・ジャパン発行)のサポートサイトです。本書籍で使用するソースコードがまとめられています。ファイル構成|フォルダ名 |説明                         ||:--        |:--                          ||ch01       |1章で使用するソースコード    ||ch02       |2章で使用するソースコード    ||...        |...                          ||ch08       |8章で使用するソースコード    ||common     |共通で使用するソースコード   ||dataset    |データセット用のソースコード |ソースコードの解説は本書籍をご覧ください。Pythonと外部ライブラリソースコードを実行するには、下記のソフトウェアが必要です。※Pythonのバージョンは、3系を利用します。実行方法各章のフォルダへ移動して、Pythonコマンドを実行します。$ cd ch01
$ python man.py

$ cd ../ch05
$ python train_nueralnet.py
クラウドサービスでの実行本書のコードは次の表にあるボタンをクリックすることで、AWSの無料の計算環境である上に実行できます(事前にが必要です)。SageMaker Studio Labの使い方はをご覧ください。で最新情報が取得できます。|フォルダ名 |Amazon SageMaker Studio Lab|:--        |:--                          ||ch01       |||ch02       |||ch03       |||ch04       |||ch05       |||ch06       |||ch07       |||ch08       |||common       ||ライセンス本リポジトリのソースコードはです。商用・非商用問わず、自由にご利用ください。正誤表本書の正誤情報は以下のページで公開しています。https://github.com/oreilly-japan/deep-learning-from-scratch/wiki/errata本ページに掲載されていない誤植など間違いを見つけた方は、までお知らせください。"
https://github.com/injetlee/Python,Python脚本。模拟登录知乎， 爬虫，操作excel，微信公众号，远程开机,"欢迎关注我的微信公众号【智能制造社区】左手代码，右手制造，分享智能制造相关技术和业务，包括 Python, C#, 数据库，工业大数据、物联网技术及MES/ERP/SAP等系统。可以通过微信公众号加我好友内容列表SQL 数据库其他"
https://github.com/donnemartin/system-design-primer,Learn how to design large-scale systems. Prep for the system design interview.  Includes Anki flashcards.,"[<marko.inline.RawText object at 0x0000015930037688>]Help The System Design PrimerMotivationLearn how to design large-scale systemsLearning how to design scalable systems will help you become a better engineer.System design is a broad topic.  There is a vast amount of resources scattered throughout the web on system design principles.This repo is an organized collection of resources to help you learn how to build systems at scale.Learn from the open source communityThis is a continually updated, open source project. are welcome!Prep for the system design interviewIn addition to coding interviews, system design is a required component of the technical interview process at many tech companies.Practice common system design interview questions and compare your results with sample solutions: discussions, code, and diagrams.Additional topics for interview prep:Anki flashcardsThe provided  use spaced repetition to help you retain key system design concepts.Great for use while on-the-go.Coding Resource: Interactive Coding ChallengesLooking for resources to help you prep for the ?Check out the sister repo , which contains an additional Anki deck:ContributingFeel free to submit pull requests to help:Content that needs some polishing is placed .Review the .Index of system design topicsStudy guideQ: For interviews, do I need to know everything here?A: No, you don't need to know everything here to prepare for the interview.What you are asked in an interview depends on variables such as:More experienced candidates are generally expected to know more about system design.  Architects or team leads might be expected to know more than individual contributors.  Top tech companies are likely to have one or more design interview rounds.Start broad and go deeper in a few areas.  It helps to know a little about various key system design topics.  Adjust the following guide based on your timeline, experience, what positions you are interviewing for, and which companies you are interviewing with.| | Short | Medium | Long ||---|---|---|---|| Read through the  to get a broad understanding of how systems work | :+1: | :+1: | :+1: || Read through a few articles in the  for the companies you are interviewing with | :+1: | :+1: | :+1: || Read through a few  | :+1: | :+1: | :+1: || Review  | :+1: | :+1: | :+1: || Work through  | Some | Many | Most || Work through  | Some | Many | Most || Review  | Some | Many | Most |How to approach a system design interview questionThe system design interview is an open-ended conversation.  You are expected to lead it.You can use the following steps to guide the discussion.  To help solidify this process, work through the  section using the following steps.Step 1: Outline use cases, constraints, and assumptionsGather requirements and scope the problem.  Ask questions to clarify use cases and constraints.  Discuss assumptions.Step 2: Create a high level designOutline a high level design with all important components.Step 3: Design core componentsDive into details for each core component.  For example, if you were asked to , discuss:Step 4: Scale the designIdentify and address bottlenecks, given the constraints.  For example, do you need the following to address scalability issues?Discuss potential solutions and trade-offs.  Everything is a trade-off.  Address bottlenecks using .Back-of-the-envelope calculationsYou might be asked to do some estimates by hand.  Refer to the  for the following resources:Source(s) and further readingCheck out the following links to get a better idea of what to expect:System design interview questions with solutions| Question | ||---|---|| Design Pastebin.com (or Bit.ly) |  || Design the Twitter timeline and search (or Facebook feed and search) |  || Design a web crawler |  || Design Mint.com |  || Design the data structures for a social network |  || Design a key-value store for a search engine |  || Design Amazon's sales ranking by category feature |  || Design a system that scales to millions of users on AWS |  || Add a system design question |  |Design Pastebin.com (or Bit.ly)Design the Twitter timeline and search (or Facebook feed and search)Design a web crawlerDesign Mint.comDesign the data structures for a social networkDesign a key-value store for a search engineDesign Amazon's sales ranking by category featureDesign a system that scales to millions of users on AWSObject-oriented design interview questions with solutions| Question | ||---|---|| Design a hash map |   || Design a least recently used cache |   || Design a call center |   || Design a deck of cards |   || Design a parking lot |   || Design a chat server |   || Design a circular array |   || Add an object-oriented design question |  |System design topics: start hereNew to system design?First, you'll need a basic understanding of common principles, learning about what they are, how they are used, and their pros and cons.Step 1: Review the scalability video lectureStep 2: Review the scalability articleNext stepsNext, we'll look at high-level trade-offs:Keep in mind that everything is a trade-off.Then we'll dive into more specific topics such as DNS, CDNs, and load balancers.Performance vs scalabilityA service is scalable if it results in increased performance in a manner proportional to resources added. Generally, increasing performance means serving more units of work, but it can also be to handle larger units of work, such as when datasets grow.1Another way to look at performance vs scalability:Source(s) and further readingLatency vs throughputLatency is the time to perform some action or to produce some result.Throughput is the number of such actions or results per unit of time.Generally, you should aim for maximal throughput with acceptable latency.Source(s) and further readingAvailability vs consistencyCAP theoremIn a distributed computer system, you can only support two of the following guarantees:Networks aren't reliable, so you'll need to support partition tolerance.  You'll need to make a software tradeoff between consistency and availability.CP - consistency and partition toleranceWaiting for a response from the partitioned node might result in a timeout error.  CP is a good choice if your business needs require atomic reads and writes.AP - availability and partition toleranceResponses return the most readily available version of the data available on any node, which might not be the latest.  Writes might take some time to propagate when the partition is resolved.AP is a good choice if the business needs to allow for  or when the system needs to continue working despite external errors.Source(s) and further readingConsistency patternsWith multiple copies of the same data, we are faced with options on how to synchronize them so clients have a consistent view of the data.  Recall the definition of consistency from the  - Every read receives the most recent write or an error.Weak consistencyAfter a write, reads may or may not see it.  A best effort approach is taken.This approach is seen in systems such as memcached.  Weak consistency works well in real time use cases such as VoIP, video chat, and realtime multiplayer games.  For example, if you are on a phone call and lose reception for a few seconds, when you regain connection you do not hear what was spoken during connection loss.Eventual consistencyAfter a write, reads will eventually see it (typically within milliseconds).  Data is replicated asynchronously.This approach is seen in systems such as DNS and email.  Eventual consistency works well in highly available systems.Strong consistencyAfter a write, reads will see it.  Data is replicated synchronously.This approach is seen in file systems and RDBMSes.  Strong consistency works well in systems that need transactions.Source(s) and further readingAvailability patternsThere are two complementary patterns to support high availability: fail-over and replication.Fail-overActive-passiveWith active-passive fail-over, heartbeats are sent between the active and the passive server on standby.  If the heartbeat is interrupted, the passive server takes over the active's IP address and resumes service.The length of downtime is determined by whether the passive server is already running in 'hot' standby or whether it needs to start up from 'cold' standby.  Only the active server handles traffic.Active-passive failover can also be referred to as master-slave failover.Active-activeIn active-active, both servers are managing traffic, spreading the load between them.If the servers are public-facing, the DNS would need to know about the public IPs of both servers.  If the servers are internal-facing, application logic would need to know about both servers.Active-active failover can also be referred to as master-master failover.Disadvantage(s): failoverReplicationMaster-slave and master-masterThis topic is further discussed in the  section:Availability in numbersAvailability is often quantified by uptime (or downtime) as a percentage of time the service is available.  Availability is generally measured in number of 9s--a service with 99.99% availability is described as having four 9s.99.9% availability - three 9s| Duration            | Acceptable downtime||---------------------|--------------------|| Downtime per year   | 8h 45min 57s       || Downtime per month  | 43m 49.7s          || Downtime per week   | 10m 4.8s           || Downtime per day    | 1m 26.4s           |99.99% availability - four 9s| Duration            | Acceptable downtime||---------------------|--------------------|| Downtime per year   | 52min 35.7s        || Downtime per month  | 4m 23s             || Downtime per week   | 1m 5s              || Downtime per day    | 8.6s               |Availability in parallel vs in sequenceIf a service consists of multiple components prone to failure, the service's overall availability depends on whether the components are in sequence or in parallel.In sequenceOverall availability decreases when two components with availability < 100% are in sequence:Availability (Total) = Availability (Foo) * Availability (Bar)
If both  and  each had 99.9% availability, their total availability in sequence would be 99.8%.In parallelOverall availability increases when two components with availability < 100% are in parallel:Availability (Total) = 1 - (1 - Availability (Foo)) * (1 - Availability (Bar))
If both  and  each had 99.9% availability, their total availability in parallel would be 99.9999%.Domain name systemA Domain Name System (DNS) translates a domain name such as www.example.com to an IP address.DNS is hierarchical, with a few authoritative servers at the top level.  Your router or ISP provides information about which DNS server(s) to contact when doing a lookup.  Lower level DNS servers cache mappings, which could become stale due to DNS propagation delays.  DNS results can also be cached by your browser or OS for a certain period of time, determined by the .Services such as  and  provide managed DNS services.  Some DNS services can route traffic through various methods:Disadvantage(s): DNSSource(s) and further readingContent delivery networkA content delivery network (CDN) is a globally distributed network of proxy servers, serving content from locations closer to the user.  Generally, static files such as HTML/CSS/JS, photos, and videos are served from CDN, although some CDNs such as Amazon's CloudFront support dynamic content.  The site's DNS resolution will tell clients which server to contact.Serving content from CDNs can significantly improve performance in two ways:Push CDNsPush CDNs receive new content whenever changes occur on your server.  You take full responsibility for providing content, uploading directly to the CDN and rewriting URLs to point to the CDN.  You can configure when content expires and when it is updated.  Content is uploaded only when it is new or changed, minimizing traffic, but maximizing storage.Sites with a small amount of traffic or sites with content that isn't often updated work well with push CDNs.  Content is placed on the CDNs once, instead of being re-pulled at regular intervals.Pull CDNsPull CDNs grab new content from your server when the first user requests the content.  You leave the content on your server and rewrite URLs to point to the CDN.  This results in a slower request until the content is cached on the CDN.A  determines how long content is cached.  Pull CDNs minimize storage space on the CDN, but can create redundant traffic if files expire and are pulled before they have actually changed.Sites with heavy traffic work well with pull CDNs, as traffic is spread out more evenly with only recently-requested content remaining on the CDN.Disadvantage(s): CDNSource(s) and further readingLoad balancerLoad balancers distribute incoming client requests to computing resources such as application servers and databases.  In each case, the load balancer returns the response from the computing resource to the appropriate client.  Load balancers are effective at:Load balancers can be implemented with hardware (expensive) or with software such as HAProxy.Additional benefits include:To protect against failures, it's common to set up multiple load balancers, either in  or  mode.Load balancers can route traffic based on various metrics, including:Layer 4 load balancingLayer 4 load balancers look at info at the  to decide how to distribute requests.  Generally, this involves the source, destination IP addresses, and ports in the header, but not the contents of the packet.  Layer 4 load balancers forward network packets to and from the upstream server, performing .Layer 7 load balancingLayer 7 load balancers look at the  to decide how to distribute requests.  This can involve contents of the header, message, and cookies.  Layer 7 load balancers terminate network traffic, reads the message, makes a load-balancing decision, then opens a connection to the selected server.  For example, a layer 7 load balancer can direct video traffic to servers that host videos while directing more sensitive user billing traffic to security-hardened servers.At the cost of flexibility, layer 4 load balancing requires less time and computing resources than Layer 7, although the performance impact can be minimal on modern commodity hardware.Horizontal scalingLoad balancers can also help with horizontal scaling, improving performance and availability.  Scaling out using commodity machines is more cost efficient and results in higher availability than scaling up a single server on more expensive hardware, called Vertical Scaling.  It is also easier to hire for talent working on commodity hardware than it is for specialized enterprise systems.Disadvantage(s): horizontal scalingDisadvantage(s): load balancerSource(s) and further readingReverse proxy (web server)A reverse proxy is a web server that centralizes internal services and provides unified interfaces to the public.  Requests from clients are forwarded to a server that can fulfill it before the reverse proxy returns the server's response to the client.Additional benefits include:Load balancer vs reverse proxyDisadvantage(s): reverse proxySource(s) and further readingApplication layerSeparating out the web layer from the application layer (also known as platform layer) allows you to scale and configure both layers independently.  Adding a new API results in adding application servers without necessarily adding additional web servers.  The single responsibility principle advocates for small and autonomous services that work together.  Small teams with small services can plan more aggressively for rapid growth.Workers in the application layer also help enable .MicroservicesRelated to this discussion are , which can be described as a suite of independently deployable, small, modular services.  Each service runs a unique process and communicates through a well-defined, lightweight mechanism to serve a business goal. 1Pinterest, for example, could have the following microservices: user profile, follower, feed, search, photo upload, etc.Service DiscoverySystems such as , , and  can help services find each other by keeping track of registered names, addresses, and ports.   help verify service integrity and are often done using an  endpoint.  Both Consul and Etcd have a built in  that can be useful for storing config values and other shared data.Disadvantage(s): application layerSource(s) and further readingDatabaseRelational database management system (RDBMS)A relational database like SQL is a collection of data items organized in tables.ACID is a set of properties of relational database .There are many techniques to scale a relational database: master-slave replication, master-master replication, federation, sharding, denormalization, and SQL tuning.Master-slave replicationThe master serves reads and writes, replicating writes to one or more slaves, which serve only reads.  Slaves can also replicate to additional slaves in a tree-like fashion.  If the master goes offline, the system can continue to operate in read-only mode until a slave is promoted to a master or a new master is provisioned.Disadvantage(s): master-slave replicationMaster-master replicationBoth masters serve reads and writes and coordinate with each other on writes.  If either master goes down, the system can continue to operate with both reads and writes.Disadvantage(s): master-master replicationDisadvantage(s): replicationSource(s) and further reading: replicationFederationFederation (or functional partitioning) splits up databases by function.  For example, instead of a single, monolithic database, you could have three databases: forums, users, and products, resulting in less read and write traffic to each database and therefore less replication lag.  Smaller databases result in more data that can fit in memory, which in turn results in more cache hits due to improved cache locality.  With no single central master serializing writes you can write in parallel, increasing throughput.Disadvantage(s): federationSource(s) and further reading: federationShardingSharding distributes data across different databases such that each database can only manage a subset of the data.  Taking a users database as an example, as the number of users increases, more shards are added to the cluster.Similar to the advantages of , sharding results in less read and write traffic, less replication, and more cache hits.  Index size is also reduced, which generally improves performance with faster queries.  If one shard goes down, the other shards are still operational, although you'll want to add some form of replication to avoid data loss.  Like federation, there is no single central master serializing writes, allowing you to write in parallel with increased throughput.Common ways to shard a table of users is either through the user's last name initial or the user's geographic location.Disadvantage(s): shardingSource(s) and further reading: shardingDenormalizationDenormalization attempts to improve read performance at the expense of some write performance.  Redundant copies of the data are written in multiple tables to avoid expensive joins.  Some RDBMS such as  and Oracle support  which handle the work of storing redundant information and keeping redundant copies consistent.Once data becomes distributed with techniques such as  and , managing joins across data centers further increases complexity.  Denormalization might circumvent the need for such complex joins.In most systems, reads can heavily outnumber writes 100:1 or even 1000:1.  A read resulting in a complex database join can be very expensive, spending a significant amount of time on disk operations.Disadvantage(s): denormalizationSource(s) and further reading: denormalizationSQL tuningSQL tuning is a broad topic and many  have been written as reference.It's important to benchmark and profile to simulate and uncover bottlenecks.Benchmarking and profiling might point you to the following optimizations.Tighten up the schemaUse good indicesAvoid expensive joinsPartition tablesTune the query cacheSource(s) and further reading: SQL tuningNoSQLNoSQL is a collection of data items represented in a key-value store, document store, wide column store, or a graph database.  Data is denormalized, and joins are generally done in the application code.  Most NoSQL stores lack true ACID transactions and favor .BASE is often used to describe the properties of NoSQL databases.  In comparison with the , BASE chooses availability over consistency.In addition to choosing between , it is helpful to understand which type of NoSQL database best fits your use case(s).  We'll review key-value stores, document stores, wide column stores, and graph databases in the next section.Key-value storeA key-value store generally allows for O(1) reads and writes and is often backed by memory or SSD.  Data stores can maintain keys in , allowing efficient retrieval of key ranges.  Key-value stores can allow for storing of metadata with a value.Key-value stores provide high performance and are often used for simple data models or for rapidly-changing data, such as an in-memory cache layer.  Since they offer only a limited set of operations, complexity is shifted to the application layer if additional operations are needed.A key-value store is the basis for more complex systems such as a document store, and in some cases, a graph database.Source(s) and further reading: key-value storeDocument storeA document store is centered around documents (XML, JSON, binary, etc), where a document stores all information for a given object.  Document stores provide APIs or a query language to query based on the internal structure of the document itself.  Note, many key-value stores include features for working with a value's metadata, blurring the lines between these two storage types.Based on the underlying implementation, documents are organized by collections, tags, metadata, or directories.  Although documents can be organized or grouped together, documents may have fields that are completely different from each other.Some document stores like  and  also provide a SQL-like language to perform complex queries.   supports both key-values and documents.Document stores provide high flexibility and are often used for working with occasionally changing data.Source(s) and further reading: document storeWide column storeA wide column store's basic unit of data is a column (name/value pair).  A column can be grouped in column families (analogous to a SQL table).  Super column families further group column families.  You can access each column independently with a row key, and columns with the same row key form a row.  Each value contains a timestamp for versioning and for conflict resolution.Google introduced  as the first wide column store, which influenced the open-source  often-used in the Hadoop ecosystem, and  from Facebook.  Stores such as BigTable, HBase, and Cassandra maintain keys in lexicographic order, allowing efficient retrieval of selective key ranges.Wide column stores offer high availability and high scalability.  They are often used for very large data sets.Source(s) and further reading: wide column storeGraph databaseIn a graph database, each node is a record and each arc is a relationship between two nodes.  Graph databases are optimized to represent complex relationships with many foreign keys or many-to-many relationships.Graphs databases offer high performance for data models with complex relationships, such as a social network.  They are relatively new and are not yet widely-used; it might be more difficult to find development tools and resources.  Many graphs can only be accessed with .Source(s) and further reading: graphSource(s) and further reading: NoSQLSQL or NoSQLReasons for SQL:Reasons for NoSQL:Sample data well-suited for NoSQL:Source(s) and further reading: SQL or NoSQLCacheCaching improves page load times and can reduce the load on your servers and databases.  In this model, the dispatcher will first lookup if the request has been made before and try to find the previous result to return, in order to save the actual execution.Databases often benefit from a uniform distribution of reads and writes across its partitions.  Popular items can skew the distribution, causing bottlenecks.  Putting a cache in front of a database can help absorb uneven loads and spikes in traffic.Client cachingCaches can be located on the client side (OS or browser), , or in a distinct cache layer.CDN caching are considered a type of cache.Web server caching and caches such as  can serve static and dynamic content directly.  Web servers can also cache requests, returning responses without having to contact application servers.Database cachingYour database usually includes some level of caching in a default configuration, optimized for a generic use case.  Tweaking these settings for specific usage patterns can further boost performance.Application cachingIn-memory caches such as Memcached and Redis are key-value stores between your application and your data storage.  Since the data is held in RAM, it is much faster than typical databases where data is stored on disk.  RAM is more limited than disk, so  algorithms such as  can help invalidate 'cold' entries and keep 'hot' data in RAM.Redis has the following additional features:There are multiple levels you can cache that fall into two general categories: database queries and objects:Generally, you should try to avoid file-based caching, as it makes cloning and auto-scaling more difficult.Caching at the database query levelWhenever you query the database, hash the query as a key and store the result to the cache.  This approach suffers from expiration issues:Caching at the object levelSee your data as an object, similar to what you do with your application code.  Have your application assemble the dataset from the database into a class instance or a data structure(s):Suggestions of what to cache:When to update the cacheSince you can only store a limited amount of data in cache, you'll need to determine which cache update strategy works best for your use case.Cache-asideThe application is responsible for reading and writing from storage.  The cache does not interact with storage directly.  The application does the following:def get_user(self, user_id):
    user = cache.get(""user.{0}"", user_id)
    if user is None:
        user = db.query(""SELECT * FROM users WHERE user_id = {0}"", user_id)
        if user is not None:
            key = ""user.{0}"".format(user_id)
            cache.set(key, json.dumps(user))
    return user
 is generally used in this manner.Subsequent reads of data added to cache are fast.  Cache-aside is also referred to as lazy loading.  Only requested data is cached, which avoids filling up the cache with data that isn't requested.Disadvantage(s): cache-asideWrite-throughThe application uses the cache as the main data store, reading and writing data to it, while the cache is responsible for reading and writing to the database:Application code:set_user(12345, {""foo"":""bar""})
Cache code:def set_user(user_id, values):
    user = db.query(""UPDATE Users WHERE id = {0}"", user_id, values)
    cache.set(user_id, user)
Write-through is a slow overall operation due to the write operation, but subsequent reads of just written data are fast.  Users are generally more tolerant of latency when updating data than reading data.  Data in the cache is not stale.Disadvantage(s): write throughWrite-behind (write-back)In write-behind, the application does the following:Disadvantage(s): write-behindRefresh-aheadYou can configure the cache to automatically refresh any recently accessed cache entry prior to its expiration.Refresh-ahead can result in reduced latency vs read-through if the cache can accurately predict which items are likely to be needed in the future.Disadvantage(s): refresh-aheadDisadvantage(s): cacheSource(s) and further readingAsynchronismAsynchronous workflows help reduce request times for expensive operations that would otherwise be performed in-line.  They can also help by doing time-consuming work in advance, such as periodic aggregation of data.Message queuesMessage queues receive, hold, and deliver messages.  If an operation is too slow to perform inline, you can use a message queue with the following workflow:The user is not blocked and the job is processed in the background.  During this time, the client might optionally do a small amount of processing to make it seem like the task has completed.  For example, if posting a tweet, the tweet could be instantly posted to your timeline, but it could take some time before your tweet is actually delivered to all of your followers.[<marko.inline.RawText object at 0x000001592FF20D08>] is useful as a simple message broker but messages can be lost.[<marko.inline.RawText object at 0x000001592FF20F48>] is popular but requires you to adapt to the 'AMQP' protocol and manage your own nodes.[<marko.inline.RawText object at 0x000001592FF4E1C8>] is hosted but can have high latency and has the possibility of messages being delivered twice.Task queuesTasks queues receive tasks and their related data, runs them, then delivers their results.  They can support scheduling and can be used to run computationally-intensive jobs in the background.[<marko.inline.RawText object at 0x000001592FF4E588>] has support for scheduling and primarily has python support.Back pressureIf queues start to grow significantly, the queue size can become larger than memory, resulting in cache misses, disk reads, and even slower performance.   can help by limiting the queue size, thereby maintaining a high throughput rate and good response times for jobs already in the queue.  Once the queue fills up, clients get a server busy or HTTP 503 status code to try again later.  Clients can retry the request at a later time, perhaps with .Disadvantage(s): asynchronismSource(s) and further readingCommunicationHypertext transfer protocol (HTTP)HTTP is a method for encoding and transporting data between a client and a server.  It is a request/response protocol: clients issue requests and servers issue responses with relevant content and completion status info about the request.  HTTP is self-contained, allowing requests and responses to flow through many intermediate routers and servers that perform load balancing, caching, encryption, and compression.A basic HTTP request consists of a verb (method) and a resource (endpoint).  Below are common HTTP verbs:| Verb | Description | Idempotent* | Safe | Cacheable ||---|---|---|---|---|| GET | Reads a resource | Yes | Yes | Yes || POST | Creates a resource or trigger a process that handles data | No | No | Yes if response contains freshness info || PUT | Creates or replace a resource | Yes | No | No || PATCH | Partially updates a resource | No | No | Yes if response contains freshness info || DELETE | Deletes a resource | Yes | No | No |*Can be called many times without different outcomes.HTTP is an application layer protocol relying on lower-level protocols such as TCP and UDP.Source(s) and further reading: HTTPTransmission control protocol (TCP)TCP is a connection-oriented protocol over an .  Connection is established and terminated using a .  All packets sent are guaranteed to reach the destination in the original order and without corruption through:If the sender does not receive a correct response, it will resend the packets.  If there are multiple timeouts, the connection is dropped.  TCP also implements  and .  These guarantees cause delays and generally result in less efficient transmission than UDP.To ensure high throughput, web servers can keep a large number of TCP connections open, resulting in high memory usage.  It can be expensive to have a large number of open connections between web server threads and say, a  server.   can help in addition to switching to UDP where applicable.TCP is useful for applications that require high reliability but are less time critical.  Some examples include web servers, database info, SMTP, FTP, and SSH.Use TCP over UDP when:User datagram protocol (UDP)UDP is connectionless.  Datagrams (analogous to packets) are guaranteed only at the datagram level.  Datagrams might reach their destination out of order or not at all.  UDP does not support congestion control.  Without the guarantees that TCP support, UDP is generally more efficient.UDP can broadcast, sending datagrams to all devices on the subnet.  This is useful with  because the client has not yet received an IP address, thus preventing a way for TCP to stream without the IP address.UDP is less reliable but works well in real time use cases such as VoIP, video chat, streaming, and realtime multiplayer games.Use UDP over TCP when:Source(s) and further reading: TCP and UDPRemote procedure call (RPC)In an RPC, a client causes a procedure to execute on a different address space, usually a remote server.  The procedure is coded as if it were a local procedure call, abstracting away the details of how to communicate with the server from the client program.  Remote calls are usually slower and less reliable than local calls so it is helpful to distinguish RPC calls from local calls.  Popular RPC frameworks include , , and .RPC is a request-response protocol:Sample RPC calls:GET /someoperation?data=anId

POST /anotheroperation
{
  ""data"":""anId"";
  ""anotherdata"": ""another value""
}
RPC is focused on exposing behaviors.  RPCs are often used for performance reasons with internal communications, as you can hand-craft native calls to better fit your use cases.Choose a native library (aka SDK) when:HTTP APIs following REST tend to be used more often for public APIs.Disadvantage(s): RPCRepresentational state transfer (REST)REST is an architectural style enforcing a client/server model where the client acts on a set of resources managed by the server.  The server provides a representation of resources and actions that can either manipulate or get a new representation of resources.  All communication must be stateless and cacheable.There are four qualities of a RESTful interface:Sample REST calls:GET /someresources/anId

PUT /someresources/anId
{""anotherdata"": ""another value""}
REST is focused on exposing data.  It minimizes the coupling between client/server and is often used for public HTTP APIs.  REST uses a more generic and uniform method of exposing resources through URIs, , and actions through verbs such as GET, POST, PUT, DELETE, and PATCH.  Being stateless, REST is great for horizontal scaling and partitioning.Disadvantage(s): RESTRPC and REST calls comparison| Operation | RPC | REST ||---|---|---|| Signup    | POST /signup | POST /persons || Resign    | POST /resign{""personid"": ""1234""} | DELETE /persons/1234 || Read a person | GET /readPerson?personid=1234 | GET /persons/1234 || Read a person’s items list | GET /readUsersItemsList?personid=1234 | GET /persons/1234/items || Add an item to a person’s items | POST /addItemToUsersItemsList{""personid"": ""1234"";""itemid"": ""456""} | POST /persons/1234/items{""itemid"": ""456""} || Update an item    | POST /modifyItem{""itemid"": ""456"";""key"": ""value""} | PUT /items/456{""key"": ""value""} || Delete an item | POST /removeItem{""itemid"": ""456""} | DELETE /items/456 |Source(s) and further reading: REST and RPCSecurityThis section could use some updates.  Consider !Security is a broad topic.  Unless you have considerable experience, a security background, or are applying for a position that requires knowledge of security, you probably won't need to know more than the basics:Source(s) and further readingAppendixYou'll sometimes be asked to do 'back-of-the-envelope' estimates.  For example, you might need to determine how long it will take to generate 100 image thumbnails from disk or how much memory a data structure will take.  The Powers of two table and Latency numbers every programmer should know are handy references.Powers of two tablePower           Exact Value         Approx Value        Bytes
---------------------------------------------------------------
7                             128
8                             256
10                           1024   1 thousand           1 KB
16                         65,536                       64 KB
20                      1,048,576   1 million            1 MB
30                  1,073,741,824   1 billion            1 GB
32                  4,294,967,296                        4 GB
40              1,099,511,627,776   1 trillion           1 TB
Source(s) and further readingLatency numbers every programmer should knowLatency Comparison Numbers
--------------------------
L1 cache reference                           0.5 ns
Branch mispredict                            5   ns
L2 cache reference                           7   ns                      14x L1 cache
Mutex lock/unlock                           25   ns
Main memory reference                      100   ns                      20x L2 cache, 200x L1 cache
Compress 1K bytes with Zippy            10,000   ns       10 us
Send 1 KB bytes over 1 Gbps network     10,000   ns       10 us
Read 4 KB randomly from SSD*           150,000   ns      150 us          ~1GB/sec SSD
Read 1 MB sequentially from memory     250,000   ns      250 us
Round trip within same datacenter      500,000   ns      500 us
Read 1 MB sequentially from SSD*     1,000,000   ns    1,000 us    1 ms  ~1GB/sec SSD, 4X memory
HDD seek                            10,000,000   ns   10,000 us   10 ms  20x datacenter roundtrip
Read 1 MB sequentially from 1 Gbps  10,000,000   ns   10,000 us   10 ms  40x memory, 10X SSD
Read 1 MB sequentially from HDD     30,000,000   ns   30,000 us   30 ms 120x memory, 30X SSD
Send packet CA->Netherlands->CA    150,000,000   ns  150,000 us  150 ms

Notes
-----
1 ns = 10^-9 seconds
1 us = 10^-6 seconds = 1,000 ns
1 ms = 10^-3 seconds = 1,000 us = 1,000,000 ns
Handy metrics based on numbers above:Latency numbers visualizedSource(s) and further readingAdditional system design interview questions| Question | Reference(s) ||---|---|| Design a file sync service like Dropbox |  || Design a search engine like Google |  || Design a scalable web crawler like Google |  || Design Google docs |  || Design a key-value store like Redis |  || Design a cache system like Memcached |  || Design a recommendation system like Amazon's |  || Design a tinyurl system like Bitly |  || Design a chat app like WhatsApp | | Design a picture sharing system like Instagram |  || Design the Facebook news feed function |  || Design the Facebook timeline function |  || Design the Facebook chat function |  || Design a graph search function like Facebook's |  || Design a content delivery network like CloudFlare |  || Design a trending topic system like Twitter's |  || Design a random ID generation system |  || Return the top k requests during a time interval |  || Design a system that serves data from multiple data centers |  || Design an online multiplayer card game |  || Design a garbage collection system |  || Design an API rate limiter |  || Design a Stock Exchange (like NASDAQ or Binance) |  || Add a system design question |  |Real world architecturesDon't focus on nitty gritty details for the following articles, instead:|Type | System | Reference(s) ||---|---|---|| Data processing | MapReduce - Distributed data processing from Google |  || Data processing | Spark - Distributed data processing from Databricks |  || Data processing | Storm - Distributed data processing from Twitter |  || | | || Data store | Bigtable - Distributed column-oriented database from Google |  || Data store | HBase - Open source implementation of Bigtable |  || Data store | Cassandra - Distributed column-oriented database from Facebook | | Data store | DynamoDB - Document-oriented database from Amazon |  || Data store | MongoDB - Document-oriented database |  || Data store | Spanner - Globally-distributed database from Google |  || Data store | Memcached - Distributed memory caching system |  || Data store | Redis - Distributed memory caching system with persistence and value types |  || | | || File system | Google File System (GFS) - Distributed file system |  || File system | Hadoop File System (HDFS) - Open source implementation of GFS |  || | | || Misc | Chubby - Lock service for loosely-coupled distributed systems from Google |  || Misc | Dapper - Distributed systems tracing infrastructure | | Misc | Kafka - Pub/sub message queue from LinkedIn |  || Misc | Zookeeper - Centralized infrastructure and services enabling synchronization |  || | Add an architecture |  |Company architectures| Company | Reference(s) ||---|---|| Amazon |  || Cinchcast |  || DataSift |  || Dropbox |  || ESPN |  || Google |  || Instagram |  || Justin.tv |  || Facebook |  || Flickr |  || Mailbox |  || Netflix |  || Pinterest |  || Playfish |  || PlentyOfFish |  || Salesforce |  || Stack Overflow |  || TripAdvisor |  || Tumblr |  || Twitter |  || Uber |  || WhatsApp |  || YouTube |  |Company engineering blogsSource(s) and further readingLooking to add a blog?  To avoid duplicating work, consider adding your company blog to the following repo:Under developmentInterested in adding a section or helping complete one in-progress?  !CreditsCredits and sources are provided throughout this repo.Special thanks to:Contact infoFeel free to contact me to discuss any issues, questions, or comments.My contact info can be found on my .LicenseI am providing code and resources in this repository to you under an open source license.  Because this is my personal repository, the license you receive to my code and resources is from me and not my employer (Facebook).Copyright 2017 Donne Martin

Creative Commons Attribution 4.0 International License (CC BY 4.0)

http://creativecommons.org/licenses/by/4.0/
"
https://github.com/wbt5/real-url,获取斗鱼&虎牙&哔哩哔哩&抖音&快手等 58 个直播平台的真实流媒体地址(直播源)和弹幕，直播源可在 PotPlayer、flv.js 等播放器中播放。,Real-Url说明没想到还有这么多朋友发 issue 和邮件咨询问题，感谢大家的支持🎈！因为有时很忙，回复和提交代码的周期会有点长，抱歉哦😋这个仓库存放的是：获取一些直播平台真实流媒体地址（直播源）和弹幕的 Python 代码实现。获取的地址经测试，均可在 PotPlayer、VLC、DPlayer(flv.js + hls.js)等播放器中播放。目前已实现：59 个直播平台的直播源获取：斗鱼直播、虎牙直播、哔哩哔哩直播、战旗直播、网易 CC 直播、火猫直播、企鹅电竞、YY 直播、一直播、快手直播、花椒直播、映客直播、西瓜直播、触手直播（已倒闭）、NOW 直播、抖音直播，爱奇艺直播、酷狗直播、龙珠直播、PPS 奇秀直播、六间房、17 直播、来疯直播、优酷轮播台、网易 LOOK 直播、千帆直播、陌陌直播、小米直播、迅雷直播、京东直播、企鹅体育、人人直播、棉花糖直播、九秀直播、羚萌直播、95秀、新浪疯播、红人直播、艾米直播、KK直播、酷我聚星、乐嗨直播、秀色直播、星光直播、我秀直播、热猫直播、艺气山直播、AcFun 直播、猫耳FM、畅秀阁、Twitch、TikTok、央视频、PP体育、zhibotv、腾讯体育直播、爱奇艺体育直播、liveU、bigolive、咪咕视频体育。18 个直播平台的弹幕获取：斗鱼直播、虎牙直播、哔哩哔哩直播、快手直播、火猫直播、企鹅电竞、花椒直播、映客直播、网易 CC 直播、酷狗直播、龙珠直播、PPS 奇秀、搜狐千帆、战旗直播、来疯直播、网易 LOOK 直播、AcFun 直播、艺气山直播。运行反馈有直播平台失效或新增其他平台解析的，可发 。更新2021.11.7：:sparkles:新增咪咕体育。2021.8.15：:sparkles:新增 liveU、bigolive。2021.7.4：:art:更新哔哩哔哩直播源；:bug:修复Acfun直播弹幕；:bug:修复企鹅电竞弹幕。2021.6.20：:sparkles:新增爱奇艺体育直播。2021.6.13：:bug:修复腾讯体育。2021.6.12：:bug:修复斗鱼直播。2021.05.22：:sparkles:新增腾讯体育直播。2021.05.15：:art:更新爱奇艺、:bug:修复战旗直播。2021.05.13： :sparkles:新增 zhibotv。2021.05.05：:sparkles:新增 PP体育。2021.05.03：:sparkles:新增 央视频。2021.05.02：:sparkles:新增 Twitch、TikTok。2021.05.01：:sparkles:新增畅秀阁、猫耳FM。2020.12.20：修复直播源：抖音、艺气山、花椒、快手、来疯、龙珠、PPS、人人直播、17live 可能需要挂代理。2020.10.17：修复：西瓜直播、YY直播。2020.09.26：更新：虎牙直播源；注释掉未完成的 YY 直播弹幕功能。2020.09.12：新增：斗鱼添加一个从PC网页端获取直播源的方法，可选线路和清晰度；新增requirements.txt文件；更新代码。2020.08.18：更新快手直播源，现在播放链接需要带参数；更新快手直播弹幕，直接用 protobuf 序列化；新增 AcFun、艺气山两个平台的弹幕功能。2020.08.08：新增 AcFun 直播、艺气山直播；更新：哔哩哔哩直播、虎牙直播、红人直播；优化：斗鱼直播。2020.07.31：新增 19 个直播平台，详见上面说明；更新YY直播，现在可以获取最高画质；优化战旗直播、优酷直播代码；2020.07.25：新增网易 LOOK 直播弹幕获取；修复斗鱼直播源；新增陌陌直播源。2020.07.19：新增来疯直播弹幕获取2020.07.18：新增酷狗、龙珠、PPS奇秀、搜狐千帆、战旗直播等5个平台的弹幕获取2020.07.11：新增网易CC直播弹幕获取2020.07.05：新增花椒直播、映客直播弹幕获取；更新虎牙直播源2020.06.25：新增🐧企鹅电竞弹幕获取2020.06.19：新增火猫直播弹幕获取2020.06.18：新增弹幕功能2020.05.30：更新虎牙直播。2020.05.25：更新哔哩哔哩直播。2020.05.23：更新17直播、虎牙直播2020.05.19：更新火猫、快手、酷狗、PPS2020.05.08：新增优酷轮播台、look 直播、千帆直播；2020.05.01：新增优酷的来疯直播。2020.04.30：新增17直播。2020.04.24：修复虎牙、哔哩哔哩、快手、爱奇艺。2020.02.26：更新一直播。2020.01.18：更新抖音直播。2020.01.10：新增酷狗直播、龙珠直播、PPS奇秀直播、六间房。2020.01.09：新增爱奇艺直播。2020.01.07：新增抖音直播；删除一个直播平台。2020.01.03：修复快手直播，请求移动网页版。 2019.12.31：修复快手直播。 2019.12.07：修复哔哩哔哩直播。2019.12.04：更新斗鱼直播，新增一种获取方式。2019.11.24：新增收米直播。2019.11.18：新增西瓜直播；触手直播；NOW直播。2019.11.18：新增一直播；快手直播；花椒直播；映客直播。2019.11.17：新增火猫直播；新增企鹅电竞；新增YY直播。2019.11.16：新增战旗tv直播源；新增网易CC直播。2019.11.09：新增哔哩哔哩直播源。2019.11.03：新增虎牙直播源。2019.11.02：修复斗鱼预览地址获取的方法；新增未开播房间的判断。鸣谢感谢  提供的 free JetBrains Open Source license
https://github.com/encode/apistar,The Web API toolkit. 🛠,"Community: https://discuss.apistar.org 🤔 💭 🤓 💬 😎Documentation: https://docs.apistar.com 📘Requirements: Python 3.6+API Star is a toolkit for working with OpenAPI or Swagger schemas. It allows you to:You can use it to build static documentation, integrate it within a Web framework,or use it as the client library for interacting with other APIs.QuickstartInstall API Star:$ pip3 install apistar
Let's take a look at some of the functionality the toolkit provides...We'll start by creating an OpenAPI schema, :openapi: 3.0.0
info:
  title: Widget API
  version: '1.0'
  description: An example API for widgets
servers:
  - url: https://www.example.org/
paths:
  /widgets:
    get:
      summary: List all the widgets.
      operationId: listWidgets
      parameters:
      - in: query
        name: search
        description: Filter widgets by this search term.
        schema:
          type: string
Let's also create a configuration file :schema:
  path: schema.yaml
  format: openapi
We're now ready to start using the  command line tool.We can validate our OpenAPI schema:$ apistar validate
✓ Valid OpenAPI schema.
Or build developer documentation for our API:$ apistar docs --serve
✓ Documentation available at ""http://127.0.0.1:8000/"" (Ctrl+C to quit)
We can also make API requests to the server referenced in the schema:$ apistar request listWidgets search=cogwheel
Where did the server go?With version 0.6 onwards the API Star project is being focused as aframework-agnostic suite of API tooling. The plan is to build out thisfunctionality in a way that makes it appropriate for use either as a stand-alonetool, or together with a large range of frameworks.The 0.5 branch remains available on GitHub, and can be installed from PyPIwith . Any further development of the API Starserver would likely need to be against a fork of that, under a new maintainer.If you're looking for a high-performance Python-based async framework, thenI would instead recommend ."
https://github.com/facebookresearch/Detectron,"FAIR's research platform for object detection research, implementing popular algorithms like Mask R-CNN and RetinaNet.","Detectron is deprecated. Please see DetectronDetectron is Facebook AI Research's software system that implements state-of-the-art object detection algorithms, including . It is written in Python and powered by the  deep learning framework.At FAIR, Detectron has enabled numerous research projects, including: , , , , , , , , and .IntroductionThe goal of Detectron is to provide a high-quality, high-performancecodebase for object detection research. It is designed to be flexible in orderto support rapid implementation and evaluation of novel research. Detectronincludes implementations of the following object detection algorithms:using the following backbone network architectures:Additional backbone architectures may be easily implemented. For more details about these models, please see  below.UpdateLicenseDetectron is released under the . See the  file for additional details.Citing DetectronIf you use Detectron in your research or wish to refer to the baseline results published in the , please use the following BibTeX entry.@misc{Detectron2018,
  author =       {Ross Girshick and Ilija Radosavovic and Georgia Gkioxari and
                  Piotr Doll\'{a}r and Kaiming He},
  title =        {Detectron},
  howpublished = {\url{https://github.com/facebookresearch/detectron}},
  year =         {2018}
}
Model Zoo and BaselinesWe provide a large set of baseline results and trained models available for download in the .InstallationPlease find installation instructions for Caffe2 and Detectron in .Quick Start: Using DetectronAfter installation, please see  for brief tutorials covering inference and training with Detectron.Getting HelpTo start, please check the  section of our installation instructions as well as our . If you couldn't find help there, try searching our GitHub issues. We intend the issues page to be a forum in which the community collectively troubleshoots problems.If bugs are found, we appreciate pull requests (including adding Q&A's to  and improving our installation instructions and troubleshooting documents). Please see  for more information about contributing to Detectron.References"
https://github.com/tkarras/progressive_growing_of_gans,"Progressive Growing of GANs for Improved Quality, Stability, and Variation","Progressive Growing of GANs for Improved Quality, Stability, and Variation&mdash; Official TensorFlow implementation of the ICLR 2018 paperTero Karras (NVIDIA), Timo Aila (NVIDIA), Samuli Laine (NVIDIA), Jaakko Lehtinen (NVIDIA and Aalto University)Picture: Two imaginary celebrities that were dreamed up by a random number generator.Abstract:We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024&#9733;&#9733;&#9733; NEW: ResourcesAll the material, including source code, is made freely available for non-commercial use under the Creative Commons  license. Feel free to use any of the material in your own work, as long as you give us appropriate credit by mentioning the title and author list of our paper.VersionsThere are two different versions of the source code. The TensorFlow version is newer and more polished, and we generally recommend it as a starting point if you are looking to experiment with our technique, build upon it, or apply it to novel datasets. The original Theano version, on the other hand, is what we used to produce all the results shown in our paper. We recommend using it if &mdash; and only if &mdash; you are looking to reproduce our exact results for benchmark datasets like CIFAR-10, MNIST-RGB, and CelebA.The main differences are summarized in the following table:| Feature                           | TensorFlow version                            | Original Theano version   || :-------------------------------- | :-------------------------------------------: | :-----------------------: || Branch                            |  (this branch) |  || Multi-GPU support                 | Yes                                           | No                        || FP16 mixed-precision support      | Yes                                           | No                        || Performance                       | High                                          | Low                       || Training time for CelebA-HQ       | 2 days (8 GPUs)2 weeks (1 GPU)            | 1&ndash;2 months          || Repro CelebA-HQ results           | Yes &ndash; very close                        | Yes &ndash; identical     || Repro LSUN results                | Yes &ndash; very close                        | Yes &ndash; identical     || Repro CIFAR-10 results            | No                                            | Yes &ndash; identical     || Repro MNIST mode recovery         | No                                            | Yes &ndash; identical     || Repro ablation study (Table 1)    | No                                            | Yes &ndash; identical     || Dataset format                    | TFRecords                                     | HDF5                      || Backwards compatibility           | Can import networkstrained with Theano    | N/A                       || Code quality                      | Reasonable                                    | Somewhat messy            |System requirementsImporting and using pre-trained networksAll pre-trained networks found on Google Drive, as well as ones produced by the training script, are stored as Python PKL files. They can be imported using the standard  mechanism as long as two conditions are met: (1) The directory containing the Progressive GAN code repository must be included in the PYTHONPATH environment variable, and (2) a  object must have been created beforehand and set as default. Each PKL file contains 3 instances of :# Import official CelebA-HQ networks.
with open('karras2018iclr-celebahq-1024x1024.pkl', 'rb') as file:
    G, D, Gs = pickle.load(file)
    # G = Instantaneous snapshot of the generator, mainly useful for resuming a previous training run.
    # D = Instantaneous snapshot of the discriminator, mainly useful for resuming a previous training run.
    # Gs = Long-term average of the generator, yielding higher-quality results than the instantaneous snapshot.
It is also possible to import networks that were produced using the Theano implementation, as long as they do not employ any features that are not natively supported by the TensorFlow version (minibatch discrimination, batch normalization, etc.). To enable Theano network import, however, you must use  in place of :# Import Theano versions of the official CelebA-HQ networks.
import misc
G, D, Gs = misc.load_pkl('200-celebahq-1024x1024/network-final.pkl')
Once you have imported the networks, you can call  to produce a set of images for given latent vectors, or  to include the generator network in a larger TensorFlow expression. For further details, please consult the example script found on Google Drive. Instructions:Preparing datasets for trainingThe Progressive GAN code repository contains a command-line tool for recreating bit-exact replicas of the datasets that we used in the paper. The tool also provides various utilities for operating on the datasets:usage: dataset_tool.py [-h] <command> ...

    display             Display images in dataset.
    extract             Extract images from dataset.
    compare             Compare two datasets.
    create_mnist        Create dataset for MNIST.
    create_mnistrgb     Create dataset for MNIST-RGB.
    create_cifar10      Create dataset for CIFAR-10.
    create_cifar100     Create dataset for CIFAR-100.
    create_svhn         Create dataset for SVHN.
    create_lsun         Create dataset for single LSUN category.
    create_celeba       Create dataset for CelebA.
    create_celebahq     Create dataset for CelebA-HQ.
    create_from_images  Create dataset from a directory full of images.
    create_from_hdf5    Create dataset from legacy HDF5 archive.

Type ""dataset_tool.py <command> -h"" for more information.
The datasets are represented by directories containing the same image data in several resolutions to enable efficient streaming. There is a separate  file for each resolution, and if the dataset contains labels, they are stored in a separate file as well:> python dataset_tool.py create_cifar10 datasets/cifar10 ~/downloads/cifar10
> ls -la datasets/cifar10
drwxr-xr-x  2 user user         7 Feb 21 10:07 .
drwxrwxr-x 10 user user        62 Apr  3 15:10 ..
-rw-r--r--  1 user user   4900000 Feb 19 13:17 cifar10-r02.tfrecords
-rw-r--r--  1 user user  12350000 Feb 19 13:17 cifar10-r03.tfrecords
-rw-r--r--  1 user user  41150000 Feb 19 13:17 cifar10-r04.tfrecords
-rw-r--r--  1 user user 156350000 Feb 19 13:17 cifar10-r05.tfrecords
-rw-r--r--  1 user user   2000080 Feb 19 13:17 cifar10-rxx.labels
The  commands take the standard version of a given dataset as input and produce the corresponding  files as output. Additionally, the  command requires a set of data files representing deltas with respect to the original CelebA dataset. These deltas (27.6GB) can be downloaded from .Note about module versions: Some of the dataset commands require specific versions of Python modules and system libraries (e.g. pillow, libjpeg), and they will give an error if the versions do not match. Please heed the error messages &mdash; there is no way to get the commands to work other than installing these specific versions.Training networksOnce the necessary datasets are set up, you can proceed to train your own networks. The general procedure is as follows:By default,  is configured to train a 1024x1024 network for CelebA-HQ using a single-GPU. This is expected to take about two weeks even on the highest-end NVIDIA GPUs. The key to enabling faster training is to employ multiple GPUs and/or go for a lower-resolution dataset. To this end,  contains several examples for commonly used datasets, as well as a set of ""configuration presets"" for multi-GPU training. All of the presets are expected to yield roughly the same image quality for CelebA-HQ, but their total training time can vary considerably:For reference, the expected output of each configuration preset for CelebA-HQ can be found in Other noteworthy config options:Analyzing resultsTraining results can be analyzed in several ways:"
https://github.com/public-api-lists/public-api-lists,A collective list of free APIs for use in software and web development 🚀 (Clone of https://github.com/public-apis/public-apis),"Public API ListsA collective list of free APIs for use in software and web development.For information on contributing to this project, please see the .IndexAnimals|                                            API                                             | Description                                 |   Auth   | HTTPS |  CORS   || :----------------------------------------------------------------------------------------: | ------------------------------------------- | :------: | :---: | :-----: ||                                     | Daily cat facts                             |    No    |  Yes  |   No    ||                                                         | Pictures of cats from Tumblr                |  |  Yes  | Unknown ||                                                               | Cat as a service (cats pictures and gifs)   |    No    |  Yes  | Unknown ||                                                            | Based on the Stanford Dogs Dataset          |    No    |  Yes  |   Yes   ||                                                                 | Random fox images with different tags       |    No    |  Yes  |   Yes   ||                                                                | Cat for every HTTP Status                   |    No    |  Yes  | Unknown ||                                                              | Dogs for every HTTP status code             |    No    |  Yes  | Unknown ||                                            | IUCN Red List of Threatened Species         |  |  No   | Unknown ||                                    | Movement and Migration data of animals      |    No    |  Yes  | Unknown ||                                  | Adoption                                    |   |  Yes  |   Yes   ||                                             | Random pictures of big cats                 |    No    |  Yes  |   Yes   ||                                                    | Random pictures of cats                     |    No    |  Yes  |   Yes   ||                                                   | Random pictures of dogs                     |    No    |  Yes  |   Yes   ||                                                    | Random pictures of foxes                    |    No    |  Yes  |   No    ||  | Adoption                                    |    No    |  Yes  | Unknown ||                                                        | Random pictures of Shibu Inu, cats or birds |    No    |  Yes  |   Yes   |[<marko.inline.RawText object at 0x000001592FFB7188>]Anime|                                    API                                    | Description                                       |   Auth   | HTTPS |  CORS   || :-----------------------------------------------------------------------: | ------------------------------------------------- | :------: | :---: | :-----: ||                   | Anime discovery & tracking                        |   |  Yes  | Unknown ||                             | Random anime quote generation                     |    No    |  Yes  | Unknown ||  | Anime industry news                               |    No    |  Yes  |   Yes   ||                                                 | Unofficial MyAnimeList API                        |    No    |  Yes  |   Yes   ||                                      | Anime discovery platform                          |   |  Yes  | Unknown ||                    | Scan anime image to get specific detail           |  |  Yes  | Unknown ||                                 | ad-free manga reader offering high-quality images |    No    |  Yes  | Unknown |[<marko.inline.RawText object at 0x000001592FDDAA48>]Anti-Malware|                                  API                                  | Description                  |   Auth   | HTTPS |  CORS   || :-------------------------------------------------------------------: | ---------------------------- | :------: | :---: | :-----: ||                               | IP/domain/URL reputation     |  |  Yes  | Unknown ||   | Google Link/Domain Flagging  |  |  Yes  | Unknown ||                                      | Metacert Link Flagging       |  |  Yes  | Unknown ||  | VirusTotal File/URL Analysis |  |  Yes  | Unknown ||                     | Website reputation           |  |  Yes  | Unknown |[<marko.inline.RawText object at 0x0000015930043F48>]Art & Design|                                 API                                  | Description                           |   Auth   | HTTPS |  CORS   || :------------------------------------------------------------------: | ------------------------------------- | :------: | :---: | :-----: ||                                | Design                                |  |  Yes  | Unknown ||              | Smithsonian Design Museum             |  |  Yes  | Unknown ||                         | Design                                |   |  No   | Unknown ||           | European Museum and Galleries content |  |  Yes  | Unknown ||  | Art                                   |  |  No   | Unknown ||                        | Icons                                 |  |  Yes  | Unknown ||              | Icons                                 |   |  Yes  | Unknown ||              | Icons                                 |   |  No   | Unknown ||                      | Art                                   |  |  Yes  | Unknown ||            | Art                                   |    No    |  Yes  | Unknown |[<marko.inline.RawText object at 0x000001592FFBA7C8>]Books|                                    API                                    | Description                                                                              |   Auth   | HTTPS |  CORS   || :-----------------------------------------------------------------------: | ---------------------------------------------------------------------------------------- | :------: | :---: | :-----: ||                               | Bhagavad Gita text                                                                       |   |  Yes  |   Yes   ||                    | Books                                                                                    |    No    |  No   | Unknown ||                                 | Books                                                                                    |  |  Yes  | Unknown ||                       | Books                                                                                    |   |  Yes  | Unknown ||                     | Books, book covers and related data                                                      |    No    |  Yes  | Unknown ||  | Books, book covers and related data                                                      |    No    |  Yes  | Unknown ||                         | Gods and poets, their categories, and the verse meters, with the mandal and sukta number |    No    |  Yes  | Unknown ||        | Descriptions of all nouns (names, places, animals, things) from vedic literature         |    No    |  Yes  | Unknown |[<marko.inline.RawText object at 0x000001592FD70908>]Business|                                    API                                     | Description                                                                   |   Auth   | HTTPS |  CORS   || :------------------------------------------------------------------------: | ----------------------------------------------------------------------------- | :------: | :---: | :-----: ||                          | Non-profit charity data                                                       |  |  No   | Unknown ||                         | Search for company logos and embed them in your projects                      |  |  Yes  | Unknown ||                            | Search domain for availability, lookup DNS & whois record of a domain         |  |  Yes  |   Yes   ||                                   | Registered Domain Names Search                                                |    No    |  Yes  | Unknown ||                             | Hire freelancers to get work done                                             |   |  Yes  | Unknown ||                           | Flexible, RESTful access to the user's inbox                                  |   |  Yes  | Unknown ||                | Collect, configure and analyze your data to reach the right audience          |   |  Yes  | Unknown ||  | Validate email address to improve deliverability                              |  |  Yes  | Unknown ||                                         | Email Service                                                                 |  |  Yes  | Unknown ||                                         | Email Service                                                                 |  |  Yes  | Unknown ||                                      | Trademark Search                                                              |    No    |  No   | Unknown ||                                 | Repetiti 3d Printer Management Service, access and control 3d Printers easily |  |  Yes  |   Yes   ||                                              | Friendly website analytics made for humans                                    |    No    |  Yes  | Unknown ||                                    | Boards, lists and cards to help you organize and prioritize your projects     |   |  Yes  | Unknown ||                                     | Email Finder for B2B sales and email marketing                                |  |  Yes  |   Yes   |[<marko.inline.RawText object at 0x000001592FD70788>]Calendar|                                       API                                       | Description                                                                                            |   Auth   | HTTPS |  CORS   || :-----------------------------------------------------------------------------: | ------------------------------------------------------------------------------------------------------ | :------: | :---: | :-----: ||               | National, regional, and religious holidays for 120+ countries & 100+ years                             |   Yes    |  Yes  |   Yes   ||  | Seach histories from wikipedia for a particular day                                                    |    No    |  Yes  | Unknown ||                                 | Worldwide Holidays and Working Days                                                                    |  |  Yes  |   Yes   ||                                | Catholic liturgical calendar                                                                           |    No    |  No   | Unknown ||                            | Lookup for a name and returns nameday date                                                             |    No    |  No   | Unknown ||           | Display, create and modify Google calendar events                                                      |   |  Yes  | Unknown ||                    | Convert between Gregorian and Hebrew, fetch Shabbat and Holiday times, etc                             |    No    |  No   | Unknown ||                                            | Enterprise-grade holiday and observance API for developers by developers                               |  |  Yes  |   Yes   ||                                              | Historical data regarding holidays                                                                     |  |  Yes  | Unknown ||                                          | Simple API for checking working, non-working or short days for Russia, Ukraine, Belarus and Kazakhstan |    No    |  Yes  |   Yes   ||                                              | Public holidays for more than 90 countries                                                             |    No    |  Yes  |   No    ||                                     | Provides namedays for multiple countries                                                               |    No    |  Yes  |   Yes   ||                              | Database of ICS files for non working days                                                             |    No    |  Yes  | Unknown ||                        | Check if a date is a Russian holiday or not                                                            |    No    |  Yes  |   No    ||             | Database of official time zones and corresponding iCal VTIMEZONE blocks                                |    No    |  Yes  | Unknown |[<marko.inline.RawText object at 0x000001592FDC80C8>]Cloud Storage & File Sharing|                         API                          | Description              |   Auth   | HTTPS |  CORS   || :--------------------------------------------------: | ------------------------ | :------: | :---: | :-----: ||           | File Sharing and Storage |    No    |  Yes  | Unknown ||                     | File Sharing and Storage |   |  Yes  | Unknown ||         | File Sharing and Storage |   |  Yes  | Unknown ||  | File Sharing and Storage |   |  Yes  | Unknown ||                 | File Sharing and Storage |   |  Yes  | Unknown ||                 | Plain Text Storage       |  |  Yes  | Unknown ||       | File Sharing             |  |  Yes  |   Yes   |[<marko.inline.RawText object at 0x000001592FE10A08>]Continuous Integration|                           API                           | Description                                                                                    |   Auth   | HTTPS |  CORS   || :-----------------------------------------------------: | ---------------------------------------------------------------------------------------------- | :------: | :---: | :-----: ||  | Automate the software development process using continuous integration and continuous delivery |  |  Yes  | Unknown ||                | Codeship is a Continuous Integration Platform in the cloud                                     |  |  Yes  | Unknown ||             | Sync your GitHub projects with Travis CI to test your code in minutes                          |  |  Yes  | Unknown |[<marko.inline.RawText object at 0x000001592FDCAA48>]Cryptocurrency|                                   API                                    | Description                                                                           |   Auth   | HTTPS |  CORS   || :----------------------------------------------------------------------: | ------------------------------------------------------------------------------------- | :------: | :---: | :-----: ||  | Exchange for Trading Cryptocurrencies based in China                                  |  |  Yes  | Unknown ||                       | Digital Asset Price Data for the blockchain industry                                  |  |  Yes  | Unknown ||               | Financial and Technical Data related to the Bitcoin Network                           |    No    |  Yes  | Unknown ||                          | Real-Time Cryptocurrency derivatives trading platform based in Hong Kong              |  |  Yes  | Unknown ||                                   | Cryptocurrency Trading Platform                                                       |  |  Yes  | Unknown ||                                  | Bitcoin Payment, Wallet & Transaction Data                                            |  |  Yes  | Unknown ||                             | Bitcoin Payment, Wallet & Transaction Data                                            |    No    |  Yes  | Unknown ||                                      | Real-time crypto data from multiple exchanges via a single unified API, and much more |  |  Yes  | Unknown ||                                       | All Currency Exchanges integrate under a single api                                   |  |  Yes  |   No    ||                               | Bitcoin, Bitcoin Cash, Litecoin and Ethereum Prices                                   |  |  Yes  | Unknown ||                        | Cryptocurrency Trading Platform                                                       |  |  Yes  | Unknown ||                                  | Bitcoin Price Index                                                                   |    No    |  No   | Unknown ||                                 | Cryptocurrency Price, Market, and Developer/Social Data                               |    No    |  Yes  |   Yes   ||                                 | Interacting with Coinigy Accounts and Exchange Directly                               |  |  Yes  | Unknown ||                                        | Real-time Crypto Currency Exchange Rates                                              |  |  Yes  | Unknown ||                                     | Crypto Currency Prices                                                                |  |  Yes  | Unknown ||              | Cryptocurrencies prices, volume and more                                              |    No    |  Yes  | Unknown ||                           | Cryptocurrencies Prices                                                               |  |  Yes  | Unknown ||                                | Cryptocurrencies prices, volume and more                                              |    No    |  Yes  |   Yes   ||                              | Live Cryptocurrency data                                                              |    No    |  Yes  | Unknown ||                          | Multi-blockchain data aggregator                                                      |   Yes    |  Yes  | Unknown ||                       | Cryptocurrencies Comparison                                                           |    No    |  Yes  | Unknown ||                           | Cryptocurrencies Exchange Rates                                                       |    No    |  Yes  | Unknown ||                                    | Currency Conversion API                                                               |  |  Yes  | Unknown ||                                      | Blockchain Assets Exchange                                                            |    No    |  Yes  | Unknown ||                               | Cryptocurrencies Exchange                                                             |    No    |  Yes  | Unknown ||                       | Crypto asset exchange for trading Marketplace                                         |  |  Yes  | Unknown ||                           | Automated cryptocurrency exchange service                                             |    No    |  No   |   Yes   ||                                    | Largest Crypto Mining Marketplace                                                     |  |  Yes  | Unknown ||                             | US based digital asset exchange                                                       |  |  Yes  | Unknown ||               | Cryptocurrencies Prices                                                               |  |  Yes  | Unknown |[<marko.inline.RawText object at 0x000001592FFBE088>]Currency Exchange|                                                     API                                                      | Description                                                   |   Auth   | HTTPS |  CORS   || :----------------------------------------------------------------------------------------------------------: | ------------------------------------------------------------- | :------: | :---: | :-----: ||                                                 | Forex currency market data                                    |  |  Yes  | Unknown ||                                                          | Convert one currency to another with reliable exchanges rates |  |  Yes  |   Yes   ||                                                      | Exchange rates and currency conversion                        |  |  Yes  | Unknown ||  | A collection of exchange rates                                |    No    |  Yes  | Unknown ||                                                          | Free currency conversion                                      |  |  Yes  |   Yes   ||                                                            | Exchange rates with currency conversion                       |  |  Yes  |   Yes   ||                                                                                   | Exchange rates and currency conversion                        |  |  Yes  | Unknown ||                                                               | Exchange rates, currency conversion and time series           |    No    |  Yes  |   Yes   ||                                                                         | Real-time exchange rates, historical rates and currency conversion      |  |  Yes  |   No    ||                                                                               | Free exchange rates and historical rates                      |    No    |  Yes  | Unknown |[<marko.inline.RawText object at 0x000001592FFD4348>]Data Validation|                                     API                                     | Description                                                           |   Auth   | HTTPS |  CORS   || :-------------------------------------------------------------------------: | --------------------------------------------------------------------- | :------: | :---: | :-----: ||               | Validate email addresses, phone numbers, VAT numbers and domain names |  |  Yes  |   Yes   ||                               | Instant email address verification to filter legit addresses          |  |  Yes  |   Yes   ||                                   | Language detection                                                    |    No    |  Yes  | Unknown ||                                                  | US Address Verification                                               |  |  Yes  | Unknown ||                                     | Email address validation                                              |    No    |  Yes  | Unknown ||                                       | Open Source phone number validation                                   |    No    |  Yes  | Unknown ||                                           | Phone number validation                                               |    No    |  Yes  | Unknown ||                                      | Content validator against profanity & obscenity                       |    No    |  No   | Unknown ||  | Enter address data quickly with real-time address suggestions         |  |  Yes  |   Yes   ||         | Extract postal addresses from any text including emails               |  |  Yes  |   Yes   ||      | Validate and append data for any US postal address                    |  |  Yes  |   Yes   ||                                             | VAT number validation                                                 |    No    |  Yes  | Unknown ||                                            | Phone number validation & carrier lookup                              |  |  Yes  |   Yes   ||                                           | Email address validation                                              |    No    |  Yes  |   Yes   |[<marko.inline.RawText object at 0x000001592FFBEF08>]Development|                                                 API                                                 | Description                                                                                         |       Auth       | HTTPS |  CORS   || :-------------------------------------------------------------------------------------------------: | --------------------------------------------------------------------------------------------------- | :--------------: | :---: | :-----: ||                                                   | Project to promote open source collaboration during December                                        |        No        |  Yes  |   No    ||                                                                         | Estimates the age from a first name                                                                 |        No        |  Yes  |   Yes   ||                                                                    | Chrome based screenshot API for developers                                                          |          |  Yes  | Unknown ||                                                              | Wikipedia for Web APIs, OpenAPI/Swagger specs for public APIs                                       |        No        |  Yes  | Unknown ||                                                                   | Return a site's meta tags in JSON format                                                            |   |  Yes  | Unknown ||                                                                   | Find random activities to fight boredom                                                             |        No        |  Yes  | Unknown ||                                                   | Easily make screenshots of web pages in any screen size, as any device                              |          |  Yes  | Unknown ||                                                      | Library info on CDNJS                                                                               |        No        |  Yes  | Unknown ||                                                               | Structured changelog metadata from open source projects                                             |        No        |  Yes  | Unknown ||                                                                     | Free and simple counting service. You can use it to track page hits and specific events             |        No        |  Yes  |   Yes   ||                                        | Status of all DigitalOcean services                                                                 |        No        |  Yes  | Unknown ||       | Generate dynamic and static QR Codes                                                                |  |  Yes  |   Yes   ||                                           | A simple JSON store API                                                                             |        No        |  Yes  |   Yes   ||                                                        | A tool to detect face                                                                               |           |  Yes  | Unknown ||                                                            | Send static html form submissions to Google Sheets, Email, Slack or Telegram                        |        No        |  Yes  |   Yes   ||                                                                 | Estimates a gender from a first name                                                                |        No        |  Yes  |   Yes   ||                                                           | Make use of GitHub repositories, code and user info programmatically                                |           |  Yes  |   Yes   ||                                                            | Automate GitLab interaction programmatically                                                        |           |  Yes  | Unknown ||                                                           | Chat for GitHub                                                                                     |           |  Yes  | Unknown ||                                                               | Test endpoints for client and server HTTP/2 protocol support                                        |        No        |  Yes  | Unknown ||  | Convert text to speech                                                                              |          |  Yes  |   Yes   ||                                                        | Generate charts, QR codes and graph images                                                          |        No        |  Yes  |   Yes   ||                                                              | Retrieve structured data from a website or RSS feed                                                 |          |  Yes  | Unknown ||                                              | WHOIS domain name lookup                                                                            |          |  Yes  | Unknown ||                                                         | Public IP Address API with company, ASN and hosting detection support                               |        No        |  Yes  |   Yes   ||                                        | Unlimited free IP Address API with useful information                                               |        No        |  Yes  | Unknown ||                                                                      | A simple IP Address API                                                                             |        No        |  Yes  | Unknown ||                                                               | Another simple IP Address API                                                                       |        No        |  Yes  | Unknown ||                                            | Package info and download stats on jsDelivr CDN                                                     |        No        |  Yes  |   Yes   ||                                                              | Convert JSON to JSONP (on-the-fly) for easy cross-domain data requests using client-side JavaScript |        No        |  Yes  | Unknown ||                                                                     | Free JSON storage service. Ideal for small scale Web apps, Websites and Mobile apps                 |          |  Yes  |   Yes   ||                                                                    | Compile and run source code                                                                         |        No        |  Yes  | Unknown ||                                                | Uncovers the technologies used on websites and URL to thumbnail                                     |        No        |  Yes  | Unknown ||                      | Unofficial REST API for choosealicense.com                                                          |        No        |  Yes  |   No    ||                                            | Live Coding Streaming                                                                               |           |  Yes  | Unknown ||                                                   | Retrieve vendor details and other information regarding a given MAC address or an OUI               |          |  Yes  |   Yes   ||                                                             | Estimate the nationality of a first name                                                            |        No        |  Yes  |   Yes   ||                                                                      | Multiple spam filtering service                                                                     |        No        |  Yes  |   Yes   ||                                                          | Tool for testing APIs                                                                               |          |  Yes  | Unknown ||                                                                 | Scraping and crawling anticaptcha service                                                           |          |  Yes  | Unknown ||                                             | A collective list of free JSON APIs for use in web development                                      |        No        |  Yes  | Unknown ||                                                             | Push notifications for Android & iOS                                                                |          |  Yes  | Unknown ||                                                     | Create new QR Code or decode existing one                                                           |          |  Yes  |   Yes   ||                                                                     | Create an easy to read QR code and URL shortener                                                    |        No        |  Yes  |   Yes   ||                                                                       | Generate and decode / read QR code graphics                                                         |        No        |  Yes  | Unknown ||                                                                 | Generate chart and graph images                                                                     |        No        |  Yes  |   Yes   ||                                                                         | A hosted REST-API ready to respond to your AJAX requests                                            |        No        |  Yes  | Unknown ||                                                             | Easily build scalable web scrapers                                                                  |          |  Yes  | Unknown ||                                                      | Create pixel-perfect website screenshots                                                            |          |  Yes  |   Yes   ||                                                          | Free capture a full screenshot of the website with high resolution and without watermarks           |          |  Yes  |   Yes   ||                                                     | Service for generating data and executing logic to create various games and puzzles                 |        No        |  Yes  |   Yes   ||                                                                  | ALL-CAPS AS A SERVICE                                                                               |        No        |  No   | Unknown ||                                                      | Q&A forum for developers                                                                            |           |  Yes  | Unknown ||                                                                  | Convert Unixtime to DateTime and vice versa                                                         |        No        |  Yes  |   Yes   ||                                                  | Integration developer utility APIs                                                                  |        No        |  Yes  | Unknown ||                   | Generate link from image(Integration developer utility APIs)                                        |        No        |  Yes  |   YES   ||                                                                     | Geolocation API,ASN API,IP Ranges API,IP Firewall API,Domain API                                    |       Yes        |  Yes  |   Yes   ||                                                                  | Geolocation API,lookup DNS & whois record of a domain                                               |        No        |  Yes  |   Yes   ||                                                               | Real-time search engines SERP API                                                                   |          |  Yes  |   No    |[<marko.inline.RawText object at 0x000001592FD70708>]Dictionaries|                         API                         | Description                                                     |   Auth   | HTTPS |  CORS   || :-------------------------------------------------: | --------------------------------------------------------------- | :------: | :---: | :-----: ||           | Word definitions, pronunciations, synonyms, antonyms and others |  |  Yes  |   Yes   ||        | Dictionary and Thesaurus Data                                   |  |  Yes  | Unknown ||                       | Definitions with example sentence and photo if available        |  |  Yes  |   Yes   ||  | Dictionary Data                                                 |  |  Yes  |   No    ||              | Dictionary Data                                                 |  |  No   | Unknown ||                   | Definitions and synonyms for more than 150,000 words            |  |  Yes  | Unknown |[<marko.inline.RawText object at 0x000001592FF43488>]Disasters|                         API                         | Description                       |   Auth   | HTTPS |  CORS   || :-------------------------------------------------: | --------------------------------- | :------: | :---: | :-----: ||  | Earthquake Data                   |  |  Yes  | Unknown ||         | All types of disaster data        |  |  Yes  | Unknown ||        | Events and natural disasters data |  oAuth   |  Yes  | Unknown |[<marko.inline.RawText object at 0x000001592FF43408>]Documents & Productivity|                                        API                                        | Description                                                                |   Auth   | HTTPS |  CORS   || :-------------------------------------------------------------------------------: | -------------------------------------------------------------------------- | :------: | :---: | :-----: ||  | HTML/URL to PDF/PNG, Office documents to PDF, image conversion             |  |  Yes  |   Yes   ||                                          | Authentication,Emails sending,File uploading and shearing, forms filluping |  |  Yes  | Unknown ||                                               | Generate dynamic PDFs with JSON to PDF API based on LaTeX                  |  |  Yes  | Unknown ||                                                     | File Sharing                                                               |    No    |  Yes  | Unknown ||                               | Web parser                                                                 |  |  Yes  | Unknown ||                                             | HTML or URL to PDF                                                         |  |  Yes  |   No    ||                                                   | HTML/URL to PDF                                                            |  |  Yes  | Unknown ||                                         | Bookmarking service                                                        |   |  Yes  | Unknown ||                                                   | Data from XML or JSON to PDF, HTML or Image                                |  |  Yes  | Unknown ||                                                   | Provides screenshot, HTML to PDF and content extraction APIs               |  |  Yes  | Unknown ||                                           | Todo Lists                                                                 |   |  Yes  | Unknown ||                                            | Free vector file converting API                                            |    No    |  No   |   Yes   ||                              | Convert files using Vertopal API                                           |  |  Yes  |   No    ||                                        | Automated time tracking leaderboards for programmers                       |    No    |  Yes  | Unknown ||                                 | High resolution, retina display and responsive screenshot                  |  |  Yes  |   Yes   |[<marko.inline.RawText object at 0x000001592FD96E88>]Education|                                        API                                        | Description                                                                |   Auth   | HTTPS |  CORS   || :-------------------------------------------------------------------------------: | -------------------------------------------------------------------------- | :------: | :---: | :-----: ||  | Current International Affairs, quizzes and more             |  |  Yes  |   No   |[<marko.inline.RawText object at 0x000001592FDB9548>]Environment|                                                  API                                                   | Description                                                                    |   Auth   | HTTPS |  CORS   || :----------------------------------------------------------------------------------------------------: | ------------------------------------------------------------------------------ | :------: | :---: | :-----: ||                                                                  | Air quality and weather data                                                   |  |  Yes  | Unknown ||                       | Green Power Index for Germany (Grünstromindex/GSI)                             |    No    |  No   |   Yes   ||                                                                      | Open air quality data                                                          |  |  Yes  | Unknown ||                                                                           | Real-time air quality index                                                    |  |  No   | Unknown ||                                            | Energy production photovoltaic (PV) energy systems                             |  |  Yes  | Unknown ||  | The Official Carbon Intensity API for Great Britain developed by National Grid |    No    |  Yes  | Unknown |[<marko.inline.RawText object at 0x000001592FDB9948>]Events|                                                               API                                                               | Description                           |   Auth   | HTTPS |  CORS   || :-----------------------------------------------------------------------------------------------------------------------------: | ------------------------------------- | :------: | :---: | :-----: ||                                                                           | Find events                           |   |  Yes  | Unknown ||  | Sell tickets anywhere                 |  |  Yes  | Unknown ||                                                                                       | Search events, venues and performers  |  |  Yes  | Unknown ||                                        | Search events, attractions, or venues |  |  Yes  | Unknown |[<marko.inline.RawText object at 0x000001593003FBC8>]Finance|                                   API                                    | Description                                                   |   Auth   | HTTPS |  CORS   || :----------------------------------------------------------------------: | ------------------------------------------------------------- | :------: | :---: | :-----: ||                             | Realtime and historical stock data                            |  |  Yes  | Unknown ||                | Stock, Futures and Forex Market Data                          |  |  Yes  | Unknown ||             | Stock information and data                                    |    No    |  Yes  | Unknown ||                                  | Realtime stock data                                           |  |  Yes  |   Yes   ||                                         | Realtime & Historical Stock and Market Data                   |  |  Yes  |   Yes   ||                                  | Spreadbetting and CFD Market Data                             |  |  Yes  | Unknown ||                                               | Connect with users’ bank accounts and access transaction data |  |  Yes  | Unknown ||                               | Indian Financial Systems Code (Bank Branch Codes)             |    No    |  Yes  | Unknown ||  | All Banks IFSC Code data,Search by IFSc or other details      |    No    |  Yes  | Unknown ||     | ACH/NACHA Bank Routing Numbers                                |    No    |  Yes  | Unknown ||                                  | US equity/option market data (delayed, intraday, historical)  |   |  Yes  |   Yes   ||                   | Market data provider                                          |  |  Yes  | Unknown ||                                   | Budgeting & Planning                                          |   |  Yes  |   Yes   |[<marko.inline.RawText object at 0x000001592FDB9508>]Food & Drink|                                       API                                        | Description                                       |   Auth   | HTTPS |  CORS   || :------------------------------------------------------------------------------: | ------------------------------------------------- | :------: | :---: | :-----: ||                                           | Recipe Search                                     |  |  Yes  | Unknown ||                                                      | Alcohol                                           |  |  Yes  | Unknown ||                                  | Breweries, Cideries and Craft Beer Bottle Shops   |    No    |  Yes  |   Yes   ||                           | Food Products Database                            |    No    |  Yes  | Unknown ||                                                   | Brewdog Beer Recipes                              |    No    |  Yes  | Unknown ||                             | Food                                              |    No    |  No   | Unknown ||                                   | Food and Recipes                                  |  |  Yes  | Unknown ||                                 | Community-driven taco database                    |    No    |  No   | Unknown ||  | Food & Drink Reviews                              |    No    |  Yes  | Unknown ||                            | Cocktail Recipes                                  |  |  Yes  |   Yes   ||                                    | Meal Recipes                                      |  |  Yes  |   Yes   ||                           | NYPL human-transcribed historical menu collection |  |  No   | Unknown ||                                     | Past online whisky auctions statistical data      |    No    |  Yes  | Unknown ||                                               | Parse recipe ingredients                          |  |  Yes  |   Yes   |[<marko.inline.RawText object at 0x000001592FD7F208>]Fraud Prevention|                                             API                                              | Description                                                           |   Auth   | HTTPS |  CORS   || :------------------------------------------------------------------------------------------: | --------------------------------------------------------------------- | :------: | :---: | :-----: ||                      | Screen order information using AI to detect frauds                    |  |  Yes  | Unknown ||      | Global identity verification with phone, address, email and IP        |  |  Yes  | Unknown ||    | Phone reputation to detect spammy phones                              |  |  Yes  | Unknown ||       | Get an owner’s name, address, demographics based on the phone number  |  |  Yes  | Unknown ||  | Phone number validation, line_type, carrier append                    |  |  Yes  | Unknown ||     | Get normalized physical address, residents, address type and validity |  |  Yes  | Unknown |[<marko.inline.RawText object at 0x000001592FF336C8>]Games & Comics|                                API                                | Description                                                                                                  |      Auth       | HTTPS |  CORS   || :---------------------------------------------------------------: | ------------------------------------------------------------------------------------------------------------ | :-------------: | :---: | :-----: ||                             | Amiibo Information                                                                                           |       No        |  No   |   Yes   ||                              | Blizzard Entertainment                                                                                       |         |  Yes  | Unknown ||        | Blue Archive Game data API Characters                                                                        |       No        |  Yes  | Unknown ||                 | Jokes                                                                                                        |       No        |  No   | Unknown ||               | Clash of Clans Game Information                                                                              |         |  Yes  | Unknown ||                  | Clash Royale Game Information                                                                                |         |  Yes  | Unknown ||     | Comics                                                                                                       |       No        |  Yes  | Unknown ||                        | Deck of Cards                                                                                                |       No        |  No   | Unknown ||              | Bungie Platform API                                                                                          |         |  Yes  | Unknown ||                               | Provides information about Player stats , Match stats, Rankings for Dota 2                                   |       No        |  Yes  | Unknown ||                    | Reference for 5th edition spells, classes, monsters, and more                                                |       No        |  No   |   No    ||                           | Third-Party Developer Documentation                                                                          |          |  Yes  | Unknown ||                           | Final Fantasy XIV Game data API                                                                              |       No        |  Yes  |   Yes   ||                               | Fortnite Stats & Cosmetics                                                                                   |         |  Yes  |   Yes   ||                   | Fortnite Stats                                                                                               |         |  Yes  | Unknown ||                                 | Random images of Forza cars                                                                                  |       No        |  Yes  | Unknown ||          | Video Games                                                                                                  |       No        |  Yes  | Unknown ||          | Guild Wars 2 Game Information                                                                                |         |  Yes  | Unknown ||                             | Halo 5 and Halo Wars 2 Information                                                                           |         |  Yes  | Unknown ||                          | Hearthstone Cards Information                                                                                |  |  Yes  | Unknown ||                                | Hypixel player stats                                                                                         |         |  Yes  | Unknown ||                                  | Video Game Database                                                                                          |         |  Yes  | Unknown ||                           | Joke of the day and large category of jokes accessible via REST API                                          |         |  Yes  |   Yes   ||                           | Programming, Miscellaneous and Dark Jokes                                                                    |       No        |  Yes  |   Yes   ||              | Programming and general jokes                                                                                |       No        |  Yes  | Unknown ||                                     | Jeopardy Question Database                                                                                   |       No        |  No   | Unknown ||                | Magic The Gathering Game Information                                                                         |       No        |  No   | Unknown ||                              | Marvel Comics                                                                                                |         |  No   | Unknown ||                                      | Cross Platform Mod API                                                                                       |         |  Yes  | Unknown ||                  | Trivia Questions                                                                                             |       No        |  Yes  | Unknown ||                            | E-sports games and results                                                                                   |         |  Yes  | Unknown ||  | PUBG Stats                                                                                                   |         |  Yes  | Unknown ||                                      | Pokémon Information                                                                                          |       No        |  Yes  | Unknown ||                               | Pokémon TCG Information                                                                                      |       No        |  Yes  | Unknown ||                                    | Puns and Pun Memes                                                                                           |       No        |  Yes  |   Yes   ||               | Random Facts from hundreds of categories                                                                     |         |  Yes  |   Yes   ||                      | All the Rick and Morty information, including images                                                         |       No        |  Yes  |   Yes   ||                     | League of Legends Game Information                                                                           |         |  Yes  | Unknown ||             | Scoresaber Stats                                                                                             |       No        |  Yes  | Unknown ||                          | Magic: The Gathering database                                                                                |       No        |  Yes  |   Yes   ||    | Steam Client Interaction                                                                                     |          |  Yes  | Unknown ||                            | All SuperHeroes and Villains data from all universes under a single API                                      |         |  Yes  | Unknown ||                                  | A Multilanguage Pokémon TCG Database with Cards Pictures and most of the informations contained on the cards |       No        |  Yes  |   Yes   ||                        | The dumbest things Donald Trump has ever said                                                                |       No        |  Yes  | Unknown ||                 | Wargaming.net info and stats                                                                                 |         |  Yes  |   No    ||                                 | Retrieve xkcd comics as JSON                                                                                 |       No        |  Yes  |   No    |[<marko.inline.RawText object at 0x000001592FF25D88>]Geocoding|                                                          API                                                           | Description                                                                                         |      Auth      | HTTPS |  CORS   || :--------------------------------------------------------------------------------------------------------------------: | --------------------------------------------------------------------------------------------------- | :------------: | :---: | :-----: ||                             | Get all administrative divisions of a country                                                       |       No       |  Yes  |   Yes   ||                                                                    | Address database of France, geocoding and reverse                                                   |       No       |  Yes  | Unknown ||                                                                                   | A (country/region/city) in-cascade location API                                                     |        |  No   | Unknown ||                                                                            | Create/customize digital maps based on Bing Maps data                                               |        |  Yes  | Unknown ||                                                                   | Convert British OSGB36 easting and northing (British National Grid) to WGS84 latitude and longitude |       No       |  Yes  |   Yes   ||                                                                       | Open APIs for select European cities                                                                |       No       |  Yes  | Unknown ||                                                                                           | Get your visitors' country from their IP                                                            |       No       |  Yes  |   Yes   ||                                                                                  | Daum Maps provide multiple APIs for Korean maps                                                     |        |  No   | Unknown ||                                                                                     | Free geo ip information, no registration required. 15k/hour rate limit                              |       No       |  Yes  |   Yes   ||                                                                           | French geographical data                                                                            |       No       |  Yes  | Unknown ||                                                                                     | Address geocoding / reverse geocoding in bulk                                                       |        |  Yes  | Unknown ||                                                                                     | Provides worldwide forward/reverse geocoding, batch geocoding and geoparsing                        |       No       |  Yes  | Unknown ||                                                                                 | Enterprise-grade geocoding and geoparsing                                                           |        |  Yes  |   Yes   ||                                                              | Geocoding of city name by using latitude and longitude coordinates                                  |        |  Yes  | Unknown ||                                                                                              | IP geolocation with ChatOps integration                                                             |       No       |  Yes  |   Yes   ||                                                                                           | Forward and Reverse Geocoding with 2500 free daily limit                                            |        |  Yes  |   Yes   ||                                                                                  | IP geolocation and currency conversion                                                              |       No       |  Yes  |   Yes   ||                                                      | A cloud-based platform for planetary-scale environmental data analysis                              |        |  Yes  | Unknown ||                                                                      | Create/customize digital maps based on Google Maps data                                             |        |  Yes  | Unknown ||                                                             | Get hello translation following user language                                                       |       No       |  Yes  | Unknown ||                                                                                 | Create/customize digital maps based on HERE Maps data                                               |        |  Yes  | Unknown ||                                                           | Get all Indian cities in a clean JSON Format                                                        |       No       |  Yes  |   Yes   ||                                                                                 | Map an IP to a country                                                                              |       No       |  Yes  | Unknown ||                                                                                | Find geolocation with ip address                                                                    |       No       |  Yes  | Unknown ||                                                                                 | Lookup country, region, city, time-zone, currency and language of an IP                             |        |  Yes  |   Yes   ||                                                                                       | Find location with ip address                                                                       |       No       |  No   | Unknown ||                                                                                        | Find IP address location information                                                                |       No       |  Yes  | Unknown ||                                                                                   | Geolocation API that returns extra information about an IP address                                  |        |  Yes  | Unknown ||                                                                            | Free IP Geolocation API                                                                             |       No       |  Yes  | Unknown ||                                                      | IP geolocation web service to get more than 55 parameters                                           |        |  Yes  | Unknown ||                                                                            | Free IP geolocation API to geolocate user's location information                                    |        |  Yes  |   Yes   ||                                                            | Detect proxy and VPN using IP address                                                               |        |  Yes  | Unknown ||                                                                   | Locate your visitors by IP with country details                                                     |       No       |  Yes  |   Yes   ||                                                                                    | Free Geolocation tools and APIs for country, region, city and time zone lookup by IP address        |        |  Yes  | Unknown ||                                                                                         | Locate and identify website visitors by IP address                                                  |        |  Yes  | Unknown ||                                                                             | Find geolocation of any IP address with IP Geolocation API                                          |        |  Yes  |   Yes   ||                                                                                     | A free API to determine if a lat/lon is on water or land                                            |  |  Yes  |   Yes   ||                                                        | IP Geolocation API. Collecting data from multiple sources                                           |       No       |  Yes  | Unknown ||                                                               | Locate and get detailed information on IP address                                                   |       No       |  Yes  |   Yes   ||                                                                              | Provides forward/reverse geocoding and batch geocoding                                              |        |  Yes  |   Yes   ||                                                                            | Create/customize beautiful digital maps                                                             |        |  Yes  | Unknown ||                                                                         | Mexico RESTful zip codes API                                                                        |       No       |  Yes  | Unknown ||                                                                           | Singapore Land Authority REST API services for Singapore addresses                                  |        |  Yes  | Unknown ||                                                                                    | Forward and reverse geocoding using open data                                                       |        |  Yes  |   Yes   ||                                                                 | Navigation, geolocation and geographical data                                                       |         |  No   | Unknown ||  | Provide geolocation data based on postcode for Dutch addresses                                      |       No       |  No   | Unknown ||                                                                                    | Postcode lookup & Geolocation for the UK                                                            |       No       |  Yes  |   Yes   ||                                                                             | Get information about countries via a RESTful API                                                   |       No       |  Yes  |   Yes   ||                                                                                        | IP Geolocation and Threat Intelligence API                                                          |        |  Yes  |   Yes   ||                                                                               | Discover and share maps with friends                                                                |        |  Yes  | Unknown ||                                                       | Validate and append data for any US ZipCode                                                         |        |  Yes  |   Yes   ||                                                                                         | Brazil RESTful zip codes API                                                                        |       No       |  Yes  | Unknown ||                                                                                | US zip code distance, radius and location API                                                       |        |  Yes  | Unknown ||                                                                                  | Get information about place such as country, city, state, etc                                       |       No       |  No   | Unknown ||                                                                                  | Get the country, state, and city of any US zip-code                                                 |       No       |  Yes  | Unknown |[<marko.inline.RawText object at 0x000001592FDB9048>]Government|                                         API                                         | Description                                                                               |   Auth   | HTTPS |  CORS   || :---------------------------------------------------------------------------------: | ----------------------------------------------------------------------------------------- | :------: | :---: | :-----: ||                | Access to the laws of British Columbia                                                    |    No    |  No   | Unknown ||                                    | Authoritative information on U.S. programs, events, services and more                     |  |  Yes  | Unknown ||                  | The US Census Bureau provides various APIs and data sets on demographics and businesses   |    No    |  Yes  | Unknown ||                    | Lyon(FR) City Open Data                                                                   |  |  Yes  | Unknown ||                 | Nantes(FR) City Open Data                                                                 |  |  Yes  | Unknown ||                                 | Prague(CZ) City Open Data                                                                 |    No    |  No   | Unknown ||                                                         | The primary platform for Open Source and code sharing for the U.S. Federal Government     |  |  Yes  | Unknown ||                                     | Formatted and geolocated Colorado public data                                             |    No    |  Yes  | Unknown ||                       | Colorado State Government Open Data                                                       |    No    |  Yes  | Unknown ||                                            | US Public Data                                                                            |    No    |  Yes  | Unknown ||                                                    | US Government Data                                                                        |  |  Yes  | Unknown ||            | Contains D.C. government public datasets, including crime, GIS, financial data, and so on |    No    |  Yes  | Unknown ||                                      | Web services and data sets from the US Environmental Protection Agency                    |    No    |  Yes  | Unknown ||                                          | Information on campaign donations in federal elections                                    |  |  Yes  | Unknown ||  | The Daily Journal of the United States Government                                         |    No    |  Yes  | Unknown ||                  | UK food hygiene rating data API                                                           |    No    |  No   | Unknown ||                               | Australian Government Open Data                                                           |    No    |  Yes  | Unknown ||                                     | Belgium Government Open Data                                                              |    No    |  Yes  | Unknown ||                                  | Canadian Government Open Data                                                             |    No    |  No   | Unknown ||                                 | French Government Open Data                                                               |  |  Yes  | Unknown ||                                       | Indian Government Open Data                                                               |  |  Yes  | Unknown ||                                   | Italy Government Open Data                                                                |    No    |  Yes  | Unknown ||                            | New Zealand Government Open Data                                                          |    No    |  Yes  | Unknown ||                                    | Poland Government Open Data                                                               |    No    |  Yes  | Unknown ||                                      | Romania Government Open Data                                                              |    No    |  No   | Unknown ||                                      | Taiwan Government Open Data                                                               |    No    |  Yes  | Unknown ||                                        | United States Government Open Data                                                        |    No    |  Yes  | Unknown ||                           | Find Canadian Government Representatives                                                  |    No    |  Yes  | Unknown ||                                      | US federal spending data                                                                  |    No    |  Yes  | Unknown |[<marko.inline.RawText object at 0x000001592FF40B48>]Health|                                      API                                      | Description                                                                                  |   Auth   | HTTPS |  CORS   || :---------------------------------------------------------------------------: | -------------------------------------------------------------------------------------------- | :------: | :---: | :-----: ||                                         | Logging and retrieving diabetes information                                                  |    No    |  No   | Unknown ||                                           | Influenza-like symptoms with geotracking                                                     |    No    |  No   | Unknown ||                       | Educational content about the US Health Insurance Marketplace                                |    No    |  Yes  | Unknown ||  | List of all healthcare diagnosis codes                                                       |    No    |  Yes  | Unknown ||                                | NLP that extracts mentions of clinical concepts from text, gives access to clinical ontology |  |  Yes  | Unknown ||                                     | Makeup Information                                                                           |    No    |  No   | Unknown ||                               | Access to the data from the CMS - medicare.gov                                               |    No    |  Yes  | Unknown ||                     | National Plan & Provider Enumeration System, info on healthcare providers registered in US   |    No    |  Yes  | Unknown ||                              | Worlds largest verified nutrition database                                                   |  |  Yes  | Unknown ||                                                | Public FDA data about drugs, devices and foods                                               |    No    |  Yes  | Unknown ||                       | National Nutrient Database for Standard Reference                                            |    No    |  Yes  | Unknown |[<marko.inline.RawText object at 0x00000159300546C8>]Jobs|                                           API                                            | Description                                                                |   Auth   | HTTPS |  CORS   || :--------------------------------------------------------------------------------------: | -------------------------------------------------------------------------- | :------: | :---: | :-----: ||                                           | Job board aggregator                                                       |  |  Yes  | Unknown ||                                      | Job search engine                                                          |  |  No   | Unknown ||                                           | Jobs for software developers                                               |    No    |  Yes  | Unknown ||                                                  | Jobs with GraphQL                                                          |    No    |  Yes  |   Yes   ||                                                | Job board aggregator                                                       |  |  Yes  | Unknown ||                                  | Job aggregator                                                             |  |  Yes  | Unknown ||                                                 | Job search engine                                                          |  |  Yes  | Unknown ||                                               | Job search engine                                                          |  |  No   | Unknown ||  | Job titles, skills and related jobs data                                   |    No    |  No   | Unknown ||                                                 | Job board aggregator                                                       |  |  Yes  | Unknown ||                                 | Tap into a list of current jobs openings with the United States government |    No    |  Yes  | Unknown ||                                     | Job board and company profiles                                             |  |  Yes  | Unknown ||                                                  | Freelance job board and management system                                  |   |  Yes  | Unknown ||                                                 | US government job board                                                    |  |  Yes  | Unknown ||                                   | Job search app and website                                                 |  |  Yes  | Unknown |[<marko.inline.RawText object at 0x000001592FD4D708>]Machine Learning|                                        API                                        | Description                                             |   Auth   | HTTPS |  CORS   || :-------------------------------------------------------------------------------: | ------------------------------------------------------- | :------: | :---: | :-----: ||  | Image captioning, face recognition, NSFW classification |  |  Yes  |   Yes   ||                     | AI for code review                                      |    No    |  Yes  | Unknown ||                                               | Natural Language Processing                             |  |  Yes  | Unknown ||                                                        | Data Analytics                                          |  |  Yes  | Unknown ||                                                   | A time series analysis API                              |  |  Yes  |   Yes   ||                                          | Forecasting API for timeseries data                     |  |  Yes  | Unknown ||                                                          | Natural Language Processing                             |   |  Yes  | Unknown |[<marko.inline.RawText object at 0x000001592FFB3108>]Music|                                                       API                                                        | Description                                                                       |   Auth   | HTTPS |  CORS   || :--------------------------------------------------------------------------------------------------------------: | --------------------------------------------------------------------------------- | :------: | :---: | :-----: ||                                                                 | Automated Music Mastering                                                         |  |  Yes  |   Yes   ||                                        | Music Events                                                                      |    No    |  Yes  | Unknown ||                                                                       | Music                                                                             |   |  Yes  | Unknown ||                                                                    | Music                                                                             |   |  Yes  | Unknown ||                                                                      | Music Samples                                                                     |  |  Yes  | Unknown ||                                                                                | Crowdsourced lyrics and music knowledge                                           |   |  Yes  | Unknown ||                                                               | Music genre generator                                                             |    No    |  Yes  | Unknown ||  | Software products                                                                 |    No    |  Yes  | Unknown ||                                                                | Music                                                                             |   |  Yes  | Unknown ||                                                                              | Get music libraries, playlists, charts, and perform out of KKBOX's platform       |   |  Yes  | Unknown ||                                                                                 | Music                                                                             |  |  Yes  | Unknown ||                                                                    | Simple API to retrieve the lyrics of a song                                       |    No    |  Yes  | Unknown ||                                                                  | Music                                                                             |   |  Yes  |   Yes   ||                                  | Music                                                                             |    No    |  Yes  | Unknown ||                                                                   | Music                                                                             |  |  Yes  | Unknown ||                                                               | Download curated playlists of streaming tracks (YouTube, SoundCloud, etc...)      |      |  Yes  |   No    ||                                                          | Similarities search based on song lyrics                                          |      |  Yes  | Unknown ||                                                                   | Music Events                                                                      |   |  Yes  | Unknown ||                                                                  | Provides guitar, bass and drums tabs and chords                                   |    No    |  Yes  | Unknown ||                                                                  | Allow users to upload and share sounds                                            |   |  Yes  | Unknown ||                                              | View Spotify music catalog, manage users' libraries, get recommendations and more |   |  Yes  | Unknown ||                                                                       | Similar artist API (also works for movies and TV shows)                           |  |  Yes  | Unknown ||                                                            | Music                                                                             |  |  Yes  | Unknown ||                                                                     | Crowdsourced lyrics and music knowledge                                           |  |  Yes  | Unknown |[<marko.inline.RawText object at 0x000001592FFB6D48>]News|                                 API                                 | Description                                                                                 |   Auth   | HTTPS |  CORS   || :-----------------------------------------------------------------: | ------------------------------------------------------------------------------------------- | :------: | :---: | :-----: ||                        | Search for news and metadata from Associated Press                                          |  |  Yes  | Unknown ||  | Provides access to millions of pages of historic US newspapers from the Library of Congress |    No    |  No   | Unknown ||                            | Latest news published in various news sources, blogs and forums                             |  |  Yes  |   Yes   ||                    | RSS reader                                                                                  |   |  Yes  | Unknown ||                     | Provides news                                                                               |  |  Yes  | Unknown ||                                         | Headlines currently published on a range of news sources and blogs                          |  |  Yes  | Unknown ||                                   | Personalized news listening experience from NPR                                             |   |  Yes  | Unknown ||                | Access all the content the Guardian creates, categorised by tags and section                |  |  Yes  | Unknown ||                | RSS reader                                                                                  |  |  Yes  | Unknown |[<marko.inline.RawText object at 0x000001592FFDF3C8>]Open Data|                                     API                                     | Description                                                                                        |   Auth   | HTTPS |  CORS   || :-------------------------------------------------------------------------: | -------------------------------------------------------------------------------------------------- | :------: | :---: | :-----: ||                                   | Unofficial US Federal Government API Development                                                   |    No    |  No   | Unknown ||                                | The Internet Archive                                                                               |    No    |  Yes  | Unknown ||                                         | United States ham radio callsigns                                                                  |    No    |  Yes  | Unknown ||                                                  | Location Information Prediction                                                                    |  |  Yes  | Unknown ||                               | News articles and public datasets                                                                  |  |  Yes  | Unknown ||          | Broadest collection of public data                                                                 |  |  Yes  |   Yes   ||                                    | Mobile Device Description                                                                          |    No    |  Yes  | Unknown ||                     | Address search via the French Government                                                           |    No    |  Yes  | Unknown ||                             | Information of fruit trees of the world                                                            |    No    |  Yes  |   No    ||                                   | Get JSON formatted summary with title, description and preview image for any requested URL         |  |  Yes  |   Yes   ||                           | Marijuana strains, races, flavors and effects                                                      |  |  No   | Unknown ||                                         | Extract structured data from any website                                                           |    No    |  Yes  |   Yes   ||  | Data on corporate entities and directors in many countries                                         |  |  Yes  | Unknown ||                                            | Stock Market Data                                                                                  |    No    |  Yes  | Unknown ||              | Recreational areas, federal lands, historic sites, museums, and other attractions/resources(US)    |  |  Yes  | Unknown ||                                          | Content Curation Service                                                                           |  |  No   | Unknown ||                                 | Quality of Life Data                                                                               |    No    |  Yes  | Unknown ||         | University names, countries and domains                                                            |    No    |  Yes  | Unknown ||                                   | Courses, lecture videos, detailed information for courses etc. for the University of Oslo (Norway) |    No    |  Yes  | Unknown ||                                  | More than 1.5 million barcode numbers from all around the world                                    |  |  Yes  | Unknown ||                   | Collaboratively edited knowledge base operated by the Wikimedia Foundation                         |   |  Yes  | Unknown ||                    | Mediawiki Encyclopedia                                                                             |    No    |  Yes  | Unknown ||                     | Find Local Business                                                                                |   |  Yes  | Unknown |[<marko.inline.RawText object at 0x000001592FFDFA88>]Open Source Projects|                           API                           | Description                    |   Auth   | HTTPS |  CORS   || :-----------------------------------------------------: | ------------------------------ | :------: | :---: | :-----: ||  | Drupal.org                     |    No    |  Yes  | Unknown ||      | Evil Insults                   |    No    |  Yes  |   Yes   ||                 | Open source software libraries |  |  Yes  | Unknown |[<marko.inline.RawText object at 0x000001592FFD68C8>]Patent|                                      API                                      | Description                       |   Auth   | HTTPS |  CORS   || :---------------------------------------------------------------------------: | --------------------------------- | :------: | :---: | :-----: ||                                             | European patent search system api |   |  Yes  | Unknown ||  | Taiwan patent search system api   |  |  Yes  | Unknown ||   | USA patent api services           |    No    |  Yes  | Unknown |[<marko.inline.RawText object at 0x000001592FFDBDC8>]Personality|                                    API                                    | Description                                                            |   Auth   | HTTPS |  CORS   || :-----------------------------------------------------------------------: | ---------------------------------------------------------------------- | :------: | :---: | :-----: ||                                  | Generate random advice slips                                           |    No    |  Yes  | Unknown ||                               | JSON API for hand curated Chuck Norris jokes                           |    No    |  Yes  | Unknown ||                                         | FavQs allows you to collect, discover and share your favorite quotes   |  |  Yes  | Unknown ||                                             | Fuck Off As A Service                                                  |    No    |  No   | Unknown ||                                | Inspirational Quotes                                                   |    No    |  No   | Unknown ||                           | Get random Hindi quotes of different categories.                       |    No    |  Yes  |   Yes   ||                           | The largest selection of dad jokes on the internet                     |    No    |  Yes  | Unknown ||                                           | REST API for random Kanye West quotes                                  |    No    |  Yes  |   Yes   ||                        | Community of readers and writers offering unique perspectives on ideas |   |  Yes  | Unknown ||                                   | JSON API for a random meme scraped from reddit                         |    No    |  Yes  | Unknown ||                           | Memes on Narendra Modi                                                 |    No    |  Yes  | Unknown ||            | An api which generates quotes from programmers                         |    No    |  Yes  |   Yes   ||             | REST API for more than 5000 famous quotes                              |    No    |  Yes  | Unknown ||                        | Inspirational Quotes                                                   |    No    |  Yes  | Unknown ||                             | Get random riddles on every API call.                                  |    No    |  Yes  |   Yes   ||                             | Assess, collect and analyze Personality                                |    No    |  Yes  | Unknown ||                               | Api & web archive for the things Donald Trump has said                 |    No    |  Yes  | Unknown |[<marko.inline.RawText object at 0x0000015930051BC8>]Photography|                          API                          | Description                                                |   Auth   | HTTPS |  CORS   || :---------------------------------------------------: | ---------------------------------------------------------- | :------: | :---: | :-----: ||         | Flickr Services                                            |   |  Yes  | Unknown ||  | Build applications using the world's most powerful imagery |   |  Yes  | Unknown ||           | Jiffier GIFs                                               |   |  Yes  | Unknown ||            | Get all your gifs                                          |  |  Yes  | Unknown ||                    | Upload images                                              |  |  Yes  | Unknown ||                    | Images                                                     |   |  Yes  | Unknown ||                 | Images from Unsplash                                       |    No    |  Yes  | Unknown ||                    | Image Background removal                                   |  |  Yes  |   Yes   ||                  | Free Stock Photos and Videos                               |  |  Yes  |   Yes   ||   | Photography                                                |  |  Yes  | Unknown ||                | Resizable kitten placeholder images                        |    No    |  Yes  | Unknown ||         | URL 2 Image                                                |    No    |  Yes  | Unknown ||            | Photography                                                |   |  Yes  | Unknown ||             | Wallpapers                                                 |  |  Yes  | Unknown |[<marko.inline.RawText object at 0x0000015930026F48>]Science & Math|                                      API                                       | Description                                                                                             |   Auth   | HTTPS |  CORS   || :----------------------------------------------------------------------------: | ------------------------------------------------------------------------------------------------------- | :------: | :---: | :-----: ||                                       | Multiple astronomy data sources                                                                         |    No    |  Yes  | Unknown ||                                         | Access the world's Open Access research papers                                                          |  |  Yes  | Unknown ||                                                 | Global Biodiversity Information Facility                                                                |    No    |  Yes  |   Yes   ||                   | Access millions of museum specimens from organizations around the world                                 |    No    |  Yes  | Unknown ||                     | High Energy Physics info. system                                                                        |    No    |  Yes  | Unknown ||                                | Integrated Taxonomic Information System                                                                 |    No    |  Yes  | Unknown ||                   | Upcoming Space Launches                                                                                 |    No    |  Yes  | Unknown ||                              | Asterank.com Information                                                                                |    No    |  No   | Unknown ||                                                    | NASA data, including imagery                                                                            |    No    |  Yes  | Unknown ||                                                | Symbolic and Arithmetic Math Calculator                                                                 |    No    |  Yes  | Unknown ||                                      | Number of the day, random number generation, number facts and anything else you want to do with numbers |  |  Yes  |   Yes   ||                                                | Facts about numbers                                                                                     |    No    |  No   | Unknown ||                          | ISS astronauts, current location, etc                                                                   |    No    |  No   | Unknown ||                              | Repository and archive for study designs, research materials, data, manuscripts, etc                    |    No    |  Yes  | Unknown ||                                           | A free, open, dataset about research and scholarly activities                                           |    No    |  Yes  | Unknown ||                                | Company, vehicle, launchpad and launch data                                                             |    No    |  Yes  | Unknown ||                            | Sunset and sunrise times for a given latitude and longitude                                             |    No    |  Yes  | Unknown ||  | Earthquakes data real-time                                                                              |    No    |  Yes  | Unknown ||                          | Water quality and level info for rivers and lakes                                                       |    No    |  Yes  | Unknown ||    | World Data                                                                                              |    No    |  No   | Unknown |[<marko.inline.RawText object at 0x0000015930030048>]Security|                                             API                                             | Description                                                                            |   Auth   | HTTPS |  CORS   || :-----------------------------------------------------------------------------------------: | -------------------------------------------------------------------------------------- | :------: | :---: | :-----: ||                                                           | Search engine for Internet connected host and devices                                  |  |  Yes  |   No    ||                                                  | Chrome extension risk scoring                                                          |  |  Yes  | Unknown ||                                          | Passwords which have previously been exposed in data breaches                          |  |  Yes  | Unknown ||  | U.S. National Vulnerability Database                                                   |    No    |  Yes  | Unknown ||                                    | Domain and IP related information such as current and historical WHOIS and DNS records |  |  Yes  | Unknown ||                                                       | Search engine for Internet connected devices                                           |  |  Yes  | Unknown ||                                                    | UK Police data                                                                         |    No    |  Yes  | Unknown |[<marko.inline.RawText object at 0x0000015930050BC8>]Shopping|                                  API                                  | Description                                                                |   Auth   | HTTPS |  CORS   || :-------------------------------------------------------------------: | -------------------------------------------------------------------------- | :------: | :---: | :-----: ||  | Products, Buying Options, Categories, Recommendations, Stores and Commerce |  |  Yes  | Unknown ||                         | Database of different types of Bra Sizes                                   |   |  Yes  | Unknown ||                                 | Sell and Buy on eBay                                                       |   |  Yes  | Unknown ||                                      | Wegmans Food Markets                                                       |  |  Yes  | Unknown |[<marko.inline.RawText object at 0x0000015930020488>]Social|                                     API                                      | Description                                                                                       |   Auth   | HTTPS |  CORS   || :--------------------------------------------------------------------------: | ------------------------------------------------------------------------------------------------- | :------: | :---: | :-----: ||                                   | Access to pending and sent updates in Buffer                                                      |   |  Yes  | Unknown ||                               | Team Collaboration Software                                                                       |   |  Yes  | Unknown ||  | Database of malicious Discord accounts                                                            |  |  Yes  | Unknown ||                       | Make bots for Discord, integrate Discord onto an external platform                                |   |  Yes  | Unknown ||                                   | Communicate with Disqus data                                                                      |   |  Yes  | Unknown ||                                  | Facebook Login, Share on FB, Social Plugins, Analytics and more                                   |   |  Yes  | Unknown ||                               | Interact with Foursquare users and places (geolocation-based checkins, photos, tips, events, etc) |   |  Yes  | Unknown ||                                | Asks someone to fuck off                                                                          |    No    |  Yes  | Unknown ||                   | Get Social Media profiles and contact Information                                                 |   |  Yes  | Unknown ||                               | Social news for CS and entrepreneurship                                                           |    No    |  Yes  | Unknown ||                             | Instagram Login, Share on Instagram, Social Plugins and more                                      |   |  Yes  | Unknown ||                              | Data about Meetups from Meetup.com                                                                |  |  Yes  | Unknown ||                                         | Seamless Social Networking features, API, SDK to any app                                          |  |  Yes  | Unknown ||        | Get Open Collective data                                                                          |    No    |  Yes  | Unknown ||                                | The world's catalog of ideas                                                                      |   |  Yes  | Unknown ||                                    | Boosted version of the Telegram bot API                                                           |  |  Yes  | Unknown ||                                      | Homepage of the internet                                                                          |   |  Yes  | Unknown ||                                               | Team Instant Messaging                                                                            |   |  Yes  | Unknown ||                            | Simplified HTTP version of the MTProto API for bots                                               |   |  Yes  | Unknown ||             | Read and write Telegram data                                                                      |  |  Yes  | Unknown ||                           | A freecycling community with thousands of free items posted every day                             |   |  Yes  |   Yes   ||                               | Read and write Tumblr Data                                                                        |   |  Yes  | Unknown ||                                          | Game Streaming API                                                                                |   |  Yes  | Unknown ||                              | Read and write Twitter data                                                                       |   |  Yes  |   No    ||                                                | Read and write vk data                                                                            |   |  Yes  | Unknown |[<marko.inline.RawText object at 0x0000015930056E88>]Sports & Fitness|                                  API                                  | Description                                                                                 |      Auth       | HTTPS |  CORS   || :-------------------------------------------------------------------: | ------------------------------------------------------------------------------------------- | :-------------: | :---: | :-----: ||                                  | Ballldontlie provides access to stats data from the NBA                                     |       No        |  Yes  |   Yes   ||              | Bikewise is a place to learn about and report bike crashes, hazards and thefts              |       No        |  Yes  | Unknown ||                   | Official JSON API providing real-time league, team and player statistics about the CFL      |         |  Yes  |   No    ||                 | The Cartola FC API serves to check the partial points of your team                          |       No        |  Yes  | Unknown ||                                | City Bikes around the world                                                                 |       No        |  No   | Unknown ||                                    | F1 data from the beginning of the world championships in 1950                               |       No        |  Yes  | Unknown ||                                      | Fitbit Information                                                                          |          |  Yes  | Unknown ||        | Embed codes for goals and highlights from Premier League, Bundesliga, Serie A and many more |       No        |  Yes  |   Yes   ||            | Predictions for upcoming football matches, odds, results and stats                          |  |  Yes  | Unknown ||                | Football Data                                                                               |       No        |  No   | Unknown ||                       | JCDecaux's self-service bicycles                                                            |         |  Yes  | Unknown ||  | Current and historical NBA Statistics                                                       |       No        |  Yes  | Unknown ||              | NHL historical data and statistics                                                          |       No        |  Yes  | Unknown ||  | List of and ressources related to sports                                                    |       No        |  Yes  |   Yes   ||                                | Connect with athletes, activities and more                                                  |          |  Yes  | Unknown ||                                | Query sports data, including teams, players, games, scores and statistics                   |       No        |  No   |   No    ||                     | Crowd-Sourced Sports Data and Artwork                                                       |         |  Yes  |   Yes   ||                                | Workout manager data as exercises, muscles or equipment                                     |         |  Yes  | Unknown ||                                | Ultimate Cricket data API - Score, Scorecard, Players data, Fantasy API                                     |         |  Yes  | Unknown |[<marko.inline.RawText object at 0x0000015930056388>]Test Data|                               API                                | Description                                                              |   Auth   | HTTPS |  CORS   || :--------------------------------------------------------------: | ------------------------------------------------------------------------ | :------: | :---: | :-----: ||                   | A Meatier Lorem Ipsum Generator                                          |    No    |  Yes  | Unknown ||                 | Generate random pixel-art avatars                                        |    No    |  Yes  |   No    ||                                  | Service to generate test and fake data                                   |  |  Yes  |   Yes   ||               | Generates abstract avatar image                                          |    No    |  Yes  |   Yes   ||           | Fake data for testing and prototyping                                    |    No    |  No   | Unknown ||                                  | The ""lorem ipsum"" generator that doesn't suck                            |    No    |  No   | Unknown ||                                          | Free and public API that generates random and fake people's data in JSON |    No    |  Yes  |   No    ||                    | Tiny API mocking microservice for generating fake JSON data              |    No    |  Yes  |   No    ||                               | Generates random user data                                               |    No    |  Yes  | Unknown ||                                 | Generate random robot/alien avatars                                      |    No    |  Yes  | Unknown ||  | Generates real-life faces of people who do not exist                     |    No    |  Yes  | Unknown ||                        | Generate random fake names                                               |    No    |  Yes  | Unknown ||                  | Generate UUIDs                                                           |    No    |  Yes  |   No    ||                                   | Generate yes or no randomly                                              |    No    |  Yes  | Unknown ||                   | Random data generator                                                    |  |  Yes  |   Yes   |[<marko.inline.RawText object at 0x000001593005AB88>]Text Analysis|                                                            API                                                            | Description                                                                                         |   Auth   | HTTPS |  CORS   || :-----------------------------------------------------------------------------------------------------------------------: | --------------------------------------------------------------------------------------------------- | :------: | :---: | :-----: ||                                                                            | A collection of information retrieval and natural language APIs                                     |  |  Yes  | Unknown ||                                           | Natural language processing and text analysis                                                       |  |  Yes  |   Yes   ||                                                                             | Detects text language                                                                               |  |  Yes  | Unknown ||                                                    | Natural language understanding technology, including sentiment, entity and syntax analysis          |  |  Yes  | Unknown ||                                                 | Suite of Text Analysis APIs such as sentiment analysis, keyword extract and named entity extraction |  |  Yes  |   Yes   ||                                                                              | Text Analytics with sentiment analysis, categorization & named entity extraction                    |   |  Yes  | Unknown ||  | Natural language processing for advanced text analysis                                              |   |  Yes  | Unknown ||                                                                                | Japanese tokenizer and morphological analysis web API                                               |    No    |  Yes  |   Yes   |[<marko.inline.RawText object at 0x000001593005A9C8>]Tracking|                       API                        | Description                                                             |   Auth   | HTTPS |  CORS   || :----------------------------------------------: | ----------------------------------------------------------------------- | :------: | :---: | :-----: ||                  | An API to query Brazilian ZIP codes and orders easily, quickly and free |    No    |  No   | Unknown ||    | Provides information about parcels in transport                         |  |  No   | Unknown ||        | Shipment and Address information                                        |  |  Yes  | Unknown ||  | Small application that measures your keyboard/mouse usage               |    No    |  Yes  | Unknown |[<marko.inline.RawText object at 0x000001593005A508>]Transportation|                                                                API                                                                 | Description                                                                                      |   Auth   | HTTPS |  CORS   || :--------------------------------------------------------------------------------------------------------------------------------: | ------------------------------------------------------------------------------------------------ | :------: | :---: | :-----: ||                                                                                | Access real-time and historical data of any and all airborne aircraft                            |    No    |  Yes  | Unknown ||                                                                           | Aeronautical information in digital media produced by the Department of Airspace Control (DECEA) |  |  No   | Unknown ||                                                                   | Travel Search - Limited usage                                                                    |  |  Yes  | Unknown ||                                                                                       | Stations and predicted arrivals for BART                                                         |  |  No   | Unknown ||                                                                                              | Search car sharing trips                                                                         |  |  Yes  | Unknown ||                       | Transitland API                                                                                  |    No    |  Yes  | Unknown ||                                                                                  | A-to-B routing with turn-by-turn instructions                                                    |  |  Yes  | Unknown ||                                                                                              | Open APIs that deliver services in or regarding Iceland                                          |    No    |  Yes  | Unknown ||                                                                                                  | Audio guide for travellers                                                                       |  |  Yes  | Unknown ||                                                                      | Delays in subway lines                                                                           |    No    |  No   |   No    ||                                                                                                  | The open API for building cool stuff with transport data                                         |  |  Yes  | Unknown ||                                                                       | Global public registry of electric vehicle charging locations                                    |    No    |  Yes  | Unknown ||                                                           | Provides safe restroom access for transgender, intersex and gender nonconforming individuals     |    No    |  Yes  | Unknown ||                                                                                  | Schiphol                                                                                         |  |  Yes  | Unknown ||                                                      | Transit Aggregation                                                                              |    No    |  Yes  | Unknown ||                                                   | Marta                                                                                            |    No    |  No   | Unknown ||                                                                      | Auckland Transport                                                                               |    No    |  Yes  | Unknown ||                                                                                | Belgian transport API                                                                            |    No    |  Yes  | Unknown ||                                                   | Bordeaux Métropole public transport and more (France)                                            |  |  Yes  | Unknown ||                                                                      | MBTA API                                                                                         |    No    |  No   | Unknown ||                                                                  | Budapest public transport API                                                                    |    No    |  Yes  | Unknown ||                                                              | CTA                                                                                              |    No    |  No   | Unknown ||                                                     | Czech transport API                                                                              |    No    |  Yes  | Unknown ||                                                    | RTD                                                                                              |    No    |  No   | Unknown ||                                                                      | Finnish transport API                                                                            |    No    |  Yes  | Unknown ||                                                          | Deutsche Bahn (DB) API                                                                           |  |  No   | Unknown ||                                      | Grenoble public transport                                                                        |    No    |  No   |   No    ||                                                                    | Honolulu Transportation Information                                                              |  |  No   | Unknown ||                                                                         | India Public Transport API                                                                       |  |  Yes  | Unknown ||                                                          | Data about buses routes, parking and traffic                                                     |  |  Yes  | Unknown ||                                                                             | TfL API                                                                                          |    No    |  Yes  | Unknown ||                                                                    | TfGM transport network data                                                                      |  |  Yes  |   No    ||                                                       | OC Transpo next bus arrival API                                                                  |    No    |  No   | Unknown ||                                                         | Live schedules made simple                                                                       |    No    |  No   | Unknown ||                                              | RATP Open Data API                                                                               |    No    |  No   | Unknown ||                                                                  | SEPTA APIs                                                                                       |    No    |  No   | Unknown ||  | SPTrans                                                                                          |   |  No   | Unknown ||                                                                                | Public Transport consumer                                                                        |   |  Yes  | Unknown ||                                                                    | Official Swiss Public Transport Open Data                                                        |  |  Yes  | Unknown ||                                                                         | Swiss public transport API                                                                       |    No    |  Yes  | Unknown ||                                                             | NS, only trains                                                                                  |  |  No   | Unknown ||                                                    | OVAPI, country-wide public transport                                                             |    No    |  Yes  | Unknown ||                                                                        | TTC                                                                                              |    No    |  Yes  | Unknown ||                                                | NextBus API                                                                                      |    No    |  No   | Unknown ||                                                                  | TransLink                                                                                        |   |  Yes  | Unknown ||                                                                        | Washington Metro transport API                                                                   |   |  Yes  | Unknown ||                                                                                         | Uber ride requests and price estimation                                                          |   |  Yes  |   Yes   ||                                                                     | Platform for public transport data in emerging cities                                            |   |  Yes  | Unknown |[<marko.inline.RawText object at 0x0000015930056148>]URL Shorteners|                                    API                                     | Description                                        |   Auth   | HTTPS |  CORS   || :------------------------------------------------------------------------: | -------------------------------------------------- | :------: | :---: | :-----: ||                              | URL shortener and link management                  |   |  Yes  | Unknown ||                                       | URL shortener service                              |      |  Yes  |   Yes   ||  | Monitor, compare and optimize your marketing links |  |  Yes  | Unknown ||                       | Custom URL shortener for sharing branded links     |  |  Yes  | Unknown ||                                   | URL shortener and track shorten urls               |  |  Yes  |   Yes   |[<marko.inline.RawText object at 0x000001593005AFC8>]Vehicle|                                  API                                   | Description                                                                                      |   Auth   | HTTPS |  CORS   || :--------------------------------------------------------------------: | ------------------------------------------------------------------------------------------------ | :------: | :---: | :-----: ||  | Vehicles information from Fundação Instituto de Pesquisas Econômicas - Fipe                      |    No    |  Yes  | Unknown ||            | Vehicle data and vin decoder, specs, plates, market value, ownership cost and images             |  |  Yes  | Unknown ||          | Vehicle info, pricing, configuration, plus much more                                             |  |  Yes  |   No    ||               | Telematics data, remotely access vehicle functions, car configurator, locate service dealers     |  |  Yes  |   No    ||                                | NHTSA Product Information Catalog and Vehicle Listing                                            |    No    |  Yes  | Unknown ||                                  | Lock and unlock vehicles and get data like odometer reading and location. Works on most new cars |   |  Yes  |   Yes   |[<marko.inline.RawText object at 0x000001592FF1BF48>]Video|                                                API                                                 | Description                                                                        |   Auth   | HTTPS |  CORS   || :------------------------------------------------------------------------------------------------: | ---------------------------------------------------------------------------------- | :------: | :---: | :-----: ||                                            | Game Of Thrones API                                                                |    No    |  Yes  | Unknown ||                                            | Breaking Bad API                                                                   |    No    |  Yes  | Unknown ||                              | Some Breaking Bad quotes                                                           |    No    |  Yes  | Unknown ||                                     | TV programme of Czech TV                                                           |    No    |  No   | Unknown ||                                                   | Dailymotion Developer API                                                          |   |  Yes  | Unknown ||                                                        | Dune API                                                                           |    No    |  Yes  | Unknown ||                                                          | Harry Potter API                                                                   |  |  Yes  |   Yes   ||                                                      | Movie information                                                                  |  |  Yes  | Unknown ||                                        | API for actor Owen Wilson's ""wow"" exclamations in movies                           |    No    |  Yes  |   Yes   ||                                                                   | Data from the Harry Potter Universe: Characters, Movies, Books, Spells and Potions |    No    |  Yes  | Unknown ||  | Television                                                                         |    No    |  Yes  | Unknown ||  | A pretty simple API to let you retrieve some of the best quotes from South Park, mmkay! | No | Yes | Yes ||                                                                           | Information on all things Star Trek                                                |    No    |  Yes  |   Yes   ||                                                | Community-based movie data                                                         |  |  Yes  | Unknown ||                                                                | Movie and TV Data                                                                  |  |  Yes  |   Yes   ||                                                             | Television data                                                                    |  |  Yes  | Unknown ||                                                                 | TV Show Data                                                                       |    No    |  No   | Unknown ||                                                               | Vimeo Developer API                                                                |   |  Yes  | Unknown ||                                                   | Add YouTube functionality to your sites and apps                                   |   |  Yes  | Unknown ||                                                                  | Develop video applications with cloud-based video editing API                      |  |  Yes  | Unknown |[<marko.inline.RawText object at 0x000001592FE38C88>]Weather|                                  API                                   | Description                                        |   Auth   | HTTPS |  CORS   || :--------------------------------------------------------------------: | -------------------------------------------------- | :------: | :---: | :-----: ||                       | Weather, especially for Astroweather               |    No    |  No   | Unknown ||                         | Weather                                            |  |  Yes  | Unknown ||                                    | Weather                                            |  |  Yes  |   No    ||                         | Weather                                            |    No    |  Yes  |   No    ||          | Weather                                            |    No    |  Yes  |   Yes   ||  | Weather and climate data                           |    No    |  Yes  | Unknown ||                 | Weather and climate data                           |  |  Yes  | Unknown ||               | Weather and weather webcams                        |    No    |  No   | Unknown ||                                   | Global weather forecast API for non-commercial use |    No    |  Yes  |   Yes   ||                                         | Real-time UV Index Forecast                        |  |  Yes  | Unknown ||                         | Weather                                            |  |  No   | Unknown ||                                   | Global marine weather from multiple sources        |  |  Yes  |   Yes   ||             | Weather, for Japan                                 |    No    |  No   |   No    ||                             | Weather                                            |  |  Yes  | Unknown ||                  | Weather                                            |  |  Yes  | Unknown |[<marko.inline.RawText object at 0x000001592FEFA948>]"
https://github.com/PAIR-code/lit,The Learning Interpretability Tool: Interactively analyze ML models to understand their behavior in an extensible and framework agnostic interface.,"🔥 Learning Interpretability Tool (LIT)The Learning Interpretability Tool (🔥LIT, formerly known as the LanguageInterpretability Tool) is a visual, interactive ML model-understanding tool thatsupports text, image, and tabular data. It can be run as a standalone server, orinside of notebook environments such as Colab, Jupyter, and Google Cloud VertexAI notebooks.LIT is built to answer questions such as:LIT supports a variety of debugging workflows through a browser-based UI.Features include:LIT has a  with live demos, tutorials,a setup guide and more.Stay up to date on LIT by joining the.For a broader overview, check out  and the.DocumentationDownload and InstallationLIT can be run via container image, installed via  or built from source.Building from source is necessary if you update any of the front-end or coreback-end code.Build container imageBuild the image using  or :git clone https://github.com/PAIR-code/lit.git && cd lit
docker build --file Dockerfile --tag lit-nlp .
See the  for detailed instructions on using thedefault LIT Docker image, running LIT as a containerized web app in differentscenarios, and how to creating your own LIT images.pip installationpip install lit-nlp
The  installation will install all necessary prerequisite packages for useof the core LIT package.It does not install the prerequisites for the provided demos, so you need toinstall those yourself. See for the list ofpackages required to run the demos.Install from sourceClone the repo:git clone https://github.com/PAIR-code/lit.git && cd lit
Note: be sure you are running Python 3.10. If you have a different version onyour system, use the  instructions below to set up a Python 3.10environment.Set up a Python environment with :python -m venv .venv
source .venv/bin/activate
Or set up a Python environment using :conda create --name lit-nlp
conda activate lit-nlp
conda install python=3.10
conda install pip
Once you have the environment, install LIT's dependencies:python -m pip install -r requirements.txt
python -m pip install cudnn cupti  # optional, for GPU support
python -m pip install torch  # optional, for PyTorch

# Build the frontend
(cd lit_nlp; yarn && yarn build)
Note: Use the  option to install every dependency requiredfor the LIT library, its test suite, and the built-in examples. You can alsoinstall subsets of these using the  (core library), (test suite), (examples), and/or any combination thereof.Note: if you see running  on Ubuntu/Debian, be sure you have the.Running LITExplore a collection of hosted demos on the.Quick-start: classification and regressionTo explore classification and regression models tasks from the popular:python -m lit_nlp.examples.glue_demo --port=5432 --quickstart
Or, using :docker run --rm -e DEMO_NAME=glue_demo -p 5432:5432 -t lit-nlp --quickstart
Navigate to http://localhost:5432 to access the LIT UI.Your default view will be a fine-tuned on the,but you can switch to or using the toolbar or thegear icon in the upper right.Quick-start: language modelingTo explore predictions from a pre-trained language model (BERT or GPT-2), run:python -m lit_nlp.examples.lm_demo --models=bert-base-uncased --port=5432
Or, using :docker run --rm -e DEMO_NAME=lm_demo -p 5432:5432 -t lit-nlp --models=bert-base-uncased
And navigate to http://localhost:5432 for the UI.Notebook usageColab notebooks showing the use of LIT inside of notebooks can be found at.We provide a simple.Run all the cells to see LIT on an example classification model in the notebook.More ExamplesSee . Most are run similarly to thequickstart example above:python -m lit_nlp.examples.<example_name> --port=5432 [optional --args]
User GuideTo learn about LIT's features, check out the , orwatch this .Adding your own models or dataYou can easily run LIT with your own model by creating a custom launcher, similar to those in . Thebasic steps are:For a full walkthrough, see.Extending LIT with new componentsLIT is easy to extend with new interpretability components, generators, andmore, both on the frontend or the backend. See our  to getstarted.Pull Request ProcessTo make code changes to LIT, please work off of the  branch and(PRs) against that branch. The  branch is for stable releases, and it isexpected that the  branch will always be ahead of . areencouraged, especially for first-time contributors or contributors working oncomplex tasks (e.g., Google Summer of Code contributors). Please use these tocommunicate ideas and implementations with the LIT team, in addition to issues.Prior to sending your PR or marking a Draft PR as ""Ready for Review"", please runthe Python and TypeScript linters on your code to ensure compliance withGoogle's  and Style Guides.# Run Pylint on your code using the following command from the root of this repo
pushd lit_nlp & pylint & popd

# Run ESLint on your code using the following command from the root of this repo
pushd lit_nlp & yarn lint & popd
Citing LITIf you use LIT as part of your work, please cite:@misc{tenney2020language,
    title={The Language Interpretability Tool: Extensible, Interactive Visualizations and Analysis for {NLP} Models},
    author={Ian Tenney and James Wexler and Jasmijn Bastings and Tolga Bolukbasi and Andy Coenen and Sebastian Gehrmann and Ellen Jiang and Mahima Pushkarna and Carey Radebaugh and Emily Reif and Ann Yuan},
    booktitle = ""Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations"",
    year = ""2020"",
    publisher = ""Association for Computational Linguistics"",
    pages = ""107--118"",
    url = ""https://www.aclweb.org/anthology/2020.emnlp-demos.15"",
}
DisclaimerThis is not an official Google product.LIT is a research project and under active development by a small team. Therewill be some bugs and rough edges, but we're releasing at an early stage becausewe think it's pretty useful already. We want LIT to be an open platform, not awalled garden, and we would love your suggestions and feedback - drop us a linein the ."
https://github.com/shibing624/pycorrector,pycorrector is a toolkit for text error correction. 文本纠错，实现了Kenlm，MacBERT，GPT等纠错模型，开箱即用。," |  |  |  pycorrector: useful python text correction toolkitpycorrector: 中文文本纠错工具。支持中文音似、形似、语法错误纠正，python3开发。pycorrector实现了Kenlm、ConvSeq2Seq、BERT、MacBERT、ELECTRA、ERNIE、Transformer等多种模型的文本纠错，并在SigHAN数据集评估各模型的效果。GuideQuestion中文文本纠错任务，常见错误类型：当然，针对不同业务场景，这些问题并不一定全部存在，比如拼音输入法、语音识别校对关注音似错误；五笔输入法、OCR校对关注形似错误，搜索引擎query纠错关注所有错误类型。本项目重点解决其中的""音似、形字、语法、专名错误""等类型。Solution规则的解决思路依据语言模型检测错别字位置，通过拼音音似特征、笔画五笔编辑距离特征及语言模型困惑度特征纠正错别字。深度模型的解决思路PS：Feature思考DemoOfficial Demo: https://www.mulanai.com/product/corrector/HuggingFace Demo: https://huggingface.co/spaces/shibing624/pycorrectorrun example:  to see the demo:python examples/gradio_demo.py
Evaluation提供评估脚本：评估结果评估数据集：SIGHAN2015测试集GPU：Tesla V100，显存 32 GB| Model Name | Model Hub Link                                                                                                      | Backbone                     | GPU | Precision | Recall | F1 | QPS     ||:--------|:--------------------------------------------------------------------------------------------------------------------|:-----------------------------|:----|:----------| :--| :--- |:--------|| Rule | -                                                                                                                   | kenlm                        | CPU | 0.6860    | 0.1529 | 0.2500 | 9       || BERT-CSC                  | -                                                                                                                   | bert-base-chinese            | GPU | 0.8029    | 0.4052 | 0.5386 | 2       || BART-CSC                  |                          | fnlp/bart-base-chinese       | GPU | 0.6984    | 0.6354 | 0.6654 | 58      || T5-CSC                    | -                                                                                                                   | byt5-small                   | GPU | 0.5220    | 0.3941 | 0.4491 | 111     || Mengzi-T5-CSC             |  | mengzi-t5-base               | GPU | 0.8321    | 0.6390 | 0.7229 | 214     || ConvSeq2Seq-CSC           | -                                                                                                                   | ConvSeq2Seq                  | GPU | 0.2415    | 0.1436 | 0.1801 | 6       || ChatGLM-6B-CSC            |                        | ChatGLM                      | GPU | 0.5263    | 0.4052 | 0.4579 | 4       || MacBERT-CSC           |                    | hfl/chinese-macbert-base | GPU | 0.8254  | 0.7311 | 0.7754 | 224 |结论Installpip install -U pycorrector
orpip install -r requirements.txt

git clone https://github.com/shibing624/pycorrector.git
cd pycorrector
pip install --no-deps .
通过以上两种方法的任何一种完成安装都可以。如果不想安装依赖包，直接使用docker拉取安装好的部署环境即可。安装依赖docker run -it -v ~/.pycorrector:/root/.pycorrector shibing624/pycorrector:0.0.2
后续调用python使用即可，该镜像已经安装好kenlm、pycorrector等包，具体参见。使用示例：pip install kenlm
pip install -r requirements.txt
Usage文本纠错example: import pycorrector

corrected_sent, detail = pycorrector.correct('少先队员因该为老人让坐')
print(corrected_sent, detail)
output:少先队员应该为老人让座 [('因该', '应该', 4, 6), ('坐', '座', 10, 11)]
错误检测example: import pycorrector

idx_errors = pycorrector.detect('少先队员因该为老人让坐')
print(idx_errors)
output:[['因该', 4, 6, 'word'], ['坐', 10, 11, 'char']]
成语、专名纠错example: import sys

sys.path.append("".."")
from pycorrector.proper_corrector import ProperCorrector

m = ProperCorrector()
x = [
    '报应接中迩来',
    '今天在拼哆哆上买了点苹果',
]

for i in x:
    print(i, ' -> ', m.proper_correct(i))
output:报应接中迩来  ->  ('报应接踵而来', [('接中迩来', '接踵而来', 2, 6)])
今天在拼哆哆上买了点苹果  ->  ('今天在拼多多上买了点苹果', [('拼哆哆', '拼多多', 3, 6)])
自定义混淆集通过加载自定义混淆集，支持用户纠正已知的错误，包括两方面功能：1）【提升准确率】误杀加白；2）【提升召回率】补充召回。example: import pycorrector

error_sentences = [
    '买iphonex，要多少钱',
    '共同实际控制人萧华、霍荣铨、张旗康',
]
for line in error_sentences:
    print(pycorrector.correct(line))

print('*' * 42)
pycorrector.set_custom_confusion_path_or_dict('./my_custom_confusion.txt')
for line in error_sentences:
    print(pycorrector.correct(line))
output:('买iphonex，要多少钱', [])   # ""iphonex""漏召，应该是""iphoneX""
('共同实际控制人萧华、霍荣铨、张启康', [['张旗康', '张启康', 14, 17]]) # ""张启康""误杀，应该不用纠
*****************************************************
('买iphonex，要多少钱', [['iphonex', 'iphoneX', 1, 8]])
('共同实际控制人萧华、霍荣铨、张旗康', [])
iPhone差 iPhoneX
张旗康 张旗康
自定义语言模型默认提供下载并使用的kenlm语言模型文件是2.8G，内存小的电脑使用程序可能会吃力些。支持用户加载自己训练的kenlm语言模型，或使用2014版人民日报数据训练的模型，模型小（140M），准确率稍低，模型下载地址：。example：from pycorrector import Corrector
import os

pwd_path = os.path.abspath(os.path.dirname(__file__))
lm_path = os.path.join(pwd_path, './people2014corpus_chars.klm')
model = Corrector(language_model_path=lm_path)

corrected_sent, detail = model.correct('少先队员因该为老人让坐')
print(corrected_sent, detail)
output:少先队员应该为老人让座 [('因该', '应该', 4, 6), ('坐', '座', 10, 11)]
英文拼写纠错支持英文单词级别的拼写错误纠正。example：import pycorrector

sent = ""what happending? how to speling it, can you gorrect it?""
corrected_text, details = pycorrector.en_correct(sent)
print(sent, '=>', corrected_text)
print(details)
output:what happending? how to speling it, can you gorrect it?
=> what happening? how to spelling it, can you correct it?
[('happending', 'happening', 5, 15), ('speling', 'spelling', 24, 31), ('gorrect', 'correct', 44, 51)]
中文简繁互换支持中文繁体到简体的转换，和简体到繁体的转换。example：import pycorrector

traditional_sentence = '憂郁的臺灣烏龜'
simplified_sentence = pycorrector.traditional2simplified(traditional_sentence)
print(traditional_sentence, '=>', simplified_sentence)

simplified_sentence = '忧郁的台湾乌龟'
traditional_sentence = pycorrector.simplified2traditional(simplified_sentence)
print(simplified_sentence, '=>', traditional_sentence)
output:憂郁的臺灣烏龜 => 忧郁的台湾乌龟
忧郁的台湾乌龟 => 憂郁的臺灣烏龜
命令行模式支持批量文本纠错python -m pycorrector -h
usage: __main__.py [-h] -o OUTPUT [-n] [-d] input

@description:

positional arguments:
  input                 the input file path, file encode need utf-8.

optional arguments:
  -h, --help            show this help message and exit
  -o OUTPUT, --output OUTPUT
                        the output file path.
  -n, --no_char         disable char detect mode.
  -d, --detail          print detail info
case：python -m pycorrector input.txt -o out.txt -n -d
Deep Model Usage本项目的初衷之一是比对、共享各种文本纠错方法，抛砖引玉的作用，如果对大家在文本纠错任务上有一点小小的启发就是我莫大的荣幸了。主要使用了多种深度模型应用于文本纠错任务，分别是前面小节介绍的、、、、、、，各模型方法内置于文件夹下，有详细指导，各模型可独立运行，相互之间无依赖。pip install -r requirements-dev.txt
使用方法各模型均可独立的预处理数据、训练、预测。MacBert4csc模型[推荐]基于MacBERT改变网络结构的中文拼写纠错模型，模型已经开源在HuggingFace Models：模型网络结构：详细教程参考example：使用pycorrector调用纠错：import sys

sys.path.append("".."")
from pycorrector.macbert.macbert_corrector import MacBertCorrector

if __name__ == '__main__':
    error_sentences = [
        '真麻烦你了。希望你们好好的跳无',
        '少先队员因该为老人让坐',
        '机七学习是人工智能领遇最能体现智能的一个分知',
        '一只小鱼船浮在平净的河面上',
        '我的家乡是有明的渔米之乡',
    ]

    m = MacBertCorrector(""shibing624/macbert4csc-base-chinese"")
    for line in error_sentences:
        correct_sent, err = m.macbert_correct(line)
        print(""query:{} => {}, err:{}"".format(line, correct_sent, err))
output：query:真麻烦你了。希望你们好好的跳无 => 真麻烦你了。希望你们好好的跳舞, err:[('无', '舞', 14, 15)]
query:少先队员因该为老人让坐 => 少先队员应该为老人让坐, err:[('因', '应', 4, 5)]
query:机七学习是人工智能领遇最能体现智能的一个分知 => 机器学习是人工智能领域最能体现智能的一个分知, err:[('七', '器', 1, 2), ('遇', '域', 10, 11)]
query:一只小鱼船浮在平净的河面上 => 一只小鱼船浮在平净的河面上, err:[]
query:我的家乡是有明的渔米之乡 => 我的家乡是有名的渔米之乡, err:[('明', '名', 6, 7)]
使用原生transformers库调用纠错：import operator
import torch
from transformers import BertTokenizerFast, BertForMaskedLM
device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")

tokenizer = BertTokenizerFast.from_pretrained(""shibing624/macbert4csc-base-chinese"")
model = BertForMaskedLM.from_pretrained(""shibing624/macbert4csc-base-chinese"")
model.to(device)

texts = [""今天新情很好"", ""你找到你最喜欢的工作，我也很高心。""]

text_tokens = tokenizer(texts, padding=True, return_tensors='pt').to(device)
with torch.no_grad():
    outputs = model(**text_tokens)

def get_errors(corrected_text, origin_text):
    sub_details = []
    for i, ori_char in enumerate(origin_text):
        if ori_char in [' ', '“', '”', '‘', '’', '\n', '…', '—', '擤']:
            # add unk word
            corrected_text = corrected_text[:i] + ori_char + corrected_text[i:]
            continue
        if i >= len(corrected_text):
            break
        if ori_char != corrected_text[i]:
            if ori_char.lower() == corrected_text[i]:
                # pass english upper char
                corrected_text = corrected_text[:i] + ori_char + corrected_text[i + 1:]
                continue
            sub_details.append((ori_char, corrected_text[i], i, i + 1))
    sub_details = sorted(sub_details, key=operator.itemgetter(2))
    return corrected_text, sub_details

result = []
for ids, (i, text) in zip(outputs.logits, enumerate(texts)):
    _text = tokenizer.decode((torch.argmax(ids, dim=-1) * text_tokens.attention_mask[i]),
                             skip_special_tokens=True).replace(' ', '')
    corrected_text, details = get_errors(_text, text)
    print(text, ' => ', corrected_text, details)
    result.append((corrected_text, details))
print(result)
output:今天新情很好  =>  今天心情很好 [('新', '心', 2, 3)]
你找到你最喜欢的工作，我也很高心。  =>  你找到你最喜欢的工作，我也很高兴。 [('心', '兴', 15, 16)]
模型文件：macbert4csc-base-chinese
    ├── config.json
    ├── added_tokens.json
    ├── pytorch_model.bin
    ├── special_tokens_map.json
    ├── tokenizer_config.json
    └── vocab.txt
ErnieCSC模型基于ERNIE的中文拼写纠错模型，模型已经开源在的模型库中。模型网络结构：详细教程参考example：使用pycorrector调用纠错：
from pycorrector.ernie_csc.ernie_csc_corrector import ErnieCSCCorrector

if __name__ == '__main__':
    error_sentences = [
        '真麻烦你了。希望你们好好的跳无',
        '少先队员因该为老人让坐',
        '机七学习是人工智能领遇最能体现智能的一个分知',
        '一只小鱼船浮在平净的河面上',
        '我的家乡是有明的渔米之乡',
    ]
    corrector = ErnieCSCCorrector(""csc-ernie-1.0"")
    for line in error_sentences:
        result = corrector.ernie_csc_correct(line)[0]
        print(""query:{} => {}, err:{}"".format(line, result['target'], result['errors']))
output:
query:真麻烦你了。希望你们好好的跳无 => 真麻烦你了。希望你们好好的跳舞, err:[{'position': 14, 'correction': {'无': '舞'}}]
query:少先队员因该为老人让坐 => 少先队员应该为老人让座, err:[{'position': 4, 'correction': {'因': '应'}}, {'position': 10, 'correction': {'坐': '座'}}]
query:机七学习是人工智能领遇最能体现智能的一个分知 => 机器学习是人工智能领域最能体现智能的一个分知, err:[{'position': 1, 'correction': {'七': '器'}}, {'position': 10, 'correction': {'遇': '域'}}]
query:一只小鱼船浮在平净的河面上 => 一只小鱼船浮在平净的河面上, err:[]
query:我的家乡是有明的渔米之乡 => 我的家乡是有名的渔米之乡, err:[{'position': 6, 'correction': {'明': '名'}}]

使用PaddleNLP库调用纠错：可以使用PaddleNLP提供的Taskflow工具来对输入的文本进行一键纠错，具体使用方法如下:
from paddlenlp import Taskflow

text_correction = Taskflow(""text_correction"")
text_correction('遇到逆竟时，我们必须勇于面对，而且要愈挫愈勇，这样我们才能朝著成功之路前进。')
text_correction('人生就是如此，经过磨练才能让自己更加拙壮，才能使自己更加乐观。')

output:
[{'source': '遇到逆竟时，我们必须勇于面对，而且要愈挫愈勇，这样我们才能朝著成功之路前进。',
    'target': '遇到逆境时，我们必须勇于面对，而且要愈挫愈勇，这样我们才能朝著成功之路前进。',
    'errors': [{'position': 3, 'correction': {'竟': '境'}}]}]

[{'source': '人生就是如此，经过磨练才能让自己更加拙壮，才能使自己更加乐观。',
    'target': '人生就是如此，经过磨练才能让自己更加茁壮，才能使自己更加乐观。',
    'errors': [{'position': 18, 'correction': {'拙': '茁'}}]}]

Bart模型from transformers import BertTokenizerFast
from textgen import BartSeq2SeqModel

tokenizer = BertTokenizerFast.from_pretrained('shibing624/bart4csc-base-chinese')
model = BartSeq2SeqModel(
    encoder_type='bart',
    encoder_decoder_type='bart',
    encoder_decoder_name='shibing624/bart4csc-base-chinese',
    tokenizer=tokenizer,
    args={""max_length"": 128, ""eval_batch_size"": 128})
sentences = [""少先队员因该为老人让坐""]
print(model.predict(sentences))
output:['少先队员应该为老人让座']
如果需要训练Bart模型，请参考 https://github.com/shibing624/textgen/blob/main/examples/seq2seq/training_bartseq2seq_zh_demo.pyRelease models基于SIGHAN+Wang271K中文纠错数据集训练的Bart模型，已经release到HuggingFace Models:ConvSeq2Seq模型 模型使用示例:训练data example:# train.txt:
你说的是对，跟那些失业的人比起来你也算是辛运的。	你说的是对，跟那些失业的人比起来你也算是幸运的。
cd seq2seq
python train.py
训练sighan数据集（2104条样本），200个epoch，单卡P40GPU训练耗时：3分钟。预测python infer.py
output：Release models基于SIGHAN2015数据集训练的convseq2seq模型，已经release到github:Dataset| 数据集                          | 语料 |                                                                                下载链接                                                                                 | 压缩包大小 ||:-----------------------------| :--------- |:-------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-----:|| SIGHAN+Wang271K中文纠错数据集 | SIGHAN+Wang271K(27万条) |                                 | 106M  || 原始SIGHAN数据集            | SIGHAN13 14 15 |                                                                                                             | 339K  || 原始Wang271K数据集          | Wang271K |                                       |  93M  || 人民日报2014版语料            | 人民日报2014版 |                                                                        | 383M  || NLPCC 2018 GEC官方数据集    | NLPCC2018-GEC |                                                                                 | 114M  || NLPCC 2018+HSK熟语料      | nlpcc2018+hsk+CGED |    | 215M  || NLPCC 2018+HSK原始语料     | HSK+Lang8 |    |  81M  || 中文纠错比赛数据汇总             | Chinese Text Correction（CTC） |                                                                                                           |   -   || NLPCC 2023中文语法纠错数据集    | NLPCC 2023 Sharedtask1 |                                                    | 125M  |说明：SIGHAN+Wang271K中文纠错数据集，数据格式：[
    {
        ""id"": ""B2-4029-3"",
        ""original_text"": ""晚间会听到嗓音，白天的时候大家都不会太在意，但是在睡觉的时候这嗓音成为大家的恶梦。"",
        ""wrong_ids"": [
            5,
            31
        ],
        ""correct_text"": ""晚间会听到噪音，白天的时候大家都不会太在意，但是在睡觉的时候这噪音成为大家的恶梦。""
    }
]
字段解释：自有数据集可以使用自己数据集训练纠错模型，把自己数据集标注好，保存为跟训练样本集一样的json格式，然后加载数据训练模型即可。Language Model语言模型对于纠错步骤至关重要，当前默认使用的是从千兆中文文本训练的中文语言模型，提供人民日报2014版语料训练得到的轻量版语言模型。大家可以用中文维基（繁体转简体，pycorrector.utils.text_utils下有此功能）等语料数据训练通用的语言模型，或者也可以用专业领域语料训练更专用的语言模型。更适用的语言模型，对于纠错效果会有比较好的提升。尊重版权，传播请注明出处。TodoContactCitation如果你在研究中使用了pycorrector，请按如下格式引用：APA:Xu, M. Pycorrector: Text error correction tool (Version 0.4.2) [Computer software]. https://github.com/shibing624/pycorrector
BibTeX:@misc{Xu_Pycorrector_Text_error,
  title={Pycorrector: Text error correction tool},
  author={Ming Xu},
  year={2021},
  howpublished={\url{https://github.com/shibing624/pycorrector}},
}
Licensepycorrector 的授权协议为 Apache License 2.0，可免费用做商业用途。请在产品说明中附加pycorrector的链接和授权协议。Contribute项目代码还很粗糙，如果大家对代码有所改进，欢迎提交回本项目，在提交之前，注意以下两点：之后即可提交PR。Reference"
https://github.com/taobao/nginx-book,Nginx开发从入门到精通,".. nginx_book documentation master file, created bysphinx-quickstart on Wed Feb 29 17:58:19 2012.You can adapt this file completely to your liking, but it should at leastcontain the root  directive.Nginx开发从入门到精通缘起++++++nginx由于出色的性能，在世界范围内受到了越来越多人的关注，在淘宝内部它更是被广泛的使用，众多的开发以及运维同学都迫切的想要了解nginx模块的开发和它的内部原理，但是国内却没有一本关于这方面的书，源于此我们决定自己来写一本。本书的作者为淘宝核心系统服务器平台组的成员，本书写作的思路是从模块开发逐渐过渡到nginx原理剖析。书籍的内容会定期在这里更新，欢迎大家提出宝贵意见，不管是本书的内容问题，还是字词错误，都欢迎大家提交issue(章节标题的左侧有评注按钮)，我们会及时的跟进。.. topic:: 更新历史.. csv-table:: 
   :header: 日期, 描述
   :widths: 20, 160
   :quote: $
   :delim: |

   2012/03/01|创建目录大纲
   2012/03/28|增加了样章
   2012/05/25|更新样章
   2012/06/08|增加第5章
   2012/06/11|增加第4章
   2012/06/26|增加第6章(event module)
   2012/06/27|更新第5章部分内容
   2012/07/04|更新第6章event module部分内容
   2012/07/12|增加第12章（请求头读取，subrequest解析）
   2012/08/14|增加第2章(nginx基础架构及基础概念)
   2012/08/14|增加第2章(ngx_str_t数据结构介绍)
   2012/08/17|增加第7章(模块开发高级篇之变量)
   2012/08/25|增加第11章(nginx的启动阶段部分内容)
   2012/09/26|增加第2章(ngx_array_t,ngx_hash_t及ngx_pool_t介绍)
   2012/10/08|增加第11章(配置解析综述)
   2012/10/12|增加第2章(ngx_hash_wildcard_t,ngx_hash_combined_t及ngx_hash_keys_arrays_t介绍)
   2012/10/21|增加第2章(ngx_chain_t,ngx_list_t及ngx_buf_t介绍)
   2012/11/09|增加第12章(请求体的读取和丢弃解析)
   2012/11/24|更新第2章(ngx_buf_t的部分字段以及其他一些书写错误和表达)
   2012/12/18|更新第11章(解析http块)
   2012/12/10|增加第3章的内容
   2012/12/28|补充和完善了第3章的内容
   2013/01/25|增加了第2章(nginx的配置系统)
   2013/02/18|增加了第2章(nginx的模块化体系结构, nginx的请求处理)
   2013/03/05|增加了第12章部分内容(多阶段请求处理)
   2013/03/08|完成第11章第1节(配置解析综述、ngx_http_block)
   2013/04/16|完成第9章第1节(源码目录结构、configure原理)
   2013/09/30|完成第12章部分内容(多阶段执行链各个阶段解析)
   2013/10/11|完成第12章部分内容(filter解析)
   2013/10/11|完成第12章部分内容(ssl解析)
版权申明++++++++++++本书的著作权归作者淘宝核心系统服务器平台组成员所有。你可以：你不可以：目录++++++书籍浏览 (http://tengine.taobao.org/book/index.html)团队成员++++++++++++叔度 (http://blog.zhuzhaoyuan.com)雕梁 (http://www.pagefault.info)文景 (http://yaoweibin.cn)李子 (http://blog.lifeibo.com)卫越 (http://blog.sina.com.cn/u/1929617884)袁茁 (http://yzprofile.me)小熊 (http://dinic.iteye.com)吉兆 (http://jizhao.blog.chinaunix.net)静龙 (http://blog.csdn.net/fengmo_q)竹权 (http://weibo.com/u/2199139545)公远 (http://100continue.iteye.com/)布可 (http://weibo.com/sifeierss)"
https://github.com/fake-useragent/fake-useragent,Up-to-date simple useragent faker with real world database,"fake-useragentUp-to-date simple useragent faker with real world database.FeaturesInstallationpip install fake-useragent
Or if you have multiple Python / pip versions installed, use :pip3 install fake-useragent
UsageSimple usage examples below, see also next chapters in this readme for more advanced usages:from fake_useragent import UserAgent
ua = UserAgent()

# Get a random browser user-agent string
print(ua.random)

# Or get user-agent string from a specific browser
print(ua.chrome)
# Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36
print(ua.google)
# Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_4) AppleWebKit/537.13 (KHTML, like Gecko) Chrome/24.0.1290.1 Safari/537.13
print(ua['google chrome'])
# Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36
print(ua.firefox)
# Mozilla/5.0 (X11; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/115.0
print(ua.ff)
# Mozilla/5.0 (X11; Linux x86_64; rv:102.0) Gecko/20100101 Firefox/102.0
print(ua.safari)
# Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.2 Safari/605.1.15
Additional usageAdditional features that fake-useragent now offers since v1.2.0.If you want to specify your own browser list, you can do that via the  argument (default is: ).This example will only return random useragents from Edge and Chrome:from fake_useragent import UserAgent
ua = UserAgent(browsers=['edge', 'chrome'])
ua.random
Note: Fakeuser-agent knowns about: Chrome, Edge, Firefox and Safari. Other browsers are not popular enough and aren't part of our dataset we use.If you want to specify your own operating systems, you can do that via the  argument (default is: ).In this example you will only get Linux useragents back:from fake_useragent import UserAgent
ua = UserAgent(os='linux')
ua.random
If you want to return more popular useragent strings, you can play with the  argument (default is: , meaning all useragents will match).In this example you get only useragents that have a minimum usage percentage of 1.3% (or higher):from fake_useragent import UserAgent
ua = UserAgent(min_percentage=1.3)
ua.random
Hint: Of-course you can combine all those arguments to you liking!User-agent Python DictionarySince version 1.3.0 we now also offer you the following ""get"" properties which return the whole Python dictionary of the UA, instead of only the user-agent string:from fake_useragent import UserAgent
ua = UserAgent()

# Random user-agent dictionary (object)
ua.getRandom
# {'percent': 0.8, 'useragent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36 Edg/116.0.1938.76', 'system': 'Edge 116.0 Win10', 'browser': 'edge', 'version': 116.0, 'os': 'win10'}

# More get properties:
ua.getFirefox
# {'percent': 0.3, 'useragent': 'Mozilla/5.0 (Windows NT 10.0; rv:109.0) Gecko/20100101 Firefox/118.0', 'system': 'Firefox 118.0 Win10', 'browser': 'firefox', 'version': 118.0, 'os': 'win10'}
ua.getChrome
ua.getSafari
ua.getEdge

# And a method with an argument.
# This is exactly the same as using: ua.getFirefox
ua.getBrowser('firefox')
NotesYou can override the fallback string using the  parameter, in very rare cases something failed:import fake_useragent

ua = fake_useragent.UserAgent(fallback='your favorite Browser')
# in case if something went wrong, one more time it is REALLY!!! rare case
ua.random == 'your favorite Browser'
If you will try to get unknown browser:from fake_useragent import UserAgent
ua = UserAgent()
print(ua.unknown)
#Error occurred during getting browser: randm, but was suppressed with fallback.
#Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36
If you need to safe some attributes from overriding them in UserAgent by  methoduse  you can pass there attributes names.At least this will prevent you from raising FakeUserAgentError when attribute not found.For example, when using fakeuseragent with  you need to:import fake_useragent

ua = fake_useragent.UserAgent(safe_attrs=('__injections__',))
Please, do not use if you don't understand why you need this.This is magic for rarely extreme case.Experiencing issues?Make sure that you using latest version!pip install --upgrade fake-useragent
Or if that isn't working, try to install the latest package version like this ( is an example, check what the ):pip install fake-useragent==1.3.0
Check version via the Python console:import fake_useragent

print(fake_useragent.VERSION)
And you are always welcome to post .Please do not forget to mention the version that you are using.For DevelopersSince GitHub Actions is unable to reach willshouse.com and has Cloudflare protection. We can run the script below to automatically scrape the user-agent strings from the external data source. The script will copy the  file to the  directory. Execute:./update_data_file.sh
The data JSON file is part of the Python package, see . Read more about .Python Virtual EnvironmentWe encourage to use Python virtual environment before installing Pip packages, like so:python -m virtualenv env
source env/bin/activate
Testspip install -r requirements.txt
tox
LintingTo fix imports:pip install -r requirements.txt
ruff --select=""I"" --fix .
Fix black code formatting errors:pip install -r requirements.txt
black .
ChangelogAuthorsYou can visit ."
https://github.com/miso-belica/sumy,Module for automatic summarization of text documents and HTML pages.,"Automatic text summarizer Simple library and command line utility for extracting summary from HTMLpages or plain texts. The package also contains simple evaluationframework for text summaries. Implemented summarization methods are described in the . I also maintain a list of  of the summarizers in various programming languages.Is my natural language supported?There is a  it is. But if not it is  it.InstallationMake sure you have  3.6+ and(,)installed. Run simply (preferred way):$ [sudo] pip install sumy
$ [sudo] pip install git+git://github.com/miso-belica/sumy.git  # for the fresh version
UsageThanks to some good soul out there, the easiest way to try sumy is in your browser at https://huggingface.co/spaces/issam9/sumy_spaceSumy contains command line utility for quick summarization of documents.$ sumy lex-rank --length=10 --url=https://en.wikipedia.org/wiki/Automatic_summarization # what's summarization?
$ sumy lex-rank --language=uk --length=30 --url=https://uk.wikipedia.org/wiki/Україна
$ sumy luhn --language=czech --url=https://www.zdrojak.cz/clanky/automaticke-zabezpeceni/
$ sumy edmundson --language=czech --length=3% --url=https://cs.wikipedia.org/wiki/Bitva_u_Lipan
$ sumy --help # for more info
Various evaluation methods for some summarization method can be executedby commands below:$ sumy_eval lex-rank reference_summary.txt --url=https://en.wikipedia.org/wiki/Automatic_summarization
$ sumy_eval lsa reference_summary.txt --language=czech --url=https://www.zdrojak.cz/clanky/automaticke-zabezpeceni/
$ sumy_eval edmundson reference_summary.txt --language=czech --url=https://cs.wikipedia.org/wiki/Bitva_u_Lipan
$ sumy_eval --help # for more info
If you don't want to bother by the installation, you can try it as a container.$ docker run --rm misobelica/sumy lex-rank --length=10 --url=https://en.wikipedia.org/wiki/Automatic_summarization
Python APIOr you can use sumy like a library in your project. Create file  () with the code below to test it.# -*- coding: utf-8 -*-

from __future__ import absolute_import
from __future__ import division, print_function, unicode_literals

from sumy.parsers.html import HtmlParser
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.lsa import LsaSummarizer as Summarizer
from sumy.nlp.stemmers import Stemmer
from sumy.utils import get_stop_words


LANGUAGE = ""english""
SENTENCES_COUNT = 10


if __name__ == ""__main__"":
    url = ""https://en.wikipedia.org/wiki/Automatic_summarization""
    parser = HtmlParser.from_url(url, Tokenizer(LANGUAGE))
    # or for plain text files
    # parser = PlaintextParser.from_file(""document.txt"", Tokenizer(LANGUAGE))
    # parser = PlaintextParser.from_string(""Check this out."", Tokenizer(LANGUAGE))
    stemmer = Stemmer(LANGUAGE)

    summarizer = Summarizer(stemmer)
    summarizer.stop_words = get_stop_words(LANGUAGE)

    for sentence in summarizer(parser.document, SENTENCES_COUNT):
        print(sentence)
Interesting projects using sumyI found some interesting projects while browsing the internet or sometimes people wrote me an e-mail with questions, and I was curious how they use the sumy :)"
https://github.com/Fewbytes/rubber-docker,A workshop on Linux containers: Rebuild Docker from Scratch,"Docker From Scratch WorkshopPreparatory Talkcovers all the basics you'll need for this workshop, including:The WorkshopUse  while advancing through the levels, adding more features to your container.Remember to go over each level's readme, and if things get rough -you can always find the solution for level N in the level N+1 skeleton.The linux python moduleNot all the necessary system calls are exposed in python's standard library.In addition, we want to preserve the semantics of the system calls and use them as if we were writing C.We therefore wrote a python module called linux (take a look at ) which exposes the relevant system calls.Have a look at the  for more info.QuickstartThere are currently 3 options to start the workshop by yourself:The workshop material is checked out at  on the instance:Before starting the workshop, go over the prep docs in the  folder.Start the workshop at .PR stuffThis workshop has been publicly given in many places starting February 2016.FAQWhy did you create this?Because we feel the only way to truly understand something to build it from scratch - and Linux containers are a very hyped and poorly understood technologyCan I use this repository to conduct my own public/private workshop?Of course! If you do, please consider letting us know on Twitter (@nukemberg and @nocoot) and of course send feedback.This workshop doesn't cover seccomp/user containers/whateverYes, no way we can cover the entire featureset of a real container engine. We tried to concentrate on thing we believe are important for understanding how containers workI found a bug!See contributions belowContributionsContributions are welcome! If you found a bug or something to improve feel free to open an issue or a pull request. Please note that the entire repository is under MIT license and your contribution will be under that license.SponsorsWe'd like to thank our friends at  for kindly providing their platform, and allowing us to deliver this and other workshops without worrying about infrastructure.If you plan to deliver this workshop yourself, we highly encourage you to ."
https://github.com/django-extensions/django-extensions,This is a repository for collecting global custom management extensions for the Django Framework. ,"===================Django Extensions.. image:: https://img.shields.io/pypi/l/django-extensions.svg:target: https://raw.githubusercontent.com/django-extensions/django-extensions/master/LICENSE.. image:: https://github.com/django-extensions/django-extensions/actions/workflows/compile_catalog.yml/badge.svg:target: https://github.com/django-extensions/django-extensions/actions.. image:: https://github.com/django-extensions/django-extensions/actions/workflows/linters.yml/badge.svg:target: https://github.com/django-extensions/django-extensions/actions.. image:: https://github.com/django-extensions/django-extensions/actions/workflows/precommit.yml/badge.svg:target: https://github.com/django-extensions/django-extensions/actions.. image:: https://github.com/django-extensions/django-extensions/actions/workflows/pytest.yml/badge.svg:target: https://github.com/django-extensions/django-extensions/actions.. image:: https://github.com/django-extensions/django-extensions/actions/workflows/security.yml/badge.svg:target: https://github.com/django-extensions/django-extensions/actions.. image:: https://img.shields.io/pypi/v/django-extensions.svg:target: https://pypi.python.org/pypi/django-extensions/:alt: Latest PyPI version.. image:: https://img.shields.io/pypi/wheel/django-extensions.svg:target: https://pypi.python.org/pypi/django-extensions/:alt: Supports Wheel format.. image:: https://coveralls.io/repos/django-extensions/django-extensions/badge.svg?branch=master:target: https://coveralls.io/r/django-extensions/django-extensions?branch=master:alt: CoverageDjango Extensions is a collection of custom extensions for the Django Framework.Getting StartedThe easiest way to figure out what Django Extensions are all about is to watch the__ (). In a couple to help show you even more.RequirementsDjango Extensions requires Django 3.2 or later.Getting ItYou can get Django Extensions by using pip::$ pip install django-extensions
If you want to install it from source, grab the git repository from GitHub and run setup.py::$ git clone git://github.com/django-extensions/django-extensions.git
$ cd django-extensions
$ python setup.py install
Installing ItTo enable  in your project you need to add it to  in your projects file:.. code-block:: pythonINSTALLED_APPS = (
    ...
    'django_extensions',
    ...
)
Using ItGenerate (and view) a graphviz graph of app models::$ python manage.py graph_models -a -o myapp_models.png
Produce a tab-separated list of  tuples for a project::$ python manage.py show_urls
Check templates for rendering errors::$ python manage.py validate_templates
Run the enhanced django shell::$ python manage.py shell_plus
Run the enhanced django runserver, (requires Werkzeug install)::$ python manage.py runserver_plus
Getting InvolvedOpen Source projects can always use more help. Fixing a problem, documenting a feature, addingtranslation in your language. If you have some time to spare and like to help us, here are the places to do so:DocumentationYou can view documentation online at:Or you can look at the docs/ directory in the repository.SupportDjango Extensions is free and always will be. It is developed and maintained by developers in an Open Source manner.Any support is welcome. You could help by writing documentation, pull-requests, report issues and/or translations.Please remember that nobody is paid directly to develop or maintain Django Extensions so we do have to divide our timebetween putting food on the table, family, this project and the rest of life :-)__ https://ericholscher.com/blog/2008/sep/12/screencast-django-command-extensions/__ https://vimeo.com/1720508__ https://www.youtube.com/watch?v=1F6G3ONhr4k"
https://github.com/ryankiros/neural-storyteller,A recurrent neural network for generating little stories about images,"neural-storytellerneural-storyteller is a recurrent neural network that generates little stories about images. This repository contains code for generating stories with your own images, as well as instructions for training new models. has made an awesome blog post with lots of results .Some more results from an older model trained on Adventure books can be found .The whole approach contains 4 components:The 'style-shifting' operation is what allows our model to transfer standard image captions to the style of stories from novels. The only source of supervision in our models is from  captions. That is, we did not collect any new training data to directly predict stories given images.Style shifting was inspired by  but the technical details are completely different.How does it work?We first train a recurrent neural network (RNN) decoder on romance novels. Each passage from a novel is mapped to a skip-thought vector. The RNN then conditions on the skip-thought vector and aims to generate the passage that it has encoded. We use romance novels collected from the BookCorpus .Parallel to this, we train a visual-semantic embedding between COCO images and captions. In this model, captions and images are mapped into a common vector space. After training, we can embed new images and retrieve captions.Given these models, we need a way to bridge the gap between retrieved image captions and passages in novels. That is, if we had a function F that maps a collection of image caption vectors x to a book passage vector F(x), then we could feed F(x) to the decoder to get our story. There is no such parallel data, so we need to construct F another way.It turns out that skip-thought vectors have some intriguing properties that allow us to construct F in a really simple way. Suppose we have 3 vectors: an image caption x, a ""caption style"" vector c and a ""book style"" vector b. Then we define F asF(x) = x - c + bwhich intuitively means: keep the ""thought"" of the caption, but replace the image caption style with that of a story. Then, we simply feed F(x) to the decoder.How do we construct c and b? Here, c is the mean of the skip-thought vectors for Microsoft COCO training captions. We set b to be the mean of the skip-thought vectors for romance novel passages that are of length > 100.What kind of biases work?Skip-thought vectors are sensitive to:For the last point, if you bias using text all written the same way the stories you get will also be written the same way.What can the decoder be trained on?We use romance novels, but that is because we have over 14 million passages to train on. Anything should work, provided you have a lot of text! If you want to train your own decoder, you can use the code available  Any models trained there can be substituted here.DependenciesThis code is written in python. To use it you will need:For running on CPU, you will need to install  and its python interface.Getting startedYou will first need to download some pre-trained models and style vectors. Most of the materials are available in a single compressed file, which you can obtain by runningwget http://www.cs.toronto.edu/~rkiros/neural_storyteller.zip
Included is a pre-trained decoder on romance novels, the decoder dictionary, caption and romance style vectors, MS COCO training captions and a pre-trained image-sentence embedding model.Next, you need to obtain the pre-trained skip-thoughts encoder. Go  and follow the instructions on the main page to obtain the pre-trained model.Finally, we need the VGG-19 ConvNet parameters. You can obtain them by runningwget https://s3.amazonaws.com/lasagne/recipes/pretrained/imagenet/vgg19.pkl
Note that this model is for non-commercial use only. Once you have all the materials, open  and specify the locations of all of the models and style vectors that you downloaded.For running on CPU, you will need to download the VGG-19 prototxt and model by:wget http://www.robots.ox.ac.uk/~vgg/software/very_deep/caffe/VGG_ILSVRC_19_layers.caffemodel
wget https://gist.githubusercontent.com/ksimonyan/3785162f95cd2d5fee77/raw/bb2b4fe0a9bb0669211cf3d0bc949dfdda173e9e/VGG_ILSVRC_19_layers_deploy.prototxt
You also need to modify pycaffe and model path in , and modify the flag in line 8 as:FLAG_CPU_MODE = True
Generating a storyThe images directory contains some sample images that you can try the model on. In order to generate a story, open Ipython and run the following:import generate
z = generate.load_all()
generate.story(z, './images/ex1.jpg')
If everything works, it will first print out the nearest COCO captions to the image (predicted by the visual-semantic embedding model). Then it will print out a story.Generation optionsThere are 2 knobs that can be tuned for generation: the number of retrieved captions to condition on as well as the beam search width. The defaults aregenerate.story(z, './images/ex1.jpg', k=100, bw=50)
where k is the number of captions to condition on and bw is the beam width. These are reasonable defaults but playing around with these can give you very different outputs! The higher the beam width, the longer it takes to generate a story.If you bias by song lyrics, you can turn on the lyric flag which will print the output in multiple lines by comma delimiting.  contains an additional bias vector called  which is the mean of skip-thought vectors across Taylor Swift lyrics. If you point  to this vector in , you can generate captions in the style of Taylor Swift lyrics. For example:generate.story(z, './images/ex1.jpg', lyric=True)
should outputYou re the only person on the beach right now
you know
I do n't think I will ever fall in love with you
and when the sea breeze hits me
I thought
Hey
ReferenceThis project does not have any associated paper with it. If you found this code useful, please consider citing:Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S. Zemel, Antonio Torralba, Raquel Urtasun, and Sanja Fidler. ""Skip-Thought Vectors."" arXiv preprint arXiv:1506.06726 (2015).@article{kiros2015skip,
  title={Skip-Thought Vectors},
  author={Kiros, Ryan and Zhu, Yukun and Salakhutdinov, Ruslan and Zemel, Richard S and Torralba, Antonio and Urtasun, Raquel and Fidler, Sanja},
  journal={arXiv preprint arXiv:1506.06726},
  year={2015}
}
If you also use the BookCorpus data for training new models, please also consider citing:Yukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, Sanja Fidler.""Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books."" arXiv preprint arXiv:1506.06724 (2015).@article{zhu2015aligning,
    title={Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books},
    author={Zhu, Yukun and Kiros, Ryan and Zemel, Richard and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
    journal={arXiv preprint arXiv:1506.06724},
    year={2015}
}
"
https://github.com/androguard/androguard,Reverse engineering and pentesting for Android applications ,"AndroguardInstallationPlease see the ... let's start reversing!FeaturesAndroguard is a full python tool to play with Android files.Authors: Androguard TeamAndroguard + tools: Anthony Desnos (desnos at t0t0.fr).DAD (DAD is A Decompiler): Geoffroy Gueguen (geoffroy dot gueguen at gmail dot com)DocumentationFind the documentation for master on .There are some (probably broken/outdated) examples and demos in the folders  and .Projects using AndroguardIn alphabetical orderYou are using Androguard and are not listed here? Just create a  or send us a  with your project!LicensesAndroguardCopyright (C) 2012 - 2023, Anthony Desnos (desnos at t0t0.fr)All rights reserved.Licensed under the Apache License, Version 2.0 (the ""License"");you may not use this file except in compliance with the License.You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an ""AS-IS"" BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License.DADCopyright (C) 2012 - 2016, Geoffroy Gueguen (geoffroy dot gueguen at gmail dot com)All rights reserved.Licensed under the Apache License, Version 2.0 (the ""License"");you may not use this file except in compliance with the License.You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an ""AS-IS"" BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License."
https://github.com/chriskiehl/Gooey,Turn (almost) any Python command line program into a full GUI application with one line,"GooeyTurn (almost) any Python 3 Console Program into a GUI application with one lineTable of ContentsQuick StartInstallation instructionsThe easiest way to install Gooey is via pip install Gooey 
Alternatively, you can install Gooey by cloning the project to your local directorygit clone https://github.com/chriskiehl/Gooey.git
run  python setup.py install
UsageGooey is attached to your code via a simple decorator on whichever method has your  declarations (usually ).from gooey import Gooey

@Gooey      <--- all it takes! :)
def main():
  parser = ArgumentParser(...)
  # rest of code
Different styling and functionality can be configured by passing arguments into the decorator.# options
@Gooey(advanced=Boolean,          # toggle whether to show advanced config or not 
       language=language_string,  # Translations configurable via json
       auto_start=True,           # skip config screens all together
       target=executable_cmd,     # Explicitly set the subprocess executable arguments
       program_name='name',       # Defaults to script name
       program_description,       # Defaults to ArgParse Description
       default_size=(610, 530),   # starting size of the GUI
       required_cols=1,           # number of columns in the ""Required"" section
       optional_cols=2,           # number of columns in the ""Optional"" section
       dump_build_config=False,   # Dump the JSON Gooey uses to configure itself
       load_build_config=None,    # Loads a JSON Gooey-generated configuration
       monospace_display=False)   # Uses a mono-spaced font in the output screen
)
def main():
  parser = ArgumentParser(...)
  # rest of code
        
See:  section for details on each option.Gooey will do its best to choose sensible widget defaults to display in the GUI. However, if more fine tuning is desired, you can use the drop-in replacement  in place of . This lets you control which widget displays in the GUI. See: from gooey import Gooey, GooeyParser

@Gooey
def main():
  parser = GooeyParser(description=""My Cool GUI Program!"") 
  parser.add_argument('Filename', widget=""FileChooser"")
  parser.add_argument('Date', widget=""DateChooser"")
  ...
ExamplesGooey downloaded and installed? Great! Wanna see it in action? Head over the the  to download a few ready-to-go example scripts. They'll give you a quick tour of all Gooey's various layouts, widgets, and features. What is it?Gooey converts your Console Applications into end-user-friendly GUI applications. It lets you focus on building robust, configurable programs in a familiar way, all without having to worry about how it will be presented to and interacted with by your average user. Why?Because as much as we love the command prompt, the rest of the world looks at it like an ugly relic from the early '80s. On top of that, more often than not programs need to do more than just one thing, and that means giving options, which previously meant either building a GUI, or trying to explain how to supply arguments to a Console Application. Gooey was made to (hopefully) solve those problems. It makes programs easy to use, and pretty to look at! Who is this for?If you're building utilities for yourself, other programmers, or something which produces a result that you want to capture and pipe over to another console application (e.g. *nix philosophy utils), Gooey probably isn't the tool for you. However, if you're building 'run and done,' around-the-office-style scripts, things that shovel bits from point A to point B, or simply something that's targeted at a non-programmer, Gooey is the perfect tool for the job. It lets you build as complex of an application as your heart desires all while getting the GUI side for free. How does it work?Gooey is attached to your code via a simple decorator on whichever method has your  declarations.@Gooey
def my_run_func():
  parser = ArgumentParser(...)
  # rest of code
At run-time, it parses your Python script for all references to . (The older  is currently not supported.) These references are then extracted, assigned a  based on the  they provide, and finally used to assemble the GUI.  Mappings:Gooey does its best to choose sensible defaults based on the options it finds. Currently,  are mapped to the following  components. | Parser Action    | Widget    | Example ||:----------------------|-----------|------|| store  |  TextCtrl |  || store_const | CheckBox ||| store_true | CheckBox | || store_False | CheckBox|     || version | CheckBox|     || append | TextCtrl |    || count | DropDown &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |  || Mutually Exclusive Group | RadioGroup | |choice &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|        DropDown |  |GooeyParserIf the above defaults aren't cutting it, you can control the exact widget type by using the drop-in  replacement . This gives you the additional keyword argument , to which you can supply the name of the component you want to display. Best part? You don't have to change any of your  code to use it. Drop it in, and you're good to go. Example:from argparse import ArgumentParser
....

def main(): 
    parser = ArgumentParser(description=""My Cool Gooey App!"")
    parser.add_argument('filename', help=""name of the file to process"") 
Given then above, Gooey would select a normal  as the widget type like this: However, by dropping in  and supplying a  name, you can display a much more user friendly from gooey import GooeyParser
....

def main(): 
    parser = GooeyParser(description=""My Cool Gooey App!"")
    parser.add_argument('filename', help=""name of the file to process"", widget='FileChooser') 
    
Custom Widgets:| Widget         |           Example            ||----------------|------------------------------|| DirChooser, FileChooser, MultiFileChooser, FileSaver, MultiFileSaver   |  || DateChooser/TimeChooser   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|  Please note that for both of these widgets the values passed to the application will always be in  while localized values may appear in some parts of the GUI depending on end-user settings. || PasswordField |  || Listbox |  || BlockCheckbox |   The default InlineCheck box can look less than ideal if a large help text block is present.  moves the text block to the normal position and provides a short-form  for display next to the control. Use  to control the label text ||  ColourChooser   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|  || FilterableDropdown |  || IntegerField |  || DecimalField |  || Slider |  |InternationalizationGooey is international ready and easily ported to your host language. Languages are controlled via an argument to the  decorator. @Gooey(language='russian')
def main(): 
    ... 
All program text is stored externally in  files. So adding new language support is as easy as pasting a few key/value pairs in the  directory. Thanks to some awesome , Gooey currently comes pre-stocked with over 18 different translations! Want to add another one? Submit a Global ConfigurationJust about everything in Gooey's overall look and feel can be customized by passing arguments to the decorator. | Parameter | Summary ||-----------|---------|| encoding | Text encoding to use when displaying characters (default: 'utf-8') || use_legacy_titles | Rewrites the default argparse group name from ""Positional"" to ""Required"". This is primarily for retaining backward compatibility with previous versions of Gooey (which had poor support/awareness of groups and did its own naive bucketing of arguments). || advanced | Toggles whether to show the 'full' configuration screen, or a simplified version || auto_start | Skips the configuration all together and runs the program immediately || language | Tells Gooey which language set to load from the  directory.|| target | Tells Gooey how to re-invoke itself. By default Gooey will find python, but this allows you to specify the program (and arguments if supplied).|| suppress_gooey_flag | Should be set when using a custom . Prevent Gooey from injecting additional CLI params ||program_name | The name displayed in the title bar of the GUI window. If not supplied, the title defaults to the script name pulled from . || program_description | Sets the text displayed in the top panel of the  screen. Defaults to the description pulled from . || default_size | Initial size of the window || fullscreen | start Gooey in fullscreen mode || required_cols | Controls how many columns are in the Required Arguments section  :warning: Deprecation notice: See  for modern layout controls|| optional_cols | Controls how many columns are in the Optional Arguments section  :warning: Deprecation notice: See  for modern layout controls|| dump_build_config | Saves a  copy of its build configuration on disk for reuse/editing || load_build_config | Loads a  copy of its build configuration from disk || monospace_display | Uses a mono-spaced font in the output screen  :warning: Deprecation notice: See  for modern font configuration|| image_dir | Path to the directory in which Gooey should look for custom images/icons || language_dir | Path to the directory in which Gooey should look for custom languages files || disable_stop_button | Disable the  button when running || show_stop_warning | Displays a warning modal before allowing the user to force termination of your program || force_stop_is_error | Toggles whether an early termination by the shows the success or error screen || show_success_modal | Toggles whether or not to show a summary modal after a successful run || show_failure_modal | Toggles whether or not to show a summary modal on failure || show_restart_button | Toggles whether or not to show the restart button at the end of execution || run_validators | Controls whether or not to have Gooey perform validation before calling your program || poll_external_updates | (Experimental!) When True, Gooey will call your code with a  CLI argument and use the response to fill out dynamic values in the UI (See: )|| use_cmd_args | Substitute any command line arguments provided at run time for the default values specified in the Gooey configuration || return_to_config | When True, Gooey will return to the configuration settings window upon successful run || progress_regex | A text regex used to pattern match runtime progress information. See:  for a detailed how-to || progress_expr | A python expression applied to any matches found via the . See:  for a detailed how-to || hide_progress_msg | Option to hide textual progress updates which match the . See:  for a detailed how-to || disable_progress_bar_animation | Disable the progress bar || timing_options | This contains the options for displaying time remaining and elapsed time, to be used with  and . . Contained as a dictionary with the options  and . Eg:  || show_time_remaining | Disable the time remaining text see  || hide_time_remaining_on_complete | Hide time remaining on complete screen see  || requires_shell | Controls whether or not the  argument is used when invoking your program.  || shutdown_signal | Specifies the  to send to the child process when the  button is pressed. See  in the docs for more info. || navigation | Sets the ""navigation"" style of Gooey's top level window. Options:   TABBEDSIDEBAR   || sidebar_title |  Controls the heading title above the SideBar's navigation pane. Defaults to: ""Actions"" || show_sidebar | Show/Hide the sidebar in when navigation mode ==  || body_bg_color | HEX value of the main Gooey window || header_bg_color | HEX value of the header background || header_height | height in pixels of the header || header_show_title | Show/Hide the header title || header_show_subtitle | Show/Hide the header subtitle || footer_bg_color | HEX value of the Footer background || sidebar_bg_color | HEX value of the Sidebar's background || terminal_panel_color | HEX value of the terminal's panel || terminal_font_color | HEX value of the font displayed in Gooey's terminal || terminal_font_family | Name of the Font Family to use in the terminal || terminal_font_weight | Weight of the font (, ) || terminal_font_size | Point size of the font displayed in the terminal || error_color | HEX value of the text displayed when a validation error occurs || richtext_controls | Switch on/off the console support for terminal control sequences (limited support for font weight and color). Defaults to : False. See  for additional details || menus | Show custom menu groups and items (see:  || clear_before_run | When true, previous output will be cleared from the terminal when running program again |Layout CustomizationYou can achieve fairly flexible layouts with Gooey by using a few simple customizations. At the highest level, you have several overall layout options controllable via various arguments to the Gooey decorator.|  |  |  |   ||---------------------|----------------------|----------------------|------------------------|| | | | |Grouping InputsBy default, if you're using Argparse with Gooey, your inputs will be split into two buckets:  and . However, these aren't always the most descriptive groups to present to your user. You can arbitrarily bucket inputs into logic groups and customize the layout of each. With  this is done via parser = ArgumentParser()
search_group = parser.add_argument_group(
    ""Search Options"", 
    ""Customize the search options""
)
You can add arguments to the group as normal search_group.add_argument(
    '--query', 
    help='Base search string'
) 
Which will display them as part of the group within the UI. Run ModesGooey has a handful of presentation modes so you can tailor its layout to your content type and user's level or experience. AdvancedThe default view is the ""full"" or ""advanced"" configuration screen. It has two different layouts depending on the type of command line interface it's wrapping. For most applications, the flat layout will be the one to go with, as its layout matches best to the familiar CLI schema of a primary command followed by many options (e.g. Curl, FFMPEG). On the other side is the Column Layout. This one is best suited for CLIs that have multiple paths or are made up of multiple little tools each with their own arguments and options (think: git). It displays the primary paths along the left column, and their corresponding arguments in the right. This is a great way to package a lot of varied functionality into a single app. Both views present each action in the  as a unique GUI component. It makes it ideal for presenting the program to users which are unfamiliar with command line options and/or Console Programs in general. Help messages are displayed along side each component to make it as clear as possible which each widget does.Setting the layout style:Currently, the layouts can't be explicitly specified via a parameter (on the TODO!). The layouts are built depending on whether or not there are  used in your code base. So, if you want to trigger the , you'll need to add a  to your  code. It can be toggled via the  parameter in the  decorator. @gooey(advanced=True)
def main():
    # rest of code   
    
BasicThe basic view is best for times when the user is familiar with Console Applications, but you still want to present something a little more polished than a simple terminal. The basic display is accessed by setting the  parameter in the  decorator to . @gooey(advanced=False)
def main():
    # rest of code  
No ConfigNo Config pretty much does what you'd expect: it doesn't show a configuration screen. It hops right to the  section and begins execution of the host program. This is the one for improving the appearance of little one-off scripts. To use this mode, set  in the Gooey decorator. @Gooey(auto_start=True) 
def main (): 
    ... 
MenusYou can add a Menu Bar to the top of Gooey with customized menu groups and items.Menus are specified on the main  decorator as a list of maps. @Gooey(menu=[{}, {}, ...])
Each map is made up of two key/value pairs You can have as many menu groups as you want. They're passed as a list to the  argument on the  decorator.@Gooey(menu=[{'name': 'File', 'items: []},
             {'name': 'Tools', 'items': []},
             {'name': 'Help', 'items': []}])
Individual menu items in a group are also just maps of key / value pairs. Their exact key set varies based on their , but two keys will always be present: Currently, three types of menu options are supported: About Dialog is your run-of-the-mill About Dialog. It displays program information such as name, version, and license info in a standard native AboutBox.Schema Example: {
    'type': 'AboutDialog',
    'menuTitle': 'About',
    'name': 'Gooey Layout Demo',
    'description': 'An example of Gooey\'s layout flexibility',
    'version': '1.2.1',
    'copyright': '2018',
    'website': 'https://github.com/chriskiehl/Gooey',
    'developer': 'http://chriskiehl.com/',
    'license': 'MIT'
}
MessageDialog is a generic informational dialog box. You can display anything from small alerts, to long-form informational text to the user.Schema: Example: {
    'type': 'MessageDialog',
    'menuTitle': 'Information',
    'message': 'Hey, here is some cool info for ya!',
    'caption': 'Stuff you should know'
}
Link is for sending the user to an external website. This will spawn their default browser at the URL you specify. Schema: Example:{
    'type': 'Link',
    'menuTitle': 'Visit Out Site',
    'url': 'http://www.example.com'
}
HtmlDialog gives you full control over what's displayed in the message dialog (bonus: people can copy/paste text from this one!). Schema: Example: {
    'type': 'HtmlDialog',
    'menuTitle': 'Fancy Dialog!',
    'caption': 'Demo of the HtmlDialog',
    'html': '''
    <body bgcolor=""white"">
        <img src=/path/to/your/image.png"" /> 
        <h1>Hello world!</h1> 
        <p><font color=""red"">Lorem ipsum dolor sit amet, consectetur</font></p>
    </body>
    '''
}

A full example:Two menu groups (""File"" and ""Help"") with four menu items between them. @Gooey(
    program_name='Advanced Layout Groups',
    menu=[{
        'name': 'File',
        'items': [{
                'type': 'AboutDialog',
                'menuTitle': 'About',
                'name': 'Gooey Layout Demo',
                'description': 'An example of Gooey\'s layout flexibility',
                'version': '1.2.1',
                'copyright': '2018',
                'website': 'https://github.com/chriskiehl/Gooey',
                'developer': 'http://chriskiehl.com/',
                'license': 'MIT'
            }, {
                'type': 'MessageDialog',
                'menuTitle': 'Information',
                'caption': 'My Message',
                'message': 'I am demoing an informational dialog!'
            }, {
                'type': 'Link',
                'menuTitle': 'Visit Our Site',
                'url': 'https://github.com/chriskiehl/Gooey'
            }]
        },{
        'name': 'Help',
        'items': [{
            'type': 'Link',
            'menuTitle': 'Documentation',
            'url': 'https://www.readthedocs.com/foo'
        }]
    }]
)
Dynamic ValidationBefore passing the user's inputs to your program, Gooey can optionally run a special pre-flight validation to check that all arguments pass your specified validations.  How does it work?   Gooey piggy backs on the  parameter available to most Argparse Argument types. parser.add_argument('--some-number', type=int)
parser.add_argument('--some-number', type=float)
In addition to simple builtins like  and , you can supply your own function to the  parameter to vet the incoming values. def must_be_exactly_ten(value): 
    number = int(value) 
    if number == 10:
        return number
    else: 
        raise TypeError(""Hey! you need to provide exactly the number 10!"")
        
        
def main(): 
    parser = ArgumentParser()
    parser.add_argument('--ten', type=must_be_exactly_ten)
How to enable the pre-flight validationBy default, Gooey won't run the validation. Why? This feature is fairly experimental and does a lot of intense Monkey Patching behind the scenes. As such, it's currently opt-in. You enable to validation by telling Gooey you'd like to subscribe to the  event. from gooey import Gooey, Events 

@Gooey(use_events=[Events.VALIDATE_FORM])
def main(): 
    ... 
Now, when you run Gooey, before it invokes your main program, it'll send a separate pre-validation check and record any issues raised from your  functions.  Full Code Examplefrom gooey import Gooey, Events
from argparse import ArgumentParser

def must_be_exactly_ten(value):
    number = int(value)
    if number == 10:
        return number
    else:
        raise TypeError(""Hey! you need to provide exactly the number 10!"")

@Gooey(program_name='Validation Example', use_events=[Events.VALIDATE_FORM])
def main():
    parser = ArgumentParser(description=""Checkout this validation!"")
    parser.add_argument('--ten', metavar='This field should be 10', type=must_be_exactly_ten)
    args = parser.parse_args()
    print(args)
Lifecycle Events and UI controlAs of 1.2.0, Gooey now exposes coarse grain lifecycle hooks to your program. This means you can now take additional follow-up actions in response to successful runs or failures and even control the current state of the UI itself! Currently, two primary hooks are exposed: These fire exactly when you'd expect: after your process has completed. Anatomy of an lifecycle handler:Both  and  have the same type signature. from typing import Mapping, Any, Optional
from gooey.types import PublicGooeyState  

def on_success(args: Mapping[str, Any], state: PublicGooeyState) -> Optional[PublicGooeyState]:
    """"""
    You can do anything you want in the handler including 
    returning an updated UI state for your next run!   
    """""" 
    return state
    
def on_error(args: Mapping[str, Any], state: PublicGooeyState) -> Optional[PublicGooeyState]:
    """"""
    You can do anything you want in the handler including 
    returning an updated UI state for your next run!   
    """""" 
    return state    
Attaching the handlers:Handlers are attached when instantiating the .parser = GooeyParser(
    on_success=my_success_handler,
    on_failure=my_failure_handler)
Subscribing to the lifecycle eventsJust like , these lifecycle events are opt-in. Pass the event you'd like to subscribe to into the  Gooey decorator argument. from gooey import Gooey, Events 

@Gooey(use_events=[Events.ON_SUCCESS, Events.ON_ERROR])
def main(): 
    ... 
Showing ProgressGiving visual progress feedback with Gooey is easy! If you're already displaying textual progress updates, you can tell Gooey to hook into that existing output in order to power its Progress Bar. For simple cases, output strings which resolve to a numeric representation of the completion percentage (e.g. ) can be pattern matched and turned into a progress bar status with a simple regular expression (e.g. ). For more complicated outputs, you can pass in a custom evaluation expression () to transform regular expression matches as needed. Output strings which satisfy the regular expression can be hidden from the console via the  parameter (e.g. .Regex and Processing Expression@Gooey(progress_regex=r""^progress: (?P<current>\d+)/(?P<total>\d+)$"",
       progress_expr=""current / total * 100"")
Program Output:progress: 1/100
progress: 2/100
progress: 3/100
...
There are lots of options for telling Gooey about progress as your program is running. Checkout the  repository for more detailed usage and examples! Elapsed / Remaining TimeGooey also supports tracking elapsed / remaining time when progress is used! This is done in a similar manner to that of the project . This can be enabled with , the  argument takes in a dictionary with the keys  and . The default behavior is True for  and False for . This will only work when  and  are used.@Gooey(progress_regex=r""^progress: (?P<current>\d+)/(?P<total>\d+)$"",
       progress_expr=""current / total * 100"",
       timing_options = {
        'show_time_remaining':True,
        'hide_time_remaining_on_complete':True,
    })
Customizing IconsGooey comes with a set of six default icons. These can be overridden with your own custom images/icons by telling Gooey to search additional directories when initializing. This is done via the  argument to the  decorator. @Gooey(program_name='Custom icon demo', image_dir='/path/to/my/image/directory')
def main():
    # rest of program
    
Images are discovered by Gooey based on their filenames. So, for example, in order to supply a custom configuration icon, simply place an image with the filename  in your images directory. These are the filenames which can be overridden:PackagingThanks to some , packaging Gooey as an executable is super easy. The tl;dr  version is to drop this  into the root directory of your application. Edit its contents so that the  and  are relevant to your project and the  value points to your applications root, then execute  to bundle your app into a ready-to-go executable. Detailed step by step instructions can be found . Screenshots| Flat Layout | Column Layout |Success Screen | Error Screen | Warning Dialog ||-------------|---------------|---------------|--------------|----------------||  |  |  |  |  | | Custom Groups | Tabbed Groups | Tabbed Navigation | Sidebar Navigation | Input Validation ||-------------|---------------|---------------|--------------|----------------||  |  |  |  |  | Wanna help?Code, translation, documentation, or graphics? All pull requests are welcome. Just make sure to checkout  first."
https://github.com/drduh/macOS-Security-and-Privacy-Guide,Guide to securing and improving privacy on macOS,"This guide is a collection of techniques for improving the security and privacy of a modern Apple Macintosh computer (""MacBook"") running a recent version of macOS (formerly known as ""OS X"").This guide is targeted to power users who wish to adopt enterprise-standard security, but is also suitable for novice users with an interest in improving their privacy and security on a Mac.A system is only as secure as its administrator is capable of making it. There is no one single technology, software, nor technique to guarantee perfect computer security; a modern operating system and computer is very complex, and requires numerous incremental changes to meaningfully improve one's security and privacy posture.This guide is provided on an 'as is' basis without any warranties of any kind. Only you are responsible if you break anything or get in any sort of trouble by following this guide.To suggest an improvement, please send a pull request or .This guide is also available in .BasicsStandard security best practices apply:Preparing and installing macOSThere are several ways to install macOS.The simplest way is to boot into  by holding  and  keys at boot. A system image can be downloaded and applied directly from Apple. However, this way exposes the serial number and other identifying information over the network in plain text, which may not be desired for privacy reasons.Packet capture of an unencrypted HTTP conversation during macOS recoveryAn alternative way to install macOS is to first download the latest version of macOS (Latest: macOS Ventura) from Apple via the  and create a custom installable system image.This can also be done from the Terminal using the commands outlined in .softwareupdate --list-full-installers
# latest is 13.3.1
softwareupdate -d --fetch-full-installer --full-installer-version 13.3.1
Getting macOSApple's  provides details for getting older versions of macOS.Verifying installation integrityThe macOS installation application is , which should be verified to make sure you received a legitimate copy, using the  or  commands.To verify the code signature and integrity of macOS application bundles:$ pkgutil --check-signature /Applications/Install\ macOS\ Ventura.app
Package ""Install macOS Ventura"":
   Status: signed by a certificate trusted by macOS
   Certificate Chain:
    1. Software Signing
       Expires: 2026-10-24 17:39:41 +0000
       SHA256 Fingerprint:
           D8 4D B9 6A F8 C2 E6 0A C4 C8 51 A2 1E C4 60 F6 F8 4E 02 35 BE B1
           7D 24 A7 87 12 B9 B0 21 ED 57
       ------------------------------------------------------------------------
    2. Apple Code Signing Certification Authority
       Expires: 2026-10-24 17:39:41 +0000
       SHA256 Fingerprint:
           5B DA B1 28 8F C1 68 92 FE F5 0C 65 8D B5 4F 1E 2E 19 CF 8F 71 CC
           55 F7 7D E2 B9 5E 05 1E 25 62
       ------------------------------------------------------------------------
    3. Apple Root CA
       Expires: 2035-02-09 21:40:36 +0000
       SHA256 Fingerprint:
           B0 B1 73 0E CB C7 FF 45 05 14 2C 49 F1 29 5E 6E DA 6B CA ED 7E 2C
           68 C5 BE 91 B5 A1 10 01 F0 24
Use the  command to examine an application's code signature:$ codesign -dvv /Applications/Install\ macOS\ Ventura.app
Executable=/Applications/Install macOS Ventura.app/Contents/MacOS/InstallAssistant_springboard
Identifier=com.apple.InstallAssistant.macOSVentura
Format=app bundle with Mach-O universal (x86_64 arm64)
CodeDirectory v=20400 size=640 flags=0x2000(library-validation) hashes=13+3 location=embedded
Platform identifier=14
Signature size=4523
Authority=Software Signing
Authority=Apple Code Signing Certification Authority
Authority=Apple Root CA
Signed Time=Mar 22, 2023 at 16:09:45
Info.plist entries=32
TeamIdentifier=not set
Sealed Resources version=2 rules=2 files=0
Internal requirements count=1 size=88
Creating a bootable USB installerInstead of booting from the network or using target disk mode, a bootable macOS installer can be made with the  utility included in  folder of the installer application bundle. See , or run the utility without arguments to see how it works.To create a bootable USB installer, mount a USB drive, and erase and partition it, then use the  utility:$ diskutil list
[Find disk matching correct size, usually the last disk, e.g. /dev/disk2]

$ diskutil unmountDisk /dev/disk2

$ diskutil partitionDisk /dev/disk2 1 JHFS+ Installer 100%

$ cd /Applications/Install\ macOS\ Ventura.app

$ sudo ./Contents/Resources/createinstallmedia --volume /Volumes/Installer --nointeraction
Erasing disk: 0%... 10%... 20%... 30%... 100%
Copying to disk: 0%... 10%... 20%... 30%... 40%... 50%... 60%... 70%... 80%... 90%... 100%
Making disk bootable...
Copying boot files...
Install media now available at ""/Volumes/Install macOS Catalina""
Apple also has  on doing this via the GUI Disk UtilityCreating an install imageNote Apple's AutoDMG installer  across OS versions. If you want to build a 10.14 image, for example, the following steps must be performed on macOS 10.14!To create a custom install image which can be  to a Mac (using a USB-C cable and target disk mode, for example), use .Manual wayNote The following instructions appear to work only on macOS versions before 10.13.Find  which is inside the installation application. Locate it in Terminal or with Finder, right click on the application bundle, select Show Package Contents and navigate to Contents > SharedSupport to find the file  file integrity by comparing its SHA-256 hash with others found in  or .To determine which macOS versions and builds originally shipped with or are available for a Mac, see .$ shasum -a 256 InstallESD.dmg
Mount and install the operating system to a temporary image:$ hdiutil attach -mountpoint /tmp/InstallESD ./InstallESD.dmg

$ hdiutil create -size 32g -type SPARSE -fs HFS+J -volname ""macOS"" -uid 0 -gid 80 -mode 1775 /tmp/macos.sparseimage

$ hdiutil attach -mountpoint /tmp/macos -owners on /tmp/macos.sparseimage

$ sudo installer -pkg /tmp/InstallESD/Packages/OSInstall.mpkg -tgt /tmp/macos -verbose
installer: OS Install started.
#############
[...]
The installation will take a while, so be patient. Use  in another terminal to monitor progress and check for errors.Once the installation is complete, detach, convert and verify the image:$ hdiutil detach /tmp/macos
""disk4"" unmounted.
""disk4"" ejected.

$ hdiutil detach /tmp/InstallESD
""disk3"" unmounted.
""disk3"" ejected.

$ hdiutil convert -format UDZO /tmp/macos.sparseimage -o ~/sierra.dmg
Preparing imaging engine...
[...]

$ asr imagescan --source ~/sierra.dmg
The file  is now ready to be applied over , from a bootable USB installer, booting from the network or recovery mode. The image could be further customized to include provisioned users, installed applications, preferences, for example.Target disk modeTo use Target Disk Mode, boot up the Mac you wish to image while holding the  key and connect it to another Mac using a USB-C, Thunderbolt or Firewire cable.If you don't have another Mac, boot to a USB installer, with  and other required files copied to it, by holding the Option key at boot.Use the command  to identify the disk of the connected Mac, usually Optionally,  the disk with a single pass (if previously FileVault-encrypted, the disk must first be unlocked and mounted as ):$ sudo diskutil secureErase freespace 1 /dev/disk3s2
Partition the disk to Journaled HFS+:$ sudo diskutil unmountDisk /dev/disk2

$ sudo diskutil partitionDisk /dev/disk2 1 JHFS+ macOS 100%
Restore the image to the new volume, making sure  is the disk being erased:$ sudo asr restore --source ~/sierra.dmg --target /Volumes/macOS --erase --buffersize 4m
[...]
Erase contents of /dev/disk2s2 (/Volumes/macOS)? [ny]:y
[...]
The Disk Utility application may also be used to erase the connected disk and restore  to the newly created partition.To transfer any files, copy them to a shared folder like  on the mounted disk image, e.g. Finished restore install from USB recovery bootCreating a recovery partitionUnless you have built the image with , or installed macOS to a second partition on the same Mac, you will need to create a recovery partition in order to use full disk encryption. You can do so using  or manually by following these steps:Download  and verify its integrity:$ shasum -a 256 RecoveryHDUpdate.dmg
f6a4f8ac25eaa6163aa33ac46d40f223f40e58ec0b6b9bf6ad96bdbfc771e12c  RecoveryHDUpdate.dmg
Attach and expand the installer, then run it - again ensuring  path is the newly created partition on the connected disk:$ hdiutil attach RecoveryHDUpdate.dmg

$ pkgutil --expand /Volumes/Mac\ OS\ X\ Lion\ Recovery\ HD\ Update/RecoveryHDUpdate.pkg /tmp/recovery

$ hdiutil attach /tmp/recovery/RecoveryHDUpdate.pkg/RecoveryHDMeta.dmg

$ /tmp/recovery/RecoveryHDUpdate.pkg/Scripts/Tools/dmtest ensureRecoveryPartition /Volumes/macOS/ /Volumes/Recovery\ HD\ Update/BaseSystem.dmg 0 0 /Volumes/Recovery\ HD\ Update/BaseSystem.chunklist
[...]
Creating recovery partition: finished
Run  again to make sure  now exists on . Eject the disk with  and power down the target disk mode-booted Mac.VirtualizationTo install macOS as a virtual machine (VM) using , follow the instructions above to create an image. You will not need to download and create a recovery partition manually.For the Installation Method, select Install macOS from the recovery partition. Customize any memory or CPU requirements and complete setup. The guest VM should boot into  by default.Note If the virtual machine does not boot due to a kernel panic, adjust the memory and process resource settings.In Recovery Mode, select a language, then select Utilities > Terminal from the menu bar.In the guest VM, type  - you should see a private address like On the host Mac, type  - you should see a private gateway address like . From the host Mac, you should be able to  or the equivalent guest VM address.From the host Mac, serve the installable image to the guest VM by editing  and adding the following line to the top (using the gateway address assigned to the host Mac and port 80):Listen 172.16.34.1:80
On the host Mac, link the image to the default Apache Web server directory:$ sudo ln ~/sierra.dmg /Library/WebServer/Documents
From the host Mac, start Apache in the foreground:$ sudo httpd -X
From the guest VM, install the disk image to the volume over the local network using :-bash-3.2# asr restore --source http://172.16.34.1/sierra.dmg --target /Volumes/Macintosh\ HD/ --erase --buffersize 4m
	Validating target...done
	Validating source...done
	Erase contents of /dev/disk0s2 (/Volumes/Macintosh HD)? [ny]: y
	Retrieving scan information...done
	Validating sizes...done
	Restoring  ....10....20....30....40....50....60....70....80....90....100
	Verifying  ....10....20....30....40....50....60....70....80....90....100
	Remounting target volume...done
When it's finished, stop the Apache Web server on the host Mac by pressing   at the  window and remove the image copy with In the guest VM, select Startup Disk from the menubar top-left, select the hard drive and restart. You may wish to disable the Network Adapter in VMware to configure the guest VM initially.Take and Restore from saved guest VM snapshots before and after attempting risky browsing, for example, or use a guest VM to install and operate questionable software.First bootNote Before setting up macOS, consider disconnecting networking and configuring a firewall(s) first. However,  with Touch Bar hardware  (also see next section).(Intel-based Mac only) On first boot, hold     keys to .When macOS first starts, you'll be greeted by Setup Assistant.When creating the first account, use a  without a hint.If you enter your real name at the account setup process, be aware that your  will comprise that name (e.g., John Appleseed's MacBook) and thus will appear on local networks and in various preference files.Both should be verified and updated as needed in System Preferences > Sharing or with the following commands after installation:$ sudo scutil --set ComputerName MacBook
$ sudo scutil --set LocalHostName MacBook
System activationA few words on the privacy implications of activating ""Touch Bar"" MacBook devices from your friendly anonymous security researcher:From .Admin and standard user accountsThe first user account is always an admin account. Admin accounts are members of the admin group and have access to , which allows them to usurp other accounts, in particular root, and gives them effective control over the system. Any program that the admin executes can potentially obtain the same access, making this a security risk.Utilities like  have  by concurrently running programs and many panes in System Preferences are  (pdf) (p. 61–62) for admin accounts.It is considered a best practice by  and  (pdf) (p. 41–42) to use a separate standard account for day-to-day work and use the admin account for installations and system configuration.It is not strictly required to ever log into the admin account via the macOS login screen. When a Terminal command requires administrator privileges, the system will prompt for authentication and Terminal then continues using those privileges. To that end, Apple provides some  for hiding the admin account and its home directory. This can be an elegant solution to avoid having a visible 'ghost' account.CaveatsSetupAccounts can be created and managed in System Preferences. On settled systems, it is generally easier to create a second admin account and then demote the first account. This avoids data migration. Newly installed systems can also just add a standard account.Demoting an account can be done either from the the new admin account in System Preferences – the other account must be logged out – or by executing these commands (it may not be necessary to execute both, see ):$ sudo dscl . -delete /Groups/admin GroupMembership <username>
$ sudo dscl . -delete /Groups/admin GroupMembers <GeneratedUID>
To find the “GeneratedUID” of an account:$ dscl . -read /Users/<username> GeneratedUID
See also  for more information about how macOS determines group membership.Full disk encryption provides full disk (technically, full volume) encryption on macOS.FileVault encryption protects data at rest and hardens (but ) someone with physical access from stealing data or tampering with your Mac.With much of the cryptographic operations happening , the performance penalty for FileVault is not noticeable.Like all cryptosystems, the security of FileVault greatly depends on the quality of the pseudo random number generator (PRNG).See  for more information.Turning on FileVault in System Preferences after installing macOS, rather than creating an encrypted partition for the installation first, is , because more PRNG entropy is available then.Additionally, the PRNG can be manually seeded with entropy by writing to /dev/random before enabling FileVault. This can be done by simply using the Mac for a little while before activating FileVault.It may also be possible to increase entropy with an external source, like . See  and  for more information.Enable FileVault with  or through System Preferences > Security & Privacy and reboot.If you can remember the password, there's no reason to save the recovery key. However, all encrypted data will be lost forever if without either the password or recovery key.To learn about how FileVault works, see the paper  (pdf) and related  (pdf). Also see  (pdf).Optional Enforce system hibernation and evict FileVault keys from memory instead of traditional sleep to memory:$ sudo pmset -a destroyfvkeyonstandby 1
$ sudo pmset -a hibernatemode 25
If you choose to evict FileVault keys in standby mode, you should also modify your standby and power nap settings. Otherwise, your machine may wake while in standby mode and then power off due to the absence of the FileVault key. See  for more information. These settings can be changed with:$ sudo pmset -a powernap 0
$ sudo pmset -a standby 0
$ sudo pmset -a standbydelay 0
$ sudo pmset -a autopoweroff 0
For more information, see  (pdf) and paper  (pdf)Note APFS may make evicting FileVault keys redundant - see discussion and links in .FirmwareSetting a firmware password prevents a Mac from starting up from any device other than the startup disk. It may also be set to be required on each boot. This may be useful for mitigating some attacks which require physical access to hardware.  See  for official documentation.This feature , protects against Direct Memory Access (DMA) attacks which can read your FileVault passwords and inject kernel modules such as , as the only way to reset the firmware password is through an Apple Store, or by using an , such as  or other flash IC programmer.The firmware password will activate at next boot. To validate the password, hold  during boot - you should be prompted to enter the password.The firmware password can also be managed with the  utility while booted into the OS. For example, to prompt for the firmware password when attempting to boot from a different volume:$ sudo firmwarepasswd -setpasswd -setmode command
To verify the firmware password:$ sudo firmwarepasswd -verify
Verifying Firmware Password
Enter password:
Correct
A firmware password may be bypassed by a determined attacker or Apple, with physical access to the computer.Using a As of macOS 10.15 Catalina, the  program has a new option . According to , this effectively prevents any firmware password resets, even by Apple themselves:Newer Mac models (Mac Pro, iMac Pro, Macbook with TouchBar) with  chips, which provide a secure enclave for encrypted keys, lessen the risk of EFI firmware attacks. See  for more information.See ,  and discussion in  for more information.FirewallThere are several types of firewalls available for macOS.Application layer firewallBuilt-in, basic firewall which blocks incoming connections only. This firewall does not have the ability to monitor, nor block outgoing connections.It can be controlled by the Firewall tab of Security & Privacy in System Preferences, or with the following commands.Enable the firewall with logging and stealth mode:$ sudo /usr/libexec/ApplicationFirewall/socketfilterfw --setglobalstate on
Firewall is enabled. (State = 1)

$ sudo /usr/libexec/ApplicationFirewall/socketfilterfw --setloggingmode on
Turning on log mode

$ sudo /usr/libexec/ApplicationFirewall/socketfilterfw --setstealthmode on
Stealth mode enabled
To prevent built-in software as well as code-signed, downloaded software from being whitelisted automatically:$ sudo /usr/libexec/ApplicationFirewall/socketfilterfw --setallowsigned off
Disabled allow signed built-in applications automatically

$ sudo /usr/libexec/ApplicationFirewall/socketfilterfw --setallowsignedapp off
Disabled allow signed downloaded applications automatically
After interacting with , restart the process by sending a line hangup signal:$ sudo pkill -HUP socketfilterfw
Third party firewallsPrograms such as , , ,  and  provide a good balance of usability and security.These programs are capable of monitoring and blocking incoming and outgoing network connections. However, they may require the use of a closed source .If the number of choices of allowing/blocking network connections is overwhelming, use Silent Mode with connections allowed, then periodically check the configuration to gain understanding of applications and what they are doing.It is worth noting that these firewalls can be bypassed by programs running as root or through  (pdf), but they are still worth having - just don't expect absolute protection. However, some malware actually  and doesn't execute if Little Snitch, or other security software, is installed.For more on how Little Snitch works, see the  and .Kernel level packet filteringA highly customizable, powerful, but also most complicated firewall exists in the kernel. It can be controlled with  and various configuration files.pf can also be controlled with a GUI application such as  or .There are many books and articles on the subject of pf firewall. Here's is just one example of blocking traffic by IP address.Add the following into a file called :wifi = ""en0""
ether = ""en7""
set block-policy drop
set fingerprints ""/etc/pf.os""
set ruleset-optimization basic
set skip on lo0
scrub in all no-df
table <blocklist> persist
block in log
block in log quick from no-route to any
block log on $wifi from { <blocklist> } to any
block log on $wifi from any to { <blocklist> }
antispoof quick for { $wifi $ether }
pass out proto tcp from { $wifi $ether } to any keep state
pass out proto udp from { $wifi $ether } to any keep state
pass out proto icmp from $wifi to any keep state
Then use the following commands to manipulate the firewall:Unless you're already familiar with packet filtering, spending too much time configuring pf is not recommended. It is also probably unnecessary if your Mac is behind a  on a secure home network.It is possible to use the pf firewall to block network access to entire ranges of network addresses, for example to a whole organization:Query  for the list of networks in use by an autonomous system, like :$ whois -h whois.radb.net '!gAS32934'
Copy and paste the list of networks returned into the blocklist command:$ sudo pfctl -t blocklist -T add 31.13.24.0/21 31.13.64.0/24 157.240.0.0/16
Confirm the addresses were added:$ sudo pfctl -t blocklist -T show
No ALTQ support in kernel
ALTQ related functions disabled
   31.13.24.0/21
   31.13.64.0/24
   157.240.0.0/16
Confirm network traffic is blocked to those addresses (note that DNS requests will still work):$ dig a +short facebook.com
157.240.2.35

$ curl --connect-timeout 5 -I http://facebook.com/
*   Trying 157.240.2.35...
* TCP_NODELAY set
* Connection timed out after 5002 milliseconds
* Closing connection 0
curl: (28) Connection timed out after 5002 milliseconds

$ sudo tcpdump -tqni pflog0 'host 157.240.2.35'
IP 192.168.1.1.62771 > 157.240.2.35.80: tcp 0
IP 192.168.1.1.62771 > 157.240.2.35.80: tcp 0
IP 192.168.1.1.62771 > 157.240.2.35.80: tcp 0
IP 192.168.1.1.62771 > 157.240.2.35.80: tcp 0
IP 192.168.1.1.162771 > 157.240.2.35.80: tcp 0
Outgoing TCP SYN packets are blocked, so a TCP connection is not established and thus a Web site is effectively blocked at the IP layer.To use pf to audit ""phone home"" behavior of user and system-level processes, see . See  for more inspiration.ServicesNote  does not allow disabling system services on recent macOS versions. Either temporarily disable SIP or disable services from Recovery Mode. See  for more information.See ,  and  for further recommendations.Services on macOS are managed by launchd. See , as well as  and You can also run  that shows more information about startup items.For example, to learn what a system launch daemon or agent does, start with:$ defaults read /System/Library/LaunchDaemons/com.apple.apsd.plist
Look at the  or  section to see which binary is run, in this case . To find more information about that, look at the man page with For example, if you're not interested in Apple Push Notifications, disable the service:$ sudo launchctl unload -w /System/Library/LaunchDaemons/com.apple.apsd.plist
Note Unloading services may break usability of some applications. Read the manual pages and use Google to make sure you understand what you're doing first.Be careful about disabling any system daemons you don't understand, as it may render your system unbootable. If you break your Mac, use  to fix it.Use  and  applications if you notice your Mac heating up, feeling sluggish, or generally misbehaving, as it may have resulted from your tinkering.To view the status of services:$ find /var/db/com.apple.xpc.launchd/ -type f -print -exec defaults read {} \; 2>/dev/null
Annotated lists of launch daemons and agents, the respective program executed, and the programs' hash sums are included in this repository.(Optional) Run the  script and  output to check for any discrepancies on your system, e.g.:$ diff <(python read_launch_plists.py | sort ) <(cat 16A323_launchd.csv | sort )
See also  for descriptions of services and  for another explanation.Persistent login items may also exist in these directories:See  (pdf) for more information.Spotlight SuggestionsDisable Spotlight Suggestions in both the Spotlight preferences and Safari's Search preferences to avoid your search queries being sent to Apple.Also disable Bing Web Searches in the Spotlight preferences to avoid your search queries being sent to Microsoft.See  for detailed instructions.Note This Web site and instructions may no longer work on macOS Sierra - see .For comparison to Windows 10, see HomebrewConsider using  to make software installations easier and to update userland tools (see ).Note If you have not already installed Xcode or Command Line Tools, use  to download and install them, or check Apple's developer site.:$ mkdir homebrew && curl -L https://github.com/Homebrew/brew/tarball/master | tar xz --strip 1 -C homebrew
Edit  in your shell or shell rc file to use  and . For example, , then change your login shell to Z shell with , open a new Terminal window and run .Homebrew uses SSL/TLS to talk with GitHub and verifies integrity of downloaded packages, so it's .Remember to periodically run  on trusted and secure networks to download and install software updates. To get information on a package before installation, run  and check its formula online.According to , Homebrew gathers anonymous analytics and reports these to a self-hosted InfluxDB instance.To opt out of Homebrew's analytics, you can set  in your environment or shell rc file, or use .You may also wish to enable , such as  and .DNSDNS profilesSince macOS 11 there is a very simple solution via ""DNS configuration profiles"" to:and all this without having to install a program or make cumbersome lists or settings.you can use ready-made profiles that can be easily installed via double-click and do not require any further work.Providers of such profiles are e.g::Note: all three offer digitally signed profiles, but only Quad9 offers an Apple signed profile.besides the ready-made profiles, you can also assemble .However, these profiles then do not have a signature and this is also criticized by macOS - even if the function of the profile is not affected.Hosts fileUse the  to block known malware, advertising or otherwise unwanted domains.Edit the hosts file as root, for example with . The hosts file can also be managed with the GUI app .To block a domain by  record, append any one of the following lines to :0 example.com
0.0.0.0 example.com
127.0.0.1 example.com
Note IPv6 uses the  DNS record type, rather than  record type, so you may also want to block those connections by also including  entries, like shown .There are many lists of domains available online which you can paste in, just make sure each line starts with , , , and the line  is included.Here are some popular and useful hosts lists:Append a list of hosts with the  command and confirm only non-routable addresses or comments were added:$ curl https://raw.githubusercontent.com/StevenBlack/hosts/master/hosts | sudo tee -a /etc/hosts

$ wc -l /etc/hosts
65580

$ egrep -ve ""^#|^255.255.255.255|^127.|^0.|^::1|^ff..::|^fe80::"" /etc/hosts | sort | uniq | egrep -e ""[1,2]|::""
[No output]
See  and  for more information.See the  section of this guide for more hosts blocking options.dnscryptTo encrypt outgoing DNS traffic, consider using . In combination with dnsmasq and DNSSEC, the integrity and authenticity of DNS traffic is greatly improved. and  provide a graphical user interface to dnscrypt.Install dnscrypt from Homebrew and follow the instructions to configure and start :$ brew install dnscrypt-proxy
If using in combination with Dnsmasq, find the file  by running$ brew info dnscrypt-proxy
which will show a location like Open it in a text editor, find the line starting with  and edit that line to use DNScrypt on a port other than 53, like 5355:listen_addresses = ['127.0.0.1:5355', '[::1]:5355']
Start DNSCrypt:$ sudo brew services restart dnscrypt-proxy
Make sure DNSCrypt is running:$ sudo lsof +c 15 -Pni UDP:5355
COMMAND          PID   USER   FD   TYPE             DEVICE SIZE/OFF NODE NAME
dnscrypt-proxy 15244 nobody    7u  IPv4 0x1337f85ff9f8beef      0t0  UDP 127.0.0.1:5355
dnscrypt-proxy 15244 nobody   10u  IPv6 0x1337f85ff9f8beef      0t0  UDP [::1]:5355
dnscrypt-proxy 15244 nobody   12u  IPv4 0x1337f85ff9f8beef      0t0  UDP 127.0.0.1:5355
dnscrypt-proxy 15244 nobody   14u  IPv6 0x1337f85ff9f8beef      0t0  UDP [::1]:5355
This can be accomplished by editing  as described above.You can run your own  (see also ) from a trusted location or use one of many  instead.Confirm outgoing DNS traffic is encrypted:$ sudo tcpdump -qtni en0
IP 10.8.8.8.59636 > 107.181.168.52: UDP, length 512
IP 107.181.168.52 > 10.8.8.8.59636: UDP, length 368

$ dig +short -x 128.180.155.106.49321
d0wn-us-ns4
dnscrypt-proxy also has the capability to blacklist domains, including the use of wild-cards. See the  for the options.Note Applications and programs may resolve DNS using their own provided servers. If dnscrypt-proxy is used, it is possible to disable all other, non-dnscrypt DNS traffic with the following pf rules:block drop quick on !lo0 proto udp from any to any port = 53
block drop quick on !lo0 proto tcp from any to any port = 53
See also , the  and .DnsmasqAmong other features,  is able to cache replies, prevent upstream queries for unqualified names, and block entire top-level domain names.Use in combination with DNSCrypt to additionally encrypt outgoing DNS traffic.If you don't wish to use DNSCrypt, you should at least use DNS  . Two popular alternatives are  and .(Optional)  is a set of extensions to DNS which provide to DNS clients (resolvers) origin authentication of DNS data, authenticated denial of existence, and data integrity. All answers from DNSSEC protected zones are digitally signed. The signed records are authenticated via a chain of trust, starting with a set of verified public keys for the DNS root-zone. The current root-zone trust anchors may be downloaded . There are a number of resources on DNSSEC, but probably the best one is .Install Dnsmasq (DNSSEC is optional):$ brew install dnsmasq --with-dnssec
Download :$ curl -o homebrew/etc/dnsmasq.conf https://raw.githubusercontent.com/drduh/config/master/dnsmasq.conf
Edit the file and examine all the options. To block entire levels of domains, append  or your own rules.Install and start the program (sudo is required to bind to  53):$ sudo brew services start dnsmasq
To set Dnsmasq as your local DNS server, open System Preferences > Network and select the active interface, then the DNS tab, select + and add , or use:$ sudo networksetup -setdnsservers ""Wi-Fi"" 127.0.0.1
Make sure Dnsmasq is correctly configured:$ scutil --dns | head
DNS configuration

resolver #1
  search domain[0] : whatever
  nameserver[0] : 127.0.0.1
  flags    : Request A records, Request AAAA records
  reach    : 0x00030002 (Reachable,Local Address,Directly Reachable Address)

$ networksetup -getdnsservers ""Wi-Fi""
127.0.0.1
Note Some VPN software overrides DNS settings on connect. See  and .Test DNSSEC validationTest DNSSEC validation succeeds for signed zones - the reply should have  status and contain  flag:$ dig +dnssec icann.org
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 47039
;; flags: qr rd ra ad; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 1
Test DNSSEC validation fails for zones that are signed improperly - the reply should have  status:$ dig www.dnssec-failed.org
;; ->>HEADER<<- opcode: QUERY, status: SERVFAIL, id: 15190
;; flags: qr rd ra; QUERY: 1, ANSWER: 0, AUTHORITY: 0, ADDITIONAL: 1
Captive portalWhen macOS connects to new networks, it checks for Internet connectivity and may launch a Captive Portal assistant utility application.An attacker could trigger the utility and direct a Mac to a site with malware without user interaction, so it's best to disable this feature and log in to captive portals using your regular Web browser by navigating to a non-secure HTTP page and accepting a redirect to the captive portal login interface (after disabling any custom proxy or DNS settings).$ sudo defaults write /Library/Preferences/SystemConfiguration/com.apple.captive.control.plist Active -bool false
Also see ,  and .Certificate authoritiesmacOS comes with  root authority certificates installed from for-profit corporations like Apple, Verisign, Thawte, Digicert and government agencies from China, Japan, Netherlands, U.S., and more! These Certificate Authorities (CAs) are capable of issuing SSL/TLS certificates for any domain, code signing certificates, etc.For more information, see ,  (pdf), and  (pdf).Inspect system root certificates in Keychain Access, under the System Roots tab or by using the  command line tool and  file.Disable certificate authorities through Keychain Access by marking them as Never Trust and closing the window:The risk of a  attack in which a coerced or compromised certificate authority trusted by your system issues a fake/rogue SSL certificate is quite low, but still .OpenSSLNote This section .The version of OpenSSL in Sierra is  which is . It doesn't support TLS 1.1 or newer, elliptic curve ciphers, and .Since Apple's official supported TLS library on macOS is , OpenSSL deprecated is considered deprecated (according to the . Apple's version of OpenSSL may also have patches which may .If you're going to use OpenSSL on your Mac, download and install a recent version of OpenSSL with . Note, linking brew to be used in favor of  may interfere with built-in software. See .Compare the TLS protocol and cipher between the homebrew version and the system version of OpenSSL:$ ~/homebrew/bin/openssl version; echo | ~/homebrew/bin/openssl s_client -connect github.com:443 2>&1 | grep -A2 SSL-Session
OpenSSL 1.0.2j  26 Sep 2016
SSL-Session:
    Protocol  : TLSv1.2
    Cipher    : ECDHE-RSA-AES128-GCM-SHA256

$ /usr/bin/openssl version; echo | /usr/bin/openssl s_client -connect github.com:443 2>&1 | grep -A2 SSL-Session
OpenSSL 0.9.8zh 14 Jan 2016
SSL-Session:
    Protocol  : TLSv1
    Cipher    : AES128-SHA
See also ,  and .CurlThe version of Curl which comes with macOS uses  for SSL/TLS validation.If you prefer to use OpenSSL, install with  and ensure it's the default with Download  or see the :$ curl -o ~/.curlrc https://raw.githubusercontent.com/drduh/config/master/curlrc
WebPrivoxyConsider using  as a local proxy to filter Web browsing traffic.Note macOS proxy settings are not universal; apps and services may not honor system proxy settings. Ensure the application you wish to proxy is correctly configured and manually verify connections don't leak. Additionally, it may be possible to configure the pf firewall to transparently proxy all traffic.A signed installation package for privoxy can be downloaded from  or . The signed package is  than the Homebrew version, and attracts full support from the Privoxy project.Alternatively, install and start privoxy using Homebrew:$ brew install privoxy

$ brew services start privoxy
By default, privoxy listens on localhost, TCP port 8118.Set the system HTTP proxy for your active network interface  and  (This can be done through System Preferences > Network > Advanced > Proxies):$ sudo networksetup -setwebproxy ""Wi-Fi"" 127.0.0.1 8118
(Optional) Set the system HTTPS proxy, which still allows for domain name filtering, with:$ sudo networksetup -setsecurewebproxy ""Wi-Fi"" 127.0.0.1 8118
Confirm the proxy is set:$ scutil --proxy
<dictionary> {
  ExceptionsList : <array> {
    0 : *.local
    1 : 169.254/16
  }
  FTPPassive : 1
  HTTPEnable : 1
  HTTPPort : 8118
  HTTPProxy : 127.0.0.1
}
Visit  in a browser, or with Curl:$ ALL_PROXY=127.0.0.1:8118 curl -I http://p.p/
HTTP/1.1 200 OK
Content-Length: 2401
Content-Type: text/html
Cache-Control: no-cache
Privoxy already comes with many good rules, however you can also write your own.Download  and  to get started:$ curl -o homebrew/etc/privoxy/config https://raw.githubusercontent.com/drduh/config/master/privoxy/config

$ curl -o homebrew/etc/privoxy/user.action https://raw.githubusercontent.com/drduh/config/master/privoxy/user.action
Restart Privoxy and verify traffic is blocked or redirected:$ sudo brew services restart privoxy

$ ALL_PROXY=127.0.0.1:8118 curl ads.foo.com/ -IL
HTTP/1.1 403 Request blocked by Privoxy
Content-Type: image/gif
Content-Length: 64
Cache-Control: no-cache

$ ALL_PROXY=127.0.0.1:8118 curl imgur.com/ -IL
HTTP/1.1 302 Local Redirect from Privoxy
Location: https://imgur.com/
Content-Length: 0

HTTP/1.1 200 OK
Content-Type: text/html; charset=utf-8
You can replace ad images with pictures of kittens, for example, by starting a local Web server and  to localhost.BrowserThe Web browser poses the largest security and privacy risk, as its fundamental job is to download and execute untrusted code from the Internet. This is an important statement. The unique use case of Web Browsers of operation in hostile environments, has forced them to adopt certain impressive security features. The cornerstone of Web Browser security is the Same Origin Policy (). In a few words, SOP prevents a malicious script on one page from obtaining access to sensitive data on another web page through that page's Document Object Model (DOM). If SOP is compromised, the security of the whole Web Browser is compromised.The best tip to ensure secure browsing regardless your choice of Web Browser is proper security hygiene. The majority of Web Browser exploits require social engineering attacks to achieve native code execution. Always be mindful of the links you click and be extra careful when websites ask you to download and install software. 99% percent of the time that software is malware.Another important consideration about Web Browser security is Web Extensions. Web Extensions greatly increase the attack surface of the Web Browser. This is an issue that plagues Firefox and  alike. Luckily, Web Extensions can only access specific browser APIs that are being governed by their manifest. That means we can quickly audit their behavior and remove them if they request access to information they shouldn't (why would an Ad blocker require camera access?). In the interest of security, it is best to limit your use of Web Extensions., , , and  are covered in this guide. Each Web Browser offers certain benefits and drawbacks regarding their security and privacy. It is best to make an informed choice and not necessarily commit to only one.Firefox is an excellent browser as well as being completely open source. Currently, Firefox is in a renaissance period. It replaces major parts of its infrastructure and code base under projects  and . Part of the Quantum project is to replace C++ code with . Rust is a systems programming language with a focus on security and thread safety. It is expected that Rust adoption will greatly improve the overall security posture of Firefox.Firefox offers a similar security model to Chrome: it has a , although it is not a lucrative as Chrome's. Firefox follows a six-week release cycle similar to Chrome. See discussion in issues  and  for more information about certain differences in Firefox and Chrome.Firefox supports user-supplied configuration files. See ,  and  for recommended preferences and hardening measures. Also see , an extension which allows whitelist-based, pre-emptive script blocking.Firefox is focused on user privacy. It supports  in Private Browsing mode. The tracking protection can be enabled for the default account, although it may break the browsing experience on some websites. Another feature for added privacy unique to Firefox is , similar to Chrome profiles.Previous versions of Firefox used a  that was quite invasive and offered immense freedom to developers. Sadly, that freedom also introduced a number of vulnerabilities in Firefox that greatly affected its users. You can find more information about vulnerabilities introduced by Firefox's legacy extensions in this  (pdf). Currently, Firefox only supports Web Extensions through the , which is very similar to Chrome's.Submission of Web Extensions in Firefox is free. Web Extensions in Firefox most of the time are open source, although certain Web Extensions are proprietary.Note Similar to Chrome and Safari, Firefox allows account sync across multiple devices. While stored login passwords are encrypted, Firefox does not require a password to reveal their plain text format. Firefox only displays as yes/no prompt. This is an important security issue. Keep that in mind if you sign in to your Firefox account from devices that do not belong to you and leave them unattended. The  has been raised among the Firefox community and hopefully will be resolved in the coming versions.See  for additional Firefox configuration options to improve security and privacy.Chrome is based on the open source  with certain :Chrome offers account sync between multiple devices. Part of the sync data are stored website credentials. The login passwords are encrypted and in order to access them, a user's Google account password is required. You can use your Google account to sign to your Chrome customized settings from other devices while retaining your the security of your passwords.Chrome's Web store for extensions requires a  in order to submit extensions. The low cost allows the development of many quality Open Source Web Extensions that do not aim to monetize through usage.Chrome has the largest share of global usage and is the preferred target platform for the majority of developers. Major technologies are based on Chrome's Open Source components, such as  which uses  Engine and the  framework, which is based on Chromium and node.js. Chrome's vast user base makes it the most attractive target for threat actors and security researchers. Despite under constants attacks, Chrome has retained an impressive security track record over the years. This is not a small feat.Chrome offers , ,  (including Flash, although you should disable it - see below), and carries . In addition, Google offers a very lucrative  program for reporting vulnerabilities along with its own . This means that a large number of highly talented and motivated people are constantly auditing Chrome's code base.Create separate Chrome profiles to reduce XSS risk and compartmentalize cookies/identities. In each profile, either disable Javascript in Chrome settings and manually whitelist allowed origins - or use  to manage Javascript and/or disable third-party scripts/frames. Also install  to upgrade insecure connections.Change the default search engine from Google to reduce additional tracking.Disable  (see also  (pdf)). Note that Chrome  to resolve DNS using Google's  and  public nameservers.Read  and  for more detailed, technical information.Read  and learn which  collect personal information. Users can opt-out of services and see what type of information Google has stored in .Safari is the default Web browser of macOS. It is also the most optimized browser for reducing battery use. Safari, like Chrome, has both Open Source and proprietary components. Safari is based on the open source Web Engine , which is ubiquitous among the macOS ecosystem. WebKit is used by Apple apps such as Mail, iTunes, iBooks, and the App Store. Chrome's  engine is a fork of WebKit and both engines share a number of similarities.Safari supports certain unique features that benefit user security and privacy.  enables the creation of content blocking rules without using Javascript. This rule based approach greatly improves memory use, security, and privacy. Safari 11 introduced an  system. This feature automatically removes tracking data stored in Safari after a period of non-interaction by the user from the tracker's website.Similar to Chrome and Firefox, Safari offers an invite only  for bug reporting to a select number of security researchers. The bounty program was announced during Apple's  at  2016.Web Extensions in Safari have an additional option to use native code in the Safari's sandbox environment, in addition to Web Extension APIs. Web Extensions in Safari are also distributed through Apple's App store. App store submission comes with the added benefit of Web Extension code being audited by Apple. On the other hand App store submission comes at a steep cost. Yearly  fee costs 100 USD (in contrast to Chrome's 5 dollar lifetime fee and Firefox's free submission). The high cost is prohibitive for the majority of Open Source developers. As a result, Safari has very few extensions to choose from. However, you should keep the high cost in mind when installing extensions. It is expected that most Web Extensions will have some way of monetizing usage in order to cover developer costs. Be wary of Web Extensions whose source code is not open.Safari syncs user preferences and saved passwords with . In order to be viewed in plain text, a user must input the account password of the current device. This means that users can sync data across devices with added security.Safari follows a slower release cycle than Chrome and Firefox (3-4 minor releases, 1 major release, per year). Newer features are slower to be adopted to the stable channel. Although security updates in Safari are handled independent of the stable release schedule and issued automatically through the App store. The Safari channel that follows a six-week release cycle (similar to as Chrome and Firefox) is called  and it is the recommended option instead of the stable channel of Safari.An excellent open source ad blocker for Safari that fully leverages content blockers is . See also  to disable hyperlink auditing beacons.Other Web browsersMany Chromium-derived browsers are not recommended. They are usually , , , and make dubious claims to protect privacy. See .Other miscellaneous browsers, such as , are not evaluated in this guide, so are neither recommended nor actively discouraged from use.Web browsers and privacyAll Web Browsers retain certain information about our browsing habits. That information is used for a number of reasons. One of them is to improve the overall performance of the Web Browser. Most Web Browsers offer prediction services to resolve typos or URL redirections, store analytics data of browsing patterns, crash reports and black listing of known malicious servers. Those options can be turned on and off from each Web browser's settings panel.Since Web browsers execute untrusted code from the server, it is important to understand what type of information can be accessed. The  interface gives access to information about the Web Browser's user agent. Those include information such as the operating system, Web sites' permissions, and the device's battery level. For more information about security conscious browsing and what type of information is being ""leaked"" by your browser, see ,  and .To hinder third party trackers, it is recommended to disable third-party cookies in Web browser settings. A third party cookie is a cookie associated with a file requested by a different domain than the one the user is currently viewing. Most of the time third-party cookies are used to create browsing profiles by tracking a user's movement on the web. Disabling third-party cookies prevents HTTP responses and scripts from other domains from setting cookies. Moreover, cookies are removed from requests to domains that are not the document origin domain, so cookies are only sent to the current site that is being viewed.Also be aware of , which may reveal your local or public (if connected to VPN) IP address(es). In Firefox and Chrome/Chromium this can be disabled with extensions such as  and . Disabling WebRTC in Safari is only possible with a .PluginsAdobe Flash, Oracle Java, Adobe Reader, Microsoft Silverlight (Netflix now works with ) and other plugins are  and should not be installed.If they are necessary, only use them in a disposable virtual machine and subscribe to security announcements to make sure you're always patched.See , , , and  for examples.TorTor is an anonymizing proxy which can be used for browsing the Web.Download Tor Browser from .Do not attempt to configure other browsers or applications to use Tor as you may make a mistake which will compromise anonymity.Download both the  and  signature files, then verify the disk image has been signed by Tor developers:$ cd ~/Downloads

$ file Tor*
TorBrowser-8.0.4-osx64_en-US.dmg:     bzip2 compressed data, block size = 900k
TorBrowser-8.0.4-osx64_en-US.dmg.asc: PGP signature Signature (old)

$ gpg Tor*asc
[...]
gpg: Can't check signature: No public key

$ gpg --recv 0x4E2C6E8793298290
gpg: key 0x4E2C6E8793298290: public key ""Tor Browser Developers (signing key) <torbrowser@torproject.org>"" imported
gpg: no ultimately trusted keys found
gpg: Total number processed: 1
gpg:               imported: 1

$ gpg --verify Tor*asc
gpg: assuming signed data in 'TorBrowser-8.0.4-osx64_en-US.dmg'
gpg: Signature made Mon Dec 10 07:16:22 2018 PST
gpg:                using RSA key 0xEB774491D9FF06E2
gpg: Good signature from ""Tor Browser Developers (signing key) <torbrowser@torproject.org>"" [unknown]
gpg: WARNING: This key is not certified with a trusted signature!
gpg:          There is no indication that the signature belongs to the owner.
Primary key fingerprint: EF6E 286D DA85 EA2A 4BA7  DE68 4E2C 6E87 9329 8290
     Subkey fingerprint: 1107 75B5 D101 FB36 BC6C  911B EB77 4491 D9FF 06E2
Make sure  appears in the output. The warning about the key not being certified is benign, as it has not yet been manually assigned trust.See  for more information.To finish installing Tor Browser, open the disk image and drag the it into the Applications folder, or with:$ hdiutil mount TorBrowser-8.0.4-osx64_en-US.dmg

$ cp -r /Volumes/Tor\ Browser/Tor\ Browser.app/ ~/Applications/

Verify the Tor application's code signature was made by with The Tor Project's Apple developer ID MADPSAYN6T, using the  and/or  commands:$ spctl -a -vv ~/Applications/Tor\ Browser.app
/Users/drduh/Applications/Tor Browser.app: accepted
source=Developer ID
origin=Developer ID Application: The Tor Project, Inc (MADPSAYN6T)

$ pkgutil --check-signature ~/Applications/Tor\ Browser.app
Package ""Tor Browser.app"":
   Status: signed by a certificate trusted by Mac OS X
   Certificate Chain:
    1. Developer ID Application: The Tor Project, Inc (MADPSAYN6T)
       SHA1 fingerprint: 95 80 54 F1 54 66 F3 9C C2 D8 27 7A 29 21 D9 61 11 93 B3 E8
       -----------------------------------------------------------------------------
    2. Developer ID Certification Authority
       SHA1 fingerprint: 3B 16 6C 3B 7D C4 B7 51 C9 FE 2A FA B9 13 56 41 E3 88 E1 86
       -----------------------------------------------------------------------------
    3. Apple Root CA
       SHA1 fingerprint: 61 1E 5B 66 2C 59 3A 08 FF 58 D1 4A E2 24 52 D1 98 DF 6C 60
You can also use the  command to examine an application's code signature:$ codesign -dvv ~/Applications/Tor\ Browser.app
Executable=/Users/drduh/Applications/Tor Browser.app/Contents/MacOS/firefox
Identifier=org.torproject.torbrowser
Format=app bundle with Mach-O thin (x86_64)
CodeDirectory v=20200 size=229 flags=0x0(none) hashes=4+3 location=embedded
Library validation warning=OS X SDK version before 10.9 does not support Library Validation
Signature size=4247
Authority=Developer ID Application: The Tor Project, Inc (MADPSAYN6T)
Authority=Developer ID Certification Authority
Authority=Apple Root CA
Signed Time=Dec 10, 2018 at 12:18:45 AM
Info.plist entries=24
TeamIdentifier=MADPSAYN6T
Sealed Resources version=2 rules=12 files=128
Internal requirements count=1 size=188
To view full certificate details for a signed application, extract them with  and decode it with :$ codesign -d --extract-certificates ~/Applications/Tor\ Browser.app
Executable=/Users/drduh/Applications/Tor Browser.app/Contents/MacOS/firefox

$ file codesign*
codesign0: data
codesign1: data
codesign2: data

$ openssl x509 -inform der -in codesign0 -subject -issuer -startdate -enddate -noout
subject= /UID=MADPSAYN6T/CN=Developer ID Application: The Tor Project, Inc (MADPSAYN6T)/OU=MADPSAYN6T/O=The Tor Project, Inc/C=US
issuer= /CN=Developer ID Certification Authority/OU=Apple Certification Authority/O=Apple Inc./C=US
notBefore=Apr 12 22:40:13 2016 GMT
notAfter=Apr 13 22:40:13 2021 GMT

$ openssl x509 -inform der -in codesign0  -fingerprint -noout
SHA1 Fingerprint=95:80:54:F1:54:66:F3:9C:C2:D8:27:7A:29:21:D9:61:11:93:B3:E8

$ openssl x509 -inform der -in codesign0 -fingerprint -sha256 -noout
SHA256 Fingerprint=B5:0D:47:F0:3E:CB:42:B6:68:1C:6F:38:06:2B:C2:9F:41:FA:D6:54:F1:29:D3:E4:DD:9C:C7:49:35:FF:F5:D9
Tor traffic is encrypted to the  (i.e., cannot be read by a passive network eavesdropper), but Tor use can be identified - for example, TLS handshake ""hostnames"" will show up in plaintext:$ sudo tcpdump -An ""tcp"" | grep ""www""
listening on pktap, link-type PKTAP (Apple DLT_PKTAP), capture size 262144 bytes
............."". ...www.odezz26nvv7jeqz1xghzs.com.........
.............#.!...www.bxbko3qi7vacgwyk4ggulh.com.........
.6....m.....>...:.........|../*	Z....W....X=..6...C../....................................0...0..0.......'....F./0..	*.H........0%1#0!..U....www.b6zazzahl3h3faf4x2.com0...160402000000Z..170317000000Z0'1%0#..U....www.tm3ddrghe22wgqna5u8g.net0..0..
See  and  for more information.You may wish to additionally obfuscate Tor traffic using a , such as  or .This can be done by setting up your own  or finding an existing private or public  to serve as an obfuscating entry node.For extra security, use Tor inside a  or  virtualized  or  machine.Finally, remember the Tor network provides , which is not necessarily synonymous with privacy. The Tor network does not guarantee protection against a global observer capable of traffic analysis and . See also  (pdf) and  (pdf).Also see  and its .VPNUnencrypted network traffic is being actively monitored and possibly tampered with. Encrypted traffic still exposes  and could be used to infer behavior or specific actions.It is a good idea to use a VPN with outgoing network traffic (not split tunnel) together with a trustworthy provider.  is one of many available guides for setting up a personal VPN server.Don't just blindly sign up for a VPN service without understanding the full implications and how your traffic will be routed. If you don't understand how the VPN works or are not familiar with the software used, you are probably better off without it.When choosing a VPN service or setting up your own, be sure to research the protocols, key exchange algorithms, authentication mechanisms, and type of encryption being used. Some protocols, such as , should be avoided in favor of  or Linux-based   or via a set of .Some clients may send traffic over the next available interface when VPN is interrupted or disconnected. See  for an example on how to allow traffic only over VPN.Another set of scripts to lock down your system so it will only access the internet via a VPN can be found as part of the Voodoo Privacy project -  and there is an updated guide to setting up an IPSec VPN on a virtual machine () or a docker container ().It may be worthwhile to consider the geographical location of the VPN provider. See further discussion in .Also see this  of the macOS built-in VPN L2TP/IPSec and IKEv2 client.Other open source OpenVPN clients/GUI: ,  are not evaluated in this guide, so are neither recommended nor actively discouraged from use.PGP/GPGPGP is a standard for encrypting email end to end. That means only the chosen recipients can decrypt a message, unlike regular email which is read and forever archived by providers.GPG, or GNU Privacy Guard, is a GPL-licensed open source program compliant with the PGP standard.GPG is used to verify signatures of software you download and install, as well as  or  encrypt files and text.Install from Homebrew with .If you prefer a graphical application, download and install .Download  to use recommended settings:$ curl -o ~/.gnupg/gpg.conf https://raw.githubusercontent.com/drduh/config/master/gpg.conf
See  to securely generate and store GPG keys.Read   and  encrypting and decrypting email to yourself and your friends. Get them interested in this stuff!OTROTR stands for off-the-record and is a cryptographic protocol for encrypting and authenticating conversations over instant messaging.You can use OTR on top of any existing  chat service, even Google Hangouts (which only encrypts conversations between users and the server using TLS).The first time you start a conversation with someone new, you'll be asked to verify their public key fingerprint. Make sure to do this in person or by some other secure means (e.g. GPG encrypted mail).A popular macOS GUI client for XMPP and other chat protocols is .Other XMPP clients include  and . Another relatively new XMPP chat client is , it's focused and security and has built-in support for OTR and Tor.If you want to know how OTR works, read the paper  (pdf)Viruses and malwareThere is an  amount of Mac malware in the wild. Macs aren't immune from viruses and malicious software!Some malware comes bundled with both legitimate software, such as the , and some with illegitimate software, such as  bundled with pirated programs.  is an excellent program for ridding oneself of ""garden-variety"" malware and other ""crapware"".See  (pdf) and  to learn about how garden-variety malware functions.You could periodically run a tool like  to examine persistent applications (e.g. scripts, binaries). But by then, it is probably too late. Maybe applications such as  and  will help. See warnings and caveats in  first, however. An open-source alternative could be .Anti-virus programs are a double-edged sword -- not so useful for advanced users and will likely increase attack surface against sophisticated threats; however possibly useful for catching ""garden variety"" malware on novice users' Macs. There is also the additional processing overhead to consider when using ""active"" scanning features.See  (pdf), , , ,  and .Therefore, the best anti-virus is Common Sense 2020. See discussion in .Local privilege escalation bugs are plenty on macOS, so always be careful when downloading and running untrusted programs or trusted programs from third party websites or downloaded over HTTP ().Subscribe to updates at  and  for current Mac security news.To scan an application with multiple AV products and examine its behavior, upload it to .Also check out  malware for macOS: ,  and , which is a good example of advanced malware with capabilities to hide from userland (e.g., , ). For more, see  and System Integrity Protection (SIP) is a security feature since OS X 10.11 ""El Capitan"". It is enabled by default, but , which may be necessary to change some system settings, such as deleting root certificate authorities or unloading certain launch daemons. Keep this feature on, as it is by default.From :Also see Some MacBook hardware has shipped with . To verify SIP is enabled, use the command , which should return:  Otherwise,  through Recovery Mode.Gatekeeper and XProtectGatekeeper and the quarantine system try to prevent unsigned or ""bad"" programs and files from running and opening.XProtect prevents the execution of known bad files and outdated plugin versions, but does nothing to cleanup or stop existing malware.Both offer trivial protection against common risks and are fine at default settings.See also  and .Note Quarantine stores information about downloaded files in , which may pose a privacy risk. To examine the file, simply use  or the following command:$ echo 'SELECT datetime(LSQuarantineTimeStamp + 978307200, ""unixepoch"") as LSQuarantineTimeStamp, ' \
  'LSQuarantineAgentName, LSQuarantineOriginURLString, LSQuarantineDataURLString from LSQuarantineEvent;' | \
  sqlite3 /Users/$USER/Library/Preferences/com.apple.LaunchServices.QuarantineEventsV2
See  for more information.To permanently disable this feature,  and :$ :>~/Library/Preferences/com.apple.LaunchServices.QuarantineEventsV2

$ sudo chflags schg ~/Library/Preferences/com.apple.LaunchServices.QuarantineEventsV2
Alternatively, you can also disable Gatekeeper using the following command:(See  and  for reference)Metadata and artifactsmacOS attaches metadata () to downloaded files, which can be viewed with the  and  commands:$ ls -l@ ~/Downloads/TorBrowser-8.0.4-osx64_en-US.dmg
-rw-r--r--@ 1 drduh staff 63M Jan 1 12:00 TorBrowser-8.0.4-osx64_en-US.dmg
	com.apple.metadata:kMDItemWhereFroms	  46B
	com.apple.quarantine	  57B

$ mdls ~/Downloads/TorBrowser-8.0.4-osx64_en-US.dmg
kMDItemContentCreationDate         = 2019-01-01 00:00:00 +0000
kMDItemContentCreationDate_Ranking = 2019-01-01 00:00:00 +0000
kMDItemContentModificationDate     = 2019-01-01 00:00:00 +0000
kMDItemContentType                 = ""com.apple.disk-image-udif""
kMDItemContentTypeTree             = (
    ""public.archive"",
    ""public.item"",
    ""public.data"",
    ""public.disk-image"",
    ""com.apple.disk-image"",
    ""com.apple.disk-image-udif""
)
kMDItemDateAdded                   = 2019-01-01 00:00:00 +0000
kMDItemDateAdded_Ranking           = 2019-01-01 00:00:00 +0000
kMDItemDisplayName                 = ""TorBrowser-8.0.4-osx64_en-US.dmg""
kMDItemFSContentChangeDate         = 2019-01-01 00:00:00 +0000
kMDItemFSCreationDate              = 2019-01-01 00:00:00 +0000
kMDItemFSCreatorCode               = """"
kMDItemFSFinderFlags               = 0
kMDItemFSHasCustomIcon             = (null)
kMDItemFSInvisible                 = 0
kMDItemFSIsExtensionHidden         = 0
kMDItemFSIsStationery              = (null)
kMDItemFSLabel                     = 0
kMDItemFSName                      = ""TorBrowser-8.0.4-osx64_en-US.dmg""
kMDItemFSNodeCount                 = (null)
kMDItemFSOwnerGroupID              = 5000
kMDItemFSOwnerUserID               = 501
kMDItemFSSize                      = 65840402
kMDItemFSTypeCode                  = """"
kMDItemInterestingDate_Ranking     = 2019-01-01 00:00:00 +0000
kMDItemKind                        = ""Disk Image""
kMDItemWhereFroms                  = (
    ""https://dist.torproject.org/torbrowser/8.0.4/TorBrowser-8.0.4-osx64_en-US.dmg"",
    ""https://www.torproject.org/projects/torbrowser.html.en""
)

$ xattr -l ~/Downloads/TorBrowser-8.0.4-osx64_en-US.dmg
com.apple.metadata:kMDItemWhereFroms:
00000000  62 70 6C 69 73 74 30 30 A2 01 02 5F 10 4D 68 74  |bplist00..._.Mht|
00000010  74 70 73 3A 2F 2F 64 69 73 74 2E 74 6F 72 70 72  |tps://dist.torpr|
00000020  6F 6A 65 63 74 2E 6F 72 67 2F 74 6F 72 62 72 6F  |oject.org/torbro|
[...]
com.apple.quarantine: 0081;58519ffa;Google Chrome.app;1F032CAB-F5A1-4D92-84EB-CBECA971B7BC
Metadata attributes can also be removed with the  flag:$ xattr -d com.apple.metadata:kMDItemWhereFroms ~/Downloads/TorBrowser-8.0.4-osx64_en-US.dmg

$ xattr -d com.apple.quarantine ~/Downloads/TorBrowser-8.0.4-osx64_en-US.dmg

$ xattr -l ~/Downloads/TorBrowser-8.0.4-osx64_en-US.dmg
[No output expected]
Other metadata and artifacts may be found in the directories including, but not limited to, , , , some of which is detailed below. contains historical list of volumes attached. To clear it, use the command  contains Bluetooth metadata, including device history. If Bluetooth is not used, the metadata can be cleared with:$ sudo defaults delete /Library/Preferences/com.apple.Bluetooth.plist DeviceCache
$ sudo defaults delete /Library/Preferences/com.apple.Bluetooth.plist IDSPairedDevices
$ sudo defaults delete /Library/Preferences/com.apple.Bluetooth.plist PANDevices
$ sudo defaults delete /Library/Preferences/com.apple.Bluetooth.plist PANInterfaces
$ sudo defaults delete /Library/Preferences/com.apple.Bluetooth.plist SCOAudioDevices
 contains the CUPS printer job cache. To clear it, use the commands:$ sudo rm -rfv /var/spool/cups/c0*
$ sudo rm -rfv /var/spool/cups/tmp/*
$ sudo rm -rfv /var/spool/cups/cache/job.cache*
To clear the list of iOS devices connected, use:$ sudo defaults delete /Users/$USER/Library/Preferences/com.apple.iPod.plist ""conn:128:Last Connect""
$ sudo defaults delete /Users/$USER/Library/Preferences/com.apple.iPod.plist Devices
$ sudo defaults delete /Library/Preferences/com.apple.iPod.plist ""conn:128:Last Connect""
$ sudo defaults delete /Library/Preferences/com.apple.iPod.plist Devices
$ sudo rm -rfv /var/db/lockdown/*
Quicklook thumbnail data can be cleared using the  command, but this writes to the file  in the Quicklook directories, and states that the Quicklook cache was manually cleared. Disable the thumbnail cache with It can also be manually cleared by getting the directory names with  and , then removing them:$ rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/exclusive
$ rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/index.sqlite
$ rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/index.sqlite-shm
$ rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/index.sqlite-wal
$ rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/resetreason
$ rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/thumbnails.data
Similarly, for the root user:$ sudo rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/thumbnails.fraghandler
$ sudo rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/exclusive
$ sudo rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/index.sqlite
$ sudo rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/index.sqlite-shm
$ sudo rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/index.sqlite-wal
$ sudo rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/resetreason
$ sudo rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/thumbnails.data
$ sudo rm -rfv $(getconf DARWIN_USER_CACHE_DIR)/com.apple.QuickLook.thumbnailcache/thumbnails.fraghandler
Also see .To clear Finder preferences:$ defaults delete ~/Library/Preferences/com.apple.finder.plist FXDesktopVolumePositions
$ defaults delete ~/Library/Preferences/com.apple.finder.plist FXRecentFolders
$ defaults delete ~/Library/Preferences/com.apple.finder.plist RecentMoveAndCopyDestinations
$ defaults delete ~/Library/Preferences/com.apple.finder.plist RecentSearches
$ defaults delete ~/Library/Preferences/com.apple.finder.plist SGTRecentFileSearches
Additional diagnostic files may be found in the following directories - but caution should be taken before removing any, as it may break logging or cause other issues:/var/db/CoreDuet/
/var/db/diagnostics/
/var/db/systemstats/
/var/db/uuidtext/
/var/log/DiagnosticMessages/
macOS stored preferred Wi-Fi data (including credentials) in NVRAM. To clear it, use the following commands:$ sudo nvram -d 36C28AB5-6566-4C50-9EBD-CBB920F83843:current-network
$ sudo nvram -d 36C28AB5-6566-4C50-9EBD-CBB920F83843:preferred-networks
$ sudo nvram -d 36C28AB5-6566-4C50-9EBD-CBB920F83843:preferred-count
macOS may collect sensitive information about what you type, even if user dictionary and suggestions are off. To remove them, and prevent them from being created again, use the following commands:$ rm -rfv ""~/Library/LanguageModeling/*"" ""~/Library/Spelling/*"" ""~/Library/Suggestions/*""
$ chmod -R 000 ~/Library/LanguageModeling ~/Library/Spelling ~/Library/Suggestions
$ chflags -R uchg ~/Library/LanguageModeling ~/Library/Spelling ~/Library/Suggestions
QuickLook application support metadata can be cleared and locked with the following commands:$ rm -rfv ""~/Library/Application Support/Quick Look/*""
$ chmod -R 000 ""~/Library/Application Support/Quick Look""
$ chflags -R uchg ""~/Library/Application Support/Quick Look""
Document revision metadata is stored in  and can be cleared and locked with the following commands - caution should be taken as this may break some core Apple applications:$ sudo rm -rfv /.DocumentRevisions-V100/*
$ sudo chmod -R 000 /.DocumentRevisions-V100
$ sudo chflags -R uchg /.DocumentRevisions-V100
Saved application state metadata may be cleared and locked with the following commands:$ rm -rfv ""~/Library/Saved Application State/*""
$ rm -rfv ""~/Library/Containers/<APPNAME>/Saved Application State""
$ chmod -R 000 ""~/Library/Saved Application State/""
$ chmod -R 000 ""~/Library/Containers/<APPNAME>/Saved Application State""
$ chflags -R uchg ""~/Library/Saved Application State/""
$ chflags -R uchg ""~/Library/Containers/<APPNAME>/Saved Application State""
Autosave metadata can be cleared and locked with the following commands:$ rm -rfv ""~/Library/Containers/<APP>/Data/Library/Autosave Information""
$ rm -rfv ""~/Library/Autosave Information""
$ chmod -R 000 ""~/Library/Containers/<APP>/Data/Library/Autosave Information""
$ chmod -R 000 ""~/Library/Autosave Information""
$ chflags -R uchg ""~/Library/Containers/<APP>/Data/Library/Autosave Information""
$ chflags -R uchg ""~/Library/Autosave Information""
The Siri analytics database, which is created even if the Siri launch agent disabled, can be cleared and locked with the following commands:$ rm -rfv ~/Library/Assistant/SiriAnalytics.db
$ chmod -R 000 ~/Library/Assistant/SiriAnalytics.db
$ chflags -R uchg ~/Library/Assistant/SiriAnalytics.db
 contains iTunes metadata. Recent iTunes search data may be cleared with the following command:$ defaults delete ~/Library/Preferences/com.apple.iTunes.plist recentSearches
If you do not use Apple ID-linked services, the following keys may be cleared, too, using the following commands:$ defaults delete ~/Library/Preferences/com.apple.iTunes.plist StoreUserInfo
$ defaults delete ~/Library/Preferences/com.apple.iTunes.plist WirelessBuddyID
All media played in QuickTime Player can be found in:~/Library/Containers/com.apple.QuickTimePlayerX/Data/Library/Preferences/com.apple.QuickTimePlayerX.plist
Additional metadata may exist in the following files:~/Library/Containers/com.apple.appstore/Data/Library/Preferences/com.apple.commerce.knownclients.plist
~/Library/Preferences/com.apple.commerce.plist
~/Library/Preferences/com.apple.QuickTimePlayerX.plist
PasswordsGenerate strong passwords with several programs or directly from :$ openssl rand -base64 30
qb8ZWbUU2Ri3FOAPY/1wKSFAJwMXmpQM4mZU4YbO

$ gpg --gen-random -a 0 90 | fold -w 40
3e+kfHOvovHVXxZYPgu+OOWQ1g1ttbljr+kNGv7f
loD//RsjUXYGIjfPM/bT0itsoEstyGLVUsFns8wP
zYM8VRBga+TsnxWrS7lWKfH1uvVPowzkq9kXCdvJ

$ LANG=C tr -dc 'A-F0-9' < /dev/urandom | fold -w 40 | head -n 5
45D0371481EE5E5A5C1F68EA59E69F9CA52CB321
A30B37A00302643921F205621B145E7EAF520164
B6EF38A2DA1D0586D20105502AFFF0468EA5F16A
029D6EA9F76CD64D3356E342EA154BEFEBE23387
07F468F0569579A0A06471247CABC4F4C1386E24

$ tr -dc '[:alnum:]' < /dev/urandom | fold -w 40 | head -n5
zmj8S0iuxud8y8YHjzdg7Hefu6U1KAYBiLl3aE8v
nCNpuMkWohTjQHntTzbiLQJG5zLzEHWSWaYSwjtm
R2L6M909S3ih852IkJqQFMDawCiHcpPBxlllAPrt
aZOXKVUmxhzQwVSYb6nqAbGTVMFSJOLf094bFZAb
HfgwSNlkVBXwIPQST6E6x6vDNCCasMLSSOoTUfSK

$ tr -dc '[:lower:]' < /dev/urandom | fold -w 40 | head -n5
gfvkanntxutzwxficgvavbwdvttexdezdftvvtmn
lgrsuiugwkqbtbkyggcbpbqlynwbiyxzlabstqcf
ufctdlsbyonkowzpmotxiksnsbwdzkjrjsupoqvr
hjwibdjxtmuvqricljayzkgdfztcmapsgwsubggr
bjstlmvwjczakgeetkbmwbjnidbeaerhaonpkacg

$ tr -dc '[:upper:]' < /dev/urandom | fold -w 40 | head -n5
EUHZMAOBOLNFXUNNDSTLJTPDCPVQBPUEQOLRZUQZ
HVNVKBEPAAYMXRCGVCNEZLFHNUYMRYPTWPWOOZVM
TAHEUPQJTSYQVJVYSKLURESMKWEZONXLUDHWQODB
PRDITWMAXXZLTRXEEOGOSGAWUXYDGDRJYRHUWICM
VHERIQBLBPHSIUZSGYZRDHTNAPUGJMRODIKBWZRJ

$ tr -dc '[:graph:]' < /dev/urandom | fold -w 40 | head -n5
n\T2|zUz:\C,@z9!#p3!B/[t6m:B94}q&t(^)Ol~
J%MMDbAgGdP}zrSQO!3mrP3$w!.[Ng_xx-_[C<3g
^)6V&*<2""ZOgU.mBd]iInvFKiT<dq~y\O[cdDK`V
+RE]UYPIf3:StX`y#w,.iG~g""urD)'FnDIFI_q^)
6?HRillpgvvFDBAr4[:H{^oAL<`Em7$roF=2w;1~
You can also generate passwords, even memorable ones, using Keychain Access password assistant, or a command line equivalent like .Keychains are encrypted with a  and are a pretty safe place to store credentials. See also . Also be aware that Keychain  the names corresponding to password entries.Alternatively, you can manage an encrypted passwords file yourself with GnuPG (see  and  for example).In addition to passwords, ensure eligible online accounts, such as GitHub, Google accounts, banking, have  enabled. offers affordable hardware tokens. See  and . One of two Yubikey's slots can also be programmed to emit a long, static password (which can be used in combination with a short, memorized password, for example).In Addition to Login and other PAMs, you can use Yubikey to secure your login and sudo, here is a pdf guide from . Yubikey are a bit pricey, there is cheaper alternative, but not as capable, . Here is a great guide to BackupAlways encrypt files locally before backing them up to external media or online services.One way is to use a symmetric cipher with GPG and a password of your choosing. Files can also be encrypted to a public key with GPG, with the private key stored on .To compress and encrypt a directory:$ tar zcvf - ~/Downloads | gpg -c > ~/Desktop/backup-$(date +%F-%H%M).tar.gz.gpg
tar: Removing leading '/' from member names
a Users/drduh/Downloads
a Users/drduh/Downloads/.DS_Store
a Users/drduh/Downloads/.localized
a Users/drduh/Downloads/TorBrowser-8.0.4-osx64_en-US.dmg.asc
a Users/drduh/Downloads/TorBrowser-8.0.4-osx64_en-US.dmg
To decrypt and decompress the directory:$ gpg -o ~/Desktop/decrypted-backup.tar.gz -d ~/Desktop/backup-2015-01-01-0000.tar.gz.gpg
gpg: AES256 encrypted data
gpg: encrypted with 1 passphrase

$ tar zxvf ~/Desktop/decrypted-backup.tar.gz
tar: Removing leading '/' from member names
x Users/drduh/._Downloads
x Users/drduh/Downloads/
x Users/drduh/Downloads/._.DS_Store
x Users/drduh/Downloads/.DS_Store
x Users/drduh/Downloads/.localized
x Users/drduh/Downloads/._TorBrowser-8.0.4-osx64_en-US.dmg.asc
x Users/drduh/Downloads/TorBrowser-8.0.4-osx64_en-US.dmg.asc
x Users/drduh/Downloads/._TorBrowser-8.0.4-osx64_en-US.dmg
x Users/drduh/Downloads/TorBrowser-8.0.4-osx64_en-US.dmg
You can also create and use encrypted volumes using Disk Utility or :$ hdiutil create ~/Desktop/encrypted.dmg -encryption -size 50M -volname ""secretStuff"" -fs JHFS+
Enter a new password to secure ""encrypted.dmg"":
Re-enter new password:
....................................
Created: /Users/drduh/Desktop/encrypted.img

$ hdiutil mount ~/Desktop/encrypted.dmg
Enter password to access ""encrypted.dmg"":
[...]
/Volumes/secretStuff

$ cp -v ~/Documents/passwords.txt /Volumes/secretStuff
[...]

$ hdiutil eject /Volumes/secretStuff
""disk4"" unmounted.
""disk4"" ejected.
With  you are also able to add the option . With these sparse bundles you may achieve faster backups because after the first run, the updated information and some padding needs to be transferred.A simple way to synchronize this encrypted folder to another server is using rsync:rsync --recursive --times --progress --delete --verbose --stats MyEncryptedDrive.sparsebundle user@server:/path/to/backup
See also the following applications and services: , , , , and .Wi-FimacOS remembers access points it has connected to. Like all wireless devices, the Mac will broadcast all access point names it remembers (e.g., MyHomeNetwork) each time it looks for a network, such as when waking from sleep.This is a privacy risk, so remove networks from the list in System Preferences > Network > Advanced when they are no longer needed.Also see  (pdf) and  (pdf).Saved Wi-Fi information (SSID, last connection, etc.) can be found in:/Library/Preferences/SystemConfiguration/com.apple.airport.preferences.plist
You may want to  of the network card before connecting to new and untrusted wireless networks to mitigate passive fingerprinting:$ sudo ifconfig en0 ether $(openssl rand -hex 6 | sed 's%\(..\)%\1:%g; s%.$%%')
macOS stores Wi-Fi SSIDs and passwords in NVRAM in order for Recovery Mode to access the Internet. Be sure to either clear NVRAM or de-authenticate your Mac from your Apple account, which will clear the NVRAM, before passing a Mac along. Resetting the SMC will clear some of the NVRAM, but not all.Note MAC addresses will reset to hardware defaults on each boot.Finally, WEP protection on wireless networks is  and you should only connect to WPA2 protected networks when possible.SSHFor outgoing SSH connections, use hardware or password-protected keys,  remote hosts and consider  them for added privacy. See  for recommended client options.You can also use ssh to create an  to send traffic through, similar to a VPN.For example, to use Privoxy running on a remote host port 8118:$ ssh -C -L 5555:127.0.0.1:8118 you@remote-host.tld

$ sudo networksetup -setwebproxy ""Wi-Fi"" 127.0.0.1 5555

$ sudo networksetup -setsecurewebproxy ""Wi-Fi"" 127.0.0.1 5555
Or to use an ssh connection as a :$ ssh -NCD 3000 you@remote-host.tld
By default, macOS does not have sshd or Remote Login enabled.To enable sshd and allow incoming ssh connections:$ sudo launchctl load -w /System/Library/LaunchDaemons/ssh.plist
Or use the System Preferences > Sharing menu.If enabling sshd, be sure to disable password authentication and consider further  your configuration. See  for recommended options.Confirm whether sshd is running:$ sudo lsof -Pni TCP:22
Physical accessKeep your Mac physically secure at all times. Don't leave it unattended in public spaces, such as hotels.A skilled attacker with unsupervised physical access to your computer can infect the boot ROM to install a keylogger and steal your password, for example - see .To protect against physical theft during use, you can use an anti-forensic tool like ,  or  (updated usbkill, with graphical user interface). All respond to USB events and can immediately shutdown your computer if your device is physically separated from you or an unauthorized device is connected.Consider purchasing a  for your screen to thwart shoulder surfers.Superglues or epoxy resins can also be used to disable physical access to computer ports.  and tamper-evidence seals can be applied to components to detect tampering.System monitoringOpenBSM auditmacOS has a powerful OpenBSM (Basic Security Module) auditing capability. You can use it to monitor process execution, network activity, and much more.To tail audit logs, use the  utility:$ sudo praudit -l /dev/auditpipe
header,201,11,execve(2),0,Thu Sep  1 12:00:00 2015, + 195 msec,exec arg,/Applications/.evilapp/rootkit,path,/Applications/.evilapp/rootkit,path,/Applications/.evilapp/rootkit,attribute,100755,root,wheel,16777220,986535,0,subject,drduh,root,wheel,root,wheel,412,100005,50511731,0.0.0.0,return,success,0,trailer,201,
header,88,11,connect(2),0,Thu Sep  1 12:00:00 2015, + 238 msec,argument,1,0x5,fd,socket-inet,2,443,173.194.74.104,subject,drduh,root,wheel,root,wheel,326,100005,50331650,0.0.0.0,return,failure : Operation now in progress,4354967105,trailer,88
header,111,11,OpenSSH login,0,Thu Sep  1 12:00:00 2015, + 16 msec,subject_ex,drduh,drduh,staff,drduh,staff,404,404,49271,::1,text,successful login drduh,return,success,0,trailer,111,
See the manual pages for , ,  and other files in Note although  says the  flag will synchronize the audit configuration, it appears necessary to reboot for changes to take effect.See articles on  and  for more information.DTraceNote   with DTrace, so it is not possible to use it in recent macOS versions without disabling SIP.See  for more information.Execution lists information about all running processes.You can also view processes with Activity Monitor. and  list loaded and running user and system launch daemons and agents.NetworkList open network files:$ sudo lsof -Pni
List contents of various network-related data structures:$ sudo netstat -atln
 can be used from the command line with .Monitor DNS queries and replies:$ tshark -Y ""dns.flags.response == 1"" -Tfields \
  -e frame.time_delta \
  -e dns.qry.name \
  -e dns.a \
  -Eseparator=,
Monitor HTTP requests and responses:$ tshark -Y ""http.request or http.response"" -Tfields \
  -e ip.dst \
  -e http.request.full_uri \
  -e http.request.method \
  -e http.response.code \
  -e http.response.phrase \
  -Eseparator=/s
Monitor x509 (SSL/TLS) certificates:$ tshark -Y ""ssl.handshake.certificate"" -Tfields \
  -e ip.src \
  -e x509sat.uTF8String \
  -e x509sat.printableString \
  -e x509sat.universalString \
  -e x509sat.IA5String \
  -e x509sat.teletexString \
  -Eseparator=/s -Equote=d
Also see the simple networking monitoring application .Binary Whitelisting is a security software developed for Google's corporate Macintosh fleet and open sourced.Santa uses the  to monitor and allow/disallow binaries from executing in the kernel. Binaries can be white- or black-listed by unique hash or signing developer certificate. Santa can be used to only allow trusted code execution, or to blacklist known malware from executing on a Mac, similar to Bit9 software for Windows.Note Santa does not currently have a graphical user interface for managing rules. The following instructions are for advanced users only!To install Santa, visit the  page and download the latest disk image, the mount it and install the contained package:$ hdiutil mount ~/Downloads/santa-0.9.20.dmg

$ sudo installer -pkg /Volumes/santa-0.9.20/santa-0.9.20.pkg -tgt /
By default, Santa installs in ""Monitor"" mode (meaning, nothing gets blocked, only logged) and comes with two rules: one for Apple binaries and another for Santa software itself.Verify Santa is running and its kernel module is loaded:$ santactl status
>>> Daemon Info
  Mode                   | Monitor
  File Logging           | No
  Watchdog CPU Events    | 0  (Peak: 0.00%)
  Watchdog RAM Events    | 0  (Peak: 0.00MB)
>>> Kernel Info
  Kernel cache count     | 0
>>> Database Info
  Binary Rules           | 0
  Certificate Rules      | 2
  Events Pending Upload  | 0

$ ps -ef | grep ""[s]anta""
    0   786     1   0 10:01AM ??         0:00.39 /Library/Extensions/santa-driver.kext/Contents/MacOS/santad --syslog

$ kextstat | grep santa
  119    0 0xffffff7f822ff000 0x6000     0x6000     com.google.santa-driver (0.9.14) 693D8E4D-3161-30E0-B83D-66A273CAE026 <5 4 3 1>
Create a blacklist rule to prevent iTunes from executing:$ sudo santactl rule --blacklist --path /Applications/iTunes.app/
Added rule for SHA-256: e1365b51d2cb2c8562e7f1de36bfb3d5248de586f40b23a2ed641af2072225b3.
Try to launch iTunes - it will be blocked.$ open /Applications/iTunes.app/
LSOpenURLsWithRole() failed with error -10810 for the file /Applications/iTunes.app.
To remove the rule:$ sudo santactl rule --remove --path /Applications/iTunes.app/
Removed rule for SHA-256: e1365b51d2cb2c8562e7f1de36bfb3d5248de586f40b23a2ed641af2072225b3.
Open iTunes:$ open /Applications/iTunes.app/
[iTunes will open successfully]
Create a new, example C program:$ cat <<EOF > foo.c
> #include <stdio.h>
> main() { printf(""Hello World\n”); }
> EOF
Compile the program with GCC (requires installation of Xcode or command-line tools):$ gcc -o foo foo.c

$ file foo
foo: Mach-O 64-bit executable x86_64

$ codesign -d foo
foo: code object is not signed at all
Run it:$ ./foo
Hello World
Toggle Santa into ""Lockdown"" mode, which only allows whitelisted binaries to run:$ sudo defaults write /var/db/santa/config.plist ClientMode -int 2
Try to run the unsigned binary:$ ./foo
bash: ./foo: Operation not permitted

Santa

The following application has been blocked from executing
because its trustworthiness cannot be determined.

Path:       /Users/demouser/foo
Identifier: 4e11da26feb48231d6e90b10c169b0f8ae1080f36c168ffe53b1616f7505baed
Parent:     bash (701)
To whitelist a specific binary, determine its SHA-256 sum:$ santactl fileinfo /Users/demouser/foo
Path                 : /Users/demouser/foo
SHA-256              : 4e11da26feb48231d6e90b10c169b0f8ae1080f36c168ffe53b1616f7505baed
SHA-1                : 4506f3a8c0a5abe4cacb98e6267549a4d8734d82
Type                 : Executable (x86-64)
Code-signed          : No
Rule                 : Blacklisted (Unknown)
Add a whitelist rule:$ sudo santactl rule --whitelist --sha256 4e11da26feb48231d6e90b10c169b0f8ae1080f36c168ffe53b1616f7505baed
Added rule for SHA-256: 4e11da26feb48231d6e90b10c169b0f8ae1080f36c168ffe53b1616f7505baed.
Run it:$ ./foo
Hello World
It's allowed and works!Applications can also be whitelisted by developer certificate (so that new binary versions will not need to be manually whitelisted on each update). For example, download and run Google Chrome - it will be blocked by Santa in ""Lockdown"" mode:$ curl -sO https://dl.google.com/chrome/mac/stable/GGRO/googlechrome.dmg

$ hdiutil mount googlechrome.dmg

$ cp -r /Volumes/Google\ Chrome/Google\ Chrome.app /Applications/

$ open /Applications/Google\ Chrome.app/
LSOpenURLsWithRole() failed with error -10810 for the file /Applications/Google Chrome.app.
Whitelist the application by its developer certificate (first item in the Signing Chain):$ santactl fileinfo /Applications/Google\ Chrome.app/
Path                 : /Applications/Google Chrome.app/Contents/MacOS/Google Chrome
SHA-256              : 0eb08224d427fb1d87d2276d911bbb6c4326ec9f74448a4d9a3cfce0c3413810
SHA-1                : 9213cbc7dfaaf7580f3936a915faa56d40479f6a
Bundle Name          : Google Chrome
Bundle Version       : 2883.87
Bundle Version Str   : 55.0.2883.87
Type                 : Executable (x86-64)
Code-signed          : Yes
Rule                 : Blacklisted (Unknown)
Signing Chain:
     1. SHA-256             : 15b8ce88e10f04c88a5542234fbdfc1487e9c2f64058a05027c7c34fc4201153
        SHA-1               : 85cee8254216185620ddc8851c7a9fc4dfe120ef
        Common Name         : Developer ID Application: Google Inc.
        Organization        : Google Inc.
        Organizational Unit : EQHXZ8M8AV
        Valid From          : 2012/04/26 07:10:10 -0700
        Valid Until         : 2017/04/27 07:10:10 -0700

     2. SHA-256             : 7afc9d01a62f03a2de9637936d4afe68090d2de18d03f29c88cfb0b1ba63587f
        SHA-1               : 3b166c3b7dc4b751c9fe2afab9135641e388e186
        Common Name         : Developer ID Certification Authority
        Organization        : Apple Inc.
        Organizational Unit : Apple Certification Authority
        Valid From          : 2012/02/01 14:12:15 -0800
        Valid Until         : 2027/02/01 14:12:15 -0800

     3. SHA-256             : b0b1730ecbc7ff4505142c49f1295e6eda6bcaed7e2c68c5be91b5a11001f024
        SHA-1               : 611e5b662c593a08ff58d14ae22452d198df6c60
        Common Name         : Apple Root CA
        Organization        : Apple Inc.
        Organizational Unit : Apple Certification Authority
        Valid From          : 2006/04/25 14:40:36 -0700
        Valid Until         : 2035/02/09 13:40:36 -0800
In this case,  is the SHA-256 of Google’s Apple developer certificate (team ID EQHXZ8M8AV). To whitelist it:$ sudo santactl rule --whitelist --certificate --sha256 15b8ce88e10f04c88a5542234fbdfc1487e9c2f64058a05027c7c34fc4201153
Added rule for SHA-256: 15b8ce88e10f04c88a5542234fbdfc1487e9c2f64058a05027c7c34fc4201153.
Google Chrome should now launch, and subsequent updates to the application will continue to work as long as the code signing certificate doesn’t change or expire.To disable ""Lockdown"" mode:$ sudo defaults delete /var/db/santa/config.plist ClientMode
See  to monitor ALLOW and DENY execution decisions.A log and configuration server for Santa is available in , an open source event monitoring solution and TLS server for osquery and Santa.Zentral will support Santa in both MONITORING and LOCKDOWN operation mode. Clients need to be enrolled with a TLS connection to sync Santa Rules, all Santa events from endpoints are aggregated and logged back in Zentral. Santa events can trigger actions and notifications from within the Zentral Framework.Note Python, Bash and other interpreters are whitelisted (since they are signed by Apple's developer certificate), so Santa will not be able to block such scripts from executing. Thus, a potential non-binary program which disables Santa is a weakness (not vulnerability, since it is so by design) to take note of.MiscellaneousDisable .If you want to play music or watch videos, use  which is free and open source.If you want to use torrents, use  which is free and open source (note: like all software, even open source projects, ). You may also wish to use a block list to avoid peering with known bad hosts - see  and .Manage default file handlers with , which can be installed with . One reason to manage extensions is to prevent auto-mounting of remote file systems in Finder (see ). Here are several recommended file handlers to manage:$ duti -s com.apple.Safari afp

$ duti -s com.apple.Safari ftp

$ duti -s com.apple.Safari nfs

$ duti -s com.apple.Safari smb

$ duti -s com.apple.TextEdit public.unix-executable
Monitor system logs with the Console application or  or  commands.In systems prior to macOS Sierra (10.12), enable the  in  to restrict the sudo session to the Terminal window/tab that started it. To do so, use  and add the line .Set your screen to lock as soon as the screensaver starts:$ defaults write com.apple.screensaver askForPassword -int 1

$ defaults write com.apple.screensaver askForPasswordDelay -int 0
Expose hidden files and Library folder in Finder:$ defaults write com.apple.finder AppleShowAllFiles -bool true

$ chflags nohidden ~/Library
Show all filename extensions (so that ""Evil.jpg.app"" cannot masquerade easily).$ defaults write NSGlobalDomain AppleShowAllExtensions -bool true
Don't default to saving documents to iCloud:$ defaults write NSGlobalDomain NSDocumentSaveNewDocumentsToCloud -bool false
Enable  in Terminal (unless you use  or applications such as ).Disable crash reporter (the dialog which appears after an application crashes and prompts to report the problem to Apple):$ defaults write com.apple.CrashReporter DialogType none
Disable Bonjour :$ sudo defaults write /Library/Preferences/com.apple.mDNSResponder.plist NoMulticastAdvertisements -bool YES
 and Bluetooth features, if they aren't necessary.Consider  your applications. See  (pdf) and .Did you know Apple has not shipped a computer with TPM since ?macOS comes with this line in :Defaults env_keep += ""HOME MAIL""
Which stops sudo from changing the HOME variable when you elevate privileges. This means it will execute as root the bash dotfiles in the non-root user's home directory when you run ""sudo bash"". It is advisable to comment this line out to avoid a potentially easy way for malware or a local attacker to escalate privileges to root.If you want to retain the convenience of the root user having a non-root user's home directory, you can append an export line to /var/root/.bashrc, e.g.:export HOME=/Users/blah
Set a :$ sudo launchctl config user umask 077
Reboot, create a file in Finder and verify its permissions (macOS default allows 'group/other' read access):$ ls -ld umask*
drwx------  2 kevin  staff       64 Dec  4 12:27 umask_testing_dir
-rw-------@ 1 kevin  staff  2026566 Dec  4 12:28 umask_testing_file
Related softwareAdditional resources"
https://github.com/guelfoweb/knock,Knock Subdomain Scan,"Knock Subdomain Scan v6.1.0Knockpy is a portable and modular python3 tool designed to quickly enumerate subdomains on a target domain through passive reconnaissance and dictionary scan.Very simplypython3 knockpy.py domain.com
Table of ContentsInstallRun from folderYou need python3, pip3, git.git clone https://github.com/guelfoweb/knock.git
cd knock
pip3 install -r requirements.txt

python3 knockpy.py <DOMAIN>
Install packageAs rootgit clone https://github.com/guelfoweb/knock.git
cd knock
python3 setup.py install

knockpy <DOMAIN>
DockerKnockpy image hosted at  and automatically updated with docker run -it --rm secsi/knockpy <domain>
UsageAs a standalone command line toolKnockpy usage: knockpy [-h] [-v] [--no-local] [--no-remote] [--no-scan] [--no-http] 
               [--no-http-code CODE [CODE ...]] [--dns DNS] [-w WORDLIST] 
               [-o FOLDER] [-t SEC] [-th NUM] [--silent [{False,json,json-pretty,csv}]]
               domain

--------------------------------------------------------------------------------
* SCAN
full scan:    knockpy domain.com
quick scan:   knockpy domain.com --no-local
faster scan:  knockpy domain.com --no-local --no-http
ignore code:  knockpy domain.com --no-http-code 404 500 530
silent mode:  knockpy domain.com --silent

* SUBDOMAINS
show recon:   knockpy domain.com --no-local --no-scan

* REPORT
show report:  knockpy --report knockpy_report/domain.com_yyyy_mm_dd_hh_mm_ss.json
plot report:  knockpy --plot knockpy_report/domain.com_yyyy_mm_dd_hh_mm_ss.json
csv report:   knockpy --csv knockpy_report/domain.com_yyyy_mm_dd_hh_mm_ss.json
--------------------------------------------------------------------------------

positional arguments:
  domain                target to scan

optional arguments:
  -h, --help            show this help message and exit
  -v, --version         show program's version number and exit
  --no-local            local wordlist ignore
  --no-remote           remote wordlist ignore
  --no-scan             scanning ignore, show wordlist and exit
  --no-http             http requests ignore
                        
  --no-http-code CODE [CODE ...]
                        http code list to ignore

  --dns DNS             use custom DNS ex. 8.8.8.8                        

  -w WORDLIST           wordlist file to import
  -o FOLDER             report folder to store json results
  -t SEC                timeout in seconds
  -th NUM               threads num

  --silent [{False,json,json-pretty,csv}]
                        silent or quiet mode, default: False
Full scan$ knockpy domain.com
Knockpy uses by default a internal file wordlist.txt and a remote list obtained by scanning other sources (passive recon) through plugins. To use a custom dictionary you can use the  option and specify the path to your local dictionary. Also, you can write a new plugin to populate the wordlist with subdomains obtained from external services. Take a look at the ones in the  folder and use them as an example. Remember that some plugins, like  or , need apikey to work.The domain target can be passed via STDIN.echo ""domain.com"" | knockpy
To ignore http(s) responses with specific code, you can use the  followed by the code list . With the  option you can ignore ip list Scan with remote list only: $ knockpy domain.com --no-local
Only test subdomains obtained through passive reconnaissance using plugins. This scanning mode will be faster because it excludes the local dictionary.No scan, get remote list only: $ knockpy domain.com --no-scan --no-local
Print passive-only wordlist and exit. No scan will be performed.Use a custom DNS: $ knockpy domain.com --dns 8.8.8.8
By default it uses the pre-configured DNS on your system (ex. ).Silent mode: $ knockpy domain.com --silent
Hide terminal output and save json report in the output folder. Using  with the  option hides the banner and shows the list of subdomains to the terminal.$ knockpy domain.com --silent json
Hide terminal output and print final results in json format.$ knockpy domain.com --silent json-pretty
Hide terminal output and print final results in intented json.$ knockpy domain.com --silent csv
Hide terminal output and print final results in csv format.Note that at each scan the report will be .Output folder: $ knockpy domain.com -o /path/to/new/folder
All scans are saved in the default folder . Alternatively, you can use the  to define the new folder path or disable autosave using .ReportAt each scan the report will be automatically saved in json format inside the file with the name . If you don't like autosave you can disable using .Report example :{
    ""sub-1.domain.com"": {
        ""domain"": ""host.domain.ext"",
        ""alias"": [""sub-1.domain.com""],
        ""ipaddr"": [
            ""123.123.123.123""
        ],
        ""code"": 200,
        ""server"": ""Microsoft-IIS/8.5""
    },
    ...................................
               -- cut --
    ...................................
    ""sub-n.domain.com""{
        ""domain"": """",
        ""alias"": [],
        ""ipaddr"": [
            ""123.123.123.124""
        ],
        ""code"": 500,
        ""server"": ""nginx/1.15.6 ""
    },
    ""_meta"": {
        ""name"": ""knockpy"",
        ""version"": ""5.4.1"",
        ""time_start"": 1616353591.2510355,
        ""time_end"": 1616353930.6632543,
        ""domain"": ""domain.com"",
        ""wordlist"": 2120
    }
}
 is a reserved key that contains the basic information of the scan.Show report: $ knockpy --report knockpy_report/domain.com_yyyy_mm_dd_hh_mm_ss.json
Print the report in the terminal in a human format.Convert report in CSV: $ knockpy --csv knockpy_report/domain.com_yyyy_mm_dd_hh_mm_ss.json
Save the existing report in csv file.Plot report: $ knockpy --plot knockpy_report/domain.com_yyyy_mm_dd_hh_mm_ss.json
Plot needs these libraries: ModuleUsage as a libraryImporting knockpy as a module (dependence) in your python script is quite simple. Naturally, the package  on your system.from knockpy import knockpy
The command-line parameters can be managed with the following dictionary.params = {
    ""no_local"": False,  # [bool] local wordlist ignore
    ""no_remote"": False, # [bool] remote wordlist ignore
    ""no_scan"": False,   # [bool] scanning ignore, show wordlist
    ""no_http"": False,   # [bool] http requests ignore
    ""no_http_code"": [], # [list] http code list to ignore
    ""no_ip"": [],        # [list] ip address to ignore
    ""dns"": """",          # [str] use custom DNS ex. 8.8.8.8
    ""timeout"": 3,       # [int] timeout in seconds
    ""threads"": 30,      # [int] threads num
    ""useragent"": """",    # [str] use a custom user agent
    ""wordlist"": """"      # [str] path to custom wordlist
}
You can choose pass only the keys you want to change and keep the others with the default values. Eg:params = {
    ""no_local"": True,
    ""no_scan"": True
}
Then you can call the function  passing as values the domain and the dictionary assigned to the variable params to get the results in json format. results = knockpy.Scanning.start(""domain.com"", params)
PluginWrite your own pluginThe plugins are situated in  folder. If you want to write your own plugin it's important to pay attention to some precautions:    api_service.py
    def get(domain):
        foo
    ['sub1.domain.com', 'sub2.domain.com', ...]
    requests, json, bs4, re
Here is an example of how a plugin should be structured. You can find other examples in the  folder.import requests
import json

def get(domain):
    # servicename -> JSON: key -> subdomain
    url = ""https://servicename.com/search/?q={domain}"".format(domain=domain)
    resp = requests.get(url, timeout=5).text
    
    resp = json.loads(resp)
    
    result = []
    for item in resp['data']:
        subdomain = item['subdomain']
        if subdomain not in result:
            result.append(subdomain)

    return result
Plugin test $ knockpy domain.com --plugin-test
In this example, the output shows errors  in three plugins because they need the API key.{
    'api_shodan.py': {
        'time': '00:00:03',
        'match': 0,
        'error': True
    },
    'certspotter.py': {
        'time': '00:00:00',
        'match': 9,
        'error': False
    },
    'rapiddns.py': {
        'time': '00:00:00',
        'match': 44,
        'error': False
    },
    'hackertarget.py': {
        'time': '00:00:00',
        'match': 9,
        'error': False
    },
    'crtsh.py': {
        'time': '00:00:19',
        'match': 10,
        'error': False
    },
    'api_censys.py': {
        'time': '00:00:03',
        'match': 0,
        'error': True
    },
    'webarchive.py': {
        'time': '00:00:04',
        'match': 4,
        'error': False
    },
    'api_virustotal.py': {
        'time': '00:00:03',
        'match': 0,
        'error': True
    },
    'alienvault.py': {
        'time': '00:00:01',
        'match': 11,
        'error': False
    },
    '_results': {
        'time': '00:00:37',
        'plugins': {
            'count': 9,
            'list': ['api_shodan.py', 'certspotter.py', 'rapiddns.py', 'hackertarget.py', ...],
            'error': ['api_shodan.py', 'api_censys.py', 'api_virustotal.py']
        },
        'subdomains': {
            'count': 52,
            'list': ['admin', 'cloud', 'www', 'mail', 'calendar', 'contact', 'ftp', .....]
        }
    }
}

LicenseKnockpy is currently under development by  and it's released under the GPL 3 license."
https://github.com/streamlink/streamlink,Streamlink is a CLI utility which pipes video streams from various services into a video player,"📦 InstallationPlease take a look at the documentation for different ways of installing Streamlink:👍 FeaturesStreamlink is built on top of a plugin system which allows support for new services to be added easily.Most of the popular streaming services are supported, such as:... and many more. A list of all plugins currently included can be found on the .💡 QuickstartAfter installing, simply run:streamlink ""STREAMURL"" best
The default behavior of Streamlink is to play back streams in the , but a lot of other options and output methods are available, such as writing the stream to the filesystem, reading stream metadata, etc.For more in-depth usage, please refer to the .An  and  is available for Python implementors of Streamlink.🙏 ContributingAll contributions are welcome.Feel free to open a new thread on the issue tracker or submit a new pull request.Please read  first. Thanks!❤️ SupportIf you think that Streamlink is useful and if you want to keep the project alive, then please consider supporting its maintainers by sending a small and optionally recurring tip via the .Your support is very much appreciated, thank you!"
https://github.com/jopohl/urh,Universal Radio Hacker: Investigate Wireless Protocols Like A Boss,"The Universal Radio Hacker (URH) is a complete suite for wireless protocol investigation with native support for  common Software Defined Radios.URH allows easy demodulation of signals combined with an  detection of modulation parameters making it a breeze to identify the bits and bytes that fly over the air.As data often gets encoded before transmission, URH offers customizable decodings to crack even sophisticated encodings like CC1101 data whitening.When it comes to protocol reverse-engineering, URH is helpful in two ways. You can either manually assign protocol fields and message types or let URH automatically infer protocol fields with a .Finally, URH entails a fuzzing component aimed at stateless protocols and a simulation environment for stateful attacks.Getting startedIn order to get startedIf you like URH, please :star: this repository and . We appreciate your support!Citing URHWe encourage researchers working with URH to cite  WOOT'18 paper or directly use the following BibTeX entry.@inproceedings {220562,
author = {Johannes Pohl and Andreas Noack},
title = {Universal Radio Hacker: A Suite for Analyzing and Attacking Stateful Wireless Protocols},
booktitle = {12th {USENIX} Workshop on Offensive Technologies ({WOOT} 18)},
year = {2018},
address = {Baltimore, MD},
url = {https://www.usenix.org/conference/woot18/presentation/pohl},
publisher = {{USENIX} Association},
}
InstallationURH runs on Windows, Linux and macOS. See below for OS specific installation instructions.WindowsOn Windows, URH can be installed with its . No further dependencies are required.If you get an error about missing , run Windows Update or directly install .LinuxInstallation with pipxURH is available on  so you can install it, for example, with : pipx install urh
This is the recommended way to install URH on Linux because it comes with all native extensions precompiled.In order to access your SDR as non-root user, install the according udev rules. You can find them .Install via Package ManagerURH is included in the repositories of many linux distributions such as Arch Linux, Gentoo, Fedora, openSUSE or NixOS. There is also a package for FreeBSD.  If available, simply use your package manager to install URH.Note: For native support, you must install the according  package(s) of your SDR(s) such as  before installing URH.Docker ImagesThe official URH docker image is available . It has all native backends included and ready to operate.macOSUsing DMGIt is recommended to use at least macOS 10.14 when using the DMG available .With brewURH is available as a  so you can install it withbrew install urh
Running from source (OS-agnostic)Without installationTo execute the Universal Radio Hacker without installation, just run:git clone https://github.com/jopohl/urh/
cd urh/src/urh
./main.py
Note, before first usage the C++ extensions will be built.Installing from sourceTo install URH from source you need to have  installed. You can get them with .Once the setuptools are installed execute: git clone https://github.com/jopohl/urh/
cd urh
python setup.py install
And start the application by typing  in a terminal.ArticlesHacking stuff with URHGeneral presentations and tutorials on URHExternal decodingsSee  for a list of external decodings provided by our community! Thanks for that!ScreenshotsGet the data out of raw signalsKeep an overview even on complex protocolsRecord and send signals"
https://github.com/hephaest0s/usbkill,« usbkill » is an anti-forensic kill-switch that waits for a change on your USB ports and then immediately shuts down your computer.,"« usbkill » is an anti-forensic kill-switch that waits for a change on your USB ports and then immediately shuts down your computer.To run:sudo python usbkill.py
orsudo python3 usbkill.py
Related project; same idea, but implemented as a Linux driver: https://github.com/NateBrune/silk-guardianWhy?Some reasons to use this tool:Feature List(version 1.0-rc.4)Supported command line arguments (partially for devs):Contact - PGP/GPG Fingerprint: 8764 EF6F D5C1 7838 8D10 E061 CF84 9CE5 42D0 B12B"
https://github.com/Netflix/security_monkey,"Security Monkey monitors AWS, GCP, OpenStack, and GitHub orgs for assets and their changes over time.","NOTE: Security Monkey is in maintenance mode and will be end-of-life in 2020.Security MonkeySecurity Monkey monitors your  for policy changes and alerts on insecure configurations.  Support is available for OpenStack public and private clouds.  Security Monkey can also watch and monitor your GitHub organizations, teams, and repositories.It provides a single UI to browse and search through all of your accounts, regions, and cloud services.  The monkey remembers previous states and can show you exactly what changed, and when.Security Monkey can be extended with , , , and .It works on CPython 2.7. It is known to work on Ubuntu Linux and OS X.| Develop Branch  | Master Branch || ------------- | ------------- ||   |   ||   |  |Special Note:Netflix's support for Security Monkey has been reduced for minor bug fixes only. That being said, we are happy to accept and merge pull-requests that fix bugs and add new features as appropriate.🚨⚠️🥁🎺 PLEASE READ: BREAKING CHANGES FOR 1.0 🎺🥁⚠️🚨If you are upgrading to 1.0 for the first time, please review the  and the documents as there is a new deployment pattern for Security Monkey. Also, new IAM permissions have been added.Project resourcesInstance DiagramThe components that make up Security Monkey are as follows (not AWS specific):Access DiagramSecurity Monkey accesses accounts to scan via credentials it is provided (""Role Assumption"" where available)."
https://github.com/EpistasisLab/tpot,A Python Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming.,"Master status: Development status: Package information: To try the  TPOT2 (alpha) please go !TPOT stands for Tree-based Pipeline Optimization Tool. Consider TPOT your Data Science Assistant. TPOT is a Python Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming.TPOT will automate the most tedious part of machine learning by intelligently exploring thousands of possible pipelines to find the best one for your data.Once TPOT is finished searching (or you get tired of waiting), it provides you with the Python code for the best pipeline it found so you can tinker with the pipeline from there.TPOT is built on top of scikit-learn, so all of the code it generates should look familiar... if you're familiar with scikit-learn, anyway.TPOT is still under active development and we encourage you to check back on this repository regularly for updates.For further information about TPOT, please see the .LicensePlease see the  for the licensing and usage information for TPOT.Generally, we have licensed TPOT to make it as widely usable as possible.InstallationWe maintain the  in the documentation. TPOT requires a working installation of Python.UsageTPOT can be used  or .Click on the corresponding links to find more information on TPOT usage in the documentation.ExamplesClassificationBelow is a minimal working example with the optical recognition of handwritten digits dataset.from tpot import TPOTClassifier
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split

digits = load_digits()
X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target,
                                                    train_size=0.75, test_size=0.25, random_state=42)

tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2, random_state=42)
tpot.fit(X_train, y_train)
print(tpot.score(X_test, y_test))
tpot.export('tpot_digits_pipeline.py')
Running this code should discover a pipeline that achieves about 98% testing accuracy, and the corresponding Python code should be exported to the  file and look similar to the following:import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline, make_union
from sklearn.preprocessing import PolynomialFeatures
from tpot.builtins import StackingEstimator
from tpot.export_utils import set_param_recursive

# NOTE: Make sure that the outcome column is labeled 'target' in the data file
tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)
features = tpot_data.drop('target', axis=1)
training_features, testing_features, training_target, testing_target = \
            train_test_split(features, tpot_data['target'], random_state=42)

# Average CV score on the training set was: 0.9799428471757372
exported_pipeline = make_pipeline(
    PolynomialFeatures(degree=2, include_bias=False, interaction_only=False),
    StackingEstimator(estimator=LogisticRegression(C=0.1, dual=False, penalty=""l1"")),
    RandomForestClassifier(bootstrap=True, criterion=""entropy"", max_features=0.35000000000000003, min_samples_leaf=20, min_samples_split=19, n_estimators=100)
)
# Fix random state for all the steps in exported pipeline
set_param_recursive(exported_pipeline.steps, 'random_state', 42)

exported_pipeline.fit(training_features, training_target)
results = exported_pipeline.predict(testing_features)
RegressionSimilarly, TPOT can optimize pipelines for regression problems. Below is a minimal working example with the practice Boston housing prices data set.from tpot import TPOTRegressor
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split

housing = load_boston()
X_train, X_test, y_train, y_test = train_test_split(housing.data, housing.target,
                                                    train_size=0.75, test_size=0.25, random_state=42)

tpot = TPOTRegressor(generations=5, population_size=50, verbosity=2, random_state=42)
tpot.fit(X_train, y_train)
print(tpot.score(X_test, y_test))
tpot.export('tpot_boston_pipeline.py')
which should result in a pipeline that achieves about 12.77 mean squared error (MSE), and the Python code in  should look similar to:import numpy as np
import pandas as pd
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import PolynomialFeatures
from tpot.export_utils import set_param_recursive

# NOTE: Make sure that the outcome column is labeled 'target' in the data file
tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)
features = tpot_data.drop('target', axis=1)
training_features, testing_features, training_target, testing_target = \
            train_test_split(features, tpot_data['target'], random_state=42)

# Average CV score on the training set was: -10.812040755234403
exported_pipeline = make_pipeline(
    PolynomialFeatures(degree=2, include_bias=False, interaction_only=False),
    ExtraTreesRegressor(bootstrap=False, max_features=0.5, min_samples_leaf=2, min_samples_split=3, n_estimators=100)
)
# Fix random state for all the steps in exported pipeline
set_param_recursive(exported_pipeline.steps, 'random_state', 42)

exported_pipeline.fit(training_features, training_target)
results = exported_pipeline.predict(testing_features)
Check the documentation for .Contributing to TPOTWe welcome you to  for bugs or enhancements to work on. If you have an idea for an extension to TPOT, please  so we can discuss it.Before submitting any contributions, please review our .Having problems or have questions about TPOT?Please  to see if your issue has already been attended to. If it hasn't,  on this repository so we can review your issue.Citing TPOTIf you use TPOT in a scientific publication, please consider citing at least one of the following papers:Trang T. Le, Weixuan Fu and Jason H. Moore (2020). . Bioinformatics.36(1): 250-256.BibTeX entry:@article{le2020scaling,
  title={Scaling tree-based automated machine learning to biomedical big data with a feature set selector},
  author={Le, Trang T and Fu, Weixuan and Moore, Jason H},
  journal={Bioinformatics},
  volume={36},
  number={1},
  pages={250--256},
  year={2020},
  publisher={Oxford University Press}
}
Randal S. Olson, Ryan J. Urbanowicz, Peter C. Andrews, Nicole A. Lavender, La Creis Kidd, and Jason H. Moore (2016). . Applications of Evolutionary Computation, pages 123-137.BibTeX entry:@inbook{Olson2016EvoBio,
    author={Olson, Randal S. and Urbanowicz, Ryan J. and Andrews, Peter C. and Lavender, Nicole A. and Kidd, La Creis and Moore, Jason H.},
    editor={Squillero, Giovanni and Burelli, Paolo},
    chapter={Automating Biomedical Data Science Through Tree-Based Pipeline Optimization},
    title={Applications of Evolutionary Computation: 19th European Conference, EvoApplications 2016, Porto, Portugal, March 30 -- April 1, 2016, Proceedings, Part I},
    year={2016},
    publisher={Springer International Publishing},
    pages={123--137},
    isbn={978-3-319-31204-0},
    doi={10.1007/978-3-319-31204-0_9},
    url={http://dx.doi.org/10.1007/978-3-319-31204-0_9}
}
Randal S. Olson, Nathan Bartley, Ryan J. Urbanowicz, and Jason H. Moore (2016). . Proceedings of GECCO 2016, pages 485-492.BibTeX entry:@inproceedings{OlsonGECCO2016,
    author = {Olson, Randal S. and Bartley, Nathan and Urbanowicz, Ryan J. and Moore, Jason H.},
    title = {Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science},
    booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference 2016},
    series = {GECCO '16},
    year = {2016},
    isbn = {978-1-4503-4206-3},
    location = {Denver, Colorado, USA},
    pages = {485--492},
    numpages = {8},
    url = {http://doi.acm.org/10.1145/2908812.2908918},
    doi = {10.1145/2908812.2908918},
    acmid = {2908918},
    publisher = {ACM},
    address = {New York, NY, USA},
}
Alternatively, you can cite the repository directly with the following DOI:Support for TPOTTPOT was developed in the  at the  with funding from the  under grant R01 AI117694. We are incredibly grateful for the support of the NIH and the University of Pennsylvania during the development of this project.The TPOT logo was designed by Todd Newmuis, who generously donated his time to the project."
https://github.com/boto/boto,"For the latest version of boto, see https://github.com/boto/boto3 -- Python interface to Amazon Web Services","Deprecation noticeThis package is no longer maintained and has been replaced by . package or the __, please open an issue on their respective repositories.botoboto 2.49.0Released: 11-July-2018.. image:: https://pypip.in/d/boto/badge.svg:target: https://pypi.python.org/pypi/boto/IntroductionBoto is a Python package that provides interfaces to Amazon Web Services.Currently, all features work with Python 2.6 and 2.7. Work is under way tosupport Python 3.3+ in the same codebase. Modules are being ported one ata time with the help of the open source community, so please check belowfor compatibility with Python 3.3+.To port a module to Python 3.3+, please view our _and the _. If you would like, you can open an issue to letothers know about your work in progress. Tests must pass on Python2.6, 2.7, 3.3, and 3.4 for pull requests to be accepted.ServicesAt the moment, boto supports:The goal of boto is to support the full breadth and depth of AmazonWeb Services.  In addition, boto provides support for other publicservices such as Google Storage in addition to private cloud systemslike Eucalyptus, OpenStack and Open Nebula.Boto is developed mainly using Python 2.6.6 and Python 2.7.3 on Mac OSXand Ubuntu Maverick.  It is known to work on other Linux distributionsand on Windows.  Most of Boto requires no additional libraries or packagesother than those that are distributed with Python.  Efforts are madeto keep boto compatible with Python 2.5.x but no guarantees are made.InstallationInstall via _:::$ pip install boto
Install from source:::$ git clone git://github.com/boto/boto.git
$ cd boto
$ python setup.py install
ChangeLogsTo see what has changed over time in boto, you can check out therelease notes at Finding Out More About BotoThe main source code repository for boto can be found on . model for branching._ is also available. The online documentation includesfull API documentation as well as Getting Started Guides for many of the botomodules.Boto releases can be found on the _.Join our IRC channel  on FreeNode.Webchat IRC channel: http://webchat.freenode.net/?channels=botoJoin the _.Getting Started with BotoYour credentials can be passed into the methods that createconnections.  Alternatively, boto will check for the existence of thefollowing environment variables to ascertain your credentials:AWS_ACCESS_KEY_ID - Your AWS Access Key IDAWS_SECRET_ACCESS_KEY - Your AWS Secret Access KeyCredentials and other boto-related settings can also be stored in aboto config file.  See _ for details... _Contributing Guidelines: https://github.com/boto/boto/blob/develop/CONTRIBUTING.. _Porting Guide: http://boto.readthedocs.org/en/latest/porting_guide.html.. _pip: http://www.pip-installer.org/.. _release notes: https://github.com/boto/boto/wiki.. _github.com: http://github.com/boto/boto.. _Online documentation: http://docs.pythonboto.org.. _Python Cheese Shop: http://pypi.python.org/pypi/boto.. _this: http://docs.pythonboto.org/en/latest/boto_config_tut.html.. _gitflow: http://nvie.com/posts/a-successful-git-branching-model/.. _neo: https://github.com/boto/boto/tree/neo.. _boto-users Google Group: https://groups.google.com/forum/?fromgroups#!forum/boto-users"
https://github.com/Yelp/mrjob,Run MapReduce jobs on Hadoop or Amazon Web Services,"mrjob: the Python MapReduce library.. image:: https://github.com/Yelp/mrjob/raw/master/docs/logos/logo_medium.pngmrjob is a Python 2.7/3.4+ package that helps you write and run HadoopStreaming jobs.__.. image:: https://travis-ci.org/Yelp/mrjob.png:target: https://travis-ci.org/Yelp/mrjobmrjob fully supports Amazon's Elastic MapReduce (EMR) service, which allows youto buy time on a Hadoop cluster on an hourly basis. mrjob has basic support for Google Cloud Dataproc (Dataproc)which allows you to buy time on a Hadoop cluster on a minute-by-minute basis.  It also works with your ownHadoop cluster.Some important features:InstallationAs of v0.7.0, Amazon Web Services and Google Cloud Services are optionaldepedencies. To use these, install with the  and  targets,respectively. For example:A Simple Map Reduce JobCode for this example and more live in ... code-block:: python""""""The classic MapReduce job: count the frequency of words.""""""from mrjob.job import MRJobimport reWORD_RE = re.compile(r""[\w']+"")class MRWordFreqCount(MRJob):   def mapper(self, _, line):
       for word in WORD_RE.findall(line):
           yield (word.lower(), 1)

   def combiner(self, word, counts):
       yield (word, sum(counts))

   def reducer(self, word, counts):
       yield (word, sum(counts))
if name == 'main':MRWordFreqCount.run()Try It Out!::# locally
python mrjob/examples/mr_word_freq_count.py README.rst > counts
# on EMR
python mrjob/examples/mr_word_freq_count.py README.rst -r emr > counts
# on Dataproc
python mrjob/examples/mr_word_freq_count.py README.rst -r dataproc > counts
# on your Hadoop cluster
python mrjob/examples/mr_word_freq_count.py README.rst -r hadoop > counts
Setting up EMR on AmazonSetting up Dataproc on GoogleAdvanced ConfigurationTo run in other AWS regions, upload your source tree, run , and useother advanced mrjob features, you'll need to set up . mrjob looksfor its conf file in:See _ for moreinformation.Project LinksReferenceMore InformationThanks to _(_) for the logo."
https://github.com/joblib/joblib,Computing with Python functions.,"|PyPi| |Azure| |ReadTheDocs| |Codecov| .. |PyPi| image:: https://badge.fury.io/py/joblib.svg:target: https://badge.fury.io/py/joblib:alt: Joblib version.. |Azure| image:: https://dev.azure.com/joblib/joblib/_apis/build/status/joblib.joblib?branchName=master:target: https://dev.azure.com/joblib/joblib/_build?definitionId=3&_a=summary&branchFilter=40:alt: Azure CI status.. |ReadTheDocs| image:: https://readthedocs.org/projects/joblib/badge/?version=latest:target: https://joblib.readthedocs.io/en/latest/?badge=latest:alt: Documentation Status.. |Codecov| image:: https://codecov.io/gh/joblib/joblib/branch/master/graph/badge.svg:target: https://codecov.io/gh/joblib/joblib:alt: Codecov coverageThe homepage of joblib with user documentation is located on:https://joblib.readthedocs.ioGetting the latest codeTo get the latest code using git, simply type::git clone https://github.com/joblib/joblib.git
If you don't have git installed, you can download a zipof the latest code: https://github.com/joblib/joblib/archive/refs/heads/master.zipInstallingYou can use  to install joblib::pip install joblib
from any directory or::python setup.py install
from the source directory.DependenciesWorkflow to contributeTo contribute to joblib, first create an account on . Once this is done, fork the  to have your own repository,clone it using 'git clone' on the computers where you want to work. Makeyour changes in your clone, push them to your github account, test themon several computers, and when you are happy with them, send a pullrequest to the main repository.Running the test suiteTo run the test suite, you need the pytest (version >= 3) and coverage modules.Run the test suite using::pytest joblib
from the root of the project.Building the docsTo build the docs you need to have sphinx (>=1.4) and some dependenciesinstalled::pip install -U -r .readthedocs-requirements.txt
The docs can then be built with the following command::make doc
The html docs are located in the  directory.Making a source tarballTo create a source tarball, eg for packaging or distributing, run thefollowing command::python setup.py sdist
The tarball will be created in the  directory. This command willcompile the docs, and the resulting tarball can be installed withno extra dependencies than the Python standard library. You will needsetuptool and sphinx.Making a release and uploading it to PyPIThis command is only run by project manager, to make a release, andupload in to PyPI::python setup.py sdist bdist_wheel
twine upload dist/*
Note that the documentation should automatically get updated at each gitpush. If that is not the case, try building th doc locally and resolveany doc build error (in particular when running the examples).Updating the changelogChanges are listed in the CHANGES.rst file. They must be manually updatedbut, the following git command may be used to generate the lines::git log --abbrev-commit --date=short --no-merges --sparse
"
https://github.com/ray-project/ray,Ray is a unified framework for scaling AI and Python applications. Ray consists of a core distributed runtime and a set of AI Libraries for accelerating ML workloads.,".. image:: https://github.com/ray-project/ray/raw/master/doc/source/images/ray_header_logo.png.. image:: https://readthedocs.org/projects/ray/badge/?version=master:target: http://docs.ray.io/en/master/?badge=master.. image:: https://img.shields.io/badge/Ray-Join%20Slack-blue:target: https://forms.gle/9TSdDYUgxYs8SA9e8.. image:: https://img.shields.io/badge/Discuss-Ask%20Questions-blue:target: https://discuss.ray.io/.. image:: https://img.shields.io/twitter/follow/raydistributed.svg?style=social&logo=twitter:target: https://twitter.com/raydistributedRay is a unified framework for scaling AI and Python applications. Ray consists of a core distributed runtime and a set of AI libraries for simplifying ML compute:.. image:: https://github.com/ray-project/ray/raw/master/doc/source/images/what-is-ray-padded.svg..https://docs.google.com/drawings/d/1Pl8aCYOsZCo61cmp57c7Sja6HhIygGCvSZLi_AuBuqo/editLearn more about _:Or more about _ and its key abstractions:Monitor and debug Ray applications and clusters using the __.Ray runs on any machine, cluster, cloud provider, and Kubernetes, and features a growing_.Install Ray with: . For nightly wheels, see the__... _: https://docs.ray.io/en/latest/serve/index.html.. _: https://docs.ray.io/en/latest/data/dataset.html.. _: https://docs.ray.io/en/latest/workflows/concepts.html.. _: https://docs.ray.io/en/latest/train/train.html.. _: https://docs.ray.io/en/latest/tune/index.html.. _: https://docs.ray.io/en/latest/rllib/index.html.. _: https://docs.ray.io/en/latest/ray-overview/ray-libraries.htmlWhy Ray?Today's ML workloads are increasingly compute-intensive. As convenient as they are, single-node development environments such as your laptop cannot scale to meet these demands.Ray is a unified way to scale Python and AI applications from a laptop to a cluster.With Ray, you can seamlessly scale the same code from a laptop to a cluster. Ray is designed to be general-purpose, meaning that it can performantly run any kind of workload. If your application is written in Python, you can scale it with Ray, no other infrastructure required.More InformationOlder documents:.. _: https://docs.ray.io/en/latest/ray-air/getting-started.html.. _: https://docs.ray.io/en/latest/ray-core/walkthrough.html.. _: https://docs.ray.io/en/latest/ray-core/tasks.html.. _: https://docs.ray.io/en/latest/ray-core/actors.html.. _: https://docs.ray.io/en/latest/ray-core/objects.html.. _: http://docs.ray.io/en/latest/index.html.. _: https://docs.google.com/document/d/1lAy0Owi-vPz2jEqBSaHNQcy2IBSDEHyXNOQZlGuj93c/preview.. _: https://docs.google.com/document/d/1tBw9A4j62ruI5omIJbMxly-la5w4q_TjyJgJL_jN2fI/preview.. _: https://arxiv.org/abs/2203.05072.. _: https://www.usenix.org/system/files/nsdi21-wang.pdf.. _: https://arxiv.org/abs/1712.05889.. _: https://arxiv.org/abs/1703.03924.. _: https://arxiv.org/abs/1712.09381.. _: https://arxiv.org/abs/1807.05118Getting Involved.. list-table:::widths: 25 50 25 25:header-rows: 1.. _: https://discuss.ray.io/.. _: https://github.com/ray-project/ray/issues.. _: https://stackoverflow.com/questions/tagged/ray.. _: https://www.meetup.com/Bay-Area-Ray-Meetup/.. _: https://twitter.com/raydistributed.. _: https://forms.gle/9TSdDYUgxYs8SA9e8"
https://github.com/agronholm/apscheduler,Task scheduling library for Python,".. image:: https://github.com/agronholm/apscheduler/actions/workflows/test.yml/badge.svg:target: https://github.com/agronholm/apscheduler/actions/workflows/test.yml:alt: Build Status.. image:: https://coveralls.io/repos/github/agronholm/apscheduler/badge.svg?branch=master:target: https://coveralls.io/github/agronholm/apscheduler?branch=master:alt: Code Coverage.. image:: https://readthedocs.org/projects/apscheduler/badge/?version=latest:target: https://apscheduler.readthedocs.io/en/master/?badge=latest:alt: Documentation.. warning:: The v4.0 series is provided as a pre-release and may change in abackwards incompatible fashion without any migration pathway, so do NOT use thisrelease in production!Advanced Python Scheduler (APScheduler) is a task scheduler and task queue system forPython. It can be used solely as a job queuing system if you have no need for taskscheduling. It scales both up and down, and is suitable for both trivial, single-processuse cases as well as large deployments spanning multiple nodes. Multiple schedulers andworkers can be deployed to use a shared data store to provide both a degree of highavailability and horizontal scaling.APScheduler comes in both synchronous and asynchronous flavors, making it a good fit forboth traditional, thread-based applications, and asynchronous (asyncio or Trio_)applications. Documentation and examples are provided for integrating with either WSGI_or ASGI_ compatible web applications.Support is provided for persistent storage of schedules and jobs. This means that theycan be shared among multiple scheduler/worker instances and will survive process andnode restarts.The built-in persistent data store back-ends are:The built-in event brokers (needed in scenarios with multiple schedulers and/orworkers):The built-in scheduling mechanisms (triggers) are:Different scheduling mechanisms can even be combined with so-called combining triggers(see the documentation_ for details).You can also implement your custom scheduling logic by building your own trigger class.These will be treated no differently than the built-in ones.Other notable features include:.. _Trio: https://pypi.org/project/trio/.. _WSGI: https://wsgi.readthedocs.io/en/latest/what.html.. _ASGI: https://asgi.readthedocs.io/en/latest/index.html.. _documentation: https://apscheduler.readthedocs.io/en/master/DocumentationDocumentation can be found_.SourceThe source can be browsed at _.Reporting bugsA _ is provided byGitHub.Getting helpIf you have problems or other questions, you can either:.. _GitHub discussions: https://github.com/agronholm/apscheduler/discussions/categories/q-a.. _StackOverflow: http://stackoverflow.com/questions/tagged/apscheduler"
https://github.com/facebookarchive/huxley,A testing system for catching visual regressions in Web applications.,"HuxleyWatches you browse, takes screenshots, tells you when they changeHuxley is a test-like system for catching visual regressions in Web applications. It was built by  with input from  at .Archived RepoThis is an archived project and is no longer supported or updated by Facebook or Instagram. Please do not file issues or pull-requests against this repo. If you wish to continue to develop this code yourself, we recommend you fork it.What is the problem?How does Huxley help me?Huxley runs in two modes:Record modeUsing Selenium WebDriver, Huxley opens a page and records your actions. When you press enter in the Huxley terminal, Huxley will save a screenshot.Testing a new flow is as simple as manually testing it once. Huxley will remember and re-run your ""manual"" test plan for you automatically.Playback modeYou should run Huxley in playback mode before submitting code for review and in continuous integration. Huxley will open the page and re-run your actions with WebDriver. It will take screenshots and compare them with the original screenshots. If they have changed, it will save the new screenshots and warn you that they have changed.When screenshots have changed, those screenshot changes will show up in your commit. A designer can review them to be sure they're OK. And your continuous integration system can alert you if you forgot to run Huxley.By default, Huxley will overwrite the old screenshots with new ones. That means you don't have to rewrite anything when your UI changes like you would with a traditional WebDriver test -- Huxley will just take a new screenshot for you and when it's checked in your test is updated!InstallationTutorialIn  you'll find two simple completed Huxley tests. To start from scratch, simply remove ,  and .MotivationIn  you'll find a very simple JavaScript application that implements a toggle button. The goal of Huxley is to make creating an integration for this component effortless, and to make it easy to update the test when the UI changes.Step 1: host your app somewhereFor our example, simply  to  and run  to start a basic server for our demo. In your app you may need to start up whatever framework you're using.Step 2: create a HuxleyfileA Huxleyfile describes your test. Create one that looks like this:[toggle]
url=http://localhost:8000/toggle.html
This creates a test named  that tests the URL .Step 2: record the testHuxley makes writing tests easy because it simply records your browser session -- specifically mouse clicks and key presses on a single page -- and can replay them in an automated way. To do this you need to install  and start it. It's as easy as .Then, run Huxley in record mode: . Huxley will bring up a browser using Selenium. Press enter in the Huxley console to take a screen shot of the initial page load. Then toggle the button in the browser a few times. After every click, switch back to the Huxley console to take a screen shot. When you've tested all of the functionality you want to test, simply type  and then enter in the Huxley console to exit.After confirming, Huxley will automatically record the test for you and save it to disk as . Be sure to commit the  as well as  into your repository so you can track changes to them.Step 3: playbackSimply run the  command in the same directory as the  to be sure that your app still works.Step 4: update the test with new screen shotsYou'll likely update the UI of the component a lot without changing its core functionality. Huxley can take new screen shots for you when this happens. Tweak the UI of the component in  somehow (maybe change the button color or something) and re-run . It will warn you that the UI has changed and will automatically write new screen shots for you. If you run  again, the test will pass since the screen shots were updated.The best part is, since the screen shots are checked into the repository, you can review the changes to the UI as part of the code review process if you'd like. At Instagram we have frontend engineers reviewing the JavaScript and designers reviewing the screenshots to ensure that they're pixel perfect.Step 5: run in CI modeIf you're using a continuous integration solution like  you probably don't want to automatically rerecord screen shots on failure. Simply run  to do this.Additionally, you may find that you're dissatisfied with Huxley replaying your browsing session in real-time. You can speed it up (or slow it down) by editing your  to read:[toggle]
url=http://localhost:8000/toggle.html
sleepfactor=0.5
This edit should cut the execution time in half.Best practicesIntegration tests sometimes get a bad rap for testing too much at once. We've found that if you use integration tests correctly they can be just as effective and accurate as unit tests. Simply follow a few best practices:Technical FAQWhy does Huxley stop recording when I navigate away from the page?Huxley is designed for testing JavaScript UI components at this time. We've found that you can test multiple pages by creating a new Huxley test for each URL. This is valuable even if you don't use the interactive features of Huxley because it will ensure your static pages stay pixel perfect.I can't tell what changed!It's usually best if you use an image comparison tool like  to tell what changed. But Huxley includes a simple image diff tool; simply run  with the  option to output a  which will show you the pixels that changed.How do I use a remote webdriver server?You can set the  environment variable to tell Huxley which webdriver URL to use for  mode. You can set the  environment variable to tell Huxley which webdriver URL to use for screenshots and playback. Usually you only need to use this when working in a team setting such that everyone's screenshots are taken on the same machine configuration (otherwise they'll change depending on who ran them last).Can I test responsive design?Of course! Simply add a  setting to your . The default is .Philosophical FAQWhy would you use this instead of unit testing?First of all, if you sufficiently componentize your UI, Huxley can be used as a unit testing tool.With that said, unit tests have two shortcomings today.What's the best way to use Huxley?Use it however you want! But we generally shell out to it from within an existing test runner (i.e. Django or Rails). This lets us programmatically start a test server for Huxley to hit.If you're using Python, you can use Huxley directly in a test (see ) or browse the source to see its core APIs.If you're on a team I recommend setting up webdriver on a shared server and changing the  environment variable such that everyone's screenshots are pixel perfect (see the technical FAQ above).Why is it called Huxley?Lots of test frameworks and methodologies are very opinionated about how your code should be structured or how you should write tests. Some tools are so opinionated that they're almost religious about their view of testing! We wanted a tool that got out of our way and let us fight regressions as quickly and easily as possible without being opinionated about it. So we named it after the guy who coined the term ""agnostic"", ."
https://github.com/Bitmessage/PyBitmessage,Reference client for Bitmessage: a P2P encrypted decentralised communication protocol:,"PyBitmessageBitmessage is a P2P communication protocol used to send encrypted messages toanother person or to many subscribers. It is decentralized and trustless,meaning that you need-not inherently trust any entities like root certificateauthorities. It uses strong authentication, which means that the sender of amessage cannot be spoofed. BM aims to hide metadata from passive eavesdropperslike those ongoing warrantless wiretapping programs. Hence the sender and receiverof Bitmessages stay anonymous.DevelopmentBitmessage is a collaborative project. You are welcome to submit pull requestsalthough if you plan to put a non-trivial amount of work into coding newfeatures, it is recommended that you first describe your ideas in theseparate issue.Feel welcome to join chan ""bitmessage"", BM-2cWy7cvHoq3f1rYMerRJp8PT653jjSuEdYReferences"
https://github.com/coleifer/huey,a little task queue for python,".. image:: http://media.charlesleifer.com/blog/photos/huey2-logo.pnga lightweight alternative.huey is:huey supports:.. image:: http://i.imgur.com/2EpRs.jpgAt a glance.. code-block:: pythonfrom huey import RedisHuey, crontab

huey = RedisHuey('my-app', host='redis.myapp.com')

@huey.task()
def add_numbers(a, b):
    return a + b

@huey.task(retries=2, retry_delay=60)
def flaky_task(url):
    # This task might fail, in which case it will be retried up to 2 times
    # with a delay of 60s between retries.
    return this_might_fail(url)

@huey.periodic_task(crontab(minute='0', hour='3'))
def nightly_backup():
    sync_all_data()
Calling a -decorated function will enqueue the function call forexecution by the consumer. A special result handle is returned immediately,which can be used to fetch the result once the task is finished:.. code-block:: pycon>>> from demo import add_numbers
>>> res = add_numbers(1, 2)
>>> res
<Result: task 6b6f36fc-da0d-4069-b46c-c0d4ccff1df6>

>>> res()
3
Tasks can be scheduled to run in the future:.. code-block:: pycon>>> res = add_numbers.schedule((2, 3), delay=10)  # Will be run in ~10s.
>>> res(blocking=True)  # Will block until task finishes, in ~10s.
5
For much more, check out the _or take a look at the _.Running the consumer^^^^^^^^^^^^^^^^^^^^Run the consumer with four worker processes:.. code-block:: console$ huey_consumer.py my_app.huey -k process -w 4
To run the consumer with a single worker thread (default):.. code-block:: console$ huey_consumer.py my_app.huey
If your work-loads are mostly IO-bound, you can run the consumer with threadsor greenlets instead. Because greenlets are so lightweight, you can run quite afew of them efficiently:.. code-block:: console$ huey_consumer.py my_app.huey -k greenlet -w 32
StorageHuey's design and feature-set were informed by the capabilities of the_ database. Redis is a fantastic fit for alightweight task queueing library like Huey: it's self-contained, versatile,and can be a multi-purpose solution for other web-application tasks likecaching, event publishing, analytics, rate-limiting, and more.Although Huey was designed with Redis in mind, the storage system implements asimple API and many other tools could be used instead of Redis if that's yourpreference.Huey comes with builtin support for Redis, Sqlite and in-memory storage.Documentation_.Project page_.Huey is named in honor of my cat:.. image:: http://m.charlesleifer.com/t/800x-/blog/photos/p1473037658.76.jpg?key=mD9_qMaKBAuGPi95KzXYqg"
https://github.com/SpiderLabs/Responder,"Responder is a LLMNR, NBT-NS and MDNS poisoner, with built-in HTTP/SMB/MSSQL/FTP/LDAP rogue authentication server supporting NTLMv1/NTLMv2/LMv2, Extended Security NTLMSSP and Basic HTTP authentication. ",":no_entry: [DEPRECATED] Active at https://github.com/lgandx/ResponderResponder.pyLLMNR/NBT-NS/mDNS PoisonerAuthor: Laurent Gaffie <laurent.gaffie@gmail.com > http://www.spiderlabs.comIntroResponder an LLMNR, NBT-NS and MDNS poisoner. It will answer to specific NBT-NS (NetBIOS Name Service) queries based on their name suffix (see: http://support.microsoft.com/kb/163409). By default, the tool will only answer to File Server Service request, which is for SMB.The concept behind this is to target our answers, and be stealthier on the network. This also helps to ensure that we don't break legitimate NBT-NS behavior. You can set the -r option via command line if you want to answer to the Workstation Service request name suffix.FeaturesSupports NTLMv1, NTLMv2 hashes with Extended Security NTLMSSP by default. Successfully tested from Windows 95 to Server 2012 RC, Samba and Mac OSX Lion. Clear text password is supported for NT4, and LM hashing downgrade when the --lm option is set. This functionality is enabled by default when the tool is launched.In order to redirect SQL Authentication to this tool, you will need to set the option -r (NBT-NS queries for SQL Server lookup are using the Workstation Service name suffix) for systems older than windows Vista (LLMNR will be used for Vista and higher). This server supports NTLMv1, LMv2 hashes. This functionality was successfully tested on Windows SQL Server 2005 & 2008.In order to redirect HTTP Authentication to this tool, you will need to set the option -r for Windows version older than Vista (NBT-NS queries for HTTP server lookup are sent using the Workstation Service name suffix). For Vista and higher, LLMNR will be used. This server supports NTLMv1, NTLMv2 hashes and Basic Authentication. This server was successfully tested on IE 6 to IE 10, Firefox, Chrome, Safari.Note: This module also works for WebDav NTLM authentication issued from Windows WebDav clients (WebClient). You can now send your custom files to a victim.Same as above. The folder certs/ contains 2 default keys, including a dummy private key. This is intentional, the purpose is to have Responder working out of the box. A script was added in case you need to generate your own self signed key pair.In order to redirect LDAP Authentication to this tool, you will need to set the option -r for Windows version older than Vista (NBT-NS queries for HTTP server lookup are sent using the Workstation Service name suffix). For Vista and higher, LLMNR will be used. This server supports NTLMSSP hashes and Simple Authentication (clear text authentication). This server was successfully tested on Windows Support tool ""ldp"" and LdapAdmin.This modules will collect clear text credentials.This server will answer type A queries. This is really handy when it's combined with ARP spoofing. This module will capture all HTTP requests from anyone launching Internet Explorer on the network if they have ""Auto-detect settings"" enabled. This module is highly effective. You can configure your custom PAC script in Responder.conf and inject HTML into the server's responses. See Responder.conf.This module allows to find the PDC in stealth mode.When the option -f is used, Responder will fingerprint every host who issued an LLMNR/NBT-NS query. All capture modules still work while in fingerprint mode. For MITM on Windows XP/2003 and earlier Domain members. This attack combined with the DNS module is pretty effective.DHCP Inform Spoofing. Allows you to let the real DHCP Server issue IP addresses, and then send a DHCP Inform answer to set your IP address as a primary DNS server, and your own WPAD URL.This module allows you to see NBT-NS, BROWSER, LLMNR, DNS requests on the network without poisoning any responses. Also, you can map domains, MSSQL servers, workstations passively, see if ICMP Redirects attacks are plausible on your subnet. HashesAll hashes are printed to stdout and dumped in an unique file John Jumbo compliant, using this format:(MODULE_NAME)-(HASH_TYPE)-(CLIENT_IP).txt
Log files are located in the ""logs/"" folder. Hashes will be logged and printed only once per user per hash type, unless you are using the Verbose mode (-v).Additionally, all captured hashed are logged into an SQLite database which you can configure in Responder.confConsiderationsEdit this file /etc/NetworkManager/NetworkManager.conf and comment the line: . Then kill dnsmasq with this command (as root): UsageFirst of all, please take a look at Responder.conf and tweak it for your needs.Running the tool:./Responder.py [options]
Typical Usage Example:./Responder.py -I eth0 -wrf
Options:  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -A, --analyze         Analyze mode. This option allows you to see NBT-NS,
                        BROWSER, LLMNR requests without responding.
  -I eth0, --interface=eth0
                        Network interface to use
  -b, --basic           Return a Basic HTTP authentication. Default: NTLM
  -r, --wredir          Enable answers for netbios wredir suffix queries.
                        Answering to wredir will likely break stuff on the
                        network. Default: False
  -d, --NBTNSdomain     Enable answers for netbios domain suffix queries.
                        Answering to domain suffixes will likely break stuff
                        on the network. Default: False
  -f, --fingerprint     This option allows you to fingerprint a host that
                        issued an NBT-NS or LLMNR query.
  -w, --wpad            Start the WPAD rogue proxy server. Default value is
                        False
  -u UPSTREAM_PROXY, --upstream-proxy=UPSTREAM_PROXY
                        Upstream HTTP proxy used by the rogue WPAD Proxy for
                        outgoing requests (format: host:port)
  -F, --ForceWpadAuth   Force NTLM/Basic authentication on wpad.dat file
                        retrieval. This may cause a login prompt. Default:
                        False
  --lm                  Force LM hashing downgrade for Windows XP/2003 and
                        earlier. Default: False
  -v, --verbose         Increase verbosity.
CopyrightNBT-NS/LLMNR ResponderCreated by Laurent GaffieCopyright (C) 2013 Trustwave Holdings, Inc.This program is free software: you can redistribute it and/or modifyit under the terms of the GNU General Public License as published bythe Free Software Foundation, either version 3 of the License, or(at your option) any later version.This program is distributed in the hope that it will be useful,but WITHOUT ANY WARRANTY; without even the implied warranty ofMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See theGNU General Public License for more details.You should have received a copy of the GNU General Public Licensealong with this program.  If not, see "
https://github.com/lebinh/ngxtop,Real-time metrics for nginx server,"================================================================ - real-time metrics for nginx server (and others)ngxtop parses your nginx access log and outputs useful, -like, metrics of your nginx server.So you can tell what is happening with your server in real-time.``ngxtop`` is designed to run in a short-period time just like the ``top`` command for troubleshooting and monitoring
your Nginx server at the moment. If you need a long running monitoring process or storing your webserver stats in external
monitoring / graphing system, you can try `Luameter <https://luameter.com>`_.
 tries to determine the correct location and format of nginx access log file by default, so you can just run and having a close look at all requests coming to your nginx server. But it does not limit you to nginxand the default top view.  is flexible enough for you to configure and change most of its behaviours.You can query for different things, specify your log and format, even parse remote Apache common access log with ease.See sample usages below for some ideas about what you can do with it.Installation::pip install ngxtop
Note:  is primarily developed and tested with python2 but also supports python3.Usage::Usage:
    ngxtop [options]
    ngxtop [options] (print|top|avg|sum) <var>
    ngxtop info

Options:
    -l <file>, --access-log <file>  access log file to parse.
    -f <format>, --log-format <format>  log format as specify in log_format directive.
    --no-follow  ngxtop default behavior is to ignore current lines in log
                     and only watch for new lines as they are written to the access log.
                     Use this flag to tell ngxtop to process the current content of the access log instead.
    -t <seconds>, --interval <seconds>  report interval when running in follow mode [default: 2.0]

    -g <var>, --group-by <var>  group by variable [default: request_path]
    -w <var>, --having <expr>  having clause [default: 1]
    -o <var>, --order-by <var>  order of output for default query [default: count]
    -n <number>, --limit <number>  limit the number of records included in report for top command [default: 10]
    -a <exp> ..., --a <exp> ...  add exp (must be aggregation exp: sum, avg, min, max, etc.) into output

    -v, --verbose  more verbose output
    -d, --debug  print every line and parsed record
    -h, --help  print this help message.
    --version  print version information.

    Advanced / experimental options:
    -c <file>, --config <file>  allow ngxtop to parse nginx config file for log format and location.
    -i <filter-expression>, --filter <filter-expression>  filter in, records satisfied given expression are processed.
    -p <filter-expression>, --pre-filter <filter-expression> in-filter expression to check in pre-parsing phase.
SamplesDefault output
::

    $ ngxtop
    running for 411 seconds, 64332 records processed: 156.60 req/sec

    Summary:
    |   count |   avg_bytes_sent |   2xx |   3xx |   4xx |   5xx |
    |---------+------------------+-------+-------+-------+-------|
    |   64332 |         2775.251 | 61262 |  2994 |    71 |     5 |

    Detailed:
    | request_path                             |   count |   avg_bytes_sent |   2xx |   3xx |   4xx |   5xx |
    |------------------------------------------+---------+------------------+-------+-------+-------+-------|
    | /abc/xyz/xxxx                            |   20946 |          434.693 | 20935 |     0 |    11 |     0 |
    | /xxxxx.json                              |    5633 |         1483.723 |  5633 |     0 |     0 |     0 |
    | /xxxxx/xxx/xxxxxxxxxxxxx                 |    3629 |         6835.499 |  3626 |     0 |     3 |     0 |
    | /xxxxx/xxx/xxxxxxxx                      |    3627 |        15971.885 |  3623 |     0 |     4 |     0 |
    | /xxxxx/xxx/xxxxxxx                       |    3624 |         7830.236 |  3621 |     0 |     3 |     0 |
    | /static/js/minified/utils.min.js         |    3031 |         1781.155 |  2104 |   927 |     0 |     0 |
    | /static/js/minified/xxxxxxx.min.v1.js    |    2889 |         2210.235 |  2068 |   821 |     0 |     0 |
    | /static/tracking/js/xxxxxxxx.js          |    2594 |         1325.681 |  1927 |   667 |     0 |     0 |
    | /xxxxx/xxx.html                          |    2521 |          573.597 |  2520 |     0 |     1 |     0 |
    | /xxxxx/xxxx.json                         |    1840 |          800.542 |  1839 |     0 |     1 |     0 |

View top source IPs of clients
::$ ngxtop top remote_addr
running for 20 seconds, 3215 records processed: 159.62 req/sec

top remote_addr
| remote_addr     |   count |
|-----------------+---------|
| 118.173.177.161 |      20 |
| 110.78.145.3    |      16 |
| 171.7.153.7     |      16 |
| 180.183.67.155  |      16 |
| 183.89.65.9     |      16 |
| 202.28.182.5    |      16 |
| 1.47.170.12     |      15 |
| 119.46.184.2    |      15 |
| 125.26.135.219  |      15 |
| 125.26.213.203  |      15 |
List 4xx or 5xx responses together with HTTP referer
::

    $ ngxtop -i 'status >= 400' print request status http_referer
    running for 2 seconds, 28 records processed: 13.95 req/sec

    request, status, http_referer:
    | request   |   status | http_referer   |
    |-----------+----------+----------------|
    | -         |      400 | -              |

Parse apache log from remote server with `common` format
::$ ssh user@remote_server tail -f /var/log/apache2/access.log | ngxtop -f common
running for 20 seconds, 1068 records processed: 53.01 req/sec

Summary:
|   count |   avg_bytes_sent |   2xx |   3xx |   4xx |   5xx |
|---------+------------------+-------+-------+-------+-------|
|    1068 |        28026.763 |  1029 |    20 |    19 |     0 |

Detailed:
| request_path                             |   count |   avg_bytes_sent |   2xx |   3xx |   4xx |   5xx |
|------------------------------------------+---------+------------------+-------+-------+-------+-------|
| /xxxxxxxxxx                              |     199 |        55150.402 |   199 |     0 |     0 |     0 |
| /xxxxxxxx/xxxxx                          |     167 |        47591.826 |   167 |     0 |     0 |     0 |
| /xxxxxxxxxxxxx/xxxxxx                    |      25 |         7432.200 |    25 |     0 |     0 |     0 |
| /xxxx/xxxxx/x/xxxxxxxxxxxxx/xxxxxxx      |      22 |          698.727 |    22 |     0 |     0 |     0 |
| /xxxx/xxxxx/x/xxxxxxxxxxxxx/xxxxxx       |      19 |         7431.632 |    19 |     0 |     0 |     0 |
| /xxxxx/xxxxx/                            |      18 |         7840.889 |    18 |     0 |     0 |     0 |
| /xxxxxxxx/xxxxxxxxxxxxxxxxx              |      15 |         7356.000 |    15 |     0 |     0 |     0 |
| /xxxxxxxxxxx/xxxxxxxx                    |      15 |         9978.800 |    15 |     0 |     0 |     0 |
| /xxxxx/                                  |      14 |            0.000 |     0 |    14 |     0 |     0 |
| /xxxxxxxxxx/xxxxxxxx/xxxxx               |      13 |        20530.154 |    13 |     0 |     0 |     0 |
"
https://github.com/PaddlePaddle/Paddle,PArallel Distributed Deep LEarning: Machine Learning Framework from Industrial Practice （『飞桨』核心框架，深度学习&机器学习高性能单机、分布式训练和跨平台部署）,"English |  | Welcome to the PaddlePaddle GitHub.PaddlePaddle, as the first independent R&D deep learning platform in China, has been officially open-sourced to professional communities since 2016. It is an industrial platform with advanced technologies and rich features that cover core deep learning frameworks, basic model libraries, end-to-end development kits, tools & components as well as service platforms.PaddlePaddle is originated from industrial practices with dedication and commitments to industrialization. It has been widely adopted by a wide range of sectors including manufacturing, agriculture, enterprise service, and so on while serving more than 8 million developers, 220,000 companies and generating 800,000 models. With such advantages, PaddlePaddle has helped an increasing number of partners commercialize AI.InstallationLatest PaddlePaddle Release: Our vision is to enable deep learning for everyone via PaddlePaddle.Please refer to our  to track the latest features of PaddlePaddle.Install Latest Stable Release:# CPU
pip install paddlepaddle
# GPU
pip install paddlepaddle-gpu

For more information about installation, please view Now our developers can acquire Tesla V100 online computing resources for free. If you create a program by AI Studio, you will obtain 8 hours to train models online per day. .FOUR LEADING TECHNOLOGIESDocumentationWe provide  and documentation.CommunicationCoursesCopyright and LicensePaddlePaddle is provided under the ."
https://github.com/qqwweee/keras-yolo3,A Keras implementation of YOLOv3 (Tensorflow backend),"keras-yolo3IntroductionA Keras implementation of YOLOv3 (Tensorflow backend) inspired by .Quick Startwget https://pjreddie.com/media/files/yolov3.weights
python convert.py yolov3.cfg yolov3.weights model_data/yolo.h5
python yolo_video.py [OPTIONS...] --image, for image detection mode, OR
python yolo_video.py [video_path] [output_path (optional)]
For Tiny YOLOv3, just do in a similar way, just specify model path and anchor path with  and .UsageUse --help to see usage of yolo_video.py:usage: yolo_video.py [-h] [--model MODEL] [--anchors ANCHORS]
                     [--classes CLASSES] [--gpu_num GPU_NUM] [--image]
                     [--input] [--output]

positional arguments:
  --input        Video input path
  --output       Video output path

optional arguments:
  -h, --help         show this help message and exit
  --model MODEL      path to model weight file, default model_data/yolo.h5
  --anchors ANCHORS  path to anchor definitions, default
                     model_data/yolo_anchors.txt
  --classes CLASSES  path to class definitions, default
                     model_data/coco_classes.txt
  --gpu_num GPU_NUM  Number of GPU to use, default 1
  --image            Image detection mode, will ignore all positional arguments
TrainingIf you want to use original pretrained weights for YOLOv3:1. 2. rename it as darknet53.weights3. 4. use model_data/darknet53_weights.h5 in train.pySome issues to know"
https://github.com/tebelorg/RPA-Python,Python package for doing RPA,"RPA for Python :snake:&ensp;•&ensp;&ensp;•&ensp;&ensp;•&ensp;&ensp;•&ensp;&ensp;•&ensp;&ensp;•&ensp;&ensp;•&ensp;&ensp;•&ensp;&ensp;•&ensp;&ensp;•&ensp;&ensp;•&ensp;&ensp;•&ensp;&ensp;•&ensp;&ensp;•&ensp;&ensp;•&ensp;&ensp;•&ensp;&ensp;•&ensp;To install this Python package for RPA (robotic process automation) -pip install rpa
To use it in Jupyter notebook, Python script or interactive shell -import rpa as r
Notes on operating systems and optional visual automation mode -Use CasesRPA for Python's simple and powerful API makes robotic process automation fun! You can use it to quickly automate away repetitive time-consuming tasks on websites, desktop applications, or the command line.As a bonus and token of my appreciation, any new bug reported will be appreciated with a US$200 gift card from your preferred merchant. Any feature suggestion accepted will be appreciated with a US$100 gift card.WEB AUTOMATION&ensp;:spider_web:r.init()
r.url('https://duckduckgo.com')
r.type('//*[@name=""q""]', 'decentralisation[enter]')
r.wait() # ensure results are fully loaded
r.snap('page', 'results.png')
r.close()
VISUAL AUTOMATION&ensp;:see_no_evil:r.init(visual_automation = True)
r.dclick('outlook_icon.png')
r.click('new_mail.png')
...
r.type('message_box.png', 'Hi Gillian,[enter]This is ...')
r.click('send_button.png')
r.close()
OCR AUTOMATION&ensp;🧿r.init(visual_automation = True, chrome_browser = False)
print(r.read('pdf_report_window.png'))
print(r.read('image_preview.png'))
r.hover('anchor_element.png')
print(r.read(r.mouse_x(), r.mouse_y(), r.mouse_x() + 400, r.mouse_y() + 200))
r.close()
KEYBOARD AUTOMATION&ensp;:musical_keyboard:r.init(visual_automation = True, chrome_browser = False)
r.keyboard('[cmd][space]')
r.keyboard('safari[enter]')
r.keyboard('[cmd]t')
r.keyboard('snatcher[enter]')
r.wait(2.5)
r.snap('page.png', 'results.png')
r.close()
MOUSE AUTOMATION&ensp;:mouse:r.init(visual_automation = True)
r.type(600, 300, 'neo kobe city')
r.click(900, 300)
r.snap('page.png', 'results.png')
r.hover('button_to_drag.png')
r.mouse('down')
r.hover(r.mouse_x() + 300, r.mouse_y())
r.mouse('up')
r.close()
TELEGRAM NOTIFICATION&ensp;:phone:r.telegram('1234567890', 'ID can be string or number, r.init() is not required')
r.telegram(1234567890, 'Hello World. Olá Mundo. नमस्ते दुनिया. 안녕하세요 세계. 世界,你好。')
r.telegram(1234567890, 'Use backslash n for new line\nThis is line 2 of the message')
SECURE TEMPORARY STORAGE&ensp;:package:bin_url = r.bin('secret_agent_report.pdf', 'optional password')
r.telegram(1234567890, 'Access confidential report at ' + bin_url)
API Reference&ensp;•&ensp;&ensp;•&ensp;&ensp;•&ensp;&ensp;•&ensp;&ensp;•&ensp;GENERAL NOTESSee , the , and . To send a Telegram app notification, simply  to allow receiving messages. To automate Chrome browser invisibly, use . To run 10X faster instead of normal human speed, use  (read the caveats!). Some CAPTCHAs can be solved using services like  or directly by . up to 100 MB with built-in temporary online storage, on a dedicated . You can even run RPA on your phone browser  (eg datascraping with up to 5 Colab sessions). By design this package has  and you can install, update and use it .Fully control error handling by  to raise Python exception on error, and manage with try-except. For fine-grained control on web browser file download location, use . For overriding default folder location to install and invoke TagUI (a  optimised for rpa package), use .If you are using non-English operating system and get ""invalid continuation byte"" error, you can set code page to support UTF-8 or change your Python script's encoding to your OS encoding. . Use focus() to make Windows/Mac application windows to be in focus (see here for ).ELEMENT IDENTIFIERSAn element identifier helps to tell RPA for Python exactly which element on the user interface you want to interact with. For example, //[@id='email'] is an XPath pointing to the webpage element having the id attribute 'email'.CORE FUNCTIONSFunction|Parameters|Purpose:-------|:---------|:------|,|start TagUI, auto-setup on first run||close TagUI, Chrome browser, SikuliX||for deploying package without internet||for updating package without internet| or |set to True to raise exception on error| or  or |print & log debug info to rpa_python.logBASIC FUNCTIONSFunction|Parameters|Purpose:-------|:---------|:------| (no parameter to return current URL)|go to web URL| (or x, y using visual automation)| left-click on element| (or x, y using visual automation)|right-click on element| (or x, y using visual automation)|double-click on element| (or x, y using visual automation)|move mouse to element| (or x, y),  (/)|enter text at element| (or x, y),  (or x, y)|choose dropdown option| ( is web page) (or x1, y1, x2, y2)|return element text| ( is web page), |save screenshot to file||return file content|, |save text to file|, |append text to file||ask & return user inputPRO FUNCTIONSFunction|Parameters|Purpose:-------|:---------|:------|,  (first look up @rpapybot)|send Telegram message| (using visual automation)|send keystrokes to screen| or  (using visual automation)|send mouse event to screen| (full name of app)|make application in focus| (default 5 seconds)|explicitly wait for some time| or , |save webpage table to CSV|,  (optional but recommended)|secure temporary storage| (CSS), |upload file to web element|,  (optional)|download from URL to file|,  (optional)|unzip zip file to specified location|,  (optional)|set web frame, frame() to reset| (no parameter to reset to main page, especially important when used to control another browser tab)|set context to web popup tab| (use ; between commands)|run OS command & return output| (JS code to run in browser)|run code in DOM & return output| (Python code for SikuliX)|run custom SikuliX commands| (blank returns current timeout)|change wait timeout (default 10s)keyboard() modifiers and special keys -HELPER FUNCTIONSFunction|Parameters|Purpose:-------|:---------|:------||True or False if element shows before timeout||return True or False if element is present now||return number of web elements as integer| or no parameter|put text or return clipboard text as string|,,,|return text between left & right markers|,|return text after deleting given characters||return '(x,y)' coordinates of mouse as string||return x coordinate of mouse as integer||return y coordinate of mouse as integer||return page title of current web page as string||return text content of current web page as string||return time elapsed in sec between calls as floatAbout & CreditsTagUI is a leading open-source RPA software :robot: with tens of thousands of users. It was created in 2016-2017 when I left DBS Bank as a test automation engineer, for a one-year sabbatical to Eastern Europe. Most of its code base was written in Novi Sad Serbia. In 2018, I joined AI Singapore to continue development of TagUI.Over a few months in 2019, I took on a daddy role full-time, taking care of my newborn baby girl and wife :cowboy_hat_face:🤱. In between nannying, I used my time pockets to create this Python package built on TagUI. I hope  would make life easier for Python users from different walks of life.I had been maintaining the package (and a  optimised for it) in my personal time. But , , , , , , , will be the new team maintaining this package. We're happy that tens of thousands of people use it :snake:For technical info, see its intuitive architecture below and ample comments in this .I would like to credit and express my appreciation to these amazing open-source contributors below :heart:LicenseRPA for Python is open-source software released under Apache 2.0 licenseOne Last Thing.. I rarely make product recommendations, other than the , and the open-source RPA tools I personally worked on. I'd like to recommend  available on phone and macOS.A mindmap is an intuitive way to store, organise and retrieve info, as it mimics how the mind works - relationships between different concepts and memories. It's perfect to make productive use of time pockets on the go.Below image is a Mindly example on benefits of coffee. I personally use it to map out my life for the next 13 years, reflect how to be a better husband, keep a list of traditional British foods, store supermarket member barcodes, as well as note-taking on the go. There's even a mindmap for my 3YO daughter to play with, she just enjoys dragging the nodes into the bin. So I created a dummy mindmap on standby that she can destroy.Best of all, the free version should meet the needs of most users. I have not exceeded the free limit of 100-node per mindmap, but I purchased it quite early on after using it, to support the work of the team behind this app.PS - I don't know Mindly's team, just recommending the app here because it rocks"
https://github.com/keras-team/autokeras,AutoML library for deep learning,"Official Website: AutoKeras: An AutoML system based on Keras.It is developed by DATA Lab at Texas A&M University.The goal of AutoKeras is to make machine learning accessible to everyone.Learning resourcesimport autokeras as ak

clf = ak.ImageClassifier()
clf.fit(x_train, y_train)
results = clf.predict(x_test)
InstallationTo install the package, please use the  installation as follows:pip3 install autokeras
Please follow the  for more details.Note: Currently, AutoKeras is only compatible with Python >= 3.7 and TensorFlow >= 2.8.0.CommunityAsk your questions on our .Contributing CodeHere is how we manage our project.We pick the critical issues to work on from .They will be added to this .Some of the issues will then be added to the ,which are used to plan for the releases.Refer to our  to learn the best practices.Thank all the contributors!Cite this workHaifeng Jin, François Chollet, Qingquan Song, and Xia Hu. ""AutoKeras: An AutoML Library for Deep Learning."" the Journal of machine Learning research 6 (2023): 1-6. ()Biblatex entry:@article{JMLR:v24:20-1355,
  author  = {Haifeng Jin and François Chollet and Qingquan Song and Xia Hu},
  title   = {AutoKeras: An AutoML Library for Deep Learning},
  journal = {Journal of Machine Learning Research},
  year    = {2023},
  volume  = {24},
  number  = {6},
  pages   = {1--6},
  url     = {http://jmlr.org/papers/v24/20-1355.html}
}
AcknowledgementsThe authors gratefully acknowledge the D3M program of the Defense Advanced Research Projects Agency (DARPA) administered through AFRL contract FA8750-17-2-0116; the Texas A&M College of Engineering, and Texas A&M University."
https://github.com/InsaneLife/ChineseNLPCorpus,中文自然语言处理数据集，平时做做实验的材料。欢迎补充提交合并。,"[TOC]ChineseNlpCorpus中文自然语言处理数据集，平时做做实验的材料。欢迎补充提交合并。阅读理解阅读理解数据集按照方法主要有：抽取式、分类（观点提取）。按照篇章又分为单篇章、多篇章，比如有的问题答案可能需要从多个文章中提取，每个文章可能都只是一部分，那么多篇章提取就会面临怎么合并，合并的时候怎么去掉重复的，保留补充的。| 名称     | 规模                        | 说明                         | 单位 | 论文                                                  | 下载                                                         | 评测                                                         || -------- | --------------------------- | ---------------------------- | ---- | ----------------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ || DuReader | 30万问题 140万文档 66万答案 | 问答阅读理解数据集 | 百度 |  |  |   || $DuReader_{robust}$ | 2.2万问题 | 单篇章、抽取式阅读理解数据集 | 百度 |   |  |  || CMRC 2018 | 2万问题 | 篇章片段抽取型阅读理解 | 哈工大讯飞联合实验室 |  |  |  || $DuReader_{yesno}$     | 9万                         | 观点型阅读理解数据集         | 百度                 |                                                       |  |  || $DuReader_{checklist}$ | 1万 | 抽取式数据集 | 百度 |                                                       |  |                                                              |任务型对话数据Medical DS复旦大学发布的基于百度拇指医生上真实对话数据的，面向任务型对话的中文医疗诊断数据集。| 名称       | 规模                       | 创建日期 | 作者       | 单位     | 论文                                                         | 下载                                                         || ---------- | -------------------------- | -------- | ---------- | -------- | ------------------------------------------------------------ | ------------------------------------------------------------ || Medical DS | 710个对话 67种症状 4种疾病 | 2018年   | Liu et al. | 复旦大学 |  |  |千言数据集包含知识对话、推荐对话、画像对话。详细见千言里面还有很多数据集，见:之前的一些对话数据集集中于语义理解，而工业界真实情况ASR也会有错误，往往被忽略。而是一个中文语音+NLU文本理解的对话数据集，可以从语音信号到理解端到端进行实验，例如直接从音素建模语言理解（而非word or token）。数据统计：官方说明手册：数据下载：NLPCC2018 Shared Task 4中文呢真实商用车载语音任务型对话系统的对话日志.| 名称                    | 规模               | 创建日期 | 作者        | 单位 | 论文                                                         | 下载                                                         | 评测                                                         || ----------------------- | ------------------ | -------- | ----------- | ---- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ || NLPCC2018 Shared Task 4 | 5800对话 2.6万问题 | 2018年   | zhao et al. | 腾讯 |  |   |  |NLPCC每年都会举办，包含大量中文数据集，如对话、qa、ner、情感检测、摘要等任务SMP这是一系类数据集，每年都会有新的数据集放出。SMP-2020-ECDT小样本对话语言理解数据集数据集介绍：数据集论文：https://arxiv.org/abs/2009.08138数据集下载地址：https://atmahou.github.io/attachments/FewJoint.zip小样本工具平台主页地址：https://github.com/AtmaHou/MetaDialogSMP-2019-NLU包含领域分类、意图识别和语义槽填充三项子任务的数据集。训练数据集下载：，目前只获取到训练集，如果有同学有测试集，欢迎提供。|        | Train || ------ | ----- || Domain | 24    || Intent | 29    || Slot   | 63    || Samples   | 2579  |SMP-2017中文对话意图识别数据集，官方git和数据: 数据集：|               | Train || ------------- | ----- || Train samples | 2299  || Dev samples   | 770   || Test samples  | 666   || Domain        | 31    |论文：文本分类新闻分类情感/观点/评论 倾向性分析| 数据集                  | 数据概览                                                     | 下载                                                         || ----------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ || ChnSentiCorp_htl_all    | 7000 多条酒店评论数据，5000 多条正向评论，2000 多条负向评论  |  || waimai_10k              | 某外卖平台收集的用户评价，正向 4000 条，负向 约 8000 条      |  || online_shopping_10_cats | 10 个类别，共 6 万多条评论数据，正、负向评论各约 3 万条， 包括书籍、平板、手机、水果、洗发水、热水器、蒙牛、衣服、计算机、酒店 |  || weibo_senti_100k        | 10 万多条，带情感标注 新浪微博，正负向评论约各 5 万条        |  || simplifyweibo_4_moods   | 36 万多条，带情感标注 新浪微博，包含 4 种情感， 其中喜悦约 20 万条，愤怒、厌恶、低落各约 5 万条 |  || dmsc_v2                 | 28 部电影，超 70 万 用户，超 200 万条 评分/评论 数据         |  || yf_dianping             | 24 万家餐馆，54 万用户，440 万条评论/评分数据                |  || yf_amazon               | 52 万件商品，1100 多个类目，142 万用户，720 万条评论/评分数据 |  || 百度千言情感分析数据集  | 包括句子级情感分类（Sentence-level Sentiment Classification）、评价对象级情感分类（Aspect-level Sentiment Classification）、观点抽取（Opinion Target Extraction） |  |实体识别&词性标注&分词另外这三个链接里面数据集也挺全的，链接：句法&语义解析语义解析| 数据集  | 单/多表 | 语言 | 复杂度 | 数据库/表格 | 训练集 | 验证集 | 测试集 | 文档                                                         || :-----: | :-----: | :--: | :----: | :---------: | :----: | :----: | :----: | ------------------------------------------------------------ || NL2SQL  |   单    | 中文 |  简单  | 5,291/5,291 | 41,522 | 4,396  | 8,141  |                    || CSpider |   多    | 中英 |  复杂  |   166/876   | 6,831  |  954   | 1,906  |                   ||  DuSQL  |   多    | 中文 |  复杂  |   200/813   | 22,521 | 2,482  | 3,759  |  |信息抽取搜索匹配千言文本相似度百度千言文本相似度，主要包含LCQMC/BQ Corpus/PAWS-X，见，丰富文本匹配的数据，可以作为目标匹配数据集的源域数据，进行多任务学习/迁移学习。OPPO手机搜索排序OPPO手机搜索排序query-title语义匹配数据集。链接:https://pan.baidu.com/s/1Hg2Hubsn3GEuu4gubbHCzw 提取码:7p3n网页搜索结果评价(SogouE)推荐系统| 数据集      | 数据概览                                                     | 下载地址                                                     || ----------- | ------------------------------------------------------------ | ------------------------------------------------------------ || ez_douban   | 5 万多部电影（3 万多有电影名称，2 万多没有电影名称），2.8 万 用户，280 万条评分数据 |  || dmsc_v2     | 28 部电影，超 70 万 用户，超 200 万条 评分/评论 数据         |  || yf_dianping | 24 万家餐馆，54 万用户，440 万条评论/评分数据                |  || yf_amazon   | 52 万件商品，1100 多个类目，142 万用户，720 万条评论/评分数据 |  |百科数据维基百科维基百科会定时将语料库打包发布：百度百科只能自己爬，爬取得链接：提取码 neqs 。 指代消歧CoNLL 2012 ： 预训练：（词向量or模型）BERTBERT变种模型：| 模型                                                         | 参数 | git                                                          || ------------------------------------------------------------ | ---- | ------------------------------------------------------------ ||  | 108M |               ||  | 108M |  ||  | 38M  |  ||  | 108M | 、ernie模型转成tensorflow模型: ||  | 334M |            ||  | 209M |  ||  | 59M  |     ||  |      |     ||  | 4M   |     ||  | 108M |  ||  | 330M |  |ELMO腾讯词向量腾讯AI实验室公开的中文词向量数据集包含800多万中文词汇，其中每个词对应一个200维的向量。下载地址：上百种预训练中文词向量中文完形填空数据集中华古诗词数据库最全中华古诗词数据集，唐宋两朝近一万四千古诗人, 接近5.5万首唐诗加26万宋诗. 两宋时期1564位词人，21050首词。保险行业语料库汉语拆字字典英文可以做char embedding，中文不妨可以试试拆字中文数据集平台NLP工具THULAC：  ：包括中文分词、词性标注功能。HanLP： 哈工大LTP  NLPIR jieba  百度千言数据集："
https://github.com/pwr-Solaar/Solaar,Linux device manager for Logitech devices,"title: Solaarlayout: defaultSolaar is a Linux manager for many Logitech keyboards, mice, and trackpadsthat connect wirelessly to a USB , Bolt, Lightspeed, or Nano receiver;connect directly via a USB cable; or connect via Bluetooth.Solaar does not work with peripherals from other companies.Documentation here is for the current version of Solaar.Some Linux distributions distribute old versions of Solaar.If you are using an old version and something described here does not work you should upgradeusing one of the methods described below.Solaar runs as a regular user process, albeit with direct access to the Linux interfacethat lets it directly communicate with the Logitech devices it manages using specialLogitech-proprietary (HID++) commands.Each Logitech device implements a different subset of these commands.Solaar is thus only able to make the changes to devices that devices implement.Solaar is not a device driver and does not process normal input from devices.It is thus unable to fix problems that arise from incorrect handling ofmouse movements or keycodes by Linux drivers or other software.Solaar can be used as a GUI application, the usual case, or via its command-line interface.The Solaar GUI is meant to run continuously in the background,monitoring devices, making changes to them, and responding to some messages they emit.To this end, it is useful to have Solaar start at user login so thatchanges made to devices by Solaar are applied at login and throughout the user's session.Both Solaar interfaces are able to list the connected devices andshow information about each device, often including battery status.Solaar is able to pair and unpair devices withreceivers as supported by the device and receiver.Solaar can also control some changeable settings of devices,such as scroll wheel direction and function key behavior.Solaar keeps track of most of these settings on a per-computer basis,because devices forget most settings when powered down,and the GUI application restores them whenever a device connects.For more information on how to use Solaar see,and for more information on its capabilities see.Solaar's GUI normally uses an icon in the system tray and starts with its main window visible.This aspect of Solaar depends on having an active system tray, which is not the defaultsituation for recent versions of Gnome.  For information on to set up a system tray under Gnome see.Solaar's GUI can be started in several waysFor more information on Solaar's command-line interface use the help option,as in .Solaar has progressed past version 1.1. Problems with earlier versions shouldnot be reported as bugs. Instead, upgrade to a recent version or manually installthe current version from .Some capabilities of Solaar have been developed by observing the behavior ofLogitech receivers and devices and generalizing from these observations.If your Logitech receiver or device behaves strangely this may be caused byan incorrect behavior generalization.Please report such experiences by creating an issue in.Supported DevicesSolaar will detect all devices paired with supported Unifying, Bolt, Lightspeed, or Nanoreceivers, and at the very least display some basic information about them.Solaar will detect some Logitech devices that connect via a USB cable or Bluetooth.Solaar can pair and unpair a Logitech device showing the Unifying logo(Solaar's version of the )with any Unifying receiver,and pair and unpair a Logitech device showing the Bolt logowith any Bolt receiver,andcan pair and unpair Lightspeed devices with Lightspeed receivers for the same model.Solaar can pair some Logitech devices with Logitech Nano receivers, but not all Logitechdevices can be paired with Nano receivers.Logitech devices without a Unifying or Bolt logogenerally cannot be paired with Unifying or Bolt receivers.Solaar does not handle connecting or disconnecting via Bluetooth,which is done using the usual Bluetooth mechanisms.For a partial list of supported devicesand their features, see .Prebuilt packagesUp-to-date prebuilt packages are available for some Linux distros(e.g., Fedora 33+) in their standard repositories.If a recent version of Solaar is notavailable from the standard repositories for your distribution, you can tryone of these packages.Solaar is available from some other repositoriesbut they are several versions behind the current version.Solaar uses a standard system tray implementation; solaar-gnome3 is no longer required for Gnome or Unity integration.Manual installationSee for the step-by-step procedure for manual installation.Known IssuesContributing to SolaarContributions to Solaar are very welcome.Solaar has complete or partial translations of its GUI strings in several languages.If you want to update a translation or add a new one see  for more information.If you find a bug, please check first if it has already been reported. If yes, please add additional information you may have to the existing issue. If not, please open a new bug report issue. If you can provide a fix for it, please also open a GitHub pull request. Label your commits using the naming conventions in recent commits to Solaar.If you want to add a new feature to Solaar, feel free to open a feature request issue to discuss your proposal.There are also usually several open issues for enhancements that have already been requested.LicenseThis software is distributed under the terms of the.ThanksThis project began as a third-hand clone of 'slogitech-solar-k750 project on GitHub (no longer available). It was developedfurther thanks to the diggings in Logitech's HID++ protocol done by many otherpeople:Also, thanks to Douglas Wagner, Julien Gascard, and Peter Wu for helping withapplication testing and supporting new devices."
https://github.com/tqdm/tqdm,":zap: A Fast, Extensible Progress Bar for Python and CLI","|Logo|tqdm|Py-Versions| |Versions| |Conda-Forge-Status| |Docker| |Snapcraft||Build-Status| |Coverage-Status| |Branch-Coverage-Status| |Codacy-Grade| |Libraries-Rank| |PyPI-Downloads||LICENCE| |OpenHub-Status| |binder-demo| |awesome-python| derives from the Arabic word taqaddum (تقدّم) which can mean ""progress,""and is an abbreviation for ""I love you so much"" in Spanish (te quiero demasiado).Instantly make your loops show a smart progress meter - just wrap anyiterable with , and you're done!.. code:: pythonfrom tqdm import tqdm
for i in tqdm(range(10000)):
    ...
 can be also used as a convenient shortcut for.|Screenshot||Video| |Slides| |Merch|It can also be executed as a module with pipes:.. code:: sh$ seq 9999999 | tqdm --bytes | wc -l
75.2MB [00:00, 217MB/s]
9999999

$ tar -zcf - docs/ | tqdm --bytes --total `du -sb docs/ | cut -f1` \
    > backup.tgz
 32%|██████████▍                      | 8.89G/27.9G [00:42<01:31, 223MB/s]
Overhead is low -- about 60ns per iteration (80ns with ), and isunit tested against performance regression.By comparison, the well-established__ hasan 800ns/iter overhead.In addition to its low overhead,  uses smart algorithms to predictthe remaining time and to skip unnecessary iteration displays, which allowsfor a negligible overhead in most cases. works on any platform(Linux, Windows, Mac, FreeBSD, NetBSD, Solaris/SunOS),in any console or in a GUI, and is also friendly with IPython/Jupyter notebooks. does not require any dependencies (not even !), justPython and an environment supporting  and control characters... contents:: Table of contents:backlinks: top:local:InstallationLatest PyPI stable release
|Versions| |PyPI-Downloads| |Libraries-Dependents|

.. code:: sh

    pip install tqdm

Latest development release on GitHub
|GitHub-Status| |GitHub-Stars| |GitHub-Commits| |GitHub-Forks| |GitHub-Updated|Pull and install pre-release  branch:.. code:: shpip install ""git+https://github.com/tqdm/tqdm.git@devel#egg=tqdm""
Latest Conda release
|Conda-Forge-Status|

.. code:: sh

    conda install -c conda-forge tqdm

Latest Snapcraft release
|Snapcraft|There are 3 channels to choose from:.. code:: shsnap install tqdm  # implies --stable, i.e. latest tagged release
snap install tqdm  --candidate  # master branch
snap install tqdm  --edge  # devel branch
Note that  binaries are purely for CLI use (not -able), andautomatically set up  tab-completion.Latest Docker release
|Docker|

.. code:: sh

    docker pull tqdm/tqdm
    docker run -i --rm tqdm/tqdm --help

Other
~~~~~

There are other (unofficial) places where ``tqdm`` may be downloaded, particularly for CLI use:

|Repology|

.. |Repology| image:: https://repology.org/badge/tiny-repos/python:tqdm.svg
   :target: https://repology.org/project/python:tqdm/versions

Changelog
---------

The list of all changes is available either on GitHub's Releases:
|GitHub-Status|, on the
`wiki <https://github.com/tqdm/tqdm/wiki/Releases>`__, or on the
`website <https://tqdm.github.io/releases>`__.


Usage
-----

``tqdm`` is very versatile and can be used in a number of ways.
The three main ones are given below.

Iterable-based
~~~~~~~~~~~~~~

Wrap ``tqdm()`` around any iterable:

.. code:: python

    from tqdm import tqdm
    from time import sleep

    text = """"
    for char in tqdm([""a"", ""b"", ""c"", ""d""]):
        sleep(0.25)
        text = text + char

``trange(i)`` is a special optimised instance of ``tqdm(range(i))``:

.. code:: python

    from tqdm import trange

    for i in trange(100):
        sleep(0.01)

Instantiation outside of the loop allows for manual control over ``tqdm()``:

.. code:: python

    pbar = tqdm([""a"", ""b"", ""c"", ""d""])
    for char in pbar:
        sleep(0.25)
        pbar.set_description(""Processing %s"" % char)

Manual
~~~~~~

Manual control of ``tqdm()`` updates using a ``with`` statement:

.. code:: python

    with tqdm(total=100) as pbar:
        for i in range(10):
            sleep(0.1)
            pbar.update(10)

If the optional variable ``total`` (or an iterable with ``len()``) is
provided, predictive stats are displayed.

``with`` is also optional (you can just assign ``tqdm()`` to a variable,
but in this case don't forget to ``del`` or ``close()`` at the end:

.. code:: python

    pbar = tqdm(total=100)
    for i in range(10):
        sleep(0.1)
        pbar.update(10)
    pbar.close()

Module
~~~~~~

Perhaps the most wonderful use of ``tqdm`` is in a script or on the command
line. Simply inserting ``tqdm`` (or ``python -m tqdm``) between pipes will pass
through all ``stdin`` to ``stdout`` while printing progress to ``stderr``.

The example below demonstrate counting the number of lines in all Python files
in the current directory, with timing information included.

.. code:: sh

    $ time find . -name '*.py' -type f -exec cat \{} \; | wc -l
    857365

    real    0m3.458s
    user    0m0.274s
    sys     0m3.325s

    $ time find . -name '*.py' -type f -exec cat \{} \; | tqdm | wc -l
    857366it [00:03, 246471.31it/s]
    857365

    real    0m3.585s
    user    0m0.862s
    sys     0m3.358s

Note that the usual arguments for ``tqdm`` can also be specified.

.. code:: sh

    $ find . -name '*.py' -type f -exec cat \{} \; |
        tqdm --unit loc --unit_scale --total 857366 >> /dev/null
    100%|█████████████████████████████████| 857K/857K [00:04<00:00, 246Kloc/s]

Backing up a large directory?

.. code:: sh

    $ tar -zcf - docs/ | tqdm --bytes --total `du -sb docs/ | cut -f1` \
      > backup.tgz
     44%|██████████████▊                   | 153M/352M [00:14<00:18, 11.0MB/s]

This can be beautified further:

.. code:: sh

    $ BYTES=$(du -sb docs/ | cut -f1)
    $ tar -cf - docs/ \
      | tqdm --bytes --total ""$BYTES"" --desc Processing | gzip \
      | tqdm --bytes --total ""$BYTES"" --desc Compressed --position 1 \
      > ~/backup.tgz
    Processing: 100%|██████████████████████| 352M/352M [00:14<00:00, 30.2MB/s]
    Compressed:  42%|█████████▎            | 148M/352M [00:14<00:19, 10.9MB/s]

Or done on a file level using 7-zip:

.. code:: sh

    $ 7z a -bd -r backup.7z docs/ | grep Compressing \
      | tqdm --total $(find docs/ -type f | wc -l) --unit files \
      | grep -v Compressing
    100%|██████████████████████████▉| 15327/15327 [01:00<00:00, 712.96files/s]

Pre-existing CLI programs already outputting basic progress information will
benefit from ``tqdm``'s ``--update`` and ``--update_to`` flags:

.. code:: sh

    $ seq 3 0.1 5 | tqdm --total 5 --update_to --null
    100%|████████████████████████████████████| 5.0/5 [00:00<00:00, 9673.21it/s]
    $ seq 10 | tqdm --update --null  # 1 + 2 + ... + 10 = 55 iterations
    55it [00:00, 90006.52it/s]

FAQ and Known Issues
--------------------

|GitHub-Issues|

The most common issues relate to excessive output on multiple lines, instead
of a neat one-line progress bar.

- Consoles in general: require support for carriage return (``CR``, ``\r``).

  * Some cloud logging consoles which don't support ``\r`` properly
    (`cloudwatch <https://github.com/tqdm/tqdm/issues/966>`__,
    `K8s <https://github.com/tqdm/tqdm/issues/1319>`__) may benefit from
    ``export TQDM_POSITION=-1``.

- Nested progress bars:

  * Consoles in general: require support for moving cursors up to the
    previous line. For example,
    `IDLE <https://github.com/tqdm/tqdm/issues/191#issuecomment-230168030>`__,
    `ConEmu <https://github.com/tqdm/tqdm/issues/254>`__ and
    `PyCharm <https://github.com/tqdm/tqdm/issues/203>`__ (also
    `here <https://github.com/tqdm/tqdm/issues/208>`__,
    `here <https://github.com/tqdm/tqdm/issues/307>`__, and
    `here <https://github.com/tqdm/tqdm/issues/454#issuecomment-335416815>`__)
    lack full support.
  * Windows: additionally may require the Python module ``colorama``
    to ensure nested bars stay within their respective lines.

- Unicode:

  * Environments which report that they support unicode will have solid smooth
    progressbars. The fallback is an ``ascii``-only bar.
  * Windows consoles often only partially support unicode and thus
    `often require explicit ascii=True <https://github.com/tqdm/tqdm/issues/454#issuecomment-335416815>`__
    (also `here <https://github.com/tqdm/tqdm/issues/499>`__). This is due to
    either normal-width unicode characters being incorrectly displayed as
    ""wide"", or some unicode characters not rendering.

- Wrapping generators:

  * Generator wrapper functions tend to hide the length of iterables.
    ``tqdm`` does not.
  * Replace ``tqdm(enumerate(...))`` with ``enumerate(tqdm(...))`` or
    ``tqdm(enumerate(x), total=len(x), ...)``.
    The same applies to ``numpy.ndenumerate``.
  * Replace ``tqdm(zip(a, b))`` with ``zip(tqdm(a), b)`` or even
    ``zip(tqdm(a), tqdm(b))``.
  * The same applies to ``itertools``.
  * Some useful convenience functions can be found under ``tqdm.contrib``.

- `No intermediate output in docker-compose <https://github.com/tqdm/tqdm/issues/771>`__:
  use ``docker-compose run`` instead of ``docker-compose up`` and ``tty: true``.

- Overriding defaults via environment variables:
  e.g. in CI/cloud jobs, ``export TQDM_MININTERVAL=5`` to avoid log spam.
  This override logic is handled by the ``tqdm.utils.envwrap`` decorator
  (useful independent of ``tqdm``).

If you come across any other difficulties, browse and file |GitHub-Issues|.

Documentation
-------------

|Py-Versions| |README-Hits| (Since 19 May 2016)

.. code:: python

    class tqdm():
      """"""
      Decorate an iterable object, returning an iterator which acts exactly
      like the original iterable, but prints a dynamically updating
      progressbar every time a value is requested.
      """"""

      @envwrap(""TQDM_"")  # override defaults via env vars
      def __init__(self, iterable=None, desc=None, total=None, leave=True,
                   file=None, ncols=None, mininterval=0.1,
                   maxinterval=10.0, miniters=None, ascii=None, disable=False,
                   unit='it', unit_scale=False, dynamic_ncols=False,
                   smoothing=0.3, bar_format=None, initial=0, position=None,
                   postfix=None, unit_divisor=1000, write_bytes=False,
                   lock_args=None, nrows=None, colour=None, delay=0):

Parameters
~~~~~~~~~~

* iterable  : iterable, optional  
    Iterable to decorate with a progressbar.
    Leave blank to manually manage the updates.
* desc  : str, optional  
    Prefix for the progressbar.
* total  : int or float, optional  
    The number of expected iterations. If unspecified,
    len(iterable) is used if possible. If float(""inf"") or as a last
    resort, only basic progress statistics are displayed
    (no ETA, no progressbar).
    If ``gui`` is True and this parameter needs subsequent updating,
    specify an initial arbitrary large positive number,
    e.g. 9e9.
* leave  : bool, optional  
    If [default: True], keeps all traces of the progressbar
    upon termination of iteration.
    If ``None``, will leave only if ``position`` is ``0``.
* file  : ``io.TextIOWrapper`` or ``io.StringIO``, optional  
    Specifies where to output the progress messages
    (default: sys.stderr). Uses ``file.write(str)`` and ``file.flush()``
    methods.  For encoding, see ``write_bytes``.
* ncols  : int, optional  
    The width of the entire output message. If specified,
    dynamically resizes the progressbar to stay within this bound.
    If unspecified, attempts to use environment width. The
    fallback is a meter width of 10 and no limit for the counter and
    statistics. If 0, will not print any meter (only stats).
* mininterval  : float, optional  
    Minimum progress display update interval [default: 0.1] seconds.
* maxinterval  : float, optional  
    Maximum progress display update interval [default: 10] seconds.
    Automatically adjusts ``miniters`` to correspond to ``mininterval``
    after long display update lag. Only works if ``dynamic_miniters``
    or monitor thread is enabled.
* miniters  : int or float, optional  
    Minimum progress display update interval, in iterations.
    If 0 and ``dynamic_miniters``, will automatically adjust to equal
    ``mininterval`` (more CPU efficient, good for tight loops).
    If > 0, will skip display of specified number of iterations.
    Tweak this and ``mininterval`` to get very efficient loops.
    If your progress is erratic with both fast and slow iterations
    (network, skipping items, etc) you should set miniters=1.
* ascii  : bool or str, optional  
    If unspecified or False, use unicode (smooth blocks) to fill
    the meter. The fallback is to use ASCII characters "" 123456789#"".
* disable  : bool, optional  
    Whether to disable the entire progressbar wrapper
    [default: False]. If set to None, disable on non-TTY.
* unit  : str, optional  
    String that will be used to define the unit of each iteration
    [default: it].
* unit_scale  : bool or int or float, optional  
    If 1 or True, the number of iterations will be reduced/scaled
    automatically and a metric prefix following the
    International System of Units standard will be added
    (kilo, mega, etc.) [default: False]. If any other non-zero
    number, will scale ``total`` and ``n``.
* dynamic_ncols  : bool, optional  
    If set, constantly alters ``ncols`` and ``nrows`` to the
    environment (allowing for window resizes) [default: False].
* smoothing  : float, optional  
    Exponential moving average smoothing factor for speed estimates
    (ignored in GUI mode). Ranges from 0 (average speed) to 1
    (current/instantaneous speed) [default: 0.3].
* bar_format  : str, optional  
    Specify a custom bar string formatting. May impact performance.
    [default: '{l_bar}{bar}{r_bar}'], where
    l_bar='{desc}: {percentage:3.0f}%|' and
    r_bar='| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, '
    '{rate_fmt}{postfix}]'
    Possible vars: l_bar, bar, r_bar, n, n_fmt, total, total_fmt,
    percentage, elapsed, elapsed_s, ncols, nrows, desc, unit,
    rate, rate_fmt, rate_noinv, rate_noinv_fmt,
    rate_inv, rate_inv_fmt, postfix, unit_divisor,
    remaining, remaining_s, eta.
    Note that a trailing "": "" is automatically removed after {desc}
    if the latter is empty.
* initial  : int or float, optional  
    The initial counter value. Useful when restarting a progress
    bar [default: 0]. If using float, consider specifying ``{n:.3f}``
    or similar in ``bar_format``, or specifying ``unit_scale``.
* position  : int, optional  
    Specify the line offset to print this bar (starting from 0)
    Automatic if unspecified.
    Useful to manage multiple bars at once (eg, from threads).
* postfix  : dict or ``*``, optional  
    Specify additional stats to display at the end of the bar.
    Calls ``set_postfix(**postfix)`` if possible (dict).
* unit_divisor  : float, optional  
    [default: 1000], ignored unless ``unit_scale`` is True.
* write_bytes  : bool, optional  
    Whether to write bytes. If (default: False) will write unicode.
* lock_args  : tuple, optional  
    Passed to ``refresh`` for intermediate output
    (initialisation, iterating, and updating).
* nrows  : int, optional  
    The screen height. If specified, hides nested bars outside this
    bound. If unspecified, attempts to use environment height.
    The fallback is 20.
* colour  : str, optional  
    Bar colour (e.g. 'green', '#00ff00').
* delay  : float, optional  
    Don't display until [default: 0] seconds have elapsed.

Extra CLI Options
~~~~~~~~~~~~~~~~~

* delim  : chr, optional  
    Delimiting character [default: '\n']. Use '\0' for null.
    N.B.: on Windows systems, Python converts '\n' to '\r\n'.
* buf_size  : int, optional  
    String buffer size in bytes [default: 256]
    used when ``delim`` is specified.
* bytes  : bool, optional  
    If true, will count bytes, ignore ``delim``, and default
    ``unit_scale`` to True, ``unit_divisor`` to 1024, and ``unit`` to 'B'.
* tee  : bool, optional  
    If true, passes ``stdin`` to both ``stderr`` and ``stdout``.
* update  : bool, optional  
    If true, will treat input as newly elapsed iterations,
    i.e. numbers to pass to ``update()``. Note that this is slow
    (~2e5 it/s) since every input must be decoded as a number.
* update_to  : bool, optional  
    If true, will treat input as total elapsed iterations,
    i.e. numbers to assign to ``self.n``. Note that this is slow
    (~2e5 it/s) since every input must be decoded as a number.
* null  : bool, optional  
    If true, will discard input (no stdout).
* manpath  : str, optional  
    Directory in which to install tqdm man pages.
* comppath  : str, optional  
    Directory in which to place tqdm completion.
* log  : str, optional  
    CRITICAL|FATAL|ERROR|WARN(ING)|[default: 'INFO']|DEBUG|NOTSET.

Returns
~~~~~~~

* out  : decorated iterator.  

.. code:: python

    class tqdm():
      def update(self, n=1):
          """"""
          Manually update the progress bar, useful for streams
          such as reading files.
          E.g.:
          >>> t = tqdm(total=filesize) # Initialise
          >>> for current_buffer in stream:
          ...    ...
          ...    t.update(len(current_buffer))
          >>> t.close()
          The last line is highly recommended, but possibly not necessary if
          ``t.update()`` will be called in such a way that ``filesize`` will be
          exactly reached and printed.

          Parameters
          ----------
          n  : int or float, optional
              Increment to add to the internal counter of iterations
              [default: 1]. If using float, consider specifying ``{n:.3f}``
              or similar in ``bar_format``, or specifying ``unit_scale``.

          Returns
          -------
          out  : bool or None
              True if a ``display()`` was triggered.
          """"""

      def close(self):
          """"""Cleanup and (if leave=False) close the progressbar.""""""

      def clear(self, nomove=False):
          """"""Clear current bar display.""""""

      def refresh(self):
          """"""
          Force refresh the display of this bar.

          Parameters
          ----------
          nolock  : bool, optional
              If ``True``, does not lock.
              If [default: ``False``]: calls ``acquire()`` on internal lock.
          lock_args  : tuple, optional
              Passed to internal lock's ``acquire()``.
              If specified, will only ``display()`` if ``acquire()`` returns ``True``.
          """"""

      def unpause(self):
          """"""Restart tqdm timer from last print time.""""""

      def reset(self, total=None):
          """"""
          Resets to 0 iterations for repeated use.

          Consider combining with ``leave=True``.

          Parameters
          ----------
          total  : int or float, optional. Total to use for the new bar.
          """"""

      def set_description(self, desc=None, refresh=True):
          """"""
          Set/modify description of the progress bar.

          Parameters
          ----------
          desc  : str, optional
          refresh  : bool, optional
              Forces refresh [default: True].
          """"""

      def set_postfix(self, ordered_dict=None, refresh=True, **tqdm_kwargs):
          """"""
          Set/modify postfix (additional stats)
          with automatic formatting based on datatype.

          Parameters
          ----------
          ordered_dict  : dict or OrderedDict, optional
          refresh  : bool, optional
              Forces refresh [default: True].
          kwargs  : dict, optional
          """"""

      @classmethod
      def write(cls, s, file=sys.stdout, end=""\n""):
          """"""Print a message via tqdm (without overlap with bars).""""""

      @property
      def format_dict(self):
          """"""Public API for read-only member access.""""""

      def display(self, msg=None, pos=None):
          """"""
          Use ``self.sp`` to display ``msg`` in the specified ``pos``.

          Consider overloading this function when inheriting to use e.g.:
          ``self.some_frontend(**self.format_dict)`` instead of ``self.sp``.

          Parameters
          ----------
          msg  : str, optional. What to display (default: ``repr(self)``).
          pos  : int, optional. Position to ``moveto``
            (default: ``abs(self.pos)``).
          """"""

      @classmethod
      @contextmanager
      def wrapattr(cls, stream, method, total=None, bytes=True, **tqdm_kwargs):
          """"""
          stream  : file-like object.
          method  : str, ""read"" or ""write"". The result of ``read()`` and
              the first argument of ``write()`` should have a ``len()``.

          >>> with tqdm.wrapattr(file_obj, ""read"", total=file_obj.size) as fobj:
          ...     while True:
          ...         chunk = fobj.read(chunk_size)
          ...         if not chunk:
          ...             break
          """"""

      @classmethod
      def pandas(cls, *targs, **tqdm_kwargs):
          """"""Registers the current `tqdm` class with `pandas`.""""""

    def trange(*args, **tqdm_kwargs):
        """"""Shortcut for `tqdm(range(*args), **tqdm_kwargs)`.""""""

Convenience Functions
.. code:: pythondef tqdm.contrib.tenumerate(iterable, start=0, total=None,
                            tqdm_class=tqdm.auto.tqdm, **tqdm_kwargs):
    """"""Equivalent of `numpy.ndenumerate` or builtin `enumerate`.""""""

def tqdm.contrib.tzip(iter1, *iter2plus, **tqdm_kwargs):
    """"""Equivalent of builtin `zip`.""""""

def tqdm.contrib.tmap(function, *sequences, **tqdm_kwargs):
    """"""Equivalent of builtin `map`.""""""
Submodules
.. code:: python

    class tqdm.notebook.tqdm(tqdm.tqdm):
        """"""IPython/Jupyter Notebook widget.""""""

    class tqdm.auto.tqdm(tqdm.tqdm):
        """"""Automatically chooses beween `tqdm.notebook` and `tqdm.tqdm`.""""""

    class tqdm.asyncio.tqdm(tqdm.tqdm):
      """"""Asynchronous version.""""""
      @classmethod
      def as_completed(cls, fs, *, loop=None, timeout=None, total=None,
                       **tqdm_kwargs):
          """"""Wrapper for `asyncio.as_completed`.""""""

    class tqdm.gui.tqdm(tqdm.tqdm):
        """"""Matplotlib GUI version.""""""

    class tqdm.tk.tqdm(tqdm.tqdm):
        """"""Tkinter GUI version.""""""

    class tqdm.rich.tqdm(tqdm.tqdm):
        """"""`rich.progress` version.""""""

    class tqdm.keras.TqdmCallback(keras.callbacks.Callback):
        """"""Keras callback for epoch and batch progress.""""""

    class tqdm.dask.TqdmCallback(dask.callbacks.Callback):
        """"""Dask callback for task progress.""""""


``contrib``
+++++++++++

The ``tqdm.contrib`` package also contains experimental modules:

- ``tqdm.contrib.itertools``: Thin wrappers around ``itertools``
- ``tqdm.contrib.concurrent``: Thin wrappers around ``concurrent.futures``
- ``tqdm.contrib.slack``: Posts to `Slack <https://slack.com>`__ bots
- ``tqdm.contrib.discord``: Posts to `Discord <https://discord.com>`__ bots
- ``tqdm.contrib.telegram``: Posts to `Telegram <https://telegram.org>`__ bots
- ``tqdm.contrib.bells``: Automagically enables all optional features

  * ``auto``, ``pandas``, ``slack``, ``discord``, ``telegram``

Examples and Advanced Usage
---------------------------

- See the `examples <https://github.com/tqdm/tqdm/tree/master/examples>`__
  folder;
- import the module and run ``help()``;
- consult the `wiki <https://github.com/tqdm/tqdm/wiki>`__;

  * this has an
    `excellent article <https://github.com/tqdm/tqdm/wiki/How-to-make-a-great-Progress-Bar>`__
    on how to make a **great** progressbar;

- check out the `slides from PyData London <https://tqdm.github.io/PyData2019/slides.html>`__, or
- run the |binder-demo|.

Description and additional stats
Custom information can be displayed and updated dynamically on  barswith the  and  arguments:.. code:: pythonfrom tqdm import tqdm, trange
from random import random, randint
from time import sleep

with trange(10) as t:
    for i in t:
        # Description will be displayed on the left
        t.set_description('GEN %i' % i)
        # Postfix will be displayed on the right,
        # formatted automatically based on argument's datatype
        t.set_postfix(loss=random(), gen=randint(1,999), str='h',
                      lst=[1, 2])
        sleep(0.1)

with tqdm(total=10, bar_format=""{postfix[0]} {postfix[1][value]:>8.2g}"",
          postfix=[""Batch"", {""value"": 0}]) as t:
    for i in range(10):
        sleep(0.1)
        t.postfix[1][""value""] = i / 2
        t.update()
Points to remember when using  in the  string:Additional  parameters may also be defined by overriding, and the bar itself may be modified using :.. code:: pythonfrom tqdm import tqdm
class TqdmExtraFormat(tqdm):
    """"""Provides a `total_time` format parameter""""""
    @property
    def format_dict(self):
        d = super(TqdmExtraFormat, self).format_dict
        total_time = d[""elapsed""] * (d[""total""] or 0) / max(d[""n""], 1)
        d.update(total_time=self.format_interval(total_time) + "" in total"")
        return d

for i in TqdmExtraFormat(
      range(9), ascii="" .oO0"",
      bar_format=""{total_time}: {percentage:.0f}%|{bar}{r_bar}""):
    if i == 4:
        break
.. code::00:00 in total: 44%|0000.     | 4/9 [00:00<00:00, 962.93it/s]
Note that  also supports a format specifier .This means a fixed bar with right-justified text may be created by using:Nested progress bars
``tqdm`` supports nested progress bars. Here's an example:

.. code:: python

    from tqdm.auto import trange
    from time import sleep

    for i in trange(4, desc='1st loop'):
        for j in trange(5, desc='2nd loop'):
            for k in trange(50, desc='3rd loop', leave=False):
                sleep(0.01)

For manual control over positioning (e.g. for multi-processing use),
you may specify ``position=n`` where ``n=0`` for the outermost bar,
``n=1`` for the next, and so on.
However, it's best to check if ``tqdm`` can work without manual ``position``
first.

.. code:: python

    from time import sleep
    from tqdm import trange, tqdm
    from multiprocessing import Pool, RLock, freeze_support

    L = list(range(9))

    def progresser(n):
        interval = 0.001 / (n + 2)
        total = 5000
        text = ""#{}, est. {:<04.2}s"".format(n, interval * total)
        for _ in trange(total, desc=text, position=n):
            sleep(interval)

    if __name__ == '__main__':
        freeze_support()  # for Windows support
        tqdm.set_lock(RLock())  # for managing output contention
        p = Pool(initializer=tqdm.set_lock, initargs=(tqdm.get_lock(),))
        p.map(progresser, L)

Note that in Python 3, ``tqdm.write`` is thread-safe:

.. code:: python

    from time import sleep
    from tqdm import tqdm, trange
    from concurrent.futures import ThreadPoolExecutor

    L = list(range(9))

    def progresser(n):
        interval = 0.001 / (n + 2)
        total = 5000
        text = ""#{}, est. {:<04.2}s"".format(n, interval * total)
        for _ in trange(total, desc=text):
            sleep(interval)
        if n == 6:
            tqdm.write(""n == 6 completed."")
            tqdm.write(""`tqdm.write()` is thread-safe in py3!"")

    if __name__ == '__main__':
        with ThreadPoolExecutor() as p:
            p.map(progresser, L)

Hooks and callbacks
~~~~~~~~~~~~~~~~~~~

``tqdm`` can easily support callbacks/hooks and manual updates.
Here's an example with ``urllib``:

**``urllib.urlretrieve`` documentation**

    | [...]
    | If present, the hook function will be called once
    | on establishment of the network connection and once after each block read
    | thereafter. The hook will be passed three arguments; a count of blocks
    | transferred so far, a block size in bytes, and the total size of the file.
    | [...]

.. code:: python

    import urllib, os
    from tqdm import tqdm
    urllib = getattr(urllib, 'request', urllib)

    class TqdmUpTo(tqdm):
        """"""Provides `update_to(n)` which uses `tqdm.update(delta_n)`.""""""
        def update_to(self, b=1, bsize=1, tsize=None):
            """"""
            b  : int, optional
                Number of blocks transferred so far [default: 1].
            bsize  : int, optional
                Size of each block (in tqdm units) [default: 1].
            tsize  : int, optional
                Total size (in tqdm units). If [default: None] remains unchanged.
            """"""
            if tsize is not None:
                self.total = tsize
            return self.update(b * bsize - self.n)  # also sets self.n = b * bsize

    eg_link = ""https://caspersci.uk.to/matryoshka.zip""
    with TqdmUpTo(unit='B', unit_scale=True, unit_divisor=1024, miniters=1,
                  desc=eg_link.split('/')[-1]) as t:  # all optional kwargs
        urllib.urlretrieve(eg_link, filename=os.devnull,
                           reporthook=t.update_to, data=None)
        t.total = t.n

Inspired by `twine#242 <https://github.com/pypa/twine/pull/242>`__.
Functional alternative in
`examples/tqdm_wget.py <https://github.com/tqdm/tqdm/blob/master/examples/tqdm_wget.py>`__.

It is recommend to use ``miniters=1`` whenever there is potentially
large differences in iteration speed (e.g. downloading a file over
a patchy connection).

**Wrapping read/write methods**

To measure throughput through a file-like object's ``read`` or ``write``
methods, use ``CallbackIOWrapper``:

.. code:: python

    from tqdm.auto import tqdm
    from tqdm.utils import CallbackIOWrapper

    with tqdm(total=file_obj.size,
              unit='B', unit_scale=True, unit_divisor=1024) as t:
        fobj = CallbackIOWrapper(t.update, file_obj, ""read"")
        while True:
            chunk = fobj.read(chunk_size)
            if not chunk:
                break
        t.reset()
        # ... continue to use `t` for something else

Alternatively, use the even simpler ``wrapattr`` convenience function,
which would condense both the ``urllib`` and ``CallbackIOWrapper`` examples
down to:

.. code:: python

    import urllib, os
    from tqdm import tqdm

    eg_link = ""https://caspersci.uk.to/matryoshka.zip""
    response = getattr(urllib, 'request', urllib).urlopen(eg_link)
    with tqdm.wrapattr(open(os.devnull, ""wb""), ""write"",
                       miniters=1, desc=eg_link.split('/')[-1],
                       total=getattr(response, 'length', None)) as fout:
        for chunk in response:
            fout.write(chunk)

The ``requests`` equivalent is nearly identical:

.. code:: python

    import requests, os
    from tqdm import tqdm

    eg_link = ""https://caspersci.uk.to/matryoshka.zip""
    response = requests.get(eg_link, stream=True)
    with tqdm.wrapattr(open(os.devnull, ""wb""), ""write"",
                       miniters=1, desc=eg_link.split('/')[-1],
                       total=int(response.headers.get('content-length', 0))) as fout:
        for chunk in response.iter_content(chunk_size=4096):
            fout.write(chunk)

**Custom callback**

``tqdm`` is known for intelligently skipping unnecessary displays. To make a
custom callback take advantage of this, simply use the return value of
``update()``. This is set to ``True`` if a ``display()`` was triggered.

.. code:: python

    from tqdm.auto import tqdm as std_tqdm

    def external_callback(*args, **kwargs):
        ...

    class TqdmExt(std_tqdm):
        def update(self, n=1):
            displayed = super(TqdmExt, self).update(n)
            if displayed:
                external_callback(**self.format_dict)
            return displayed

``asyncio``
~~~~~~~~~~~

Note that ``break`` isn't currently caught by asynchronous iterators.
This means that ``tqdm`` cannot clean up after itself in this case:

.. code:: python

    from tqdm.asyncio import tqdm

    async for i in tqdm(range(9)):
        if i == 2:
            break

Instead, either call ``pbar.close()`` manually or use the context manager syntax:

.. code:: python

    from tqdm.asyncio import tqdm

    with tqdm(range(9)) as pbar:
        async for i in pbar:
            if i == 2:
                break

Pandas Integration
~~~~~~~~~~~~~~~~~~

Due to popular demand we've added support for ``pandas`` -- here's an example
for ``DataFrame.progress_apply`` and ``DataFrameGroupBy.progress_apply``:

.. code:: python

    import pandas as pd
    import numpy as np
    from tqdm import tqdm

    df = pd.DataFrame(np.random.randint(0, 100, (100000, 6)))

    # Register `pandas.progress_apply` and `pandas.Series.map_apply` with `tqdm`
    # (can use `tqdm.gui.tqdm`, `tqdm.notebook.tqdm`, optional kwargs, etc.)
    tqdm.pandas(desc=""my bar!"")

    # Now you can use `progress_apply` instead of `apply`
    # and `progress_map` instead of `map`
    df.progress_apply(lambda x: x**2)
    # can also groupby:
    # df.groupby(0).progress_apply(lambda x: x**2)

In case you're interested in how this works (and how to modify it for your
own callbacks), see the
`examples <https://github.com/tqdm/tqdm/tree/master/examples>`__
folder or import the module and run ``help()``.

Keras Integration
~~~~~~~~~~~~~~~~~

A ``keras`` callback is also available:

.. code:: python

    from tqdm.keras import TqdmCallback

    ...

    model.fit(..., verbose=0, callbacks=[TqdmCallback()])

Dask Integration
~~~~~~~~~~~~~~~~

A ``dask`` callback is also available:

.. code:: python

    from tqdm.dask import TqdmCallback

    with TqdmCallback(desc=""compute""):
        ...
        arr.compute()

    # or use callback globally
    cb = TqdmCallback(desc=""global"")
    cb.register()
    arr.compute()

IPython/Jupyter Integration
IPython/Jupyter is supported via the  submodule:.. code:: pythonfrom tqdm.notebook import trange, tqdm
from time import sleep

for i in trange(3, desc='1st loop'):
    for j in tqdm(range(100), desc='2nd loop'):
        sleep(0.01)
In addition to  features, the submodule provides a native Jupyterwidget (compatible with IPython v1-v4 and Jupyter), fully working nested barsand colour hints (blue: normal, green: completed, red: error/interrupt,light blue: no ETA); as demonstrated below.|Screenshot-Jupyter1||Screenshot-Jupyter2||Screenshot-Jupyter3|The  version supports percentage or pixels for overall width(e.g.:  or ).It is also possible to let  automatically choose betweenconsole or notebook versions by using the  submodule:.. code:: pythonfrom tqdm.autonotebook import tqdm
tqdm.pandas()
Note that this will issue a  if run in a notebooksince it is not meant to be possible to distinguish between and . Use  instead of  to suppressthis warning.Note that notebooks will display the bar in the cell where it was created.This may be a different cell from the one where it is used.If this is not desired, either.. code:: pythonfrom tqdm.notebook import tqdm
pbar = tqdm(..., display=False)
.. code:: python# different cell
display(pbar.container)
The  callback has a  method which can be used likewise:.. code:: pythonfrom tqdm.keras import TqdmCallback
cbk = TqdmCallback(display=False)
.. code:: python# different cell
cbk.display()
model.fit(..., verbose=0, callbacks=[cbk])
Another possibility is to have a single bar (near the top of the notebook)which is constantly re-used (using  rather than ).For this reason, the notebook version (unlike the CLI version) does notautomatically call  upon ... code:: pythonfrom tqdm.notebook import tqdm
pbar = tqdm()
.. code:: python# different cell
iterable = range(100)
pbar.reset(total=len(iterable))  # initialise with new `total`
for i in iterable:
    pbar.update()
pbar.refresh()  # force print final status but don't `close()`
Custom Integration
To change the default arguments (such as making ``dynamic_ncols=True``),
simply use built-in Python magic:

.. code:: python

    from functools import partial
    from tqdm import tqdm as std_tqdm
    tqdm = partial(std_tqdm, dynamic_ncols=True)

For further customisation,
``tqdm`` may be inherited from to create custom callbacks (as with the
``TqdmUpTo`` example `above <#hooks-and-callbacks>`__) or for custom frontends
(e.g. GUIs such as notebook or plotting packages). In the latter case:

1. ``def __init__()`` to call ``super().__init__(..., gui=True)`` to disable
   terminal ``status_printer`` creation.
2. Redefine: ``close()``, ``clear()``, ``display()``.

Consider overloading ``display()`` to use e.g.
``self.frontend(**self.format_dict)`` instead of ``self.sp(repr(self))``.

Some submodule examples of inheritance:

- `tqdm/notebook.py <https://github.com/tqdm/tqdm/blob/master/tqdm/notebook.py>`__
- `tqdm/gui.py <https://github.com/tqdm/tqdm/blob/master/tqdm/gui.py>`__
- `tqdm/tk.py <https://github.com/tqdm/tqdm/blob/master/tqdm/tk.py>`__
- `tqdm/contrib/slack.py <https://github.com/tqdm/tqdm/blob/master/tqdm/contrib/slack.py>`__
- `tqdm/contrib/discord.py <https://github.com/tqdm/tqdm/blob/master/tqdm/contrib/discord.py>`__
- `tqdm/contrib/telegram.py <https://github.com/tqdm/tqdm/blob/master/tqdm/contrib/telegram.py>`__

Dynamic Monitor/Meter
You can use a  as a meter which is not monotonically increasing.This could be because  decreases (e.g. a CPU usage monitor) or changes.One example would be recursively searching for files. The  is thenumber of objects found so far, while  is the number of those objects whichare files (rather than folders):.. code:: pythonfrom tqdm import tqdm
import os.path

def find_files_recursively(path, show_progress=True):
    files = []
    # total=1 assumes `path` is a file
    t = tqdm(total=1, unit=""file"", disable=not show_progress)
    if not os.path.exists(path):
        raise IOError(""Cannot find:"" + path)

    def append_found_file(f):
        files.append(f)
        t.update()

    def list_found_dir(path):
        """"""returns os.listdir(path) assuming os.path.isdir(path)""""""
        listing = os.listdir(path)
        # subtract 1 since a ""file"" we found was actually this directory
        t.total += len(listing) - 1
        # fancy way to give info without forcing a refresh
        t.set_postfix(dir=path[-10:], refresh=False)
        t.update(0)  # may trigger a refresh
        return listing

    def recursively_search(path):
        if os.path.isdir(path):
            for f in list_found_dir(path):
                recursively_search(os.path.join(path, f))
        else:
            append_found_file(path)

    recursively_search(path)
    t.set_postfix(dir=path)
    t.close()
    return files
Using  is a handy way to let  decide when to trigger adisplay refresh to avoid console spamming.Writing messages
This is a work in progress (see
`#737 <https://github.com/tqdm/tqdm/issues/737>`__).

Since ``tqdm`` uses a simple printing mechanism to display progress bars,
you should not write any message in the terminal using ``print()`` while
a progressbar is open.

To write messages in the terminal without any collision with ``tqdm`` bar
display, a ``.write()`` method is provided:

.. code:: python

    from tqdm.auto import tqdm, trange
    from time import sleep

    bar = trange(10)
    for i in bar:
        # Print using tqdm class method .write()
        sleep(0.1)
        if not (i % 3):
            tqdm.write(""Done task %i"" % i)
        # Can also use bar.write()

By default, this will print to standard output ``sys.stdout``. but you can
specify any file-like object using the ``file`` argument. For example, this
can be used to redirect the messages writing to a log file or class.

Redirecting writing
If using a library that can print messages to the console, editing the libraryby  replacing  with  may not be desirable.In that case, redirecting  to  is an option.To redirect , create a file-like class that will writeany input string to , and supply the arguments.A reusable canonical example is given below:.. code:: pythonfrom time import sleep
import contextlib
import sys
from tqdm import tqdm
from tqdm.contrib import DummyTqdmFile


@contextlib.contextmanager
def std_out_err_redirect_tqdm():
    orig_out_err = sys.stdout, sys.stderr
    try:
        sys.stdout, sys.stderr = map(DummyTqdmFile, orig_out_err)
        yield orig_out_err[0]
    # Relay exceptions
    except Exception as exc:
        raise exc
    # Always restore sys.stdout/err if necessary
    finally:
        sys.stdout, sys.stderr = orig_out_err

def some_fun(i):
    print(""Fee, fi, fo,"".split()[i])

# Redirect stdout to tqdm.write() (don't forget the `as save_stdout`)
with std_out_err_redirect_tqdm() as orig_stdout:
    # tqdm needs the original stdout
    # and dynamic_ncols=True to autodetect console width
    for i in tqdm(range(3), file=orig_stdout, dynamic_ncols=True):
        sleep(.5)
        some_fun(i)

# After the `with`, printing is restored
print(""Done!"")
Redirecting 
Similar to ``sys.stdout``/``sys.stderr`` as detailed above, console ``logging``
may also be redirected to ``tqdm.write()``.

Warning: if also redirecting ``sys.stdout``/``sys.stderr``, make sure to
redirect ``logging`` first if needed.

Helper methods are available in ``tqdm.contrib.logging``. For example:

.. code:: python

    import logging
    from tqdm import trange
    from tqdm.contrib.logging import logging_redirect_tqdm

    LOG = logging.getLogger(__name__)

    if __name__ == '__main__':
        logging.basicConfig(level=logging.INFO)
        with logging_redirect_tqdm():
            for i in trange(9):
                if i == 4:
                    LOG.info(""console logging redirected to `tqdm.write()`"")
        # logging restored

Monitoring thread, intervals and miniters
 implements a few tricks to increase efficiency and reduce overhead.However, consider a case with a combination of fast and slow iterations.After a few fast iterations,  will set  to alarge number. When iteration rate subsequently slows,  willremain large and thus reduce display update frequency. To address this:The monitoring thread should not have a noticeable overhead, and guaranteesupdates at least every 10 seconds by default.This value can be directly changed by setting the  ofany  instance (i.e. ).The monitor thread may be disabled application-wide by setting before instantiation of any  bar.MerchYou can buy __ now!Contributions|GitHub-Commits| |GitHub-Issues| |GitHub-PRs| |OpenHub-Status| |GitHub-Contributions| |CII Best Practices|All source code is hosted on __.Contributions are welcome.See the__file for more information.Developers who have made significant contributions, ranked by SLoC(surviving lines of code,__ ),are:==================== ======================================================== ==== ================================Name                 ID                                                       SLoC Notes==================== ======================================================== ==== ================================Casper da Costa-Luis __             ~80% primary maintainer |Gift-Casper|Stephen Larroque     __                 ~9%  team memberMartin Zugnoni       __     ~3%Daniel Ecer          __                 ~2%Richard Sheridan     __ ~1%Guangshuo Chen       __                   ~1%Helio Machado        __             ~1%Kyle Altendorf       __               <1%Noam Yorav-Raphael   __               <1%  original authorMatthew Stevens      __       <1%Hadrien Mary         __                     <1%  team memberMikhail Korobov      __                     <1%  team member==================== ======================================================== ==== ================================Ports to Other Languages
A list is available on
`this wiki page <https://github.com/tqdm/tqdm/wiki/tqdm-ports>`__.


LICENCE
-------

Open Source (OSI approved): |LICENCE|

Citation information: |DOI|

|README-Hits| (Since 19 May 2016)

.. |Logo| image:: https://tqdm.github.io/img/logo.gif
.. |Screenshot| image:: https://tqdm.github.io/img/tqdm.gif
.. |Video| image:: https://tqdm.github.io/img/video.jpg
   :target: https://tqdm.github.io/video
.. |Slides| image:: https://tqdm.github.io/img/slides.jpg
   :target: https://tqdm.github.io/PyData2019/slides.html
.. |Merch| image:: https://tqdm.github.io/img/merch.jpg
   :target: https://tqdm.github.io/merch
.. |Build-Status| image:: https://img.shields.io/github/actions/workflow/status/tqdm/tqdm/test.yml?branch=master&label=tqdm&logo=GitHub
   :target: https://github.com/tqdm/tqdm/actions/workflows/test.yml
.. |Coverage-Status| image:: https://img.shields.io/coveralls/github/tqdm/tqdm/master?logo=coveralls
   :target: https://coveralls.io/github/tqdm/tqdm
.. |Branch-Coverage-Status| image:: https://codecov.io/gh/tqdm/tqdm/branch/master/graph/badge.svg
   :target: https://codecov.io/gh/tqdm/tqdm
.. |Codacy-Grade| image:: https://app.codacy.com/project/badge/Grade/3f965571598f44549c7818f29cdcf177
   :target: https://www.codacy.com/gh/tqdm/tqdm/dashboard
.. |CII Best Practices| image:: https://bestpractices.coreinfrastructure.org/projects/3264/badge
   :target: https://bestpractices.coreinfrastructure.org/projects/3264
.. |GitHub-Status| image:: https://img.shields.io/github/tag/tqdm/tqdm.svg?maxAge=86400&logo=github&logoColor=white
   :target: https://github.com/tqdm/tqdm/releases
.. |GitHub-Forks| image:: https://img.shields.io/github/forks/tqdm/tqdm.svg?logo=github&logoColor=white
   :target: https://github.com/tqdm/tqdm/network
.. |GitHub-Stars| image:: https://img.shields.io/github/stars/tqdm/tqdm.svg?logo=github&logoColor=white
   :target: https://github.com/tqdm/tqdm/stargazers
.. |GitHub-Commits| image:: https://img.shields.io/github/commit-activity/y/tqdm/tqdm.svg?logo=git&logoColor=white
   :target: https://github.com/tqdm/tqdm/graphs/commit-activity
.. |GitHub-Issues| image:: https://img.shields.io/github/issues-closed/tqdm/tqdm.svg?logo=github&logoColor=white
   :target: https://github.com/tqdm/tqdm/issues?q=
.. |GitHub-PRs| image:: https://img.shields.io/github/issues-pr-closed/tqdm/tqdm.svg?logo=github&logoColor=white
   :target: https://github.com/tqdm/tqdm/pulls
.. |GitHub-Contributions| image:: https://img.shields.io/github/contributors/tqdm/tqdm.svg?logo=github&logoColor=white
   :target: https://github.com/tqdm/tqdm/graphs/contributors
.. |GitHub-Updated| image:: https://img.shields.io/github/last-commit/tqdm/tqdm/master.svg?logo=github&logoColor=white&label=pushed
   :target: https://github.com/tqdm/tqdm/pulse
.. |Gift-Casper| image:: https://img.shields.io/badge/dynamic/json.svg?color=ff69b4&label=gifts%20received&prefix=%C2%A3&query=%24..sum&url=https%3A%2F%2Fcaspersci.uk.to%2Fgifts.json
   :target: https://cdcl.ml/sponsor
.. |Versions| image:: https://img.shields.io/pypi/v/tqdm.svg
   :target: https://tqdm.github.io/releases
.. |PyPI-Downloads| image:: https://img.shields.io/pypi/dm/tqdm.svg?label=pypi%20downloads&logo=PyPI&logoColor=white
   :target: https://pepy.tech/project/tqdm
.. |Py-Versions| image:: https://img.shields.io/pypi/pyversions/tqdm.svg?logo=python&logoColor=white
   :target: https://pypi.org/project/tqdm
.. |Conda-Forge-Status| image:: https://img.shields.io/conda/v/conda-forge/tqdm.svg?label=conda-forge&logo=conda-forge
   :target: https://anaconda.org/conda-forge/tqdm
.. |Snapcraft| image:: https://img.shields.io/badge/snap-install-82BEA0.svg?logo=snapcraft
   :target: https://snapcraft.io/tqdm
.. |Docker| image:: https://img.shields.io/badge/docker-pull-blue.svg?logo=docker&logoColor=white
   :target: https://hub.docker.com/r/tqdm/tqdm
.. |Libraries-Rank| image:: https://img.shields.io/librariesio/sourcerank/pypi/tqdm.svg?logo=koding&logoColor=white
   :target: https://libraries.io/pypi/tqdm
.. |Libraries-Dependents| image:: https://img.shields.io/librariesio/dependent-repos/pypi/tqdm.svg?logo=koding&logoColor=white
    :target: https://github.com/tqdm/tqdm/network/dependents
.. |OpenHub-Status| image:: https://www.openhub.net/p/tqdm/widgets/project_thin_badge?format=gif
   :target: https://www.openhub.net/p/tqdm?ref=Thin+badge
.. |awesome-python| image:: https://awesome.re/mentioned-badge.svg
   :target: https://github.com/vinta/awesome-python
.. |LICENCE| image:: https://img.shields.io/pypi/l/tqdm.svg
   :target: https://raw.githubusercontent.com/tqdm/tqdm/master/LICENCE
.. |DOI| image:: https://img.shields.io/badge/DOI-10.5281/zenodo.595120-blue.svg
   :target: https://doi.org/10.5281/zenodo.595120
.. |binder-demo| image:: https://mybinder.org/badge_logo.svg
   :target: https://mybinder.org/v2/gh/tqdm/tqdm/master?filepath=DEMO.ipynb
.. |Screenshot-Jupyter1| image:: https://tqdm.github.io/img/jupyter-1.gif
.. |Screenshot-Jupyter2| image:: https://tqdm.github.io/img/jupyter-2.gif
.. |Screenshot-Jupyter3| image:: https://tqdm.github.io/img/jupyter-3.gif
.. |README-Hits| image:: https://caspersci.uk.to/cgi-bin/hits.cgi?q=tqdm&style=social&r=https://github.com/tqdm/tqdm&l=https://tqdm.github.io/img/favicon.png&f=https://tqdm.github.io/img/logo.gif
   :target: https://caspersci.uk.to/cgi-bin/hits.cgi?q=tqdm&a=plot&r=https://github.com/tqdm/tqdm&l=https://tqdm.github.io/img/favicon.png&f=https://tqdm.github.io/img/logo.gif&style=social
"
https://github.com/matplotlib/matplotlib,matplotlib: plotting with Python,"Matplotlib is a comprehensive library for creating static, animated, andinteractive visualizations in Python.Check out our  for more information.Matplotlib produces publication-quality figures in a variety of hardcopyformats and interactive environments across platforms. Matplotlib can beused in Python scripts, Python/IPython shells, web application servers,and various graphical user interface toolkits.InstallSee the ,which is generated from ContributeYou've discovered a bug or something else you want to change — excellent!You've worked out a way to fix it — even better!You want to tell us about it — best of all!Start at the !Contact is the discussion forumfor general questions and discussions and our recommended startingpoint.Our active mailing lists (which are mirrored on Discourse) are: is for coordinatingdevelopment and asking questions directly related to contributing tomatplotlib.Citing MatplotlibIf Matplotlib contributes to a project that leads to publication, pleaseacknowledge this by citing Matplotlib. isavailable."
https://github.com/qtile/qtile,":cookie: A full-featured, hackable tiling window manager written and configured in Python (X11 + Wayland)","|logo|A full-featured, hackable tiling window manager written and configured in Python|website| |pypi| |ci| |rtd| |license| |black| |coverage|FeaturesCommunityQtile is supported by a dedicated group of users. If you need any help, pleasedon't hesitate to fire off an email to our mailing list or join us on IRC. Youcan also ask questions on the discussions board.:Mailing List: https://groups.google.com/group/qtile-dev:Q&A: https://github.com/qtile/qtile/discussions/categories/q-a:IRC: irc://irc.oftc.net:6667/qtile:Discord: https://discord.gg/ehh233wCrC (Bridged with IRC)Example codeCheck out the _ repo which contains examples of users' configurations,scripts and other useful links... _: https://github.com/qtile/qtile-examplesContributingPlease report any suggestions, feature requests, bug reports, or annoyances tothe GitHub . There are also a few ,and _ for contributing in the documentation.Please also consider submitting useful scripts etc. to the qtile-examples repo(see above)... _: https://github.com/qtile/qtile/issues.. _: https://docs.qtile.org/en/latest/manual/hacking.html.. _: https://docs.qtile.org/en/latest/manual/contributing.html.. |logo| image:: https://raw.githubusercontent.com/qtile/qtile/master/logo.png:alt: Logo:target: https://qtile.org.. |website| image:: https://img.shields.io/badge/website-qtile.org-blue.svg:alt: Website:target: https://qtile.org.. |pypi| image:: https://img.shields.io/pypi/v/qtile.svg:alt: PyPI:target: https://pypi.org/project/qtile/.. |ci| image:: https://github.com/qtile/qtile/workflows/ci/badge.svg?branch=master:alt: CI status:target: https://github.com/qtile/qtile/actions.. |rtd| image:: https://readthedocs.org/projects/qtile/badge/?version=latest:alt: Read the Docs:target: https://docs.qtile.org/en/latest/.. |license| image:: https://img.shields.io/github/license/qtile/qtile.svg:alt: License:target: https://github.com/qtile/qtile/blob/master/LICENSE.. |black| image:: https://img.shields.io/badge/code%20style-black-000000.svg:alt: Codestyle:target: https://github.com/psf/black.. |coverage| image:: https://coveralls.io/repos/github/qtile/qtile/badge.svg:alt: Coverage:target: https://coveralls.io/github/qtile/qtileMaintainers| _ GPG: | _ GPG: | _ GPG: | _ GPG: | _ GPG: | _ GPG: .. _: https://github.com/tych0.. _: https://github.com/ramnes.. _: https://github.com/m-col.. _: https://github.com/flacjacket.. _: https://github.com/elparaguayo.. _: https://github.com/jwijenbergh"
https://github.com/HIPS/autograd,Efficiently computes derivatives of numpy code.,"Note: Autograd is still being maintained but is no longer actively developed.Autograd     Autograd can automatically differentiate native Python and Numpy code. It canhandle a large subset of Python's features, including loops, ifs, recursion andclosures, and it can even take derivatives of derivatives of derivatives. Itsupports reverse-mode differentiation (a.k.a. backpropagation), which means itcan efficiently take gradients of scalar-valued functions with respect toarray-valued arguments, as well as forward-mode differentiation, and the two canbe composed arbitrarily. The main intended application of Autograd isgradient-based optimization. For more information, check out the and the .Example use:>>> import autograd.numpy as np  # Thinly-wrapped numpy
>>> from autograd import grad    # The only autograd function you may ever need
>>>
>>> def tanh(x):                 # Define a function
...     y = np.exp(-2.0 * x)
...     return (1.0 - y) / (1.0 + y)
...
>>> grad_tanh = grad(tanh)       # Obtain its gradient function
>>> grad_tanh(1.0)               # Evaluate the gradient at x = 1.0
0.41997434161402603
>>> (tanh(1.0001) - tanh(0.9999)) / 0.0002  # Compare to finite differences
0.41997434264973155
We can continue to differentiate as many times as we like, and use numpy'svectorization of scalar-valued functions across many different input values:>>> from autograd import elementwise_grad as egrad  # for functions that vectorize over inputs
>>> import matplotlib.pyplot as plt
>>> x = np.linspace(-7, 7, 200)
>>> plt.plot(x, tanh(x),
...          x, egrad(tanh)(x),                                     # first  derivative
...          x, egrad(egrad(tanh))(x),                              # second derivative
...          x, egrad(egrad(egrad(tanh)))(x),                       # third  derivative
...          x, egrad(egrad(egrad(egrad(tanh))))(x),                # fourth derivative
...          x, egrad(egrad(egrad(egrad(egrad(tanh)))))(x),         # fifth  derivative
...          x, egrad(egrad(egrad(egrad(egrad(egrad(tanh))))))(x))  # sixth  derivative
>>> plt.show()
See the  for the code.DocumentationYou can find a tutorial End-to-end examplesHow to installInstall Autograd using Pip:pip install autograd
Some features require SciPy, which you can install separately or as anoptional dependency along with Autograd:pip install ""autograd[scipy]""
AuthorsAutograd was written by ,,,and many other contributors. The package is currently still being maintained,but is no longer actively developed. Please feel free to submit any bugs orfeature requests. We'd also love to hear about your experiences with autogradin general. Drop us an email!We want to thank Jasper Snoek and the rest of the HIPS group (led by Prof. RyanP. Adams) for helpful contributions and advice; Barak Pearlmutter forfoundational work on automatic differentiation and for guidance on ourimplementation; and Analog Devices Inc. (Lyric Labs) and Samsung Advanced Instituteof Technology for their generous support."
https://github.com/wiseodd/generative-models,"Collection of generative models, e.g. GAN, VAE in Pytorch and Tensorflow.","Generative ModelsCollection of generative models, e.g. GAN, VAE in Pytorch and Tensorflow.Also present here are RBM and Helmholtz Machine.Note:Generated samples will be stored in  (or , etc) directory during training.What's in it?Generative Adversarial Nets (GAN)Variational Autoencoder (VAE)Restricted Boltzmann Machine (RBM)Helmholtz MachineDependencies"
https://github.com/sympy/sympy,A computer algebra system written in pure Python,"SymPySee the  file for the list of authors.And many more people helped on the SymPy mailing list, reported bugs,helped organize SymPy's participation in the Google Summer of Code, theGoogle Highly Open Participation Contest, Google Code-In, wrote andblogged about SymPy...License: New BSD License (see the  file for details) covers allfiles in the sympy repository unless stated otherwise.Our mailing list is at.We have a community chat at . Feelfree to ask us anything there. We have a very welcoming and helpfulcommunity.DownloadThe recommended installation method is through Anaconda,You can also get the latest version of SymPy fromTo get the git version do$ git clone https://github.com/sympy/sympy.git
For other options (tarballs, debs, etc.), see.Documentation and UsageFor in-depth instructions on installation and building thedocumentation, see the .Everything is at:You can generate everything at the above site in your local copy ofSymPy by:$ cd doc
$ make html
Then the docs will be in build/html. Ifyou don't want to read that, here is a short usage:From this directory, start Python and:>>> from sympy import Symbol, cos
>>> x = Symbol('x')
>>> e = 1/cos(x)
>>> print(e.series(x, 0, 10))
1 + x**2/2 + 5*x**4/24 + 61*x**6/720 + 277*x**8/8064 + O(x**10)
SymPy also comes with a console that is a simple wrapper around theclassic python console (or IPython when available) that loads the SymPynamespace and executes some common commands for you.To start it, issue:$ bin/isympy
from this directory, if SymPy is not installed or simply:$ isympy
if SymPy is installed.InstallationSymPy has a hard dependency on the  library(version = 0.19). You should install it first, please refer to thempmath installation guide:To install SymPy using PyPI, run the following command:$ pip install sympy
To install SymPy using Anaconda, run the following command:$ conda install -c anaconda sympy
To install SymPy from GitHub source, first clone SymPy using :$ git clone https://github.com/sympy/sympy.git
Then, in the  repository that you cloned, simply run:$ pip install .
See  for more information.ContributingWe welcome contributions from anyone, even if you are new to opensource. Please read our page and the . If youare new and looking for some way to contribute, a good place to start isto look at the issues tagged .Please note that all participants in this project are expected to followour Code of Conduct. By participating in this project you agree to abideby its terms. See .TestsTo execute all tests, run:$./setup.py test
in the current directory.For the more fine-grained running of tests or doctests, use or respectively . The master branch is automatically testedby GitHub Actions.To test pull requests, use.Regenerate Experimental LaTeX Parser/LexerThe parser and lexer were generated with the toolchain in  and checked into the repo.Presently, most users should not need to regenerate these files, butif you plan to work on this feature, you will need the command-line tool (and you must ensure that it is in your ).One way to get it is:$ conda install -c conda-forge antlr=4.11.1
Alternatively, follow the instructions on the ANTLR website and downloadthe . Then export the  as instructedand instead of creating  as an alias, make it an executable filewith the following contents:#!/bin/bash
java -jar /usr/local/lib/antlr-4.11.1-complete.jar ""$@""
After making changes to , run:$ ./setup.py antlr
CleanTo clean everything (thus getting the same tree as in the repository):$ git clean -Xdf
which will clear everything ignored by , and:$ git clean -df
to clear all untracked files. You can revert the most recent changes ingit with:$ git reset --hard
WARNING: The above commands will all clear changes you may have made,and you will lose them forever. Be sure to check things with , , , and  before doing anyof those.BugsOur issue tracker is at . Pleasereport any bugs that you find. Or, even better, fork the repository onGitHub and create a pull request. We welcome all changes, big or small,and we will help you make the pull request if you are new to git (justask on our mailing list or Gitter Channel). If you further have any queries, you can find answerson Stack Overflow using the  tag.Brief HistorySymPy was started by Ondřej Čertík in 2005, he wrote some code duringthe summer, then he wrote some more code during summer 2006. In February2007, Fabian Pedregosa joined the project and helped fix many things,contributed documentation, and made it alive again. 5 students (MateuszPaprocki, Brian Jorgensen, Jason Gedge, Robert Schwarz, and Chris Wu)improved SymPy incredibly during summer 2007 as part of the GoogleSummer of Code. Pearu Peterson joined the development during the summer2007 and he has made SymPy much more competitive by rewriting the corefrom scratch, which has made it from 10x to 100x faster. Jurjen N.E. Boshas contributed pretty-printing and other patches. Fredrik Johansson haswritten mpmath and contributed a lot of patches.SymPy has participated in every Google Summer of Code since 2007. Youcan see  forfull details. Each year has improved SymPy by bounds. Most of SymPy'sdevelopment has come from Google Summer of Code students.In 2011, Ondřej Čertík stepped down as lead developer, with AaronMeurer, who also started as a Google Summer of Code student, taking hisplace. Ondřej Čertík is still active in the community but is too busywith work and family to play a lead development role.Since then, a lot more people have joined the development and somepeople have also left. You can see the full list in doc/src/aboutus.rst,or online at:The git history goes back to 2007 when development moved from svn to hg.To see the history before that point, look at.You can use git to see the biggest developers. The command:$ git shortlog -ns
will show each developer, sorted by commits to the project. The command:$ git shortlog -ns --since=""1 year""
will show the top developers from the last year.CitationTo cite SymPy in publications useA BibTeX entry for LaTeX users is@article{10.7717/peerj-cs.103,
 title = {SymPy: symbolic computing in Python},
 author = {Meurer, Aaron and Smith, Christopher P. and Paprocki, Mateusz and \v{C}ert\'{i}k, Ond\v{r}ej and Kirpichev, Sergey B. and Rocklin, Matthew and Kumar, Amit and Ivanov, Sergiu and Moore, Jason K. and Singh, Sartaj and Rathnayake, Thilina and Vig, Sean and Granger, Brian E. and Muller, Richard P. and Bonazzi, Francesco and Gupta, Harsh and Vats, Shivam and Johansson, Fredrik and Pedregosa, Fabian and Curry, Matthew J. and Terrel, Andy R. and Rou\v{c}ka, \v{S}t\v{e}p\'{a}n and Saboo, Ashutosh and Fernando, Isuru and Kulal, Sumith and Cimrman, Robert and Scopatz, Anthony},
 year = 2017,
 month = Jan,
 keywords = {Python, Computer algebra system, Symbolics},
 abstract = {
            SymPy is an open-source computer algebra system written in pure Python. It is built with a focus on extensibility and ease of use, through both interactive and programmatic applications. These characteristics have led SymPy to become a popular symbolic library for the scientific Python ecosystem. This paper presents the architecture of SymPy, a description of its features, and a discussion of select submodules. The supplementary material provides additional examples and further outlines details of the architecture and features of SymPy.
         },
 volume = 3,
 pages = {e103},
 journal = {PeerJ Computer Science},
 issn = {2376-5992},
 url = {https://doi.org/10.7717/peerj-cs.103},
 doi = {10.7717/peerj-cs.103}
}
SymPy is BSD licensed, so you are free to use it whatever you like, beit academic, commercial, creating forks or derivatives, as long as youcopy the BSD statement if you redistribute it (see the LICENSE file fordetails). That said, although not required by the SymPy license, if itis convenient for you, please cite SymPy when using it in your work andalso consider contributing all your changes back, so that we canincorporate it and all of us will benefit in the end."
https://github.com/quantopian/qgrid,"An interactive grid for sorting, filtering, and editing DataFrames in Jupyter notebooks",".. image:: https://media.quantopian.com/logos/open_source/qgrid-logo-03.png:target: https://qgrid.readthedocs.io:width: 190px:align: center:alt: qgrid=====qgridQgrid is a Jupyter notebook widget which uses _ to render pandasDataFrames within a Jupyter notebook. This allows you to explore your DataFrames with intuitive scrolling, sorting, andfiltering controls, as well as edit your DataFrames by double clicking cells.Qgrid was developed for use in _and is available for use in that environment as of June 2018.Quantopian also offers a _that includes Qgrid, Zipline, Alphalens, Pyfolio, FactSet data, and more.Announcements: Qgrid WebinarQgrid author Tim Shawver recently did a live webinar about Qgrid, and the recording of the webinar is _.This talk will be interesting both for people that are new to Qgrid, as well as longtime fans that are interested in learning more about the project.DemoClick the badge below to try out the latest beta of qgrid in Quantopian's hosted research environment. If you're already signed into Quantopian you'll be brought directly to the demo notebook. Otherwise you'll be prompted to register (it's free):.. image:: https://img.shields.io/badge/launch-quantopian-red.svg?colorB=d33015:target: https://www.quantopian.com/clone_notebook?id=5b2baee1b3d6870048620188&utm_source=github&utm_medium=web&utm_campaign=qgrid-repo|Click the badge below to try out qgrid using binder:.. image:: https://beta.mybinder.org/badge.svg:target: https://mybinder.org/v2/gh/quantopian/qgrid-notebooks/master?filepath=index.ipynb|Click the following badge to try out qgrid in Jupyterlab, also using binder:.. image:: https://mybinder.org/badge.svg:target: https://mybinder.org/v2/gh/quantopian/qgrid-notebooks/master?urlpath=lab|For both binder links, you'll see a brief loading screen while a server is being created for you in the cloud.  This shouldn't take more than a minute, and usually completes in under 10 seconds.The binder demos generally will be using the most recent stable release of qgrid, so features that were added in a recent beta version may not be available in those demos.For people who would rather not go to another page to try out qgrid for real, here's the tldr; version:    .. figure:: docs/images/filtering_demo.gif
     :align: left
     :target: docs/images/filtering_demo.gif
     :width: 200px

      A brief demo showing filtering, editing, and the `get_changed_df()` method
API DocumentationAPI documentation is hosted on _.InstallationInstalling with pip::pip install qgridjupyter nbextension enable --py --sys-prefix qgridonly required if you have not enabled the ipywidgets nbextension yetjupyter nbextension enable --py --sys-prefix widgetsnbextensionInstalling with conda::only required if you have not added conda-forge to your channels yetconda config --add channels conda-forgeconda install qgridJupyterlab InstallationFirst, go through the normal installation steps above as you normally would when using qgrid in the notebook.If you haven't already install jupyterlab and enabled ipywidgets, do that first with the following lines::pip install jupyterlabjupyter labextension install @jupyter-widgets/jupyterlab-managerInstall the qgrid-jupyterlab extension and enable::jupyter labextension install qgrid2At this point if you run jupyter lab normally with the 'jupyter lab' command, you should beable to use qgrid in notebooks as you normally would.Please Note: Jupyterlab support has been tested with jupyterlab 0.30.5 and jupyterlab-manager 0.31.3, so if you'reWhat's NewColumn-specific options (as of 1.1.0):Thanks to a significant _, Qgrid users now have the ability to set a number of options on a per column basis.  This allows you to do things like explicitly specify which column should be sortable, editable, etc.  For example, if you wanted to prevent editing on all columns except for a column named , you could do the following::col_opts = { 'editable': False }
col_defs = { 'A': { 'editable': True } }
qgrid.show_grid(df, column_options=col_opts, column_definitions=col_defs)
See the updated _ documentation for more information.Disable editing on a per-row basis (as of 1.1.0):This feature can be thought of as the first row-specific option that qgrid supports.  In particular it allows a user to specify, using python code, whether or not a particular row should be editable. For example, to make it so only rows in the grid where the  column is set to  are editable, you might use the following code::def can_edit_row(row):
    return row['status'] == 'active'

qgrid.show_grid(df, row_edit_callback=can_edit_row)
New API methods for dynamically updating an existing qgrid widget (as of 1.1.0):Adds the following new methods, which can be used to update the state of an existing Qgrid widget without having to call  to completely rebuild the widget:- `edit_cell <https://qgrid.readthedocs.io/en/latest/#qgrid.QgridWidget.edit_cell>`_
- `change_selection <https://qgrid.readthedocs.io/en/latest/#qgrid.QgridWidget.change_selection>`_
- `toggle_editable <https://qgrid.readthedocs.io/en/latest/#qgrid.QgridWidget.toggle_editable>`_
- `change_grid_option <https://qgrid.readthedocs.io/en/latest/#qgrid.QgridWidget.change_grid_option>`_ (experimental)
Improved MultiIndex Support (as of 1.0.6-beta.6):Qgrid now displays multi-indexed DataFrames with some of the index cells merged for readability, as is normally done when viewing DataFrames as a static html table.  The following image shows qgrid displaying a multi-indexed DataFrame that was returned from Quantopian's _:.. figure:: https://s3.amazonaws.com/quantopian-forums/pipeline_with_qgrid.png:align: left:target: https://s3.amazonaws.com/quantopian-forums/pipeline_with_qgrid.png:width: 100pxDependenciesQgrid runs on .  You'll also need for the installation steps below.Qgrid depends on the following three Python packages:`Jupyter notebook <https://github.com/jupyter/notebook>`_
  This is the interactive Python environment in which qgrid runs.

`ipywidgets <https://github.com/ipython/ipywidgets>`_
  In order for Jupyter notebooks to be able to run widgets, you have to also install this ipywidgets package.
  It's maintained by the Jupyter organization, the same people who created Jupyter notebook.

`Pandas <http://pandas.pydata.org/>`_
  A powerful data analysis / manipulation library for Python.  Qgrid requires that the data to be rendered as an
  interactive grid be provided in the form of a pandas DataFrame.
These are listed in _and will be automatically installed (if necessary) when qgrid is installed via pip.Compatibility=================  ===========================  ==============================  ==============================qgrid             IPython / Jupyter notebook   ipywidgets                      Jupyterlab=================  ===========================  ==============================  ==============================0.2.0             2.x                          N/A                             N/A0.3.x             3.x                          N/A                             N/A0.3.x             4.0                          4.0.x                           N/A0.3.x             4.1                          4.1.x                           N/A0.3.2             4.2                          5.x                             N/A0.3.3             5.x                          6.x                             N/A1.0.x             5.x                          7.x                             0.30.x=================  ===========================  ==============================  ==============================Running the demo notebooks locallyThere are a couple of demo notebooks in the _ repositorywhich will help you get familiar with the functionality that qgrid provides. Here are the steps to clone theqgrid-notebooks repository and open a demo notebook:#. Install qgrid by following the instructions in the _ section above, if you haven't already#. Clone the qgrid-notebooks repository from GitHub::git clone https://github.com/quantopian/qgrid-notebooks.git
#. Install the dev requirements for the repository and start the notebook server::cd qgrid-notebooks
pip install -r requirements_dev.txt
jupyter notebook
#. Click on one of the two notebooks (_ or _) that you see listed in the notebook UI in your browser.Running from source & testing your changesIf you'd like to contribute to qgrid, or just want to be able to modify the source code for your own purposes, you'llwant to clone this repository and run qgrid from your local copy of the repository.  The following steps explain howto do this.#. Clone the repository from GitHub and  into the top-level directory::git clone https://github.com/quantopian/qgrid.git
cd qgrid
#. Install the current project in _mode::pip install -e .
#. Install the node packages that qgrid depends on and build qgrid's javascript using webpack::cd js && npm install .
#. Install and enable qgrid's javascript in your local jupyter notebook environment::jupyter nbextension install --py --symlink --sys-prefix qgrid && jupyter nbextension enable --py --sys-prefix qgrid
#. If desired, install the labextension::jupyter labextension link js/
#. Run the notebook as you normally would with the following command::jupyter notebook
Manually testing server-side changes^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^If the code you need to change is in qgrid's python code, then restart the kernel of the notebook you're in andrerun any qgrid cells to see your changes take effect.Manually testing client-side changes^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^If the code you need to change is in qgrid's javascript or css code, repeat step 3 to rebuild qgrid's npm package,then refresh the browser tab where you're viewing your notebook to see your changes take effect.Running automated tests^^^^^^^^^^^^^^^^^^^^^^^There is a small python test suite which can be run locally by running the command  in the root folderof the repository.Building docs^^^^^^^^^^^^^The read-the-docs page is generated using sphinx. If you change any doc strings or want to add something to theread-the-docs page, you can preview your changes locally before submitting a PR using the following commands::pip install sphinx sphinx_rtd_theme
cd docs && make html
This will result in the  folder being populated with a new version of the read-the-docs site. Ifyou open the  file in your browser, you should be able to preview your changes.Events APIAs of qgrid 1.0.3 there are new  and  methods in qgrid which can be used to attach/detach event handlers. They're available on both the  module (see ), and on individual QgridWidget instances (see ). Previously the only way to listen for events was to use undocumented parts of the API.Having the ability to attach event handlers allows us to do some interesting things in terms of using qgrid in conjunction with other widgets/visualizations. One example is using qgrid to filter a DataFrame that's also being displayed by another visualization.If you previously used the  method to respond to qgrid events, lets see how your code might be updated to use the new  method::# Before upgrading to 1.0.3
def handle_df_change(change):
    print(change['new'])

qgrid_widget.observe(handle_df_change, names=['_df'])
When you upgrade to 1.0.3, you have more granular control over which events you do an don't listen to, but you can also replicate the previous behavior of calling  every time the state of the internal DataFrame is changed. Here's what that would look like using the new  method::# After upgrading to 1.0.3
def handle_json_updated(event, qgrid_widget):
    # exclude 'viewport_changed' events since that doesn't change the DataFrame
    if (event['triggered_by'] != 'viewport_changed'):
        print(qgrid_widget.get_changed_df())

qgrid_widget.on('json_updated', handle_json_updated)
See the _ for more examples of using these new API methods.For people who would rather not go to another page to try out the events notebook, here are a couple of gifs to give you an idea of what you can do with it.The first gif shows how you can use qgrid to filter the data that's being shown by a matplotlib scatter plot:    .. figure:: docs/images/linked_to_scatter.gif
     :align: left
     :target: docs/images/linked_to_scatter.gif
     :width: 600px

      A brief demo showing qgrid hooked up to a matplotlib plot
The second gif shows how you can move qgrid to a separate view in JupyterLab, which makes it more convenientto use in conjunction with other visualizations (in this case, a couple of  widgets):    .. figure:: docs/images/events_api.gif
     :align: left
     :target: docs/images/events_api.gif
     :width: 600px

      A brief demo showing qgrid's events api
Continuing to use qgrid 0.3.3If you're looking for the installation and usage instructions for qgrid 0.3.3 and the sample notebook that goesalong with it, please see the _ in thisrepository. The installation steps will be mostly the same. The only difference is that when you run ""pip install""you'll have to explicitly specify that you want to install version 0.3.3, like this::pip install qgrid==0.3.3If you're looking for the API docs, you can find them on the_.If you're looking for the demo notebook for 0.3.3, it's still availabe _.Qgrid 0.3.3 is not compatible with ipywidgets 7, so if you need support for ipywidgets 7, you'll need to useqgrid 1.0.ContributingAll contributions, bug reports, bug fixes, documentation improvements, enhancements, and ideas are welcome. See the_ section above for more details on local qgrid development.If you are looking to start working with the qgrid codebase, navigate to the GitHub issues tab and start lookingthrough interesting issues.Feel free to ask questions by submitting an issue with your question."
https://github.com/chubin/wttr.in,:partly_sunny: The right way to check the weather,"wttr.in — the right way to ~check~ wttr.in is a console-oriented weather forecast service that supports various informationrepresentation methods like terminal-oriented ANSI-sequences for console HTTP clients(curl, httpie, or wget), HTML for web browsers, or PNG for graphical viewers.Originally started as a small project, a wrapper for ,intended to demonstrate the power of the console-oriented services,wttr.in became a popular weather reporting service, handling tens of millions of queries daily.You can see it running here: . |  |  |  |  |  |  |  |  | UsageYou can access the service from a shell or from a Web browser like this:$ curl wttr.in
Weather for City: Paris, France

     \   /     Clear
      .-.      10 – 11 °C
   ― (   ) ―   ↑ 11 km/h
      `-’      10 km
     /   \     0.0 mm
Here is an example weather report:Or in PowerShell:Invoke-RestMethod https://wttr.in
Want to get the weather information for a specific location? You can add the desired location to the URL in yourrequest like this:$ curl wttr.in/London
$ curl wttr.in/Moscow
$ curl wttr.in/Salt+Lake+City
If you omit the location name, you will get the report for your current location based on your IP address.Use 3-letter airport codes in order to get the weather information at a certain airport:$ curl wttr.in/muc      # Weather for IATA: muc, Munich International Airport, Germany
$ curl wttr.in/ham      # Weather for IATA: ham, Hamburg Airport, Germany
Let's say you'd like to get the weather for a geographical location other than a town or city - maybe an attractionin a city, a mountain name, or some special location. Add the character  before the name to look up that speciallocation name before the weather is then retrieved:$ curl wttr.in/~Vostok+Station
$ curl wttr.in/~Eiffel+Tower
$ curl wttr.in/~Kilimanjaro
For these examples, you'll see a line below the weather forecast output that shows the geolocationresults of looking up the location:Location: Vostok Station, станция Восток, AAT, Antarctica [-78.4642714,106.8364678]
Location: Tour Eiffel, 5, Avenue Anatole France, Gros-Caillou, 7e, Paris, Île-de-France, 75007, France [48.8582602,2.29449905432]
Location: Kilimanjaro, Northern, Tanzania [-3.4762789,37.3872648]
You can also use IP-addresses (direct) or domain names (prefixed with ) to specify a location:$ curl wttr.in/@github.com
$ curl wttr.in/@msu.ru
To get detailed information online, you can access the  page:$ curl wttr.in/:help
Weather UnitsBy default the USCS units are used for the queries from the USA and the metric system for the rest of the world.You can override this behavior by adding ,  or    to a URL like this:$ curl wttr.in/Amsterdam?u  # USCS (used by default in US)
$ curl wttr.in/Amsterdam?m  # metric (SI) (used by default everywhere except US)
$ curl wttr.in/Amsterdam?M  # metric (SI), but show wind speed in m/s
If you have several options to pass, write them without delimiters in between for the one-letter options,and use  as a delimiter for the long options with values:$ curl 'wttr.in/Amsterdam?m2&lang=nl'
It would be a rough equivalent of  for the GNU CLI syntax.Supported output formats and viewswttr.in currently supports five output formats:The ANSI and HTML formats are selected based on the User-Agent string.To force plain text, which disables colors:$ curl wttr.in/?T
The PNG format can be forced by adding  to the end of the query:$ wget wttr.in/Paris.png
You can use all of the options with the PNG-format like in an URL, but you haveto separate them with  instead of  and :$ wget wttr.in/Paris_0tqp_lang=fr.png
Useful options for the PNG format:Transparency is a useful feature when weather PNGs are used to add weather data to pictures:$ convert source.jpg <( curl wttr.in/Oymyakon_tqp0.png ) -geometry +50+50 -composite target.jpg
In this example:You can embed a special wttr.in widget, that displays the weather condition for the current or a selected location, into a HTML page using the . That is how it looks like:  or on a real world web site: https://feuerwehr-eisolzried.de/.One-line outputOne-line output format is convenient to be used to show weather infoin status bar of different programs, such as tmux, weechat, etc.For one-line output format, specify additional URL parameter :$ curl wttr.in/Nuremberg?format=3
Nuremberg: 🌦 +11⁰C
Available preconfigured formats: 1, 2, 3, 4 and the custom format using the percent notation (see below).You can specify multiple locations separated with  (for repeating queries):$ curl wttr.in/Nuremberg:Hamburg:Berlin?format=3
Nuremberg: 🌦 +11⁰C
Or to process all this queries at once:$ curl -s 'wttr.in/{Nuremberg,Hamburg,Berlin}?format=3'
Nuremberg: 🌦 +11⁰C
Hamburg: 🌦 +8⁰C
Berlin: 🌦 +8⁰C
To specify your own custom output format, use the special -notation:    c    Weather condition,
    C    Weather condition textual name,
    x    Weather condition, plain-text symbol,
    h    Humidity,
    t    Temperature (Actual),
    f    Temperature (Feels Like),
    w    Wind,
    l    Location,
    m    Moon phase 🌑🌒🌓🌔🌕🌖🌗🌘,
    M    Moon day,
    p    Precipitation (mm/3 hours),
    P    Pressure (hPa),
    u    UV index (1-12),

    D    Dawn*,
    S    Sunrise*,
    z    Zenith*,
    s    Sunset*,
    d    Dusk*,
    T    Current time*,
    Z    Local timezone.

(*times are shown in the local timezone)
So, these two calls are the same:    $ curl wttr.in/London?format=3
    London: ⛅️ +7⁰C
    $ curl wttr.in/London?format=""%l:+%c+%t\n""
    London: ⛅️ +7⁰C
tmuxWhen using in , you have to escape  with , i.e. write there  instead of .The output does not contain new line by default, when the %-notation is used, but it does contain it when preconfigured format (,, etc.)are used. To have the new line in the output when the %-notation is used, use '\n' and single quotes when doing a query from the shell.In programs, that are querying the service automatically (such as tmux), it is better to use some reasonable update interval. In tmux, you can configure it with .If several,  separated locations, are specified in the query, specify update periodas an additional query parameter :set -g status-interval 60
WEATHER='#(curl -s wttr.in/London:Stockholm:Moscow\?format\=""%%l:+%%c%%20%%t%%60%%w&period=60"")'
set -g status-right ""$WEATHER ...""
WeeChatTo embed in to an IRC () client's existing status bar:/alias add wttr /exec -pipe ""/mute /set plugins.var.wttr"" url:wttr.in/Montreal?format=%l:+%c+%f+%h+%p+%P+%m+%w+%S+%s;/wait 3 /item refresh wttr
/trigger add wttr timer 60000;0;0 """" """" ""/wttr""
/item add wttr """" ""${plugins.var.wttr}""
/eval /set weechat.bar.status.items ${weechat.bar.status.items},spacer,wttr
/eval /set weechat.startup.command_after_plugins ${weechat.startup.command_after_plugins};/wttr
/wttr
conkyConky usage example:${texeci 1800 curl wttr.in/kyiv_0pq_lang=uk.png
  | convert - -transparent black $HOME/.config/conky/out.png}
${image $HOME/.config/conky/out.png -p 0,0}
Emojis supportTo see emojis in terminal, you need:For the emoji font, we recommend Noto Color Emoji, and a good alternative option would be the Emoji One font;both of them support all necessary emoji glyphs.Font configuration:$ cat ~/.config/fontconfig/fonts.conf
<?xml version=""1.0"" encoding=""UTF-8""?>
<!DOCTYPE fontconfig SYSTEM ""fonts.dtd"">
<fontconfig>
  <alias>
    <family>serif</family>
    <prefer>
      <family>Noto Color Emoji</family>
    </prefer>
  </alias>
  <alias>
    <family>sans-serif</family>
    <prefer>
      <family>Noto Color Emoji</family>
    </prefer>
  </alias>
  <alias>
    <family>monospace</family>
    <prefer>
      <family>Noto Color Emoji</family>
    </prefer>
  </alias>
</fontconfig>
(to apply the configuration, run ).In some cases,  and the terminal understanding of some emoji characters may differ, which maycause strange effects similar to that described in #579.Data-rich output format (v2)In the experimental data-rich output format, that is available under the view code ,a lot of additional weather and astronomical information is available:  $ curl v2.wttr.in/München
or  $ curl wttr.in/München?format=v2
or, if you prefer Nerd Fonts instead of Emoji,  (day) or  (night):  $ curl v2d.wttr.in/München
(The mode is experimental, and it has several limitations currently:Currently, you need some tweaks for some terminals, to get the best possible visualization.URXVTDepending on your configuration you might be taking all steps, or only a few. URXVT currently doesn't support emoji related fonts, but we can get almost the same effect using Font-Symbola. So add to your  file the following line:    xft:symbola:size=10:minspace=False
You can add it after your preferred font and it will only show up when required.Then, if you see or feel like you're having spacing issues, add this: For some reason URXVT sometimes stops deciding right the word spacing and we need to force it this way.The result, should look like:Map view (v3)In the experimental map view, that is available under the view code ,weather information about a geographical region is available:    $ curl v3.wttr.in/Bayern.sxl
or directly in browser:The map view currently supports three formats:Terminal with inline images protocols support:⟶ Detailed article: | Terminal              | Environment    | Images support | Protocol || --------------------- | --------- | ------------- | --------- || uxterm                |   X11     |   yes         |   Sixel   || mlterm                |   X11     |   yes         |   Sixel   || kitty                 |   X11     |   yes         |   Kitty   || wezterm               |   X11     |   yes         |   IIP     || Darktile              |   X11     |   yes         |   Sixel   || Jexer                 |   X11     |   yes         |   Sixel   || GNOME Terminal        |   X11     |    |   Sixel   || alacritty             |   X11     |    |  Sixel   || foot                  |  Wayland  |   yes         |   Sixel   || DomTerm               |   Web     |   yes         |   Sixel   || Yaft                  |   FB      |   yes         |   Sixel   || iTerm2                |   Mac OS X|   yes         |   IIP     || mintty                | Windows   |   yes         |   Sixel   || Windows Terminal  |   Windows     |    |   Sixel   ||  | Windows | yes         |   Sixel   |   |Different output formatsJSON outputThe JSON format is a feature providing access to wttr.in data through an easy-to-parse format, without requiring the user to create a complex script to reinterpret wttr.in's graphical output.To fetch information in JSON format, use the following syntax:$ curl wttr.in/Detroit?format=j1
This will fetch information on the Detroit region in JSON format. The j1 format code is used to allow for the use of other layouts for the JSON output.The result will look something like the following:{
	""current_condition"": [
		{
		    ""FeelsLikeC"": ""25"",
		    ""FeelsLikeF"": ""76"",
		    ""cloudcover"": ""100"",
		    ""humidity"": ""76"",
		    ""observation_time"": ""04:08 PM"",
		    ""precipMM"": ""0.2"",
		    ""pressure"": ""1019"",
		    ""temp_C"": ""22"",
		    ""temp_F"": ""72"",
		    ""uvIndex"": 5,
		    ""visibility"": ""16"",
		    ""weatherCode"": ""122"",
		    ""weatherDesc"": [
			{
			    ""value"": ""Overcast""
			}
		    ],
		    ""weatherIconUrl"": [
			{
			    ""value"": """"
			}
		    ],
		    ""winddir16Point"": ""NNE"",
		    ""winddirDegree"": ""20"",
		    ""windspeedKmph"": ""7"",
		    ""windspeedMiles"": ""4""
		}
	],
...
Most of these values are self-explanatory, aside from . The  is an enumeration which you can find at either  or .Prometheus Metrics OutputThe  Metrics format is a feature providing access to wttr.in data through an easy-to-parse format for monitoring systems, without requiring the user to create a complex script to reinterpret wttr.in's graphical output.To fetch information in Prometheus format, use the following syntax:$ curl wttr.in/Detroit?format=p1
This will fetch information on the Detroit region in Prometheus Metrics format. The  format code is used to allow for the use of other layouts for the Prometheus Metrics output.A possible configuration for Prometheus could look like this:    - job_name: 'wttr_in_detroit'
        static_configs:
            - targets: ['wttr.in']
        metrics_path: '/Detroit'
        params:
            format: ['p1']
The result will look something like the following:# HELP temperature_feels_like_celsius Feels Like Temperature in Celsius
temperature_feels_like_celsius{forecast=""current""} 7
# HELP temperature_feels_like_fahrenheit Feels Like Temperature in Fahrenheit
temperature_feels_like_fahrenheit{forecast=""current""} 45
[truncated]
...Moon phaseswttr.in can also be used to check the phase of the Moon. This example shows how to see the current Moon phasein the full-output mode:$ curl wttr.in/Moon
Get the moon phase for a particular date by adding :$ curl wttr.in/Moon@2016-12-25
The moon phase information uses  as its backend.To get the moon phase information in the online mode, use :$ curl wttr.in/London?format=%m
🌖
Keep in mind that the Unicode representation of moon phases suffers 2 caveats:See #247, #364 for the corresponding tracking issues,and  for pyphoon. Any help is welcome.Internationalization and localizationwttr.in supports multilingual locations names that can be specified in any language in the world(it may be surprising, but many locations in the world don't have an English name).The query string should be specified in Unicode (hex-encoded or not). Spaces in the query stringmust be replaced with :$ curl wttr.in/станция+Восток
Weather report: станция Восток

               Overcast
      .--.     -65 – -47 °C
   .-(    ).   ↑ 23 km/h
  (___.__)__)  15 km
               0.0 mm
The language used for the output (except the location name) does not depend on the input languageand it is either English (by default) or the preferred language of the browser (if the querywas issued from a browser) that is specified in the query headers ().The language can be set explicitly when using console clients by using command-line options like this:curl -H ""Accept-Language: fr"" wttr.in
http GET wttr.in Accept-Language:ru
The preferred language can be forced using the  option:$ curl wttr.in/Berlin?lang=de
The third option is to choose the language using the DNS name used in the query:$ curl de.wttr.in/Berlin
wttr.in is currently translated into 54 languages, and the number of supported languages is constantly growing.See  to learn more about the translation process,to see the list of supported languages and contributors, or to know how you can help to translate wttr.inin your language.Windows UsersThere are currently two Windows related issues that prevent the examples found on this page from working exactly as expected out of the box. Until Microsoft fixes the issues, there are a few workarounds. To circumvent both issues you may use a shell such as  on the  or read on for alternative solutions.Garbage characters in the outputThere is a limitation of the current Win32 version of . Until the  is resolved and rolled out in a future Windows release, it is recommended that you use Powershell’s  command instead:Missing or double wide diagonal wind direction charactersThe second issue is regarding the width of the diagonal arrow glyphs that some Windows Terminal Applications such as the default  use. At the time of writing this, ,  and Terminal Applications built on top of ConEmu such as Cmder () use these double-wide glyphs by default. The result is the same with all of these programs, either a missing character for certain wind directions or a broken table in the output or both. Some third-party Terminal Applications have addressed the wind direction glyph issue but that fix depends on the font and the Terminal Application you are using.One way to display the diagonal wind direction glyphs in your Terminal Application is to use  which is currently available in the . Windows Terminal is currently a preview release and will be rolled out as the default Terminal Application in an upcoming release. If your output is still skewed after using Windows Terminal then try maximizing the terminal window.Another way you can display the diagonal wind direction is to swap out the problematic characters with forward and backward slashes as shown .InstallationTo install the application:Install external dependencieswttr.in has the following external dependencies:After you install , install :$ go get -u github.com/schachmat/wego
$ go install github.com/schachmat/wego
Install Python dependenciesPython requirements:If you want to get weather reports as PNG files, you'll also need to install:You can install most of them using .Some python package use LLVM, so install it first:$ apt-get install llvm-7 llvm-7-dev
If  is used:$ virtualenv -p python3 ve
$ ve/bin/pip3 install -r requirements.txt
$ ve/bin/python3 bin/srv.py
Also, you need to install the geoip2 database.You can use a free database GeoLite2 that can be downloaded from (http://dev.maxmind.com/geoip/geoip2/geolite2/).Configure IP2Location (optional)If you want to use the IP2location service for IP-addresses that are not covered by GeoLite2,you have to obtain a API key of that service, and after that save into the  file:$ echo 'YOUR_IP2LOCATION_KEY' > ~/.ip2location.key
If you don't have this file, the service will be silently skipped (it is not a big problem,because the MaxMind database is pretty good).Installation with Docker/root/.wegorc
/root/.ip2location.key (optional)
/app/airports.dat
/app/GeoLite2-City.mmdb
Get a WorldWeatherOnline key and configure wegoTo get a WorldWeatherOnline API key, you must register here:https://developer.worldweatheronline.com/auth/register
After you have a WorldWeatherOnline key, you can save it into theWWO key file: Also, you have to specify the key in the  configuration:$ cat ~/.wegorc
{
	""APIKey"": ""00XXXXXXXXXXXXXXXXXXXXXXXXXXX"",
	""City"": ""London"",
	""Numdays"": 3,
	""Imperial"": false,
	""Lang"": ""en""
}
The  parameter in  is ignored.Configure wttr.inConfigure the following environment variables that define the path to the local installation, to the GeoLite database, and to the  installation. For example:export WTTR_MYDIR=""/home/igor/wttr.in""
export WTTR_GEOLITE=""/home/igor/wttr.in/GeoLite2-City.mmdb""
export WTTR_WEGO=""/home/igor/go/bin/wego""
export WTTR_LISTEN_HOST=""0.0.0.0""
export WTTR_LISTEN_PORT=""8002""
Configure the HTTP-frontend serviceIt's recommended that you also configure the web server that will be used to access the service:server {
	listen [::]:80;
	server_name  wttr.in *.wttr.in;
	access_log  /var/log/nginx/wttr.in-access.log  main;
	error_log  /var/log/nginx/wttr.in-error.log;

	location / {
	    proxy_pass         http://127.0.0.1:8002;

	    proxy_set_header   Host             $host;
	    proxy_set_header   X-Real-IP        $remote_addr;
	    proxy_set_header   X-Forwarded-For  $remote_addr;

	    client_max_body_size       10m;
	    client_body_buffer_size    128k;

	    proxy_connect_timeout      90;
	    proxy_send_timeout         90;
	    proxy_read_timeout         90;

	    proxy_buffer_size          4k;
	    proxy_buffers              4 32k;
	    proxy_busy_buffers_size    64k;
	    proxy_temp_file_write_size 64k;

	    expires                    off;
	}
}
"
https://github.com/kivy/python-for-android,Turn your Python application into an Android APK,"python-for-androidpython-for-android is a packaging tool for Python apps on Android. You cancreate your own Python distribution including the modules anddependencies you want, and bundle it in an APK or AAB along with your own code.Features include:For documentation and support, see:DocumentationFollow the to install and begin creating APKs and AABs.Quick instructions: install python-for-android with:pip install python-for-android
(for the develop branch: )Test that the install works with:p4a --version
To build any actual apps, set up the Android SDK and NDKas described in the .Use the SDK/NDK API level & NDK version as in the quickstart,other API levels may not work.With everything installed, build an APK with SDL2 with e.g.:p4a apk --private PATH_TO_YOUR_APP_CODE --package=org.example.myapp --name ""My application"" --version 0.1 --bootstrap=sdl2 --requirements=python3,kivy
If you need to deploy your app on Google Play, Android App Bundle (aab) is required since 1 August 2021:For full instructions and parameter options, see .SupportIf you need assistance, you can ask for help on our mailing list:We also have .ContributingWe love pull requests and discussing novel ideas. Check out the Kivyproject  andfeel free to improve python-for-android.See for more information about the python-for-android development andrelease model, but don't worry about the details. You just need tomake a pull request, we'll take care of the rest.The following mailing list and IRC channel are used exclusively fordiscussions about developing the Kivy framework and its sister projects:We also have .Licensepython-for-android is released under the terms of the MIT License.Please refer to the LICENSE file.HistoryIn 2015 these tools were rewritten to provide a new, easier-to-use andeasier-to-extend interface. If you'd like to browse the old toolchain, itsstatus is recorded for posterity athttps://github.com/kivy/python-for-android/tree/old_toolchain.In the last quarter of 2018 the python recipes were changed. Thenew recipe for python3 (3.7.1) had a new build system which wasapplied to the ancient python recipe, allowing us to bump the python2version number to 2.7.15. This change unified the build process forboth python recipes, and probably solved various issues detected over theyears. These unified python recipes require a minimum target api level of 21,Android 5.0 - Lollipop. If you need to build targeting anapi level below 21, you should use an older version of python-for-android(<=0.7.1).On March of 2020 we dropped support for creating apps that use Python 2. The latestpython-for-android release that supported building Python 2 was version 2019.10.6.On August of 2021, we added support for Android App Bundle (aab). As a collateral,now We support multi-arch apk.ContributorsThis project exists thanks to all the people who contribute. [].BackersThank you to all our backers! 🙏 []SponsorsSupport this project by becoming a sponsor. Your logo will show up here with a link to your website. []"
https://github.com/NervanaSystems/neon,Intel® Nervana™ reference deep learning framework committed to best performance on all hardware,"DISCONTINUATION OF PROJECT.  This project will no longer be maintained by Intel.  Intel will not provide or guarantee development of or support for this project, including but not limited to, maintenance, bug fixes, new releases or updates.  Patches to this project are no longer accepted by Intel. If you have an ongoing need to use this project, are interested in independently developing it, or would like to maintain patches for the community, please create your own fork of the project.neon is Intel's reference deep learning framework committed to  on all hardware. Designed for ease-of-use and extensibility.For fast iteration and model exploration, neon has the fastest performance among deep learning libraries (2x speed of cuDNNv4, see ).We use neon internally at Intel Nervana to solve our customers' problems across many. We are hiring across severalroles. Apply !See the  in our latest release.We want to highlight that neon v2.0.0+ has been optimized for much better performance on CPUs by enabling Intel Math Kernel Library (MKL). The DNN (Deep Neural Networks) component of MKL that is used by neon is provided free of charge and downloaded automatically as part of the neon installation.Quick InstallOn a Mac OSX or Linux machine, enter the following to download and installneon (conda users see the ), and use it to train your first multi-layer perceptron. To force a python2 or python3 install, replace  below with either  or .    git clone https://github.com/NervanaSystems/neon.git
    cd neon
    make
    . .venv/bin/activate
Starting after neon v2.2.0, the master branch of neon will be updated weekly with work-in-progress toward the next release. Check out a release tag (e.g., ""git checkout v2.2.0"") for a stable release. Or simply check out the ""latest"" release tag to get the latest stable release (i.e., ""git checkout latest"")From version 2.4.0, we re-enabled pip install. Neon can be installed using package name nervananeon.     pip install nervananeon
It is noted that  needs to be installed separately. The latest release v2.6.0 uses aeon v1.3.0.WarningUse a script to run an example    python examples/mnist_mlp.py 
Selecting a backend engine from the command lineThe gpu backend is selected by default, so the above command is equivalent to if a compatible GPU resource is found on the system:    python examples/mnist_mlp.py -b gpu
When no GPU is available, the optimized CPU (MKL) backend is now selected by default as of neon v2.1.0, which means the above command is now equivalent to:    python examples/mnist_mlp.py -b mkl
If you are interested in comparing the default mkl backend with the non-optimized CPU backend, use the following command:    python examples/mnist_mlp.py -b cpu
Use a yaml file to run an exampleAlternatively, a yaml file may be used run an example.    neon examples/mnist_mlp.yaml
To select a specific backend in a yaml file, add or modify a line that contains  to enable mkl backend, or  to enable cpu backend.  The gpu backend is selected by default if a GPU is available.Recommended Settings for neon with MKL on Intel ArchitecturesThe Intel Math Kernel Library takes advantages of the parallelization and vectorization capabilities of Intel Xeon and Xeon Phi systems. When hyperthreading is enabled on the system, we recommendthe following KMP_AFFINITY setting to make sure parallel threads are 1:1 mapped to the available physical cores.     export OMP_NUM_THREADS=<Number of Physical Cores>
    export KMP_AFFINITY=compact,1,0,granularity=fine  
or     export OMP_NUM_THREADS=<Number of Physical Cores>
    export KMP_AFFINITY=verbose,granularity=fine,proclist=[0-<Number of Physical Cores>],explicit
For more information about KMP_AFFINITY, please check .We encourage users to set out trying and establishing their own best performance settings. DocumentationThe complete documentation for neon is available. Some useful starting points are:SupportFor any bugs or feature requests please:For other questions and discussions please post a message to theGoogle groupLicenseWe are releasing  under an open source License. We welcome you to  with your use cases."
https://github.com/avinassh/rockstar,Makes you a Rockstar C++ Programmer in 2 minutes,"RockstarRockstar is one amazing library, which will make you a Rockstar Programmer in just 2 minutes. In last decade, people learned . But these days, it has come down to just . But, I wanted to do better.This repo will not only teach you Complete C++ in just 2 minutes, but also makes Open Source Contributions. You see, Open Source contributions are very important these days, especially if you can get those boxes filled with green on your Github profile. As an efficient programmer, I believe in killing two birds in just one shot.Run , be a Rockstar, show off your Github profile to everyone and bag those $200K programmer jobs. Once you become a Rockstar, every recruiter will want to hire you and there is no turning back.Installation is Python 3 only library. Rockstar programmers don't code in Python 2.pip install rockstar
UsageTime is very important. Do not waste time reading large manuals or , run these couple of lines instead:from rockstar import RockStar

rock_it_bro = RockStar(days=300)
rock_it_bro.make_me_a_rockstar()
Above script will create a git repository in the current directory. After that, create a repo on Github and push the local repo. That's all! specify number of boxes you want to fill with green. Default value is . Remember, more commits, more green, more Rockstar you are!Become a Rockstar Swift Programmer with 5 years of experience:from rockstar import RockStar

swift_code = ""print('Hello world')""
rock_it_bro = RockStar(days=1900, file_name='hello.swift', code=swift_code)
rock_it_bro.make_me_a_rockstar()
CLI MasterraceSometimes when Atom takes too long time to start, you can use the command line version instead:rockstar --days=666
TestimonialsThis is how your profile looks:Such glorious, much wow!Many people have received jobs from the big 4 after becoming a Rockstar, using and also:Feel free to send a PR and add your name.TodoLicenseThe mighty MIT license. Please check  for more details."
https://github.com/libratbag/piper,GTK application to configure gaming devices,"PiperPiper is a GTK+ application to configure gaming mice. Piper is merely agraphical frontend to the ratbagd DBus daemon, see for instructions on how to run ratbagd.If you are running piper from git, we recommend using libratbag from gitas well to make sure the latest bugfixes are applied.Supported DevicesPiper is merely a frontend, the list of supported devices depends onlibratbag. See  fora list of all known devices.  The device-specific protocols usually have tobe reverse-engineered and the features available may vary to themanufacturer's advertized features.ScreenshotsAnd if you see the mousetrap, something isn't right. Usually this means thateither ratbagd is not running (like in this screenshot), ratbagd needs to beupdated to a newer version, or some other unexpected error occured.Installing PiperSee  for how to install Piper.Building Piper from gitPiper uses the . Run the followingcommands to clone Piper and initialize the build:git clone https://github.com/libratbag/piper.git
cd piper
meson builddir --prefix=/usr/
To build or re-build after code-changes and install, run:ninja -C builddir
sudo ninja -C builddir install
Note:  is the build output directory and can be changed to any otherdirectory name.See  for whatto do when you encounter missing dependencies.ContributingYes please. It's best to contact us first to see what you could do. Note thatthe devices displayed by Piper come from libratbag.For quicker development iteration, there is a special binary that uses data files from the git directory. This removes the need toinstall piper after every code change.ninja -C builddir
./builddir/piper.devel
Note that this still requires ratbagd to run on the system bus.Piper tries to conform to Python's PEP8 style guide using the  formatter.Checking if code is formatted is done as a part of the test suite.You can check if your code passes tests before submitting changes using thefollowing command:meson -C builddir test
Sourcegit clone https://github.com/libratbag/piper.git
BugsBugs can be reported in the issue tracker on our GitHub repo:https://github.com/libratbag/piper/issuesLicenseLicensed under the GPLv2. See the file for thefull license information."
https://github.com/chrissimpkins/codeface,Typefaces for source code beautification,"Typefaces for Source Code BeautificationNew!Each typeface directory in the repository now includes a vertical metrics table ().  Combine the data in these reference tables with the new  to modify any font to the line spacing that suits your needs.  Tighten up or keep it loose.Fun Little Rant About Font CustomizationDownload Font ArchiveClick a link below to download all font binaries in the Codeface main and bitmap gallery collections:| Archive  | SHA1  | Size || :------------: |:---------------:| :-----:||  |  | 31.4 MB ||  |  | 16.5 MB |or use one of the following commands to pull a font archive with :$ curl -OL https://github.com/chrissimpkins/codeface/releases/download/font-collection/codeface-fonts.zip
$ curl -OL https://github.com/chrissimpkins/codeface/releases/download/font-collection/codeface-fonts.tar.xz
The Gallery[  ][  ][  ][  ][  ][  ][  ][  ][  ][  ][  ][  ][  ][  ][  ][  ]  [  ][  ][  ][  ][  ]  [  ][  ][  ][  ][  ][  ][  ][  ][  ][  ][  ][  ][  ][  ][  ][  ][  ]  [  ][  ][  ][  ][  ][  ][  ][  ][  ][  ][  ][  ][  ][  ][  ][  ][  ][  ][  ][  ][  ][  ][  ][  ][  ]§ The End"
https://github.com/seatgeek/fuzzywuzzy,Fuzzy String Matching in Python,This project has been renamed and moved to https://github.com/seatgeek/thefuzz[<marko.inline.RawText object at 0x000001592FDF9C88>] version 0.19.0 correlates with this project's 0.18.0 version with  replacing all instances of this project's name.PRs and issues here will need to be resubmitted to [<marko.inline.RawText object at 0x000001592FDF94C8>]
https://github.com/josephmisiti/awesome-machine-learning,"A curated list of awesome Machine Learning frameworks, libraries and software.","Awesome Machine Learning  A curated list of awesome machine learning frameworks, libraries and software (by language). Inspired by .If you want to contribute to this list (please do), send me a pull request or contact me Also, a listed repository should be deprecated if:Further resources:Table of ContentsFrameworks and LibrariesAPLGeneral-Purpose Machine LearningCGeneral-Purpose Machine LearningComputer VisionC++Computer VisionGeneral-Purpose Machine LearningNatural Language ProcessingSpeech RecognitionSequence AnalysisGesture DetectionCommon LispGeneral-Purpose Machine LearningClojureNatural Language ProcessingGeneral-Purpose Machine LearningDeep LearningData AnalysisData VisualizationInteropMiscExtraCrystalGeneral-Purpose Machine LearningElixirGeneral-Purpose Machine LearningNatural Language ProcessingErlangGeneral-Purpose Machine LearningFortranGeneral-Purpose Machine LearningData Analysis / Data VisualizationGoNatural Language ProcessingGeneral-Purpose Machine LearningSpatial analysis and geometryData Analysis / Data VisualizationComputer visionReinforcement learningHaskellGeneral-Purpose Machine LearningJavaNatural Language ProcessingGeneral-Purpose Machine LearningSpeech RecognitionData Analysis / Data VisualizationDeep LearningJavaScriptNatural Language ProcessingData Analysis / Data VisualizationGeneral-Purpose Machine LearningMiscDemos and ScriptsJuliaGeneral-Purpose Machine LearningNatural Language ProcessingData Analysis / Data VisualizationMisc Stuff / PresentationsKotlinDeep LearningLuaGeneral-Purpose Machine LearningDemos and ScriptsMatlabComputer VisionNatural Language ProcessingGeneral-Purpose Machine LearningData Analysis / Data Visualization.NETComputer VisionNatural Language ProcessingGeneral-Purpose Machine LearningData Analysis / Data VisualizationObjective CGeneral-Purpose Machine LearningOCamlGeneral-Purpose Machine LearningOpenCVOpenSource-Computer-VisionPerlData Analysis / Data VisualizationGeneral-Purpose Machine LearningPerl 6Data Analysis / Data VisualizationGeneral-Purpose Machine LearningPHPNatural Language ProcessingGeneral-Purpose Machine LearningPythonComputer VisionNatural Language ProcessingGeneral-Purpose Machine LearningData Analysis / Data VisualizationMisc Scripts / iPython Notebooks / CodebasesNeural NetworksSpiking Neural NetworksPython Survival AnalysisFederated LearningKaggle Competition Source CodeReinforcement LearningRubyNatural Language ProcessingGeneral-Purpose Machine LearningData Analysis / Data VisualizationMiscRustGeneral-Purpose Machine LearningRGeneral-Purpose Machine LearningData Manipulation | Data Analysis | Data VisualizationSASGeneral-Purpose Machine LearningData Analysis / Data VisualizationNatural Language ProcessingDemos and ScriptsScalaNatural Language ProcessingData Analysis / Data VisualizationGeneral-Purpose Machine LearningSchemeNeural NetworksSwiftGeneral-Purpose Machine LearningTensorFlowGeneral-Purpose Machine LearningToolsNeural NetworksMiscBooksCredits"
https://github.com/s3tools/s3cmd,Official s3cmd repo -- Command line tool for managing Amazon S3 and CloudFront services,"S3cmd tool for Amazon Simple Storage Service (S3)S3tools / S3cmd mailing lists:S3cmd requires Python 2.6 or newer.Python 3+ is also supported starting with S3cmd version 2.See .What is S3cmdS3cmd () is a free command line tool and client for uploading, retrieving and managing data in Amazon S3 and other cloud storage service providers that use the S3 protocol, such as Google Cloud Storage or DreamHost DreamObjects. It is best suited for power users who are familiar with command line programs. It is also ideal for batch scripts and automated backup to S3, triggered from cron, etc.S3cmd is written in Python. It's an open source project available under GNU Public License v2 (GPLv2) and is free for both commercial and private use. You will only have to pay Amazon for using their storage.Lots of features and options have been added to S3cmd, since its very first release in 2008.... we recently counted more than 60 command line options, including multipart uploads, encryption, incremental backup, s3 sync, ACL and Metadata management, S3 bucket size, bucket policies, and more!What is Amazon S3Amazon S3 provides a managed internet-accessible storage service where anyone can store any amount of data and retrieve it later again.S3 is a paid service operated by Amazon. Before storing anything into S3 you must sign up for an ""AWS"" account (where AWS = Amazon Web Services) to obtain a pair of identifiers: Access Key and Secret Key. You will need togive these keys to S3cmd. Think of them as if they were a username and password for your S3 account.Amazon S3 pricing explainedAt the time of this writing the costs of using S3 are (in USD):$0.023 per GB per month of storage space usedplus$0.00 per GB - all data uploadedplus$0.000 per GB - first 1GB / month data downloaded$0.090 per GB - up to 10 TB / month data downloaded$0.085 per GB - next 40 TB / month data downloaded$0.070 per GB - next 100 TB / month data downloaded$0.050 per GB - data downloaded / month over 150 TBplus$0.005 per 1,000 PUT or COPY or LIST requests$0.004 per 10,000 GET and all other requestsIf for instance on 1st of January you upload 2GB of photos in JPEG from your holiday in New Zealand, at the end of January you will be charged $0.05 for using 2GB of storage space for a month, $0.0 for uploading 2GB of data, and a few cents for requests. That comes to slightly over $0.06 for a complete backup of your precious holiday pictures.In February you don't touch it. Your data are still on S3 servers so you pay $0.06 for those two gigabytes, but not a single cent will be charged for any transfer. That comes to $0.05 as an ongoing cost of your backup. Not too bad.In March you allow anonymous read access to some of your pictures and your friends download, say, 1500MB of them. As the files are owned by you, you are responsible for the costs incurred. That means at the end of March you'll be charged $0.05 for storage plus $0.045 for the download traffic generated by your friends.There is no minimum monthly contract or a setup fee. What you use is what you pay for. At the beginning my bill used to be like US$0.03 or even nil.That's the pricing model of Amazon S3 in a nutshell. Check the  for more details.Needless to say that all these money are charged by Amazon itself, there is obviously no payment for using S3cmd :-)Amazon S3 basicsFiles stored in S3 are called ""objects"" and their names are officially called ""keys"". Since this is sometimes confusing for the users we often refer to the objects as ""files"" or ""remote files"". Each object belongs to exactly one ""bucket"".To describe objects in S3 storage we invented a URI-like schema in the following form:s3://BUCKET
ors3://BUCKET/OBJECT
BucketsBuckets are sort of like directories or folders with some restrictions:It is a good idea to use DNS-compatible bucket names. That for instance means you should not use upper case characters. While DNS compliance is not strictly required some features described below are not available for DNS-incompatible named buckets. One more step further is using a fully qualified domain name (FQDN) for a bucket - that has even more benefits.Look for ""Virtual Hosts"" later in this text for more details regarding FQDN named buckets.Objects (files stored in Amazon S3)Unlike for buckets there are almost no restrictions on object names. These can be any UTF-8 strings of up to 1024 bytes long. Interestingly enough the object name can contain forward slash character (/) thus a  is a valid object name. Note that there are not directories nor buckets called  and  - it is really a single object name called  and S3 does not care at all that it looks like a directory structure.The full URI of such an image could be, for example:s3://my-bucket/my/funny/picture.jpg
Public vs Private filesThe files stored in S3 can be either Private or Public. The Private ones are readable only by the user who uploaded them while the Public ones can be read by anyone. Additionally the Public files can be accessed using HTTP protocol, not only using  or a similar tool.The ACL (Access Control List) of a file can be set at the time of upload using  or  options with  or  commands (see below).Alternatively the ACL can be altered for existing remote files with  (or ) command.Simple s3cmd HowToGo to https://aws.amazon.com/s3, click the ""Sign up for web service"" button in the right column and work through the registration. You will have to supply your Credit Card details in order to allow Amazon charge you for S3 usage. At the end you should have your Access and Secret Keys.If you set up a separate IAM user, that user's access key must have at least the following permissions to do anything:Other example policies can be found at https://docs.aws.amazon.com/AmazonS3/latest/dev/example-policies-s3.htmlYou will be asked for the two keys - copy and paste them from your confirmation email or from your Amazon account page. Be careful when copying them! They are case sensitive and must be entered accurately or you'll keep getting errors about invalid signatures or similar.Remember to add s3:ListAllMyBuckets permissions to the keys or you will get an AccessDenied error while testing access.As you just started using S3 there are no buckets owned by you as of now. So the output will be empty.As mentioned above the bucket names must be unique amongst all users of S3. That means the simple names like ""test"" or ""asdf"" are already taken and you must make up something more original. To demonstrate as many features as possible let's create a FQDN-named bucket :$ s3cmd mb s3://public.s3tools.org

Bucket 's3://public.s3tools.org' created
Now you should see your freshly created bucket:$ s3cmd ls

2009-01-28 12:34  s3://public.s3tools.org
$ s3cmd ls s3://public.s3tools.org
$
It's empty, indeed.$ s3cmd put some-file.xml s3://public.s3tools.org/somefile.xml

some-file.xml -> s3://public.s3tools.org/somefile.xml  [1 of 1]
 123456 of 123456   100% in    2s    51.75 kB/s  done
Upload a two-directory tree into the bucket's virtual 'directory':$ s3cmd put --recursive dir1 dir2 s3://public.s3tools.org/somewhere/

File 'dir1/file1-1.txt' stored as 's3://public.s3tools.org/somewhere/dir1/file1-1.txt' [1 of 5]
File 'dir1/file1-2.txt' stored as 's3://public.s3tools.org/somewhere/dir1/file1-2.txt' [2 of 5]
File 'dir1/file1-3.log' stored as 's3://public.s3tools.org/somewhere/dir1/file1-3.log' [3 of 5]
File 'dir2/file2-1.bin' stored as 's3://public.s3tools.org/somewhere/dir2/file2-1.bin' [4 of 5]
File 'dir2/file2-2.txt' stored as 's3://public.s3tools.org/somewhere/dir2/file2-2.txt' [5 of 5]
As you can see we didn't have to create the  'directory'. In fact it's only a filename prefix, not a real directory and it doesn't have to be created in any way beforehand.Instead of using  with the  option, you could also use the  command:$ s3cmd sync dir1 dir2 s3://public.s3tools.org/somewhere/
$ s3cmd ls s3://public.s3tools.org

                       DIR   s3://public.s3tools.org/somewhere/
2009-02-10 05:10    123456   s3://public.s3tools.org/somefile.xml
Use --recursive (or -r) to list all the remote files:$ s3cmd ls --recursive s3://public.s3tools.org

2009-02-10 05:10    123456   s3://public.s3tools.org/somefile.xml
2009-02-10 05:13        18   s3://public.s3tools.org/somewhere/dir1/file1-1.txt
2009-02-10 05:13         8   s3://public.s3tools.org/somewhere/dir1/file1-2.txt
2009-02-10 05:13        16   s3://public.s3tools.org/somewhere/dir1/file1-3.log
2009-02-10 05:13        11   s3://public.s3tools.org/somewhere/dir2/file2-1.bin
2009-02-10 05:13         8   s3://public.s3tools.org/somewhere/dir2/file2-2.txt
$ s3cmd get s3://public.s3tools.org/somefile.xml some-file-2.xml

s3://public.s3tools.org/somefile.xml -> some-file-2.xml  [1 of 1]
 123456 of 123456   100% in    3s    35.75 kB/s  done
$ md5sum some-file.xml some-file-2.xml

39bcb6992e461b269b95b3bda303addf  some-file.xml
39bcb6992e461b269b95b3bda303addf  some-file-2.xml
Checksums of the original file matches the one of the retrieved ones. Looks like it worked :-)To retrieve a whole 'directory tree' from S3 use recursive get:$ s3cmd get --recursive s3://public.s3tools.org/somewhere

File s3://public.s3tools.org/somewhere/dir1/file1-1.txt saved as './somewhere/dir1/file1-1.txt'
File s3://public.s3tools.org/somewhere/dir1/file1-2.txt saved as './somewhere/dir1/file1-2.txt'
File s3://public.s3tools.org/somewhere/dir1/file1-3.log saved as './somewhere/dir1/file1-3.log'
File s3://public.s3tools.org/somewhere/dir2/file2-1.bin saved as './somewhere/dir2/file2-1.bin'
File s3://public.s3tools.org/somewhere/dir2/file2-2.txt saved as './somewhere/dir2/file2-2.txt'
Since the destination directory wasn't specified,  saved the directory structure in a current working directory ('.').There is an important difference between:get s3://public.s3tools.org/somewhere
andget s3://public.s3tools.org/somewhere/
(note the trailing slash) always uses the last path part, ie the word after the last slash, for naming files.In the case of  the last path part is 'somewhere' and therefore the recursive get names the local files as somewhere/dir1, somewhere/dir2, etc.On the other hand in  the last pathpart is empty and s3cmd will only create 'dir1' and 'dir2'without the 'somewhere/' prefix:$ s3cmd get --recursive s3://public.s3tools.org/somewhere/ ~/

File s3://public.s3tools.org/somewhere/dir1/file1-1.txt saved as '~/dir1/file1-1.txt'
File s3://public.s3tools.org/somewhere/dir1/file1-2.txt saved as '~/dir1/file1-2.txt'
File s3://public.s3tools.org/somewhere/dir1/file1-3.log saved as '~/dir1/file1-3.log'
File s3://public.s3tools.org/somewhere/dir2/file2-1.bin saved as '~/dir2/file2-1.bin'
See? It's  and not  as it was in the previous example.Remove everything under s3://public.s3tools.org/somewhere/$ s3cmd del --recursive s3://public.s3tools.org/somewhere/

File s3://public.s3tools.org/somewhere/dir1/file1-1.txt deleted
File s3://public.s3tools.org/somewhere/dir1/file1-2.txt deleted
...
Now try to remove the bucket:$ s3cmd rb s3://public.s3tools.org

ERROR: S3 error: 409 (BucketNotEmpty): The bucket you tried to delete is not empty
Ouch, we forgot about . We can force the bucket removal anyway:$ s3cmd rb --force s3://public.s3tools.org/

WARNING: Bucket is not empty. Removing all the objects from it first. This may take some time...
File s3://public.s3tools.org/somefile.xml deleted
Bucket 's3://public.s3tools.org/' removed
HintsThe basic usage is as simple as described in the previous section.You can increase the level of verbosity with  option and if you're really keen to know what the program does under its bonnet run it with  to see all 'debugging' output.After configuring it with  all available options are spitted into your  file. It's a text file ready to be modified in your favourite text editor.The Transfer commands (put, get, cp, mv, and sync) continue transferring even if an object fails. If a failure occurs the failure is output to stderr and the exit status will be EX_PARTIAL (2). If the option  is specified, or the config option stop_on_error is true, the transfers stop and an appropriate error code is returned.For more information refer to the .LicenseCopyright (C) 2007-2023 TGRMN Software (https://www.tgrmn.com), Sodria SAS (https://www.sodria.com/) and contributorsThis program is free software; you can redistribute it and/or modifyit under the terms of the GNU General Public License as published bythe Free Software Foundation; either version 2 of the License, or(at your option) any later version.This program is distributed in the hope that it will be useful,but WITHOUT ANY WARRANTY; without even the implied warranty ofMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See theGNU General Public License for more details."
https://github.com/nicolargo/glances,"Glances an Eye on your system. A top/htop alternative for GNU/Linux, BSD, Mac OS and Windows operating systems.","===============================Glances - An eye on your system.. image:: https://img.shields.io/pypi/v/glances.svg:target: https://pypi.python.org/pypi/Glances.. image:: https://img.shields.io/github/stars/nicolargo/glances.svg:target: https://github.com/nicolargo/glances/:alt: Github stars.. image:: https://img.shields.io/docker/pulls/nicolargo/glances:target: https://hub.docker.com/r/nicolargo/glances/:alt: Docker pull.. image:: https://pepy.tech/badge/glances/month:target: https://pepy.tech/project/glances:alt: Pypi downloads.. image:: https://github.com/nicolargo/glances/actions/workflows/test.yml/badge.svg:target: https://github.com/nicolargo/glances/actions:alt: Linux tests (GitHub Actions).. image:: https://img.shields.io/github/contributors/nicolargo/glances:target: https://github.com/nicolargo/glances/issues?q=is%3Aissue+is%3Aopen+label%3A%22needs+contributor%22:alt: Contibutors.. image:: https://scrutinizer-ci.com/g/nicolargo/glances/badges/quality-score.png?b=develop:target: https://scrutinizer-ci.com/g/nicolargo/glances/?branch=develop:alt: Code quality.. image:: https://img.shields.io/github/sponsors/nicolargo:target: https://github.com/sponsors/nicolargo:alt: Sponsors.. image:: https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&label=Follow%20%40nicolargo:target: https://twitter.com/nicolargo:alt: @nicolargoSummaryGlances is an open-source system cross-platform monitoring tool.It allows real-time monitoring of various aspects of your system such asCPU, memory, disk, network usage etc. It also allows monitoring of running processes,logged in users, temperatures, voltages, fan speeds etc.It also supports container monitoring, it supports different container managementsystems such as Docker, LXC. The information is presented in an easy to read dashboardand can also be used for remote monitoring of systems via a web interface or commandline interface. It is easy to install and use and can be customized to show onlythe information that you are interested in... image:: https://raw.githubusercontent.com/nicolargo/glances/develop/docs/_static/glances-summary.pngIn client/server mode, remote monitoring could be done via terminal,Web interface or API (XML-RPC and RESTful).Stats can also be exported to files or external time/value databases, CSV or directoutput to STDOUT... image:: https://raw.githubusercontent.com/nicolargo/glances/develop/docs/_static/glances-responsive-webdesign.pngGlances is written in Python and uses libraries to grab information fromyour system. It is based on an open architecture where developers canadd new plugins or exports modules.Project sponsorshipYou can help me to achieve my goals of improving this open-source projector just say ""thank you"" by:Any and all contributions are greatly appreciated.RequirementsNote for Python 2 usersGlances version 4 or higher do not support Python 2 (and Python 3 < 3.8).Please uses Glances version 3.4.x if you need Python 2 support.Optional dependencies:InstallationThere are several methods to test/install Glances on your system. Choose your weapon!PyPI: The standard wayGlances is on . By using PyPI, you will be using the lateststable version.To install Glances, simply use :.. code-block:: consolepip install --user glances
Note: Python headers are required to install _, a Glancesdependency. For example, on Debian/Ubuntu the simplest is  or alternatively need to install firstthe python-dev package and gcc (python-devel on Fedora/CentOS/RHEL).For Windows, just install psutil from the binary installation file.Note 2 (for the Wifi plugin): If you want to use the Wifi plugin, you needto install the wireless-tools package on your system.By default, Glances is installed without the Web interface dependencies.To install it, use the following command:.. code-block:: consolepip install --user 'glances[web]'
For a full installation (with all features):.. code-block:: consolepip install --user 'glances[all]'
To upgrade Glances to the latest version:.. code-block:: consolepip install --user --upgrade glances
The current develop branch is published to the test.pypi.org package index.If you want to test the develop version (could be instable), enter:.. code-block:: consolepip install --user -i https://test.pypi.org/simple/ Glances
Glances Auto Install script: the easy wayTo install both dependencies and the latest Glances production ready version(aka master branch), just enter the following command line:.. code-block:: consolecurl -L https://bit.ly/glances | /bin/bash
or.. code-block:: consolewget -O- https://bit.ly/glances | /bin/bash
Note: This is only supported on some GNU/Linux distributions and Mac OS X.If you want to support other distributions, please contribute to _.Docker: the fun wayGlances Docker images are availables. You can use it to monitor yourserver and all your containers !Get the Glances container:.. code-block:: consoledocker pull nicolargo/glances:latest-full
The following tags are availables:Run last version of Glances container in console mode:.. code-block:: consoledocker run --rm -e TZ=""${TZ}"" -v /var/run/docker.sock:/var/run/docker.sock:ro -v /run/user/1000/podman/podman.sock:/run/user/1000/podman/podman.sock:ro --pid host --network host -it nicolargo/glances:latest-full
Additionally, if you want to use your own glances.conf file, you cancreate your own Dockerfile:.. code-block:: consoleFROM nicolargo/glances:latest
COPY glances.conf /etc/glances.conf
CMD python -m glances -C /etc/glances.conf $GLANCES_OPT
Alternatively, you can specify something along the same lines withdocker run options (notice the  environmentvariable setting parameters for the glances startup command):.. code-block:: consoledocker run -e TZ=""${TZ}"" -v `pwd`/glances.conf:/etc/glances.conf -v /var/run/docker.sock:/var/run/docker.sock:ro -v /run/user/1000/podman/podman.sock:/run/user/1000/podman/podman.sock:ro --pid host -e GLANCES_OPT=""-C /etc/glances.conf"" -it nicolargo/glances:latest-full
Where pwd/glances.conf is a local directory containing your glances.conf file.Run the container in Web server mode:.. code-block:: consoledocker run -d --restart=""always"" -p 61208-61209:61208-61209 -e TZ=""${TZ}"" -e GLANCES_OPT=""-w"" -v /var/run/docker.sock:/var/run/docker.sock:ro -v /run/user/1000/podman/podman.sock:/run/user/1000/podman/podman.sock:ro --pid host nicolargo/glances:latest-full
For a full list of options, see the Glances _ documentation page.GNU/Linux is available on many Linux distributions, so you should beable to install it using your favorite package manager. Be aware thatwhen you use this method the operating system _ for may not be the latest version and only basics plugins are enabled.Note: The Debian package (and all other Debian-based distributions) donot include anymore the JS statics files used by the Web interface(see ). If you want to add it to your Glances installation,follow the instructions: .FreeBSDTo install the binary package:.. code-block:: console# pkg install py38-glances
To install Glances from ports:.. code-block:: console# cd /usr/ports/sysutils/py-glances/
# make install clean
macOSIf you do not want to use the glancesautoinstall script, follow this procedure.macOS users can install Glances using  or .Homebrew
.. code-block:: console

    $ brew install glances

MacPorts
.. code-block:: console$ sudo port install glances
WindowsInstall _ for Windows (Python 3.4+ ship with pip) andthen run the following command:.. code-block:: console$ pip install glances
AndroidYou need a rooted device and the _ application (available on theGoogle Play Store).Start Termux on your device and enter:.. code-block:: console$ apt update
$ apt upgrade
$ apt install clang python
$ pip install bottle
$ pip install glances
And start Glances:.. code-block:: console$ glances
You can also run Glances in server mode (-s or -w) in order to remotelymonitor your Android device.SourceTo install Glances from source:.. code-block:: console$ wget https://github.com/nicolargo/glances/archive/vX.Y.tar.gz -O - | tar xz
$ cd glances-*
# python setup.py install
Note: Python headers are required to install psutil.ChefAn awesome  cookbook is available to monitor your infrastructure:https://supermarket.chef.io/cookbooks/glances (thanks to Antoine Rouyer)PuppetYou can install Glances using : https://github.com/rverchere/puppet-glancesAnsibleA Glances  role is available: https://galaxy.ansible.com/zaxos/glances-ansible-role/UsageFor the standalone mode, just run:.. code-block:: console$ glances
For the Web server mode, run:.. code-block:: console$ glances -w
and enter the URL  in your favorite web browser.For the client/server mode, run:.. code-block:: console$ glances -s
on the server side and run:.. code-block:: console$ glances -c <ip>
on the client one.You can also detect and display all Glances servers available on yournetwork or defined in the configuration file:.. code-block:: console$ glances --browser
You can also display raw stats on stdout:.. code-block:: console$ glances --stdout cpu.user,mem.used,load
cpu.user: 30.7
mem.used: 3278204928
load: {'cpucore': 4, 'min1': 0.21, 'min5': 0.4, 'min15': 0.27}
cpu.user: 3.4
mem.used: 3275251712
load: {'cpucore': 4, 'min1': 0.19, 'min5': 0.39, 'min15': 0.27}
...
or in a CSV format thanks to the stdout-csv option:.. code-block:: console$ glances --stdout-csv now,cpu.user,mem.used,load
now,cpu.user,mem.used,load.cpucore,load.min1,load.min5,load.min15
2018-12-08 22:04:20 CEST,7.3,5948149760,4,1.04,0.99,1.04
2018-12-08 22:04:23 CEST,5.4,5949136896,4,1.04,0.99,1.04
...
or in a JSON format thanks to the stdout-json option (attribute not supported in this mode in order to have a real JSON object in output):.. code-block:: console$ glances --stdout-json cpu,mem
cpu: {""total"": 29.0, ""user"": 24.7, ""nice"": 0.0, ""system"": 3.8, ""idle"": 71.4, ""iowait"": 0.0, ""irq"": 0.0, ""softirq"": 0.0, ""steal"": 0.0, ""guest"": 0.0, ""guest_nice"": 0.0, ""time_since_update"": 1, ""cpucore"": 4, ""ctx_switches"": 0, ""interrupts"": 0, ""soft_interrupts"": 0, ""syscalls"": 0}
mem: {""total"": 7837949952, ""available"": 2919079936, ""percent"": 62.8, ""used"": 4918870016, ""free"": 2919079936, ""active"": 2841214976, ""inactive"": 3340550144, ""buffers"": 546799616, ""cached"": 3068141568, ""shared"": 788156416}
...
and RTFM, always.DocumentationFor complete documentation have a look at the readthedocs_ website.If you have any question (after RTFM!), please post it on the official Q&A _.Gateway to other servicesGlances can export stats to:  file,  file, , , ,, , , , ,, , ,  and  server.How to contribute ?If you want to contribute to the Glances project, read this _ page.There is also a chat dedicated to the Glances developers:.. image:: https://badges.gitter.im/Join%20Chat.svg:target: https://gitter.im/nicolargo/glances?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badgeAuthorNicolas Hennion (@nicolargo) .. image:: https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&label=Follow%20%40nicolargo:target: https://twitter.com/nicolargoLicenseGlances is distributed under the LGPL version 3 license. See  for more details... _psutil: https://github.com/giampaolo/psutil.. _glancesautoinstall: https://github.com/nicolargo/glancesautoinstall.. _Python: https://www.python.org/getit/.. _Termux: https://play.google.com/store/apps/details?id=com.termux.. _readthedocs: https://glances.readthedocs.io/.. _forum: https://groups.google.com/forum/?hl=en#!forum/glances-users.. _wiki: https://github.com/nicolargo/glances/wiki/How-to-contribute-to-Glances-%3F.. _package: https://repology.org/project/glances/versions.. _sponsors: https://github.com/sponsors/nicolargo.. wishlist: https://www.amazon.fr/hz/wishlist/ls/BWAAQKWFR3FI?ref=wl_share.. _issue2021: https://github.com/nicolargo/glances/issues/2021#issuecomment-1197831157.. _issue2021comment: https://github.com/nicolargo/glances/issues/2021#issuecomment-1197831157.. _Docker: https://github.com/nicolargo/glances/blob/develop/docs/docker.rst"
https://github.com/sherjilozair/char-rnn-tensorflow," Multi-layer Recurrent Neural Networks (LSTM, RNN) for character-level language models in Python using Tensorflow ","char-rnn-tensorflowMulti-layer Recurrent Neural Networks (LSTM, RNN) for character-level language models in Python using Tensorflow.Inspired from Andrej Karpathy's .RequirementsBasic UsageTo train with default parameters on the tinyshakespeare corpus, run . To access all the parameters use .To sample from a checkpointed model, .Sampling while the learning is still in progress (to check last checkpoint) works only in CPU or using another GPU.To force CPU mode, use  and  afterward(resp.  and  on Windows).To continue training after interruption or to run on more epochs, DatasetsYou can use any plain text file as input. For example you could download  as such:cd data
mkdir sherlock
cd sherlock
wget https://sherlock-holm.es/stories/plain-text/cnus.txt
mv cnus.txt input.txt
Then start train from the top level directory using A quick tip to concatenate many small disparate  files into one large training file: .TuningTuning your models is kind of a ""dark art"" at this point. In general:TensorboardTo visualize training progress, model graphs, and internal state histograms:  fire up Tensorboard and point it at your .  E.g.:$ tensorboard --logdir=./logs/
Then open a browser to  or the correct IP/Port specified.RoadmapContributingPlease feel free to:"
https://github.com/gevent/gevent,Coroutine-based concurrency library for Python,"========gevent.. image:: https://github.com/gevent/gevent/workflows/gevent%20testing/badge.svg:target: https://github.com/gevent/gevent/actions.. image:: https://ci.appveyor.com/api/projects/status/bqxl88yhpho223jg?svg=true:target: https://ci.appveyor.com/project/denik/gevent.. image:: https://coveralls.io/repos/gevent/gevent/badge.svg?branch=master&service=github:target: https://coveralls.io/github/gevent/gevent?branch=master.. include:: docs/_about.rstRead the documentation online at http://www.gevent.org.Post issues on the , discuss and ask open ended, and find announcements andinformation on the blog_ and _... include:: docs/install.rst.. _bug tracker: https://github.com/gevent/gevent/issues.. _mailing list: http://groups.google.com/group/gevent.. _blog: https://dev.nextthought.com/blog/categories/gevent.html.. _twitter (@gevent): http://twitter.com/gevent"
https://github.com/JonathanSalwan/ROPgadget,"This tool lets you search your gadgets on your binaries to facilitate your ROP exploitation. ROPgadget supports ELF, PE and Mach-O format on x86, x64, ARM, ARM64, PowerPC, SPARC, MIPS, RISC-V 64, and RISC-V Compressed architectures. ","ROPgadget ToolThis tool lets you search your gadgets on your binaries to facilitate your ROPexploitation. ROPgadget supports ELF/PE/Mach-O/Raw formats on x86, x64, ARM,ARM64, PowerPC, SPARC, MIPS, RISC-V 64, and RISC-V Compressed architectures.InstallThe easiest way is installing ROPgadget from PyPi:$ sudo apt install python3-pip
$ sudo -H python3 -m pip install ROPgadget
$ ROPgadget --help
Alternatively you can install ROPgadget from source.You have to install  first.For the Capstone's installation on nix machine:$ sudo apt install python3-pip
$ sudo -H python3 -m pip install capstone
Capstone supports multi-platforms (windows, ios, android, cygwin...). For the cross-compilation,please refer to the https://github.com/capstone-engine/capstone/blob/master/COMPILE.TXT file.After Capstone is installed, ROPgadget can be used as a standalone tool:$ python3 ROPgadget.py --help
Or installed into the Python site-packages library, and executed from $PATH.$ sudo -H python3 setup.py install
$ ROPgadget --help
Usageusage: ROPgadget.py [-h] [-v] [-c] [--binary <binary>] [--opcode <opcodes>]
                    [--string <string>] [--memstr <string>] [--depth <nbyte>]
                    [--only <key>] [--filter <key>] [--range <start-end>]
                    [--badbytes <byte>] [--rawArch <arch>] [--rawMode <mode>]
                    [--rawEndian <endian>] [--re <re>] [--offset <hexaddr>]
                    [--ropchain] [--thumb] [--console] [--norop] [--nojop]
                    [--callPreceded] [--nosys] [--multibr] [--all] [--noinstr]
                    [--dump] [--silent] [--align ALIGN] [--mipsrop <rtype>]

description:
  ROPgadget lets you search your gadgets on a binary. It supports several
  file formats and architectures and uses the Capstone disassembler for
  the search engine.

formats supported:
  - ELF
  - PE
  - Mach-O
  - Raw

architectures supported:
  - x86
  - x86-64
  - ARM
  - ARM64
  - MIPS
  - PowerPC
  - Sparc
  - RISC-V 64
  - RISC-V Compressed

optional arguments:
  -h, --help            show this help message and exit
  -v, --version         Display the ROPgadget's version
  -c, --checkUpdate     Checks if a new version is available
  --binary <binary>     Specify a binary filename to analyze
  --opcode <opcodes>    Search opcode in executable segment
  --string <string>     Search string in readable segment
  --memstr <string>     Search each byte in all readable segment
  --depth <nbyte>       Depth for search engine (default 10)
  --only <key>          Only show specific instructions
  --filter <key>        Suppress specific mnemonics
  --range <start-end>   Search between two addresses (0x...-0x...)
  --badbytes <byte>     Rejects specific bytes in the gadget's address
  --rawArch <arch>      Specify an arch for a raw file
                        x86|arm|arm64|sparc|mips|ppc|riscv
  --rawMode <mode>      Specify a mode for a raw file 32|64|arm|thumb
  --rawEndian <endian>  Specify an endianness for a raw file little|big
  --re <re>             Regular expression
  --offset <hexaddr>    Specify an offset for gadget addresses
  --ropchain            Enable the ROP chain generation
  --thumb               Use the thumb mode for the search engine (ARM only)
  --console             Use an interactive console for search engine
  --norop               Disable ROP search engine
  --nojop               Disable JOP search engine
  --callPreceded        Only show gadgets which are call-preceded
  --nosys               Disable SYS search engine
  --multibr             Enable multiple branch gadgets
  --all                 Disables the removal of duplicate gadgets
  --noinstr             Disable the gadget instructions console printing
  --dump                Outputs the gadget bytes
  --silent              Disables printing of gadgets during analysis
  --align ALIGN         Align gadgets addresses (in bytes)
  --mipsrop <rtype>     MIPS useful gadgets finder
                        stackfinder|system|tails|lia0|registers

examples:
  ROPgadget.py --binary ./test-suite-binaries/elf-Linux-x86
  ROPgadget.py --binary ./test-suite-binaries/elf-Linux-x86 --ropchain
  ROPgadget.py --binary ./test-suite-binaries/elf-Linux-x86 --depth 3
  ROPgadget.py --binary ./test-suite-binaries/elf-Linux-x86 --string ""main""
  ROPgadget.py --binary ./test-suite-binaries/elf-Linux-x86 --string ""m..n""
  ROPgadget.py --binary ./test-suite-binaries/elf-Linux-x86 --opcode c9c3
  ROPgadget.py --binary ./test-suite-binaries/elf-Linux-x86 --only ""mov|ret""
  ROPgadget.py --binary ./test-suite-binaries/elf-Linux-x86 --only ""mov|pop|xor|ret""
  ROPgadget.py --binary ./test-suite-binaries/elf-Linux-x86 --filter ""xchg|add|sub|cmov.*""
  ROPgadget.py --binary ./test-suite-binaries/elf-Linux-x86 --norop --nosys
  ROPgadget.py --binary ./test-suite-binaries/elf-Linux-x86 --range 0x08041000-0x08042000
  ROPgadget.py --binary ./test-suite-binaries/elf-Linux-x86 --string main --range 0x080c9aaa-0x080c9aba
  ROPgadget.py --binary ./test-suite-binaries/elf-Linux-x86 --memstr ""/bin/sh""
  ROPgadget.py --binary ./test-suite-binaries/elf-Linux-x86 --console
  ROPgadget.py --binary ./test-suite-binaries/elf-Linux-x86 --badbytes ""00|01-1f|7f|42""
  ROPgadget.py --binary ./test-suite-binaries/Linux_lib64.so --offset 0xdeadbeef00000000
  ROPgadget.py --binary ./test-suite-binaries/elf-ARMv7-ls --depth 5
  ROPgadget.py --binary ./test-suite-binaries/elf-ARM64-bash --depth 5
  ROPgadget.py --binary ./test-suite-binaries/raw-x86.raw --rawArch=x86 --rawMode=32
  ROPgadget.py --binary ./test-suite-binaries/elf-Linux-RISCV_64 --depth 8
How can I contribute ?Bugs/Patches/ContactPlease, report bugs, submit pull requests, etc. on GitHub at https://github.com/JonathanSalwan/ROPgadgetLicenseSee LICENSE_BSD.txt and the license header on all source files.Screenshots"
https://github.com/asweigart/pyautogui,A cross-platform GUI automation Python module for human beings. Used to programmatically control the mouse & keyboard.,"PyAutoGUIPyAutoGUI is a  cross-platform GUI automation Python module for human beings. Used to programmatically control the mouse & keyboard.Full documentation available at https://pyautogui.readthedocs.orgSimplified Chinese documentation available at https://github.com/asweigart/pyautogui/blob/master/docs/simplified-chinese.ipynbSource code available at https://github.com/asweigart/pyautoguiIf you need help installing Python, visit https://installpython3.com/DependenciesPyAutoGUI supports Python 2 and 3. If you are installing PyAutoGUI from PyPI using pip:Windows has no dependencies. The Win32 extensions do not need to be installed.macOS needs the pyobjc-core and pyobjc module installed (in that order).Linux needs the python3-xlib (or python-xlib for Python 2) module installed.Pillow needs to be installed, and on Linux you may need to install additional libraries to make sure Pillow's PNG/JPEG works correctly. See:https://stackoverflow.com/questions/7648200/pip-install-pil-e-tickets-1-no-jpeg-png-support

http://ubuntuforums.org/showthread.php?t=1751455
If you want to do development and contribute to PyAutoGUI, you will need to install these modules from PyPI:Example UsageKeyboard and Mouse ControlThe x, y coordinates used by PyAutoGUI has the 0, 0 origin coordinates in the top left corner of the screen. The x coordinates increase going to the right (just as in mathematics) but the y coordinates increase going down (the opposite of mathematics). On a screen that is 1920 x 1080 pixels in size, coordinates 0, 0 are for the top left while 1919, 1079 is for the bottom right.Currently, PyAutoGUI only works on the primary monitor. PyAutoGUI isn't reliable for the screen of a second monitor (the mouse functions may or may not work on multi-monitor setups depending on your operating system and version).All keyboard presses done by PyAutoGUI are sent to the window that currently has focus, as if you had pressed the physical keyboard key.    >>> import pyautogui
    >>> screenWidth, screenHeight = pyautogui.size() # Returns two integers, the width and height of the screen. (The primary monitor, in multi-monitor setups.)
    >>> currentMouseX, currentMouseY = pyautogui.position() # Returns two integers, the x and y of the mouse cursor's current position.
    >>> pyautogui.moveTo(100, 150) # Move the mouse to the x, y coordinates 100, 150.
    >>> pyautogui.click() # Click the mouse at its current location.
    >>> pyautogui.click(200, 220) # Click the mouse at the x, y coordinates 200, 220.
    >>> pyautogui.move(None, 10)  # Move mouse 10 pixels down, that is, move the mouse relative to its current position.
    >>> pyautogui.doubleClick() # Double click the mouse at the
    >>> pyautogui.moveTo(500, 500, duration=2, tween=pyautogui.easeInOutQuad) # Use tweening/easing function to move mouse over 2 seconds.
    >>> pyautogui.write('Hello world!', interval=0.25)  # Type with quarter-second pause in between each key.
    >>> pyautogui.press('esc') # Simulate pressing the Escape key.
    >>> pyautogui.keyDown('shift')
    >>> pyautogui.write(['left', 'left', 'left', 'left', 'left', 'left'])
    >>> pyautogui.keyUp('shift')
    >>> pyautogui.hotkey('ctrl', 'c')
Display Message Boxes    >>> import pyautogui
    >>> pyautogui.alert('This is an alert box.')
    'OK'
    >>> pyautogui.confirm('Shall I proceed?')
    'Cancel'
    >>> pyautogui.confirm('Enter option.', buttons=['A', 'B', 'C'])
    'B'
    >>> pyautogui.prompt('What is your name?')
    'Al'
    >>> pyautogui.password('Enter password (text will be hidden)')
    'swordfish'
Screenshot Functions(PyAutoGUI uses Pillow for image-related features.)    >>> import pyautogui
    >>> im1 = pyautogui.screenshot()
    >>> im1.save('my_screenshot.png')
    >>> im2 = pyautogui.screenshot('my_screenshot2.png')
You can also locate where an image is on the screen:    >>> import pyautogui
    >>> button7location = pyautogui.locateOnScreen('button.png') # returns (left, top, width, height) of matching region
    >>> button7location
    (1416, 562, 50, 41)
    >>> buttonx, buttony = pyautogui.center(button7location)
    >>> buttonx, buttony
    (1441, 582)
    >>> pyautogui.click(buttonx, buttony)  # clicks the center of where the button was found
The locateCenterOnScreen() function returns the center of this match region:    >>> import pyautogui
    >>> buttonx, buttony = pyautogui.locateCenterOnScreen('button.png') # returns (x, y) of matching region
    >>> buttonx, buttony
    (1441, 582)
    >>> pyautogui.click(buttonx, buttony)  # clicks the center of where the button was found
How Does PyAutoGUI Work?The three major operating systems (Windows, macOS, and Linux) each have different ways to programmatically control the mouse and keyboard. This can often involve confusing, obscure, and deeply technical details. The job of PyAutoGUI is to hide all of this complexity behind a simple API.SupportIf you find this project helpful and would like to support its development, ."
https://github.com/cython/cython,The most widely used Python to C compiler,"Welcome to Cython!Cython is a Python compiler that makes writing C extensions forPython as easy as Python itself.  Cython is based on Pyrex,but supports more cutting edge functionality and optimizations.Cython translates Python code to C/C++ code, but additionally supports callingC functions and declaring C types on variables and class attributes.This allows the compiler to generate very efficient C code from Cython code.This makes Cython the ideal language for wrapping external C libraries, andfor fast C modules that speed up the execution of Python code.Cython has _per month on PyPI.  You can support the Cython project via_ or_.Installation:If you already have a C compiler, just run following command::pip install Cythonotherwise, see _.License:The original Pyrex program was licensed ""free of restrictions"" (see below).Cython itself is licensed under the permissive Apache License.See _.Contributing:Want to contribute to the Cython project?Here is some _.Differences to other Python compilersStarted as a project in the early 2000s, Cython has outlived_at producing static compilers for the Python language.Similar projects that have a relevance today include:In comparison to the above, Cython providesGet the full source history:Note that Cython used to ship the full version control repository in its sourcedistribution, but no longer does so due to space constraints.  To get thefull source history from a downloaded source archive, make sure you have gitinstalled, then step into the base directory of the Cython source distributionand type::make repo
The following is from Pyrex:This is a development version of Pyrex, a languagefor writing Python extension modules.For more info, take a look at:Comments, suggestions, bug reports, etc. are mostwelcome!Copyright stuff: Pyrex is free of restrictions. Youmay use, redistribute, modify and distribute modifiedversions.The latest version of Pyrex can be found _.| Greg Ewing, Computer Science Dept| University of Canterbury| Christchurch, New ZealandA citizen of NewZealandCorp, a wholly-owned subsidiary of USA Inc."
https://github.com/wookayin/gpustat,📊 A simple command-line utility for querying and monitoring GPU status,"Just less than nvidia-smi?NOTE: This works with NVIDIA Graphics Devices only, no AMD support as of now. Contributions are welcome!Self-Promotion: A web interface of  is available (in alpha)! Check out .Quick InstallationInstall from :pip install gpustat
If you don't have root (sudo) privilege, please try installing  on user namespace: .To install the latest version (master branch) via pip:pip install git+https://github.com/wookayin/gpustat.git@master
NVIDIA Driver Requirements uses . As of now  requires , which is compatible with NVIDIA driver versions R450.00 or higher. Please upgrade the NVIDIA driver if  fails to display process information. If your NVIDIA driver is too old, you can use older  versions (). See  for more details.Python requirementsUsageOptions (Please see  for more details):TipsDefault display[0] GeForce GTX Titan X | 77°C,  96 % | 11848 / 12287 MB | python/52046(11821M)
ChangelogSee License"
https://github.com/bayesian-optimization/BayesianOptimization,A Python implementation of global optimization with gaussian processes.,"Bayesian OptimizationPure Python implementation of bayesian global optimization with gaussianprocesses.$ pip install bayesian-optimization
$ conda install -c conda-forge bayesian-optimization
This is a constrained global optimization package built upon bayesian inferenceand gaussian process, that attempts to find the maximum value of an unknownfunction in as few iterations as possible. This technique is particularlysuited for optimization of high cost functions, situations where the balancebetween exploration and exploitation is important.Quick StartSee below for a quick tour over the basics of the Bayesian Optimization package. More detailed information, other advanced features, and tips on usage/implementation can be found in the  folder. I suggest that you:How does it work?Bayesian optimization works by constructing a posterior distribution of functions (gaussian process) that best describes the function you want to optimize. As the number of observations grows, the posterior distribution improves, and the algorithm becomes more certain of which regions in parameter space are worth exploring and which are not, as seen in the picture below.As you iterate over and over, the algorithm balances its needs of exploration and exploitation taking into account what it knows about the target function. At each step a Gaussian Process is fitted to the known samples (points previously explored), and the posterior distribution, combined with a exploration strategy (such as UCB (Upper Confidence Bound), or EI (Expected Improvement)), are used to determine the next point that should be explored (see the gif below).This process is designed to minimize the number of steps required to find a combination of parameters that are close to the optimal combination. To do so, this method uses a proxy optimization problem (finding the maximum of the acquisition function) that, albeit still a hard problem, is cheaper (in the computational sense) and common tools can be employed. Therefore Bayesian Optimization is most adequate for situations where sampling the function to be optimized is a very expensive endeavor. See the references for a proper discussion of this method.This project is under active development, if you find a bug, or anything thatneeds correction, please let me know.Basic tour of the Bayesian Optimization package1. Specifying the function to be optimizedThis is a function optimization package, therefore the first and most important ingredient is, of course, the function to be optimized.DISCLAIMER: We know exactly how the output of the function below depends on its parameter. Obviously this is just an example, and you shouldn't expect to know it in a real scenario. However, it should be clear that you don't need to. All you need in order to use this package (and more generally, this technique) is a function  that takes a known set of parameters and outputs a real number.def black_box_function(x, y):
    """"""Function with unknown internals we wish to maximize.

    This is just serving as an example, for all intents and
    purposes think of the internals of this function, i.e.: the process
    which generates its output values, as unknown.
    """"""
    return -x ** 2 - (y - 1) ** 2 + 1
2. Getting StartedAll we need to get started is to instantiate a  object specifying a function to be optimized , and its parameters with their corresponding bounds, . This is a constrained optimization technique, so you must specify the minimum and maximum values that can be probed for each parameter in order for it to workfrom bayes_opt import BayesianOptimization

# Bounded region of parameter space
pbounds = {'x': (2, 4), 'y': (-3, 3)}

optimizer = BayesianOptimization(
    f=black_box_function,
    pbounds=pbounds,
    random_state=1,
)
The BayesianOptimization object will work out of the box without much tuning needed. The main method you should be aware of is , which does exactly what you think it does.There are many parameters you can pass to maximize, nonetheless, the most important ones are:optimizer.maximize(
    init_points=2,
    n_iter=3,
)
|   iter    |  target   |     x     |     y     |
-------------------------------------------------
|  1        | -7.135    |  2.834    |  1.322    |
|  2        | -7.78     |  2.0      | -1.186    |
|  3        | -19.0     |  4.0      |  3.0      |
|  4        | -16.3     |  2.378    | -2.413    |
|  5        | -4.441    |  2.105    | -0.005822 |
=================================================
The best combination of parameters and target value found can be accessed via the property .print(optimizer.max)
>>> {'target': -4.441293113411222, 'params': {'y': -0.005822117636089974, 'x': 2.104665051994087}}
While the list of all parameters probed and their corresponding target values is available via the property .for i, res in enumerate(optimizer.res):
    print(""Iteration {}: \n\t{}"".format(i, res))

>>> Iteration 0:
>>>     {'target': -7.135455292718879, 'params': {'y': 1.3219469606529488, 'x': 2.8340440094051482}}
>>> Iteration 1:
>>>     {'target': -7.779531005607566, 'params': {'y': -1.1860045642089614, 'x': 2.0002287496346898}}
>>> Iteration 2:
>>>     {'target': -19.0, 'params': {'y': 3.0, 'x': 4.0}}
>>> Iteration 3:
>>>     {'target': -16.29839645063864, 'params': {'y': -2.412527795983739, 'x': 2.3776144540856503}}
>>> Iteration 4:
>>>     {'target': -4.441293113411222, 'params': {'y': -0.005822117636089974, 'x': 2.104665051994087}}
2.1 Changing boundsDuring the optimization process you may realize the bounds chosen for some parameters are not adequate. For these situations you can invoke the method  to alter them. You can pass any combination of existing parameters and their associated new bounds.optimizer.set_bounds(new_bounds={""x"": (-2, 3)})

optimizer.maximize(
    init_points=0,
    n_iter=5,
)
|   iter    |  target   |     x     |     y     |
-------------------------------------------------
|  6        | -5.145    |  2.115    | -0.2924   |
|  7        | -5.379    |  2.337    |  0.04124  |
|  8        | -3.581    |  1.874    | -0.03428  |
|  9        | -2.624    |  1.702    |  0.1472   |
|  10       | -1.762    |  1.442    |  0.1735   |
=================================================
2.2 Sequential Domain ReductionSometimes the initial boundaries specified for a problem are too wide, and adding points to improve the response surface in regions of the solution domain is extraneous. Other times the cost function is very expensive to compute, and minimizing the number of calls is extremely beneficial.When it's worthwhile to converge on an optimal point quickly rather than try to find the optimal point, contracting the domain around the current optimal value as the search progresses can speed up the search progress considerably. Using the  the bounds of the problem can be panned and zoomed dynamically in an attempt to improve convergence.An example of using the  is shown in the . More information about this method can be found in the paper .3. Guiding the optimizationIt is often the case that we have an idea of regions of the parameter space where the maximum of our function might lie. For these situations the  object allows the user to specify points to be probed. By default these will be explored lazily (), meaning these points will be evaluated only the next time you call . This probing process happens before the gaussian process takes over.Parameters can be passed as dictionaries or as an iterable.optimizer.probe(
    params={""x"": 0.5, ""y"": 0.7},
    lazy=True,
)

optimizer.probe(
    params=[-0.3, 0.1],
    lazy=True,
)

# Will probe only the two points specified above
optimizer.maximize(init_points=0, n_iter=0)
|   iter    |  target   |     x     |     y     |
-------------------------------------------------
|  11       |  0.66     |  0.5      |  0.7      |
|  12       |  0.1      | -0.3      |  0.1      |
=================================================
4. Saving, loading and restartingBy default you can follow the progress of your optimization by setting  when instantiating the  object. If you need more control over logging/alerting you will need to use an observer. For more information about observers checkout the advanced tour notebook. Here we will only see how to use the native  object to save to and load progress from files.4.1 Saving progressfrom bayes_opt.logger import JSONLogger
from bayes_opt.event import Events
The observer paradigm works by:The  object fires a number of internal events during optimization, in particular, everytime it probes the function and obtains a new parameter-target combination it will fire an  event, which our logger will listen to.Caveat: The logger will not look back at previously probed points.logger = JSONLogger(path=""./logs.log"")
optimizer.subscribe(Events.OPTIMIZATION_STEP, logger)

# Results will be saved in ./logs.log
optimizer.maximize(
    init_points=2,
    n_iter=3,
)
By default the previous data in the json file is removed. If you want to keep working with the same logger, the  parameter in  should be set to False.4.2 Loading progressNaturally, if you stored progress you will be able to load that onto a new instance of . The easiest way to do it is by invoking the  function, from the  submodule.from bayes_opt.util import load_logs


new_optimizer = BayesianOptimization(
    f=black_box_function,
    pbounds={""x"": (-2, 2), ""y"": (-2, 2)},
    verbose=2,
    random_state=7,
)

# New optimizer is loaded with previously seen points
load_logs(new_optimizer, logs=[""./logs.log""]);
Next StepsThis introduction covered the most basic functionality of the package. Checkout the  and  notebooks in the example folder, where you will find detailed explanations and other more advanced functionality. Also, browse the examples folder for implementation tips and ideas.InstallationInstallationThe latest release can be obtained by two ways:The bleeding edge version can be installed with:pip install git+https://github.com/fmfn/BayesianOptimization.git
If you prefer, you can clone it and run the setup.py file. Use the followingcommands to get a copy from Github and install all dependencies:git clone https://github.com/fmfn/BayesianOptimization.git
cd BayesianOptimization
python setup.py install
CitationIf you used this package in your research and is interested in citing it here's how you do it:@Misc{,
    author = {Fernando Nogueira},
    title = {{Bayesian Optimization}: Open source constrained global optimization tool for {Python}},
    year = {2014--},
    url = "" https://github.com/fmfn/BayesianOptimization""
}
DependenciesReferences:"
https://github.com/burnash/gspread,Google Sheets Python API,"Google Spreadsheets Python API v4Simple interface for working with Google Sheets.Features:Installationpip install gspread
Requires Python 3.7+Basic Usageimport gspread

gc = gspread.service_account()

# Open a sheet from a spreadsheet in one go
wks = gc.open(""Where is the money Lebowski?"").sheet1

# Update a range of cells using the top left corner address
wks.update('A1', [[1, 2], [3, 4]])

# Or update a single cell
wks.update('B42', ""it's down there somewhere, let me take another look."")

# Format the header
wks.format('A1:B1', {'textFormat': {'bold': True}})
v5.12 to v6.0 Migration GuideUpgrade from Python 3.7Python 3.7 is . gspread v6 requires a minimum of Python 3.8.Change  argumentsThe first two arguments ( & ) have swapped (to  & ). Either swap them (works in v6 only), or use named arguments (works in v5 & v6).As well,  can no longer be a list, and must be a 2D array.- file.sheet1.update([""54""], ""B2"")
+ file.sheet1.update(range_name=""I7"", values=[[""54""]])
Change colors from dictionary to textv6 uses hexadecimal color representation. Change all colors to hex. You can use the compatibility function  to convert a dictionary to a hex string.- tab_color = {""red"": 1, ""green"": 0.5, ""blue"": 1}
+ tab_color = ""#FF7FFF""
file.sheet1.update_tab_color(tab_color)
Switch lastUpdateTime from property to method- age = spreadsheet.lastUpdateTime
+ age = spreadsheet.get_lastUpdateTime()
Silence warningsIn version 5 there are many warnings to mark deprecated feature/functions/methods.They can be silenced by setting the  environment variable to More ExamplesOpening a Spreadsheet# You can open a spreadsheet by its title as it appears in Google Docs
sh = gc.open('My poor gym results') # <-- Look ma, no keys!

# If you want to be specific, use a key (which can be extracted from
# the spreadsheet's url)
sht1 = gc.open_by_key('0BmgG6nO_6dprdS1MN3d3MkdPa142WFRrdnRRUWl1UFE')

# Or, if you feel really lazy to extract that key, paste the entire url
sht2 = gc.open_by_url('https://docs.google.com/spreadsheet/ccc?key=0Bm...FE&hl')
Creating a Spreadsheetsh = gc.create('A new spreadsheet')

# But that new spreadsheet will be visible only to your script's account.
# To be able to access newly created spreadsheet you *must* share it
# with your email. Which brings us to…
Sharing a Spreadsheetsh.share('otto@example.com', perm_type='user', role='writer')
Selecting a Worksheet# Select worksheet by index. Worksheet indexes start from zero
worksheet = sh.get_worksheet(0)

# By title
worksheet = sh.worksheet(""January"")

# Most common case: Sheet1
worksheet = sh.sheet1

# Get a list of all worksheets
worksheet_list = sh.worksheets()
Creating a Worksheetworksheet = sh.add_worksheet(title=""A worksheet"", rows=""100"", cols=""20"")
Deleting a Worksheetsh.del_worksheet(worksheet)
Getting a Cell Value# With label
val = worksheet.get('B1').first()

# With coords
val = worksheet.cell(1, 2).value
Getting All Values From a Row or a Column# Get all values from the first row
values_list = worksheet.row_values(1)

# Get all values from the first column
values_list = worksheet.col_values(1)
Getting All Values From a Worksheet as a List of Listslist_of_lists = worksheet.get_values()
Getting a range of valuesReceive only the cells with a value in them>>> worksheet.get_values(""A1:B4"")
[['A1', 'B1'], ['A2']]
Receive a lists of lists matching the requested sizeregardless if values are empty or not>>> worksheet.get_values(""A1:B4"", maintain_size=True)
[['A1', 'B1'], ['A2', ''], ['', ''], ['', '']]
Finding a Cell# Find a cell with exact string value
cell = worksheet.find(""Dough"")

print(""Found something at R%sC%s"" % (cell.row, cell.col))

# Find a cell matching a regular expression
amount_re = re.compile(r'(Big|Enormous) dough')
cell = worksheet.find(amount_re)
Finding All Matched Cells# Find all cells with string value
cell_list = worksheet.findall(""Rug store"")

# Find all cells with regexp
criteria_re = re.compile(r'(Small|Room-tiering) rug')
cell_list = worksheet.findall(criteria_re)
Updating Cells# Update a single cell
worksheet.update('B1', 'Bingo!')

# Update a range
worksheet.update('A1:B2', [[1, 2], [3, 4]])

# Update multiple ranges at once
worksheet.batch_update([{
    'range': 'A1:B2',
    'values': [['A1', 'B1'], ['A2', 'B2']],
}, {
    'range': 'J42:K43',
    'values': [[1, 2], [3, 4]],
}])
Documentation Ask QuestionsThe best way to get an answer to a question is to ask on .ContributorsHow to ContributePlease make sure to take a moment and read the .Report IssuesPlease report bugs and suggest features via the .Before opening an issue, search the tracker for possible duplicates. If you find a duplicate, please add a comment saying that you encountered the problem as well.Improve Documentation is as important as code. If you know how to make it more consistent, readable and clear, please submit a pull request. The documentation files are in  folder, use  markup and rendered by .Contribute codePlease make sure to read the  before making a pull request."
https://github.com/luyishisi/Anti-Anti-Spider,越来越多的网站具有反爬虫特性，有的用图片隐藏关键数据，有的使用反人类的验证码，建立反反爬虫的代码仓库，通过与不同特性的网站做斗争（无恶意）提高技术。（欢迎提交难以采集的网站）（因工作原因，项目暂停） ,"基于CNN的验证码图片识别简介本项目采用alexnet模型和letnet模型，可根据实际需要选择(在train_model.py中的train函数修改即可)95.5%
作者有话说不知不觉这个git库伴随我从16到到20年，带给我自己最棒的一段人生旅程，
整理了这份文档，希望任何想学习图片识别，玩玩卷积神经网络的同学可以最便捷的上手体验。
请谨慎使用技术，仅支持学习，不支持任何黑灰产相关
可参看：https://www.urlteam.cn/?p=1893 https://www.urlteam.cn/?p=1406
原先的Anti-Anti-Spider 全部内容移动到 原Anti-Anti-Spider 目录下
有何疑问可邮件 543429245@qq.com 咨询
模型文件下载 如果出现无法解压，可以使用：
https://www.urlteam.cn/%E5%8F%AF%E7%94%A8%E8%AE%AD%E7%BB%83%E9%9B%86%E4%B8%8E%E8%AE%AD%E5%A5%BD%E7%9A%84%E6%A8%A1%E5%9E%8B.zip
Alexnet 模型结构根据验证码的复杂度不同，训练的时间也会有较大的不同使用方法1.开始训练样本前，修改conf/config.json
2.将预处理过的数据集分成验证集和训练集，放到sample目录下
3.运行train_model.py开始训练，训练完成的模型保存至model_result中
4.将训练好的模型放置model_result，运行cnn_models/recognition.py，选定验证码，即可看到模型效果
环境配置TensorFlow CPU版本安装：TensorFlow GPU版本安装：GUP版本的安装比较麻烦，需要安装CUDA和cuDNN才能使tensorflow调动GPU下图为TensorFlow，Python，CUDA与cuDNN之间的版本对应关系：CUDA与cuDNN安装过程主要有两步：项目结构├─cnn_models
│  ├─cnn_model.py		# CNN网络类
│  └─recognition.py		# 验证训练结果
├─conf
│  └─config.json		# 配置文件
├─logs			# 模型训练日志
├─model_result	# 模型保存地址
│  └─1040		# 一套训练完成的验证码训练集及对应模型
├─sample
│  ├─test		# 训练集（训练集与验证集一般是对总数据集9:1分割）
│  └─train		# 验证集
├─src			# 配置环境所需的工具，可根据自身情况到网上下载
├─train_model.py		# 训练程序
└─verify_sample.py		# 制作数据集（打标签加图片预处理）
图片预处理注意事项alexnet输入必须为227*227的图片，所有图片预处理时可通过PIL中的函数线性转换图片形状，或者缩放后粘贴到227*227的背景中。
"
https://github.com/ytdl-org/youtube-dl,Command-line program to download videos from YouTube.com and other video sites,"youtube-dl - download videos from youtube.com or other video platformsINSTALLATIONTo install it right away for all UNIX users (Linux, macOS, etc.), type:sudo curl -L https://yt-dl.org/downloads/latest/youtube-dl -o /usr/local/bin/youtube-dl
sudo chmod a+rx /usr/local/bin/youtube-dl
If you do not have curl, you can alternatively use a recent wget:sudo wget https://yt-dl.org/downloads/latest/youtube-dl -O /usr/local/bin/youtube-dl
sudo chmod a+rx /usr/local/bin/youtube-dl
Windows users can  and place it in any location on their  except for  (e.g. do not put in ).You can also use pip:sudo -H pip install --upgrade youtube-dl
This command will update youtube-dl if you have already installed it. See the  for more information.macOS users can install youtube-dl with :brew install youtube-dl
Or with :sudo port install youtube-dl
Alternatively, refer to the  for how to check out and work with the git repository. For further options, including PGP signatures, see the .DESCRIPTIONyoutube-dl is a command-line program to download videos from YouTube.com and a few more sites. It requires the Python interpreter, version 2.6, 2.7, or 3.2+, and it is not platform specific. It should work on your Unix box, on Windows or on macOS. It is released to the public domain, which means you can modify it, redistribute it or use it however you like.youtube-dl [OPTIONS] URL [URL...]
OPTIONS-h, --help                           Print this help text and exit
--version                            Print program version and exit
-U, --update                         Update this program to latest version.
                                     Make sure that you have sufficient
                                     permissions (run with sudo if needed)
-i, --ignore-errors                  Continue on download errors, for
                                     example to skip unavailable videos in a
                                     playlist
--abort-on-error                     Abort downloading of further videos (in
                                     the playlist or the command line) if an
                                     error occurs
--dump-user-agent                    Display the current browser
                                     identification
--list-extractors                    List all supported extractors
--extractor-descriptions             Output descriptions of all supported
                                     extractors
--force-generic-extractor            Force extraction to use the generic
                                     extractor
--default-search PREFIX              Use this prefix for unqualified URLs.
                                     For example ""gvsearch2:"" downloads two
                                     videos from google videos for youtube-
                                     dl ""large apple"". Use the value ""auto""
                                     to let youtube-dl guess (""auto_warning""
                                     to emit a warning when guessing).
                                     ""error"" just throws an error. The
                                     default value ""fixup_error"" repairs
                                     broken URLs, but emits an error if this
                                     is not possible instead of searching.
--ignore-config                      Do not read configuration files. When
                                     given in the global configuration file
                                     /etc/youtube-dl.conf: Do not read the
                                     user configuration in
                                     ~/.config/youtube-dl/config
                                     (%APPDATA%/youtube-dl/config.txt on
                                     Windows)
--config-location PATH               Location of the configuration file;
                                     either the path to the config or its
                                     containing directory.
--flat-playlist                      Do not extract the videos of a
                                     playlist, only list them.
--mark-watched                       Mark videos watched (YouTube only)
--no-mark-watched                    Do not mark videos watched (YouTube
                                     only)
--no-color                           Do not emit color codes in output
Network Options:--proxy URL                          Use the specified HTTP/HTTPS/SOCKS
                                     proxy. To enable SOCKS proxy, specify a
                                     proper scheme. For example
                                     socks5://127.0.0.1:1080/. Pass in an
                                     empty string (--proxy """") for direct
                                     connection
--socket-timeout SECONDS             Time to wait before giving up, in
                                     seconds
--source-address IP                  Client-side IP address to bind to
-4, --force-ipv4                     Make all connections via IPv4
-6, --force-ipv6                     Make all connections via IPv6
Geo Restriction:--geo-verification-proxy URL         Use this proxy to verify the IP address
                                     for some geo-restricted sites. The
                                     default proxy specified by --proxy (or
                                     none, if the option is not present) is
                                     used for the actual downloading.
--geo-bypass                         Bypass geographic restriction via
                                     faking X-Forwarded-For HTTP header
--no-geo-bypass                      Do not bypass geographic restriction
                                     via faking X-Forwarded-For HTTP header
--geo-bypass-country CODE            Force bypass geographic restriction
                                     with explicitly provided two-letter ISO
                                     3166-2 country code
--geo-bypass-ip-block IP_BLOCK       Force bypass geographic restriction
                                     with explicitly provided IP block in
                                     CIDR notation
Video Selection:--playlist-start NUMBER              Playlist video to start at (default is
                                     1)
--playlist-end NUMBER                Playlist video to end at (default is
                                     last)
--playlist-items ITEM_SPEC           Playlist video items to download.
                                     Specify indices of the videos in the
                                     playlist separated by commas like: ""--
                                     playlist-items 1,2,5,8"" if you want to
                                     download videos indexed 1, 2, 5, 8 in
                                     the playlist. You can specify range: ""
                                     --playlist-items 1-3,7,10-13"", it will
                                     download the videos at index 1, 2, 3,
                                     7, 10, 11, 12 and 13.
--match-title REGEX                  Download only matching titles (regex or
                                     caseless sub-string)
--reject-title REGEX                 Skip download for matching titles
                                     (regex or caseless sub-string)
--max-downloads NUMBER               Abort after downloading NUMBER files
--min-filesize SIZE                  Do not download any videos smaller than
                                     SIZE (e.g. 50k or 44.6m)
--max-filesize SIZE                  Do not download any videos larger than
                                     SIZE (e.g. 50k or 44.6m)
--date DATE                          Download only videos uploaded in this
                                     date
--datebefore DATE                    Download only videos uploaded on or
                                     before this date (i.e. inclusive)
--dateafter DATE                     Download only videos uploaded on or
                                     after this date (i.e. inclusive)
--min-views COUNT                    Do not download any videos with less
                                     than COUNT views
--max-views COUNT                    Do not download any videos with more
                                     than COUNT views
--match-filter FILTER                Generic video filter. Specify any key
                                     (see the ""OUTPUT TEMPLATE"" for a list
                                     of available keys) to match if the key
                                     is present, !key to check if the key is
                                     not present, key > NUMBER (like
                                     ""comment_count > 12"", also works with
                                     >=, <, <=, !=, =) to compare against a
                                     number, key = 'LITERAL' (like ""uploader
                                     = 'Mike Smith'"", also works with !=) to
                                     match against a string literal and & to
                                     require multiple matches. Values which
                                     are not known are excluded unless you
                                     put a question mark (?) after the
                                     operator. For example, to only match
                                     videos that have been liked more than
                                     100 times and disliked less than 50
                                     times (or the dislike functionality is
                                     not available at the given service),
                                     but who also have a description, use
                                     --match-filter ""like_count > 100 &
                                     dislike_count <? 50 & description"" .
--no-playlist                        Download only the video, if the URL
                                     refers to a video and a playlist.
--yes-playlist                       Download the playlist, if the URL
                                     refers to a video and a playlist.
--age-limit YEARS                    Download only videos suitable for the
                                     given age
--download-archive FILE              Download only videos not listed in the
                                     archive file. Record the IDs of all
                                     downloaded videos in it.
--include-ads                        Download advertisements as well
                                     (experimental)
Download Options:-r, --limit-rate RATE                Maximum download rate in bytes per
                                     second (e.g. 50K or 4.2M)
-R, --retries RETRIES                Number of retries (default is 10), or
                                     ""infinite"".
--fragment-retries RETRIES           Number of retries for a fragment
                                     (default is 10), or ""infinite"" (DASH,
                                     hlsnative and ISM)
--skip-unavailable-fragments         Skip unavailable fragments (DASH,
                                     hlsnative and ISM)
--abort-on-unavailable-fragment      Abort downloading when some fragment is
                                     not available
--keep-fragments                     Keep downloaded fragments on disk after
                                     downloading is finished; fragments are
                                     erased by default
--buffer-size SIZE                   Size of download buffer (e.g. 1024 or
                                     16K) (default is 1024)
--no-resize-buffer                   Do not automatically adjust the buffer
                                     size. By default, the buffer size is
                                     automatically resized from an initial
                                     value of SIZE.
--http-chunk-size SIZE               Size of a chunk for chunk-based HTTP
                                     downloading (e.g. 10485760 or 10M)
                                     (default is disabled). May be useful
                                     for bypassing bandwidth throttling
                                     imposed by a webserver (experimental)
--playlist-reverse                   Download playlist videos in reverse
                                     order
--playlist-random                    Download playlist videos in random
                                     order
--xattr-set-filesize                 Set file xattribute ytdl.filesize with
                                     expected file size
--hls-prefer-native                  Use the native HLS downloader instead
                                     of ffmpeg
--hls-prefer-ffmpeg                  Use ffmpeg instead of the native HLS
                                     downloader
--hls-use-mpegts                     Use the mpegts container for HLS
                                     videos, allowing to play the video
                                     while downloading (some players may not
                                     be able to play it)
--external-downloader COMMAND        Use the specified external downloader.
                                     Currently supports aria2c,avconv,axel,c
                                     url,ffmpeg,httpie,wget
--external-downloader-args ARGS      Give these arguments to the external
                                     downloader
Filesystem Options:-a, --batch-file FILE                File containing URLs to download ('-'
                                     for stdin), one URL per line. Lines
                                     starting with '#', ';' or ']' are
                                     considered as comments and ignored.
--id                                 Use only video ID in file name
-o, --output TEMPLATE                Output filename template, see the
                                     ""OUTPUT TEMPLATE"" for all the info
--output-na-placeholder PLACEHOLDER  Placeholder value for unavailable meta
                                     fields in output filename template
                                     (default is ""NA"")
--autonumber-start NUMBER            Specify the start value for
                                     %(autonumber)s (default is 1)
--restrict-filenames                 Restrict filenames to only ASCII
                                     characters, and avoid ""&"" and spaces in
                                     filenames
-w, --no-overwrites                  Do not overwrite files
-c, --continue                       Force resume of partially downloaded
                                     files. By default, youtube-dl will
                                     resume downloads if possible.
--no-continue                        Do not resume partially downloaded
                                     files (restart from beginning)
--no-part                            Do not use .part files - write directly
                                     into output file
--no-mtime                           Do not use the Last-modified header to
                                     set the file modification time
--write-description                  Write video description to a
                                     .description file
--write-info-json                    Write video metadata to a .info.json
                                     file
--write-annotations                  Write video annotations to a
                                     .annotations.xml file
--load-info-json FILE                JSON file containing the video
                                     information (created with the ""--write-
                                     info-json"" option)
--cookies FILE                       File to read cookies from and dump
                                     cookie jar in
--cache-dir DIR                      Location in the filesystem where
                                     youtube-dl can store some downloaded
                                     information permanently. By default
                                     $XDG_CACHE_HOME/youtube-dl or
                                     ~/.cache/youtube-dl . At the moment,
                                     only YouTube player files (for videos
                                     with obfuscated signatures) are cached,
                                     but that may change.
--no-cache-dir                       Disable filesystem caching
--rm-cache-dir                       Delete all filesystem cache files
Thumbnail Options:--write-thumbnail                    Write thumbnail image to disk
--write-all-thumbnails               Write all thumbnail image formats to
                                     disk
--list-thumbnails                    Simulate and list all available
                                     thumbnail formats
Verbosity / Simulation Options:-q, --quiet                          Activate quiet mode
--no-warnings                        Ignore warnings
-s, --simulate                       Do not download the video and do not
                                     write anything to disk
--skip-download                      Do not download the video
-g, --get-url                        Simulate, quiet but print URL
-e, --get-title                      Simulate, quiet but print title
--get-id                             Simulate, quiet but print id
--get-thumbnail                      Simulate, quiet but print thumbnail URL
--get-description                    Simulate, quiet but print video
                                     description
--get-duration                       Simulate, quiet but print video length
--get-filename                       Simulate, quiet but print output
                                     filename
--get-format                         Simulate, quiet but print output format
-j, --dump-json                      Simulate, quiet but print JSON
                                     information. See the ""OUTPUT TEMPLATE""
                                     for a description of available keys.
-J, --dump-single-json               Simulate, quiet but print JSON
                                     information for each command-line
                                     argument. If the URL refers to a
                                     playlist, dump the whole playlist
                                     information in a single line.
--print-json                         Be quiet and print the video
                                     information as JSON (video is still
                                     being downloaded).
--newline                            Output progress bar as new lines
--no-progress                        Do not print progress bar
--console-title                      Display progress in console titlebar
-v, --verbose                        Print various debugging information
--dump-pages                         Print downloaded pages encoded using
                                     base64 to debug problems (very verbose)
--write-pages                        Write downloaded intermediary pages to
                                     files in the current directory to debug
                                     problems
--print-traffic                      Display sent and read HTTP traffic
-C, --call-home                      Contact the youtube-dl server for
                                     debugging
--no-call-home                       Do NOT contact the youtube-dl server
                                     for debugging
Workarounds:--encoding ENCODING                  Force the specified encoding
                                     (experimental)
--no-check-certificate               Suppress HTTPS certificate validation
--prefer-insecure                    Use an unencrypted connection to
                                     retrieve information about the video.
                                     (Currently supported only for YouTube)
--user-agent UA                      Specify a custom user agent
--referer URL                        Specify a custom referer, use if the
                                     video access is restricted to one
                                     domain
--add-header FIELD:VALUE             Specify a custom HTTP header and its
                                     value, separated by a colon ':'. You
                                     can use this option multiple times
--bidi-workaround                    Work around terminals that lack
                                     bidirectional text support. Requires
                                     bidiv or fribidi executable in PATH
--sleep-interval SECONDS             Number of seconds to sleep before each
                                     download when used alone or a lower
                                     bound of a range for randomized sleep
                                     before each download (minimum possible
                                     number of seconds to sleep) when used
                                     along with --max-sleep-interval.
--max-sleep-interval SECONDS         Upper bound of a range for randomized
                                     sleep before each download (maximum
                                     possible number of seconds to sleep).
                                     Must only be used along with --min-
                                     sleep-interval.
Video Format Options:-f, --format FORMAT                  Video format code, see the ""FORMAT
                                     SELECTION"" for all the info
--all-formats                        Download all available video formats
--prefer-free-formats                Prefer free video formats unless a
                                     specific one is requested
-F, --list-formats                   List all available formats of requested
                                     videos
--youtube-skip-dash-manifest         Do not download the DASH manifests and
                                     related data on YouTube videos
--merge-output-format FORMAT         If a merge is required (e.g.
                                     bestvideo+bestaudio), output to given
                                     container format. One of mkv, mp4, ogg,
                                     webm, flv. Ignored if no merge is
                                     required
Subtitle Options:--write-sub                          Write subtitle file
--write-auto-sub                     Write automatically generated subtitle
                                     file (YouTube only)
--all-subs                           Download all the available subtitles of
                                     the video
--list-subs                          List all available subtitles for the
                                     video
--sub-format FORMAT                  Subtitle format, accepts formats
                                     preference, for example: ""srt"" or
                                     ""ass/srt/best""
--sub-lang LANGS                     Languages of the subtitles to download
                                     (optional) separated by commas, use
                                     --list-subs for available language tags
Authentication Options:-u, --username USERNAME              Login with this account ID
-p, --password PASSWORD              Account password. If this option is
                                     left out, youtube-dl will ask
                                     interactively.
-2, --twofactor TWOFACTOR            Two-factor authentication code
-n, --netrc                          Use .netrc authentication data
--video-password PASSWORD            Video password (vimeo, youku)
Adobe Pass Options:--ap-mso MSO                         Adobe Pass multiple-system operator (TV
                                     provider) identifier, use --ap-list-mso
                                     for a list of available MSOs
--ap-username USERNAME               Multiple-system operator account login
--ap-password PASSWORD               Multiple-system operator account
                                     password. If this option is left out,
                                     youtube-dl will ask interactively.
--ap-list-mso                        List all supported multiple-system
                                     operators
Post-processing Options:-x, --extract-audio                  Convert video files to audio-only files
                                     (requires ffmpeg/avconv and
                                     ffprobe/avprobe)
--audio-format FORMAT                Specify audio format: ""best"", ""aac"",
                                     ""flac"", ""mp3"", ""m4a"", ""opus"", ""vorbis"",
                                     or ""wav""; ""best"" by default; No effect
                                     without -x
--audio-quality QUALITY              Specify ffmpeg/avconv audio quality,
                                     insert a value between 0 (better) and 9
                                     (worse) for VBR or a specific bitrate
                                     like 128K (default 5)
--recode-video FORMAT                Encode the video to another format if
                                     necessary (currently supported:
                                     mp4|flv|ogg|webm|mkv|avi)
--postprocessor-args ARGS            Give these arguments to the
                                     postprocessor
-k, --keep-video                     Keep the video file on disk after the
                                     post-processing; the video is erased by
                                     default
--no-post-overwrites                 Do not overwrite post-processed files;
                                     the post-processed files are
                                     overwritten by default
--embed-subs                         Embed subtitles in the video (only for
                                     mp4, webm and mkv videos)
--embed-thumbnail                    Embed thumbnail in the audio as cover
                                     art
--add-metadata                       Write metadata to the video file
--metadata-from-title FORMAT         Parse additional metadata like song
                                     title / artist from the video title.
                                     The format syntax is the same as
                                     --output. Regular expression with named
                                     capture groups may also be used. The
                                     parsed parameters replace existing
                                     values. Example: --metadata-from-title
                                     ""%(artist)s - %(title)s"" matches a
                                     title like ""Coldplay - Paradise"".
                                     Example (regex): --metadata-from-title
                                     ""(?P<artist>.+?) - (?P<title>.+)""
--xattrs                             Write metadata to the video file's
                                     xattrs (using dublin core and xdg
                                     standards)
--fixup POLICY                       Automatically correct known faults of
                                     the file. One of never (do nothing),
                                     warn (only emit a warning),
                                     detect_or_warn (the default; fix file
                                     if we can, warn otherwise)
--prefer-avconv                      Prefer avconv over ffmpeg for running
                                     the postprocessors
--prefer-ffmpeg                      Prefer ffmpeg over avconv for running
                                     the postprocessors (default)
--ffmpeg-location PATH               Location of the ffmpeg/avconv binary;
                                     either the path to the binary or its
                                     containing directory.
--exec CMD                           Execute a command on the file after
                                     downloading and post-processing,
                                     similar to find's -exec syntax.
                                     Example: --exec 'adb push {}
                                     /sdcard/Music/ && rm {}'
--convert-subs FORMAT                Convert the subtitles to other format
                                     (currently supported: srt|ass|vtt|lrc)
CONFIGURATIONYou can configure youtube-dl by placing any supported command line option to a configuration file. On Linux and macOS, the system wide configuration file is located at  and the user wide configuration file at . On Windows, the user wide configuration file locations are  or . Note that by default configuration file may not exist so you may need to create it yourself.For example, with the following configuration file youtube-dl will always extract the audio, not copy the mtime, use a proxy and save all videos under  directory in your home directory:# Lines starting with # are comments

# Always extract audio
-x

# Do not copy the mtime
--no-mtime

# Use this proxy
--proxy 127.0.0.1:3128

# Save all videos under Movies directory in your home directory
-o ~/Movies/%(title)s.%(ext)s
Note that options in configuration file are just the same options aka switches used in regular command line calls thus there must be no whitespace after  or , e.g.  or  but not  or .You can use  if you want to disable the configuration file for a particular youtube-dl run.You can also use  if you want to use custom configuration file for a particular youtube-dl run.Authentication with  fileYou may also want to configure automatic credentials storage for extractors that support authentication (by providing login and password with  and ) in order not to pass credentials as command line arguments on every youtube-dl execution and prevent tracking plain text passwords in the shell command history. You can achieve this using a  on a per extractor basis. For that you will need to create a  file in your  and restrict permissions to read/write by only you:touch $HOME/.netrc
chmod a-rwx,u+rw $HOME/.netrc
After that you can add credentials for an extractor in the following format, where extractor is the name of the extractor in lowercase:machine <extractor> login <login> password <password>
For example:machine youtube login myaccount@gmail.com password my_youtube_password
machine twitch login my_twitch_account_name password my_twitch_password
To activate authentication with the  file you should pass  to youtube-dl or place it in the .On Windows you may also need to setup the  environment variable manually. For example:set HOME=%USERPROFILE%
OUTPUT TEMPLATEThe  option allows users to indicate a template for the output file names.tl;dr: .The basic usage is not to set any template arguments when downloading a single file, like in . However, it may contain special sequences that will be replaced when downloading each video. The special sequences may be formatted according to . For example,  or . To clarify, that is a percent symbol followed by a name in parentheses, followed by formatting operations. Allowed names along with sequence type are:Available for the video that belongs to some logical chapter or section:Available for the video that is an episode of some series or programme:Available for the media that is a track or a part of a music album:Each aforementioned sequence when referenced in an output template will be replaced by the actual value corresponding to the sequence name. Note that some of the sequences are not guaranteed to be present since they depend on the metadata obtained by a particular extractor. Such sequences will be replaced with placeholder value provided with  ( by default).For example for  and an mp4 video with title  and id , this will result in a  file created in the current directory.For numeric sequences you can use numeric related formatting, for example,  will result in a string with view count padded with zeros up to 5 characters, like in .Output templates can also contain arbitrary hierarchical path, e.g.  which will result in downloading each video in a directory corresponding to this path template. Any missing directory will be automatically created for you.To use percent literals in an output template use . To output to stdout use .The current default template is .In some cases, you don't want special characters such as 中, spaces, or &, such as when transferring the downloaded filename to a Windows system or the filename through an 8bit-unsafe channel. In these cases, add the  flag to get a shorter title.Output template and Windows batch filesIf you are using an output template inside a Windows batch file then you must escape plain percent characters () by doubling, so that  should become . However you should not touch 's that are not plain characters, e.g. environment variables for expansion should stay intact: .Output template examplesNote that on Windows you may need to use double quotes instead of single.$ youtube-dl --get-filename -o '%(title)s.%(ext)s' BaW_jenozKc
youtube-dl test video ''_ä↭𝕐.mp4    # All kinds of weird characters

$ youtube-dl --get-filename -o '%(title)s.%(ext)s' BaW_jenozKc --restrict-filenames
youtube-dl_test_video_.mp4          # A simple file name

# Download YouTube playlist videos in separate directory indexed by video order in a playlist
$ youtube-dl -o '%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s' https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re

# Download all playlists of YouTube channel/user keeping each playlist in separate directory:
$ youtube-dl -o '%(uploader)s/%(playlist)s/%(playlist_index)s - %(title)s.%(ext)s' https://www.youtube.com/user/TheLinuxFoundation/playlists

# Download Udemy course keeping each chapter in separate directory under MyVideos directory in your home
$ youtube-dl -u user -p password -o '~/MyVideos/%(playlist)s/%(chapter_number)s - %(chapter)s/%(title)s.%(ext)s' https://www.udemy.com/java-tutorial/

# Download entire series season keeping each series and each season in separate directory under C:/MyVideos
$ youtube-dl -o ""C:/MyVideos/%(series)s/%(season_number)s - %(season)s/%(episode_number)s - %(episode)s.%(ext)s"" https://videomore.ru/kino_v_detalayah/5_sezon/367617

# Stream the video being downloaded to stdout
$ youtube-dl -o - BaW_jenozKc
FORMAT SELECTIONBy default youtube-dl tries to download the best available quality, i.e. if you want the best quality you don't need to pass any special options, youtube-dl will guess it for you by default.But sometimes you may want to download in a different format, for example when you are on a slow or intermittent connection. The key mechanism for achieving this is so-called format selection based on which you can explicitly specify desired format, select formats based on some criterion or criteria, setup precedence and much more.The general syntax for format selection is  or shorter  where  is a selector expression, i.e. an expression that describes format or formats you would like to download.tl;dr: .The simplest case is requesting a specific format, for example with  you can download the format with format code equal to 22. You can get the list of available format codes for particular video using  or . Note that these format codes are extractor specific.You can also use a file extension (currently , , , , , , , ,  are supported) to download the best quality format of a particular file extension served as a single file, e.g.  will download the best quality format with the  extension served as a single file.You can also use special names to select particular edge case formats:For example, to download the worst quality video-only format you can use .If you want to download multiple videos and they don't have the same formats available, you can specify the order of preference using slashes. Note that slash is left-associative, i.e. formats on the left hand side are preferred, for example  will download format 22 if it's available, otherwise it will download format 17 if it's available, otherwise it will download format 18 if it's available, otherwise it will complain that no suitable formats are available for download.If you want to download several formats of the same video use a comma as a separator, e.g.  will download all these three formats, of course if they are available. Or a more sophisticated example combined with the precedence feature: .You can also filter the video formats by putting a condition in brackets, as in  (or ).The following numeric meta fields can be used with comparisons , , , ,  (equals),  (not equals):Also filtering work for comparisons  (equals),  (starts with),  (ends with),  (contains) and following string meta fields:Any string comparison may be prefixed with negation  in order to produce an opposite comparison, e.g.  (does not contain).Note that none of the aforementioned meta fields are guaranteed to be present since this solely depends on the metadata obtained by particular extractor, i.e. the metadata offered by the video hoster.Formats for which the value is not known are excluded unless you put a question mark () after the operator. You can combine format filters, so  selects up to 720p videos (or videos where the height is not known) with a bitrate of at least 500 KBit/s.You can merge the video and audio of two formats into a single file using  (requires ffmpeg or avconv installed), for example  will download the best video-only format, the best audio-only format and mux them together with ffmpeg/avconv.Format selectors can also be grouped using parentheses, for example if you want to download the best mp4 and webm formats with a height lower than 480 you can use .Since the end of April 2015 and version 2015.04.26, youtube-dl uses  as the default format selection (see , ). If ffmpeg or avconv are installed this results in downloading  and  separately and muxing them together into a single file giving the best overall quality available. Otherwise it falls back to  and results in downloading the best available quality served as a single file.  is also needed for videos that don't come from YouTube because they don't provide the audio and video in two different files. If you want to only download some DASH formats (for example if you are not interested in getting videos with a resolution higher than 1080p), you can add  to your configuration file. Note that if you use youtube-dl to stream to  (and most likely to pipe it to your media player then), i.e. you explicitly specify output template as , youtube-dl still uses  format selection in order to start content delivery immediately to your player and not to wait until  and  are downloaded and muxed.If you want to preserve the old format selection behavior (prior to youtube-dl 2015.04.26), i.e. you want to download the best available quality media served as a single file, you should explicitly specify your choice with . You may want to add it to the  in order not to type it every time you run youtube-dl.Format selection examplesNote that on Windows you may need to use double quotes instead of single.# Download best mp4 format available or any other best if no mp4 available
$ youtube-dl -f 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best'

# Download best format available but no better than 480p
$ youtube-dl -f 'bestvideo[height<=480]+bestaudio/best[height<=480]'

# Download best video only format but no bigger than 50 MB
$ youtube-dl -f 'best[filesize<50M]'

# Download best format available via direct link over HTTP/HTTPS protocol
$ youtube-dl -f '(bestvideo+bestaudio/best)[protocol^=http]'

# Download the best video format and the best audio format without merging them
$ youtube-dl -f 'bestvideo,bestaudio' -o '%(title)s.f%(format_id)s.%(ext)s'
Note that in the last example, an output template is recommended as bestvideo and bestaudio may have the same file name.VIDEO SELECTIONVideos can be filtered by their upload date using the options ,  or . They accept dates in two formats:Examples:# Download only the videos uploaded in the last 6 months
$ youtube-dl --dateafter now-6months

# Download only the videos uploaded on January 1, 1970
$ youtube-dl --date 19700101

$ # Download only the videos uploaded in the 200x decade
$ youtube-dl --dateafter 20000101 --datebefore 20091231
FAQHow do I update youtube-dl?If you've followed , you can simply run  (or, on Linux, ).If you have used pip, a simple  is sufficient to update.If you have installed youtube-dl using a package manager like apt-get or yum, use the standard system update mechanism to update. Note that distribution packages are often outdated. As a rule of thumb, youtube-dl releases at least once a month, and often weekly or even daily. Simply go to https://yt-dl.org to find out the current version. Unfortunately, there is nothing we youtube-dl developers can do if your distribution serves a really outdated version. You can (and should) complain to your distribution in their bugtracker or support forum.As a last resort, you can also uninstall the version installed by your package manager and follow our manual installation instructions. For that, remove the distribution's package, with a line likesudo apt-get remove -y youtube-dl
Afterwards, simply follow :sudo wget https://yt-dl.org/downloads/latest/youtube-dl -O /usr/local/bin/youtube-dl
sudo chmod a+rx /usr/local/bin/youtube-dl
hash -r
Again, from then on you'll be able to update with .youtube-dl is extremely slow to start on WindowsAdd a file exclusion for  in Windows Defender settings.I'm getting an error  on YouTube playlistsYouTube changed their playlist format in March 2014 and later on, so you'll need at least youtube-dl 2014.07.25 to download all YouTube videos.If you have installed youtube-dl with a package manager, pip, setup.py or a tarball, please use that to update. Note that Ubuntu packages do not seem to get updated anymore. Since we are not affiliated with Ubuntu, there is little we can do. Feel free to  to the  - all they have to do is update the package to a somewhat recent version. See above for a way to update.I'm getting an error when trying to use output template: Make sure you are not using  with any of these options , , ,  or  set in command line or in a configuration file. Remove the latter if any.Do I always have to pass ?By default, youtube-dl intends to have the best options (incidentally, if you have a convincing case that these should be different, ). Therefore, it is unnecessary and sometimes harmful to copy long option strings from webpages. In particular, the only option out of  that is regularly useful is .Can you please put the  option back?Most people asking this question are not aware that youtube-dl now defaults to downloading the highest available quality as reported by YouTube, which will be 1080p or 720p in some cases, so you no longer need the  option. For some specific videos, maybe YouTube does not report them to be available in a specific high quality format you're interested in. In that case, simply request it with the  option and youtube-dl will try to download it.I get HTTP error 402 when trying to download a video. What's this?Apparently YouTube requires you to pass a CAPTCHA test if you download too much. We're , but at the moment, your best course of action is pointing a web browser to the youtube URL, solving the CAPTCHA, and restart youtube-dl.Do I need any other programs?youtube-dl works fine on its own on most sites. However, if you want to convert video/audio, you'll need  or . On some sites - most notably YouTube - videos can be retrieved in a higher quality format without sound. youtube-dl will detect whether avconv/ffmpeg is present and automatically pick the best option.Videos or video formats streamed via RTMP protocol can only be downloaded when  is installed. Downloading MMS and RTSP videos requires either  or  to be installed.I have downloaded a video but how can I play it?Once the video is fully downloaded, use any video player, such as ,  or .I extracted a video URL with , but it does not play on another machine / in my web browser.It depends a lot on the service. In many cases, requests for the video (to download/play it) must come from the same IP address and with the same cookies and/or HTTP headers. Use the  option to write the required cookies into a file, and advise your downloader to read cookies from that file. Some sites also require a common user agent to be used, use  to see the one in use by youtube-dl. You can also get necessary cookies and HTTP headers from JSON output obtained with .It may be beneficial to use IPv6; in some cases, the restrictions are only applied to IPv4. Some services (sometimes only for a subset of videos) do not restrict the video URL by IP address, cookie, or user-agent, but these are the exception rather than the rule.Please bear in mind that some URL protocols are not supported by browsers out of the box, including RTMP. If you are using , your own downloader must support these as well.If you want to play the video on a machine that is not running youtube-dl, you can relay the video content from the machine that runs youtube-dl. You can use  to let youtube-dl stream a video to stdout, or simply allow the player to download the files written by youtube-dl in turn.ERROR: no fmt_url_map or conn information found in video infoYouTube has switched to a new video info format in July 2011 which is not supported by old versions of youtube-dl. See  for how to update youtube-dl.ERROR: unable to download videoYouTube requires an additional signature since September 2012 which is not supported by old versions of youtube-dl. See  for how to update youtube-dl.Video URL contains an ampersand and I'm getting some strange output  or That's actually the output from your shell. Since ampersand is one of the special shell characters it's interpreted by the shell preventing you from passing the whole URL to youtube-dl. To disable your shell from interpreting the ampersands (or any other special characters) you have to either put the whole URL in quotes or escape them with a backslash (which approach will work depends on your shell).For example if your URL is https://www.youtube.com/watch?t=4&v=BaW_jenozKc you should end up with following command:orFor Windows you have to use the double quotes:ExtractorError: Could not find JS function u'OF'In February 2015, the new YouTube player contained a character sequence in a string that was misinterpreted by old versions of youtube-dl. See  for how to update youtube-dl.HTTP Error 429: Too Many Requests or 402: Payment RequiredThese two error codes indicate that the service is blocking your IP address because of overuse. Usually this is a soft block meaning that you can gain access again after solving CAPTCHA. Just open a browser and solve a CAPTCHA the service suggests you and after that  to youtube-dl. Note that if your machine has multiple external IPs then you should also pass exactly the same IP you've used for solving CAPTCHA with . Also you may need to pass a  HTTP header of your browser with .If this is not the case (no CAPTCHA suggested to solve by the service) then you can contact the service and ask them to unblock your IP address, or - if you have acquired a whitelisted IP address already - use the  to select another IP address.SyntaxError: Non-ASCII characterThe errorFile ""youtube-dl"", line 2
SyntaxError: Non-ASCII character '\x93' ...
means you're using an outdated version of Python. Please update to Python 2.6 or 2.7.What is this binary file? Where has the code gone?Since June 2012 () youtube-dl is packed as an executable zipfile, simply unzip it (might need renaming to  first on some systems) or clone the git repository, as laid out above. If you modify the code, you can run it by executing the  file. To recompile the executable, run .The exe throws an error due to missing To run the exe you need to install first the .On Windows, how should I set up ffmpeg and youtube-dl? Where should I put the exe files?If you put youtube-dl and ffmpeg in the same directory that you're running the command from, it will work, but that's rather cumbersome.To make a different directory work - either for ffmpeg, or for youtube-dl, or for both - simply create the directory (say, , or ), put all the executables directly in there, and then  to include that directory.From then on, after restarting your shell, you will be able to access both youtube-dl and ffmpeg (and youtube-dl will be able to find ffmpeg) by simply typing  or , no matter what directory you're in.How do I put downloads into a specific folder?Use the  to specify an , for example . If you want this for all of your downloads, put the option into your .How do I download a video starting with a ?Either prepend  or separate the ID from the options with :youtube-dl -- -wNyEUrxzFU
youtube-dl ""https://www.youtube.com/watch?v=-wNyEUrxzFU""
How do I pass cookies to youtube-dl?Use the  option, for example .In order to extract cookies from browser use any conforming browser extension for exporting cookies. For example,  (for Chrome) or  (for Firefox).Note that the cookies file must be in Mozilla/Netscape format and the first line of the cookies file must be either  or . Make sure you have correct  in the cookies file and convert newlines if necessary to correspond with your OS, namely  () for Windows and  () for Unix and Unix-like systems (Linux, macOS, etc.).  when using  is a good sign of invalid newline format.Passing cookies to youtube-dl is a good way to workaround login when a particular extractor does not implement it explicitly. Another use case is working around  some websites require you to solve in particular cases in order to get access (e.g. YouTube, CloudFlare).How do I stream directly to media player?You will first need to tell youtube-dl to stream media to stdout with , and also tell your media player to read from stdin (it must be capable of this for streaming) and then pipe former to latter. For example, streaming to  can be achieved with:youtube-dl -o - ""https://www.youtube.com/watch?v=BaW_jenozKcj"" | vlc -
How do I download only new videos from a playlist?Use download-archive feature. With this feature you should initially download the complete playlist with  that will record identifiers of all the videos in a special file. Each subsequent run with the same  will download only new videos and skip all videos that have been downloaded before. Note that only successful downloads are recorded in the file.For example, at first,youtube-dl --download-archive archive.txt ""https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re""
will download the complete  playlist and create a file . Each subsequent run will only download new videos if any:youtube-dl --download-archive archive.txt ""https://www.youtube.com/playlist?list=PLwiyx1dc3P2JR9N8gQaQN_BCvlSlap7re""
Should I add  into my config?When youtube-dl detects an HLS video, it can download it either with the built-in downloader or ffmpeg. Since many HLS streams are slightly invalid and ffmpeg/youtube-dl each handle some invalid cases better than the other, there is an option to switch the downloader if needed.When youtube-dl knows that one particular downloader works better for a given website, that downloader will be picked. Otherwise, youtube-dl will pick the best downloader for general compatibility, which at the moment happens to be ffmpeg. This choice may change in future versions of youtube-dl, with improvements of the built-in downloader and/or ffmpeg.In particular, the generic extractor (used when your website is not in the  cannot mandate one specific downloader.If you put either  or  into your configuration, a different subset of videos will fail to download correctly. Instead, it is much better to  or a pull request which details why the native or the ffmpeg HLS downloader is a better choice for your use case.Can you add support for this anime video site, or site which shows current movies for free?As a matter of policy (as well as legality), youtube-dl does not include support for services that specialize in infringing copyright. As a rule of thumb, if you cannot easily find a video that the service is quite obviously allowed to distribute (i.e. that has been uploaded by the creator, the creator's distributor, or is published under a free license), the service is probably unfit for inclusion to youtube-dl.A note on the service that they don't host the infringing content, but just link to those who do, is evidence that the service should not be included into youtube-dl. The same goes for any DMCA note when the whole front page of the service is filled with videos they are not allowed to distribute. A ""fair use"" note is equally unconvincing if the service shows copyright-protected videos in full without authorization.Support requests for services that do purchase the rights to distribute their content are perfectly fine though. If in doubt, you can simply include a source that mentions the legitimate purchase of content.How can I speed up work on my issue?(Also known as: Help, my important issue not being solved!) The youtube-dl core developer team is quite small. While we do our best to solve as many issues as possible, sometimes that can take quite a while. To speed up your issue, here's what you can do:First of all, please do report the issue . That allows us to coordinate all efforts by users and developers, and serves as a unified point. Unfortunately, the youtube-dl project has grown too large to use personal email as an effective communication channel.Please read the  below. A lot of bugs lack all the necessary information. If you can, offer proxy, VPN, or shell access to the youtube-dl developers. If you are able to, test the issue from multiple computers in multiple countries to exclude local censorship or misconfiguration issues.If nobody is interested in solving your issue, you are welcome to take matters into your own hands and submit a pull request (or coerce/pay somebody else to do so).Feel free to bump the issue from time to time by writing a small comment (""Issue is still present in youtube-dl version ...from France, but fixed from Belgium""), but please not more than once a month. Please do not declare your issue as  or .How can I detect whether a given URL is supported by youtube-dl?For one, have a look at the . Note that it can sometimes happen that the site changes its URL scheme (say, from https://example.com/video/1234567 to https://example.com/v/1234567 ) and youtube-dl reports an URL of a service in that list as unsupported. In that case, simply report a bug.It is not possible to detect whether a URL is supported or not. That's because youtube-dl contains a generic extractor which matches all URLs. You may be tempted to disable, exclude, or remove the generic extractor, but the generic extractor not only allows users to extract videos from lots of websites that embed a video from another service, but may also be used to extract video from a service that it's hosting itself. Therefore, we neither recommend nor support disabling, excluding, or removing the generic extractor.If you want to find out whether a given URL is supported, simply call youtube-dl with it. If you get no videos back, chances are the URL is either not referring to a video or unsupported. You can find out which by examining the output (if you run youtube-dl on the console) or catching an  exception if you run it from a Python program.Why do I need to go through that much red tape when filing bugs?Before we had the issue template, despite our extensive , about 80% of the issue reports we got were useless, for instance because people used ancient versions hundreds of releases old, because of simple syntactic errors (not in youtube-dl but in general shell usage), because the problem was already reported multiple times before, because people did not actually read an error message, even if it said ""please install ffmpeg"", because people did not mention the URL they were trying to download and many more simple, easy-to-avoid problems, many of whom were totally unrelated to youtube-dl.youtube-dl is an open-source project manned by too few volunteers, so we'd rather spend time fixing bugs where we are certain none of those simple problems apply, and where we can be reasonably confident to be able to reproduce the issue without asking the reporter repeatedly. As such, the output of  is really all that's required to file an issue. The issue template also guides you through some basic steps you can do, such as checking that your version of youtube-dl is current.DEVELOPER INSTRUCTIONSMost users do not need to build youtube-dl and can  or get them from their distribution.To run youtube-dl as a developer, you don't need to build anything either. Simply executepython -m youtube_dl
To run the test, simply invoke your favorite test runner, or execute a test file directly; any of the following work:python -m unittest discover
python test/test_download.py
nosetests
For Python versions 3.6 and later, you can use  to implement . The original  has not been upgraded for 3.10 and later.See item 6 of  for how to run extractor specific test cases.If you want to create a build of youtube-dl yourself, you'll needAdding support for a new siteIf you want to add support for a new site, first of all make sure this site is not dedicated to . youtube-dl does not support such sites thus pull requests adding support for them will be rejected.After you have ensured this site is distributing its content legally, you can follow this quick list (assuming your service is called ):In any case, thank you very much for your contributions!youtube-dl coding conventionsThis section introduces guidelines for writing idiomatic, robust and future-proof extractor code.Extractors are very fragile by nature since they depend on the layout of the source data provided by 3rd party media hosters out of your control and this layout tends to change. As an extractor implementer your task is not only to write code that will extract media links and metadata correctly but also to minimize dependency on the source's layout and even to make the code foresee potential future changes and be ready for that. This is important because it will allow the extractor not to break on minor layout changes thus keeping old youtube-dl versions working. Even though this breakage issue is easily fixed by emitting a new version of youtube-dl with a fix incorporated, all the previous versions become broken in all repositories and distros' packages that may not be so prompt in fetching the update from us. Needless to say, some non rolling release distros may never receive an update at all.Mandatory and optional metafieldsFor extraction to work youtube-dl relies on metadata your extractor extracts and provides to youtube-dl expressed by an  or simply info dict. Only the following meta fields in the info dict are considered mandatory for a successful extraction process by youtube-dl:In fact only the last option is technically mandatory (i.e. if you can't figure out the download location of the media the extraction does not make any sense). But by convention youtube-dl also treats  and  as mandatory. Thus the aforementioned metafields are the critical data that the extraction does not make any sense without and if any of them fail to be extracted then the extractor is considered completely broken. apart from the aforementioned ones are considered optional. That means that extraction should be tolerant to situations when sources for these fields can potentially be unavailable (even if they are always available at the moment) and future-proof in order not to break the extraction of general purpose mandatory fields.ExampleSay you have some source dictionary  that you've fetched as JSON with HTTP request and it has a key :meta = self._download_json(url, video_id)
Assume at this point 's layout is:{
    ...
    ""summary"": ""some fancy summary text"",
    ...
}
Assume you want to extract  and put it into the resulting info dict as . Since  is an optional meta field you should be ready that this key may be missing from the  dict, so that you should extract it like:description = meta.get('summary')  # correct
and not like:description = meta['summary']  # incorrect
The latter will break extraction process with  if  disappears from  at some later time but with the former approach extraction will just go ahead with  set to  which is perfectly fine (remember  is equivalent to the absence of data).Similarly, you should pass  when extracting optional data from a webpage with ,  or similar methods, for instance:description = self._search_regex(
    r'<span[^>]+id=""title""[^>]*>([^<]+)<',
    webpage, 'description', fatal=False)
With  set to  if  fails to extract  it will emit a warning and continue extraction.You can also pass , for example:description = self._search_regex(
    r'<span[^>]+id=""title""[^>]*>([^<]+)<',
    webpage, 'description', default=None)
On failure this code will silently continue the extraction with  set to . That is useful for metafields that may or may not be present.Provide fallbacksWhen extracting metadata try to do so from multiple sources. For example if  is present in several places, try extracting from at least some of them. This makes it more future-proof in case some of the sources become unavailable.ExampleSay  from the previous example has a  and you are about to extract it. Since  is a mandatory meta field you should end up with something like:title = meta['title']
If  disappears from  in future due to some changes on the hoster's side the extraction would fail since  is mandatory. That's expected.Assume that you have some another source you can extract  from, for example  HTML meta of a . In this case you can provide a fallback scenario:title = meta.get('title') or self._og_search_title(webpage)
This code will try to extract from  first and if it fails it will try extracting  from a .Regular expressionsDon't capture groups you don't useCapturing group must be an indication that it's used somewhere in the code. Any group that is not used must be non capturing.ExampleDon't capture id attribute name here since you can't use it for anything anyway.Correct:r'(?:id|ID)=(?P<id>\d+)'
Incorrect:r'(id|ID)=(?P<id>\d+)'
Make regular expressions relaxed and flexibleWhen using regular expressions try to write them fuzzy, relaxed and flexible, skipping insignificant parts that are more likely to change, allowing both single and double quotes for quoted values and so on.ExampleSay you need to extract  from the following HTML code:<span style=""position: absolute; left: 910px; width: 90px; float: right; z-index: 9999;"" class=""title"">some fancy title</span>
The code for that task should look similar to:title = self._search_regex(
    r'<span[^>]+class=""title""[^>]*>([^<]+)', webpage, 'title')
Or even better:title = self._search_regex(
    r'<span[^>]+class=([""\'])title\1[^>]*>(?P<title>[^<]+)',
    webpage, 'title', group='title')
Note how you tolerate potential changes in the  attribute's value or switch from using double quotes to single for  attribute:The code definitely should not look like:title = self._search_regex(
    r'<span style=""position: absolute; left: 910px; width: 90px; float: right; z-index: 9999;"" class=""title"">(.*?)</span>',
    webpage, 'title', group='title')
Long lines policyThere is a soft limit to keep lines of code under 80 characters long. This means it should be respected if possible and if it does not make readability and code maintenance worse.For example, you should never split long string literals like URLs or some other often copied entities over multiple lines to fit this limit:Correct:'https://www.youtube.com/watch?v=FqZTN594JQw&list=PLMYEtVRpaqY00V9W81Cwmzp6N6vZqfUKD4'
Incorrect:'https://www.youtube.com/watch?v=FqZTN594JQw&list='
'PLMYEtVRpaqY00V9W81Cwmzp6N6vZqfUKD4'
Inline valuesExtracting variables is acceptable for reducing code duplication and improving readability of complex expressions. However, you should avoid extracting variables used only once and moving them to opposite parts of the extractor file, which makes reading the linear flow difficult.ExampleCorrect:title = self._html_search_regex(r'<title>([^<]+)</title>', webpage, 'title')
Incorrect:TITLE_RE = r'<title>([^<]+)</title>'
# ...some lines of code...
title = self._html_search_regex(TITLE_RE, webpage, 'title')
Collapse fallbacksMultiple fallback values can quickly become unwieldy. Collapse multiple fallback values into a single expression via a list of patterns.ExampleGood:description = self._html_search_meta(
    ['og:description', 'description', 'twitter:description'],
    webpage, 'description', default=None)
Unwieldy:description = (
    self._og_search_description(webpage, default=None)
    or self._html_search_meta('description', webpage, default=None)
    or self._html_search_meta('twitter:description', webpage, default=None))
Methods supporting list of patterns are: , , , .Trailing parenthesesAlways move trailing parentheses after the last argument.ExampleCorrect:    lambda x: x['ResultSet']['Result'][0]['VideoUrlSet']['VideoUrl'],
    list)
Incorrect:    lambda x: x['ResultSet']['Result'][0]['VideoUrlSet']['VideoUrl'],
    list,
)
Use convenience conversion and parsing functionsWrap all extracted numeric data into safe functions from : , . Use them for string to number conversions as well.Use  for safe URL processing.Use  for safe metadata extraction from parsed JSON.Use  for uniform  or any  meta field extraction,  for uniform  extraction,  for  extraction,  for count meta fields extraction, ,  for  extraction,  for  extraction.Explore  for more useful convenience functions.More examplesSafely extract optional description from parsed JSONWhen processing complex JSON, as often returned by site API requests or stashed in web pages for ""hydration"", you can use the  utility function to handle multiple fallback values and to ensure the expected type of metadata items. The function's docstring defines how the function works: also review usage in the codebase for more examples.In this example, a text , or , is pulled from the  member of the parsed JSON , if available.description = traverse_obj(response, ('result', 'video', 0, 'summary', T(compat_str)))
 is a shorthand for a set literal; if you hate people who still run Python 2.6,  could be written as a set literal .Some extractors use the older and less capable  function in the same way.description = try_get(response, lambda x: x['result']['video'][0]['summary'], compat_str)
Safely extract more optional metadataIn this example, various optional metadata values are extracted from the  member of the parsed JSON , which is expected to be a JS object, parsed into a , with no crash if that isn't so, or if any of the target values are missing or invalid.video = traverse_obj(response, ('result', 'video', 0, T(dict))) or {}
# formerly:
# video = try_get(response, lambda x: x['result']['video'][0], dict) or {}
description = video.get('summary')
duration = float_or_none(video.get('durationMs'), scale=1000)
view_count = int_or_none(video.get('views'))
Safely extract nested listsSuppose you've extracted JSON like this into a Python data structure named  using, say, the  or  methods of :{
    ""title"": ""Example video"",
    ""comment"": ""try extracting this"",
    ""media"": [{
        ""type"": ""bad"",
        ""size"": 320,
        ""url"": ""https://some.cdn.site/bad.mp4""
    }, {
        ""type"": ""streaming"",
        ""url"": ""https://some.cdn.site/hls.m3u8""
    }, {
        ""type"": ""super"",
        ""size"": 1280,
        ""url"": ""https://some.cdn.site/good.webm""
    }],
    ""moreStuff"": ""more values"",
    ...
}
Then extractor code like this can collect the various fields of the JSON:...
from ..utils import (
    determine_ext,
    int_or_none,
    T,
    traverse_obj,
    txt_or_none,
    url_or_none,
)
...
        ...
        info_dict = {}
        # extract title and description if valid and not empty
        info_dict.update(traverse_obj(media_json, {
            'title': ('title', T(txt_or_none)),
            'description': ('comment', T(txt_or_none)),
        }))

        # extract any recognisable media formats
        fmts = []
        # traverse into ""media"" list, extract `dict`s with desired keys
        for fmt in traverse_obj(media_json, ('media', Ellipsis, {
                'format_id': ('type', T(txt_or_none)),
                'url': ('url', T(url_or_none)),
                'width': ('size', T(int_or_none)), })):
            # bad `fmt` values were `None` and removed
            if 'url' not in fmt:
                continue
            fmt_url = fmt['url']  # known to be valid URL
            ext = determine_ext(fmt_url)
            if ext == 'm3u8':
                fmts.extend(self._extract_m3u8_formats(fmt_url, video_id, 'mp4', fatal=False))
            else:
                fmt['ext'] = ext
                fmts.append(fmt)

        # sort, raise if no formats
        self._sort_formats(fmts)

        info_dict['formats'] = fmts
        ...
The extractor raises an exception rather than random crashes if the JSON structure changes so that no formats are found.EMBEDDING YOUTUBE-DLyoutube-dl makes the best effort to be a good command-line program, and thus should be callable from any programming language. If you encounter any problems parsing its output, feel free to .From a Python program, you can embed youtube-dl in a more powerful fashion, like this:from __future__ import unicode_literals
import youtube_dl

ydl_opts = {}
with youtube_dl.YoutubeDL(ydl_opts) as ydl:
    ydl.download(['https://www.youtube.com/watch?v=BaW_jenozKc'])
Most likely, you'll want to use various options. For a list of options available, have a look at . For a start, if you want to intercept youtube-dl's output, set a  object.Here's a more complete example of a program that outputs only errors (and a short message after the download is finished), and downloads/converts the video to an mp3 file:from __future__ import unicode_literals
import youtube_dl


class MyLogger(object):
    def debug(self, msg):
        pass

    def warning(self, msg):
        pass

    def error(self, msg):
        print(msg)


def my_hook(d):
    if d['status'] == 'finished':
        print('Done downloading, now converting ...')


ydl_opts = {
    'format': 'bestaudio/best',
    'postprocessors': [{
        'key': 'FFmpegExtractAudio',
        'preferredcodec': 'mp3',
        'preferredquality': '192',
    }],
    'logger': MyLogger(),
    'progress_hooks': [my_hook],
}
with youtube_dl.YoutubeDL(ydl_opts) as ydl:
    ydl.download(['https://www.youtube.com/watch?v=BaW_jenozKc'])
BUGSBugs and suggestions should be reported in the issue tracker:  ( is an alias for this). Unless you were prompted to or there is another pertinent reason (e.g. GitHub fails to accept the bug report), please do not send bug reports via personal email. For discussions, join us in the IRC channel  on freenode ().Opening a bug report or suggestionBe sure to follow instructions provided below and in the issue tracker. Complete the appropriate issue template fully. Consider whether your problem is covered by an existing issue: if so, follow the discussion there. Avoid commenting on existing duplicate issues as such comments do not add to the discussion of the issue and are liable to be treated as spam.Please include the full output of youtube-dl when run with , i.e. add  flag to your command line, copy the whole output and post it in the issue body wrapped in  for better formatting. It should look similar to this:$ youtube-dl -v <your command line>
[debug] System config: []
[debug] User config: []
[debug] Command-line args: [u'-v', u'https://www.youtube.com/watch?v=BaW_jenozKcj']
[debug] Encodings: locale cp1251, fs mbcs, out cp866, pref cp1251
[debug] youtube-dl version 2015.12.06
[debug] Git HEAD: 135392e
[debug] Python version 2.6.6 - Windows-2003Server-5.2.3790-SP2
[debug] exe versions: ffmpeg N-75573-g1d0487f, ffprobe N-75573-g1d0487f, rtmpdump 2.4
[debug] Proxy map: {}
...
Do not post screenshots of verbose logs; only plain text is acceptable.The output (including the first lines) contains important debugging information. Issues without the full output are often not reproducible and therefore do not get solved in short order, if ever.Finally please review your issue to avoid various common mistakes (you can and should use this as a checklist) listed below.Is the description of the issue itself sufficient?We often get issue reports that are hard to understand. To avoid subsequent clarifications, and to assist participants who are not native English speakers, please elaborate on what feature you are requesting, or what bug you want to be fixed.Make sure that it's obviousIf your report is shorter than two lines, it is almost certainly missing some of these, which makes it hard for us to respond to it. We're often too polite to close the issue outright, but the missing info makes misinterpretation likely. As a committer myself, I often get frustrated by these issues, since the only possible way for me to move forward on them is to ask for clarification over and over.For bug reports, this means that your report should contain the complete output of youtube-dl when called with the  flag. The error message you get for (most) bugs even says so, but you would not believe how many of our bug reports do not contain this information.If your server has multiple IPs or you suspect censorship, adding  may be a good idea to get more diagnostics. If the error is  and you cannot reproduce it from multiple countries, add  (warning: this will yield a rather large output, redirect it to the file  by adding  to your command-line) or upload the  files you get when you add  .Site support requests must contain an example URL. An example URL is a URL you might want to download, like . There should be an obvious video present. Except under very special circumstances, the main page of a video service (e.g. ) is not an example URL.Is the issue already documented?Make sure that someone has not already opened the issue you're trying to open. Search at the top of the window or browse the  of this repository. Initially, at least, use the search term  to focus on active issues. If there is an issue, feel free to write something along the lines of ""This affects me as well, with version 2015.01.01. Here is some more information on the issue: ..."". While some issues may be old, a new post into them often spurs rapid activity.Are you using the latest version?Before reporting any issue, type . This should report that you're up-to-date. About 20% of the reports we receive are already fixed, but people are using outdated versions. This goes for feature requests as well.Why are existing options not enough?Before requesting a new feature, please have a quick peek at . Many feature requests are for features that actually exist already! Please, absolutely do show off your work in the issue report and detail how the existing similar options do not solve your problem.Is there enough context in your bug report?People want to solve problems, and often think they do us a favor by breaking down their larger problems (e.g. wanting to skip already downloaded files) to a specific request (e.g. requesting us to look whether the file exists before downloading the info page). However, what often happens is that they break down the problem into two steps: One simple, and one impossible (or extremely complicated one).We are then presented with a very complicated request when the original problem could be solved far easier, e.g. by recording the downloaded video IDs in a separate file. To avoid this, you must include the greater context where it is non-obvious. In particular, every feature request that does not consist of adding support for a new site should contain a use case scenario that explains in what situation the missing feature would be useful.Does the issue involve one problem, and one problem only?Some of our users seem to think there is a limit of issues they can or should open. There is no limit of issues they can or should open. While it may seem appealing to be able to dump all your issues into one ticket, that means that someone who solves one of your issues cannot mark the issue as closed. Typically, reporting a bunch of issues leads to the ticket lingering since nobody wants to attack that behemoth, until someone mercifully splits the issue into multiple ones.In particular, every site support request issue should only pertain to services at one site (generally under a common domain, but always using the same backend technology). Do not request support for vimeo user videos, White house podcasts, and Google Plus pages in the same issue. Also, make sure that you don't post bug reports alongside feature requests. As a rule of thumb, a feature request does not include outputs of youtube-dl that are not immediately related to the feature at hand. Do not post reports of a network error alongside the request for a new video service.Is anyone going to need the feature?Only post features that you (or an incapacitated friend you can personally talk to) require. Do not post features because they seem like a good idea. If they are really useful, they will be requested by someone who requires them.Is your question about youtube-dl?It may sound strange, but some bug reports we receive are completely unrelated to youtube-dl and relate to a different, or even the reporter's own, application. Please make sure that you are actually using youtube-dl. If you are using a UI for youtube-dl, report the bug to the maintainer of the actual application providing the UI. On the other hand, if your UI for youtube-dl fails in some way you believe is related to youtube-dl, by all means, go ahead and report the bug.COPYRIGHTyoutube-dl is released into the public domain by the copyright holders.This README file was originally written by  and is likewise released into the public domain."
https://github.com/kylemcdonald/FreeWifi,How to get free wifi.,"Free WifiThis short tutorial describes a few methods for gaining access to the Internet, , from public wireless networks.This tutorial has been tested on Mac and a Raspberry Pi. It should generally work on Linux, and hasn't been tested on Windows.PreparationMake sure you do this step before you are stuck without Internet access:Ubuntu:$ sudo apt-get install python-dev
Fedora:$ sudo dnf install python-devel
Note: For Centos, substitute  with $ git clone https://github.com/kylemcdonald/FreeWifi
$ cd FreeWifi && sudo pip install -r requirements.txt
How to get additional timeIf you had free internet access but your time has run out, the first thing to try is open an incognito/private window. Here are instructions for a few browsers:An incognito/private window will temporarily clear any cookies that may have been used for tracking how much time you spent online, making you look like a ""new user"" and allowing you to log into the wireless portal again.Unfortunately, most systems track MAC addresses instead of cookies. A MAC address is a unique identifier assigned to every network interface. This means you need to get a new MAC address to get additional time. Fortunately, MAC addresses can be changed in software, without swapping the hardware. The  command line utility makes this easy by entering . If the command fails to run, try entering  to check what the name of your wireless device is first, and use that manually. After randomizing your MAC, try logging into the wireless portal again. When you're done using the Internet, run  to reset your MAC address.Note that MAC address spoofing may be interpreted as an illegal activity depending on why you do it. In some cases it is certainly not illegal: recent mobile operating systems like iOS 8+ and Android 6+ automatically randomize their MAC address when searching for wireless networks to avoid being tracked. But when , MAC address spoofing was claimed as a signal of intention to commit a crime.How to get free accessIf the network is open, but you can't get access for some reason, you can also try spoofing the MAC address of a device that is already using the network. To the router, your device and the other device will look like one device. This can cause some minor problems if they interrupt each other, but for light browsing it usually works out fine.To find the MAC addresses of other devices using the network, first you need to connect to the network. You don't need to have Internet access, just a connection. First, on Mac OS run the command  once to make sure you can sniff wireless data (you need to do this again if you restart your computer). Then run the command . You should see a progress bar immediately:Available interfaces: en0
Interface: en0
SSID: nonoinflight
Available gateways: en0
Gateway IP: 10.0.1.1
Gateway MAC: 00:e0:4b:22:96:d9
100%|██████████████████████████| 1000/1000 [00:46<00:00, 21.46it/s]
Total of 5 user(s):
27:35:96:a8:66:7f	6359 bytes
36:fe:83:9c:35:eb	9605 bytes
65:01:3c:cc:20:e8	17306 bytes
8c:6f:11:2c:f0:ee	20515 bytes
0a:4f:b2:b8:e8:56	71541 bytes
If there isn't much traffic on the network, it might take longer. If it's taking too long, type  to cancel the sniffing and print whatever results are available. Finally, we want to spoof one of these MAC addresses. For example, in this case we would enter  to try spoofing the address with the most traffic (they probably have a connection). After running that command, try to access the Internet. If you don't have a connection, try the next MAC in the list. If your Internet connection drops out while using this MAC address, try disconnecting and reconnecting to the wireless network. Note that the original user of the MAC you copied may experience these same connection drop outs if you are both actively using the network.How it works uses  to collect wireless packets. Then we look through these packets for any hints of the MAC address (BSSID) of our wireless network. Finally, we look for data packets that mention a user's MAC as well as the network BSSID (or the network gateway), and take note of that MAC using some amount of data. Then we sort the user's MACs by the total amount of data and print them out.Instead of sniffing wireless traffic, in some situations you can also use the command  to get a list of MAC addresses of devices on the wireless network. Then you can either use  to copy the address, or use  directly on Linux and OSX. For the specifics of using  look at the implementations of  inside .This repository is dedicated to Lauren McCarthy, who has taught me the most about the art of getting a good deal."
https://github.com/facebookresearch/maskrcnn-benchmark,"Fast, modular reference implementation of Instance Segmentation and Object Detection algorithms in PyTorch.","Faster R-CNN and Mask R-CNN in PyTorch 1.0maskrcnn-benchmark has been deprecated. Please see This project aims at providing the necessary building blocks for easilycreating detection and segmentation models using PyTorch 1.0.HighlightsWebcam and Jupyter notebook demoWe provide a simple webcam demo that illustrates how you can use  for inference:cd demo
# by default, it runs on the GPU
# for best results, use min-image-size 800
python webcam.py --min-image-size 800
# can also run it on the CPU
python webcam.py --min-image-size 300 MODEL.DEVICE cpu
# or change the model that you want to use
python webcam.py --config-file ../configs/caffe2/e2e_mask_rcnn_R_101_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu
# in order to see the probability heatmaps, pass --show-mask-heatmaps
python webcam.py --min-image-size 300 --show-mask-heatmaps MODEL.DEVICE cpu
# for the keypoint demo
python webcam.py --config-file ../configs/caffe2/e2e_keypoint_rcnn_R_50_FPN_1x_caffe2.yaml --min-image-size 300 MODEL.DEVICE cpu
A notebook with the demo can be found in .InstallationCheck  for installation instructions.Model Zoo and BaselinesPre-trained models, baselines and comparison with Detectron and mmdetectioncan be found in Inference in a few linesWe provide a helper class to simplify writing inference pipelines using pre-trained models.Here is how we would do it. Run this from the  folder:from maskrcnn_benchmark.config import cfg
from predictor import COCODemo

config_file = ""../configs/caffe2/e2e_mask_rcnn_R_50_FPN_1x_caffe2.yaml""

# update the config options with the config file
cfg.merge_from_file(config_file)
# manual override some options
cfg.merge_from_list([""MODEL.DEVICE"", ""cpu""])

coco_demo = COCODemo(
    cfg,
    min_image_size=800,
    confidence_threshold=0.7,
)
# load image and then run prediction
image = ...
predictions = coco_demo.run_on_opencv_image(image)
Perform training on COCO datasetFor the following examples to work, you need to first install .You will also need to download the COCO dataset.We recommend to symlink the path to the coco dataset to  as followsWe use  and  sets from # symlink the coco dataset
cd ~/github/maskrcnn-benchmark
mkdir -p datasets/coco
ln -s /path_to_coco_dataset/annotations datasets/coco/annotations
ln -s /path_to_coco_dataset/train2014 datasets/coco/train2014
ln -s /path_to_coco_dataset/test2014 datasets/coco/test2014
ln -s /path_to_coco_dataset/val2014 datasets/coco/val2014
# or use COCO 2017 version
ln -s /path_to_coco_dataset/annotations datasets/coco/annotations
ln -s /path_to_coco_dataset/train2017 datasets/coco/train2017
ln -s /path_to_coco_dataset/test2017 datasets/coco/test2017
ln -s /path_to_coco_dataset/val2017 datasets/coco/val2017

# for pascal voc dataset:
ln -s /path_to_VOCdevkit_dir datasets/voc
P.S.  =  +  ,  = You can also configure your own paths to the datasets.For that, all you need to do is to modify  topoint to the location where your dataset is stored.You can also create a new  file which implements the same two classes,and pass it as a config argument  during training.Single GPU trainingMost of the configuration files that we provide assume that we are running on 8 GPUs.In order to be able to run it on fewer GPUs, there are a few possibilities:1. Run the following without modificationspython /path_to_maskrcnn_benchmark/tools/train_net.py --config-file ""/path/to/config/file.yaml""
This should work out of the box and is very similar to what we should do for multi-GPU training.But the drawback is that it will use much more GPU memory. The reason is that we set in theconfiguration files a global batch size that is divided over the number of GPUs. So if we onlyhave a single GPU, this means that the batch size for that GPU will be 8x larger, which might leadto out-of-memory errors.If you have a lot of memory available, this is the easiest solution.2. Modify the cfg parametersIf you experience out-of-memory errors, you can reduce the global batch size. But this means thatyou'll also need to change the learning rate, the number of iterations and the learning rate schedule.Here is an example for Mask R-CNN R-50 FPN with the 1x schedule:python tools/train_net.py --config-file ""configs/e2e_mask_rcnn_R_50_FPN_1x.yaml"" SOLVER.IMS_PER_BATCH 2 SOLVER.BASE_LR 0.0025 SOLVER.MAX_ITER 720000 SOLVER.STEPS ""(480000, 640000)"" TEST.IMS_PER_BATCH 1 MODEL.RPN.FPN_POST_NMS_TOP_N_TRAIN 2000
This follows the Note that we have multiplied the number of iterations by 8x (as well as the learning rate schedules),and we have divided the learning rate by 8x.We also changed the batch size during testing, but that is generally not necessary because testingrequires much less memory than training.Furthermore, we set  as the proposals are selected for per the batch rather than per image in the default training. The value is calculated by 1000 x images-per-gpu. Here we have 2 images per GPU, therefore we set the number as 1000 x 2 = 2000. If we have 8 images per GPU, the value should be set as 8000. Note that this does not apply if  is set to  during training. See  for more details.Multi-GPU trainingWe use internally  in order to launchmulti-gpu training. This utility function from PyTorch spawns as manyPython processes as the number of GPUs we want to use, and each Pythonprocess will only use a single GPU.export NGPUS=8
python -m torch.distributed.launch --nproc_per_node=$NGPUS /path_to_maskrcnn_benchmark/tools/train_net.py --config-file ""path/to/config/file.yaml"" MODEL.RPN.FPN_POST_NMS_TOP_N_TRAIN images_per_gpu x 1000
Note we should set  follow the rule in Single-GPU training.Mixed precision trainingWe currently use  to add  support. To enable, just do Single-GPU or Multi-GPU training and set .export NGPUS=8
python -m torch.distributed.launch --nproc_per_node=$NGPUS /path_to_maskrcnn_benchmark/tools/train_net.py --config-file ""path/to/config/file.yaml"" MODEL.RPN.FPN_POST_NMS_TOP_N_TRAIN images_per_gpu x 1000 DTYPE ""float16""
If you want more verbose logging, set . See  for more details.EvaluationYou can test your model directly on single or multiple gpus. Here is an example for Mask R-CNN R-50 FPN with the 1x schedule on 8 GPUS:export NGPUS=8
python -m torch.distributed.launch --nproc_per_node=$NGPUS /path_to_maskrcnn_benchmark/tools/test_net.py --config-file ""configs/e2e_mask_rcnn_R_50_FPN_1x.yaml"" TEST.IMS_PER_BATCH 16
To calculate mAP for each class, you can simply modify a few lines in . See  for more details.AbstractionsFor more information on some of the main abstractions in our implementation, see .Adding your own datasetThis implementation adds support for COCO-style datasets.But adding support for training on a new dataset can be done as follows:from maskrcnn_benchmark.structures.bounding_box import BoxList

class MyDataset(object):
    def __init__(self, ...):
        # as you would do normally

    def __getitem__(self, idx):
        # load the image as a PIL Image
        image = ...

        # load the bounding boxes as a list of list of boxes
        # in this case, for illustrative purposes, we use
        # x1, y1, x2, y2 order.
        boxes = [[0, 0, 10, 10], [10, 20, 50, 50]]
        # and labels
        labels = torch.tensor([10, 20])

        # create a BoxList from the boxes
        boxlist = BoxList(boxes, image.size, mode=""xyxy"")
        # add the labels to the boxlist
        boxlist.add_field(""labels"", labels)

        if self.transforms:
            image, boxlist = self.transforms(image, boxlist)

        # return the image, the boxlist and the idx in your dataset
        return image, boxlist, idx

    def get_img_info(self, idx):
        # get img_height and img_width. This is used if
        # we want to split the batches according to the aspect ratio
        # of the image, as it can be more efficient than loading the
        # image from disk
        return {""height"": img_height, ""width"": img_width}
That's it. You can also add extra fields to the boxlist, such as segmentation masks(using ), or even your own instance type.For a full example of how the  is implemented, check .Once you have created your dataset, it needs to be added in a couple of places:TestingWhile the aforementioned example should work for training, we leverage thecocoApi for computing the accuracies during testing. Thus, test datasetsshould currently follow the cocoApi for now.To enable your dataset for testing, add a corresponding if statement in :if isinstance(dataset, datasets.MyDataset):
        return coco_evaluation(**args)
Finetuning from Detectron weights on custom datasetsCreate a script  like .You can decide which keys to be removed and which keys to be kept by modifying the script.Then you can simply point the converted model path in the config file by changing .For further information, please refer to .TroubleshootingIf you have issues running or compiling this code, we have compiled a list of common issues in. If your issue is not present there, please feelfree to open a new issue.CitationsPlease consider citing this project in your publications if it helps your research. The following is a BibTeX reference. The BibTeX entry requires the  LaTeX package.@misc{massa2018mrcnn,
author = {Massa, Francisco and Girshick, Ross},
title = {{maskrcnn-benchmark: Fast, modular reference implementation of Instance Segmentation and Object Detection algorithms in PyTorch}},
year = {2018},
howpublished = {\url{https://github.com/facebookresearch/maskrcnn-benchmark}},
note = {Accessed: [Insert date here]}
}
Projects using maskrcnn-benchmarkLicensemaskrcnn-benchmark is released under the MIT license. See  for additional details."
https://github.com/floydhub/dl-docker,"An all-in-one Docker image for deep learning. Contains all the popular DL frameworks (TensorFlow, Theano, Torch, Caffe, etc.)"," •  •  •  • All-in-one Docker image for Deep LearningHere are Dockerfiles to get you up and running with a fully functional deep learning machine. It contains all the popular deep learning frameworks with CPU and GPU support (CUDA and cuDNN included). The CPU version should work on Linux, Windows and OS X. The GPU version will, however, only work on Linux machines. See  for detailsIf you are not familiar with Docker, but would still like an all-in-one solution, start here: . If you know what Docker is, but are wondering why we need one for deep learning, Update: I've built a quick tool, based on dl-docker, to run your DL project on the cloud with zero setup. You can start running your Tensorflow project on AWS in <30seconds using Floyd. See . It's free to try out.Happy to take feature requests/feedback and answer questions - mail me sai@floydhub.com.SpecsThis is what you get out of the box when you create a container with the provided image/Dockerfile:SetupPrerequisitesObtaining the Docker imageYou have 2 options to obtain the Docker imageOption 1: Download the Docker image from Docker HubDocker Hub is a cloud based repository of pre-built images. You can download the image directly from here, which should be much faster than building it locally (a few minutes, based on your internet speed). Here is the automated build page for : . The image is automatically built based on the  in the Github repo.CPU Versiondocker pull floydhub/dl-docker:cpu
GPU VersionAn automated build for the GPU image is not available currently due to timeout restrictions in Docker's automated build process. I'll look into solving this in the future, but for now you'll have to build the GPU version locally using Option 2 below.Option 2: Build the Docker image locallyAlternatively, you can build the images locally. Also, since the GPU version is not available in Docker Hub at the moment, you'll have to follow this if you want to GPU version. Note that this will take an hour or two depending on your machine since it compiles a few libraries from scratch.git clone https://github.com/saiprashanths/dl-docker.git
cd dl-docker
CPU Versiondocker build -t floydhub/dl-docker:cpu -f Dockerfile.cpu .
GPU Versiondocker build -t floydhub/dl-docker:gpu -f Dockerfile.gpu .
This will build a Docker image named  and tagged either  or  depending on the tag your specify. Also note that the appropriate  has to be used.Running the Docker image as a ContainerOnce we've built the image, we have all the frameworks we need installed in it. We can now spin up one or more containers using this image, and you should be ready to CPU Versiondocker run -it -p 8888:8888 -p 6006:6006 -v /sharedfolder:/root/sharedfolder floydhub/dl-docker:cpu bash
GPU Versionnvidia-docker run -it -p 8888:8888 -p 6006:6006 -v /sharedfolder:/root/sharedfolder floydhub/dl-docker:gpu bash
Note the use of  rather than just | Parameter      | Explanation ||----------------|-------------||             | This creates an interactive terminal you can use to iteract with your container ||    | This exposes the ports inside the container so they can be accessed from the host. The format is . The default iPython Notebook runs on port 8888 and Tensorboard on 6006 || | This shares the folder  on your host machine to  inside your container. Any data written to this folder by the container will be persistent. You can modify this to anything of the format . See |   | This the image that you want to run. The format is . In our case, we use the image  and tag  or  to spin up the appropriate image ||       | This provides the default command when the container is started. Even if this was not provided, bash is the default command and just starts a Bash session. You can modify this to be whatever you'd like to be executed when your container starts. For example, you can execute . This will execute the command  and starts your Jupyter Notebook for you when the container startsSome common scenariosJupyter NotebooksThe container comes pre-installed with iPython and iTorch Notebooks, and you can use these to work with the deep learning frameworks. If you spin up the docker container with  (as shown above in the ), you will have access to these ports on your host and can access them at . The default iPython notebook uses port 8888 and Tensorboard uses port 6006. Since we expose both these ports when we run the container, we can access them both from the localhost.However, you still need to start the Notebook inside the container to be able to access it from the host. You can either do this from the container terminal by executing  or you can pass this command in directly while spinning up your container using the  CLI. The Jupyter Notebook has both Python (for TensorFlow, Caffe, Theano, Keras, Lasagne) and iTorch (for Torch) kernels.Note: If you are setting the notebook on Windows, you will need to first determine the IP address of your Docker container. This command on the Docker command-line provides the IP addressdocker-machine ip default
> <IP-address>
 is the name of the container provided by default to the container you will spin.On obtaining the IP-address, run the docker as per the  provided and start the Jupyter notebook as . Then accessing  on your host's browser should show you the notebook.Data SharingSee .Consider this: You have a script that you've written on your host machine. You want to run this in the container and get the output data (say, a trained model) back into your host. The way to do this is using a . By passing in the  to the CLI, we are sharing the folder between the host and the container, with persistence. You could copy your script into  folder on the host, execute your script from inside the container (located at ) and write the results data back to the same folder. This data will be accessible even after you kill the container.What is Docker? itself has a great answer to this question.Docker is based on the idea that one can package code along with its dependencies into a self-contained unit. In this case, we start with a base Ubuntu 14.04 image, a bare minimum OS. When we build our initial Docker image using , we install all the deep learning frameworks and its dependencies on the base, as defined by the . This gives us an image which has all the packages we need installed in it. We can now spin up as many instances of this image as we like, using the  command. Each instance is called a container. Each of these containers can be thought of as a fully functional and isolated OS with all the deep learning libraries installed in it. Why do I need a Docker?Installing all the deep learning frameworks to coexist and function correctly is an exercise in dependency hell. Unfortunately, given the current state of DL development and research, it is almost impossible to rely on just one framework. This Docker is intended to provide a solution for this use case.If you would rather install all the frameworks yourself manually, take a look at this guide: Do I really need an all-in-one container?No. The provided all-in-one solution is useful if you have dependencies on multiple frameworks (say, load a pre-trained Caffe model, finetune it, convert it to Tensorflow and continue developing there) or if you just want to play around with the various frameworks.The Docker philosophy is to build a container for each logical task/framework. If we followed this, we should have one container for each of the deep learning frameworks. This minimizes clashes between frameworks and is easier to maintain as things evolve. In fact, if you only intend to use one of the frameworks, or at least use only one framework at a time, follow this approach. You can find Dockerfiles for individual frameworks here:FAQsPerformanceRunning the DL frameworks as Docker containers should have no performance impact during runtime. Spinning up a Docker container itself is very fast and should take only a couple of seconds or lessDocker container persistenceKeep in mind that the changes made inside Docker container are not persistent. Lets say you spun up a Docker container, added and deleted a few files and then kill the container. The next time you spin up a container using the same image, all your previous changes will be lost and you will be presented with a fresh instance of the image. This is great, because if you mess up your container, you can always kill it and start afresh again. It's bad if you don't know/understand this and forget to save your work before killing the container. There are a couple of ways to work around this:How do I update/install new libraries?You can do one of:What operating systems are supported?Docker is supported on all the OSes mentioned here:  (i.e. different flavors of Linux, Windows and OS X). The CPU version (Dockerfile.cpu) will run on all the above operating systems. However, the GPU version (Dockerfile.gpu) will only run on Linux OS. This is because Docker runs inside a virtual machine on Windows and OS X. Virtual machines don't have direct access to the GPU on the host. Unless PCI passthrough is implemented for these hosts, GPU support isn't available on non-Linux OSes at the moment."
https://github.com/psf/requests,"A simple, yet elegant, HTTP library.","RequestsRequests is a simple, yet elegant, HTTP library.>>> import requests
>>> r = requests.get('https://httpbin.org/basic-auth/user/pass', auth=('user', 'pass'))
>>> r.status_code
200
>>> r.headers['content-type']
'application/json; charset=utf8'
>>> r.encoding
'utf-8'
>>> r.text
'{""authenticated"": true, ...'
>>> r.json()
{'authenticated': True, ...}
Requests allows you to send HTTP/1.1 requests extremely easily. There’s no need to manually add query strings to your URLs, or to form-encode your  &  data — but nowadays, just use the  method!Requests is one of the most downloaded Python packages today, pulling in around — according to GitHub, Requests is currently  by  repositories. You may certainly put your trust in this code.Installing Requests and Supported VersionsRequests is available on PyPI:$ python -m pip install requests
Requests officially supports Python 3.7+.Supported Features & Best–PracticesRequests is ready for the demands of building robust and reliable HTTP–speaking applications, for the needs of today.API Reference and User Guide available on Cloning the repositoryWhen cloning the Requests repository, you may need to add the  flag to avoid an error about a bad commit (see for more background):git clone -c fetch.fsck.badTimezone=ignore https://github.com/psf/requests.git
You can also apply this setting to your global Git config:git config --global fetch.fsck.badTimezone ignore
 "
https://github.com/PaddlePaddle/PaddleHub,"Awesome pre-trained models toolkit based on PaddlePaddle. (400+ models including Image, Text, Audio, Video and Cross-Modal with Easy Inference & Serving)","English | ⭐Features💥Recent Updates🌈Visualization Demo🏜️ 👓 🎤 🎧 ⭐ Thanks for Your Star🍻Welcome to join PaddleHub technical group✈️QuickStart🚁The installation of required components.# install paddlepaddle with gpu
# !pip install --upgrade paddlepaddle-gpu

# or install paddlepaddle with cpu
!pip install --upgrade paddlepaddle

# install paddlehub
!pip install --upgrade paddlehub
🛫The simplest cases of Chinese word segmentation.import paddlehub as hub

lac = hub.Module(name=""lac"")
test_text = [""今天是个好天气。""]

results = lac.cut(text=test_text, use_gpu=False, batch_size=1, return_tag=True)
print(results)
#{'word': ['今天', '是', '个', '好天气', '。'], 'tag': ['TIME', 'v', 'q', 'n', 'w']}
🛰️The simplest command of deploying lac service.!hub serving start -m lac
📚LicenseThe release of this project is certified by the Apache 2.0 license.👨‍👨‍👧‍👦ContributionWe welcome you to contribute code to PaddleHub, and thank you for your feedback."
https://github.com/JuanPotato/Legofy,Make images look as if they are made out of 1x1 LEGO blocks,"Legofy       What is it?Legofy is a python program that takes a static image or gif and makes it so that it looks as if it was built out of LEGO.RequirementsBugsIf you find a bug:Quickstart$ pip install legofy
or install from source$ git clone https://github.com/JuanPotato/Legofy.git
$ cd Legofy
$ python setup.py install
Wait! I don't know what any of this means? Use pip then, or if you really want to install from source UsageUsage: legofy [OPTIONS] IMAGE [OUTPUT]

  Legofy an image!

Options:
  --size INTEGER                  Number of bricks the longest side of the legofied image should have.
  --dither / --no-dither          Use dither algorithm to spread the color approximation error.
  --palette [all|effects|mono|solid|transparent]
                                  Palette to use based on real Lego colors.
  --help                          Show this message and exit.
PaletteThere are 3 palettes: solid (33 colors), transparent (14 colors) and effects (4 colors).You can use one of them or all the 3.$ legofy --palette solid image.jpg
$ legofy --palette transparent image.jpg
$ legofy --palette effects image.jpg
$ legofy --palette all image.jpg
There is another one palette, mono, with only 2 colors (black and white...). It's just for test and fun...TroubleshootingInstallationForks"
https://github.com/numpy/numpy,The fundamental package for scientific computing with Python.,"NumPy is the fundamental package for scientific computing with Python.It provides:Testing:NumPy requires  and .  Tests can then be run after installation with:python -c ""import numpy, sys; sys.exit(numpy.test() is False)""
Code of ConductNumPy is a community-driven open source project developed by a diverse group of. The NumPy leadership has made a strongcommitment to creating an open, inclusive, and positive community. Please read the for guidance on how to interactwith others in a way that makes our community thrive.Call for ContributionsThe NumPy project welcomes your expertise and enthusiasm!Small improvements or fixes are always appreciated. If you are considering larger contributionsto the source code, please contact us through the  first.Writing code isn’t the only way to contribute to NumPy. You can also:For more information about the ways you can contribute to NumPy, visit .If you’re unsure where to start or how your skills fit in, reach out! You canask on the mailing list or here, on GitHub, by opening a new issue or leaving acomment on a relevant issue that is already open.Our preferred channels of communication are all public, but if you’d like tospeak to us in private first, contact our community coordinators atnumpy-team@googlegroups.com or on Slack (write numpy-team@googlegroups.com foran invitation).We also have a biweekly community call, details of which are announced on themailing list. You are very welcome to join.If you are new to contributing to open source,  helps explain why, what,and how to successfully get involved."
https://github.com/Gallopsled/pwntools,CTF framework and exploit development library,"pwntools - CTF toolkitPwntools is a CTF framework and exploit development library. Written in Python, it is designed for rapid prototyping and development, and intended to make exploit writing as simple as possible.from pwn import *
context(arch = 'i386', os = 'linux')

r = remote('exploitme.example.com', 31337)
# EXPLOIT CODE GOES HERE
r.send(asm(shellcraft.sh()))
r.interactive()
DocumentationOur documentation is available at A series of tutorials is also To get you started, we've provided some example solutions for past CTF challenges in our .InstallationPwntools is best supported on 64-bit Ubuntu LTS releases (14.04, 16.04, 18.04, and 20.04).  Most functionality should work on any Posix-like distribution (Debian, Arch, FreeBSD, OSX, etc.).  Python3 is suggested, but Pwntools still works with Python 2.7.  Most of the functionality of pwntools is self-contained and Python-only.  You should be able to get running quickly withapt-get update
apt-get install python3 python3-pip python3-dev git libssl-dev libffi-dev build-essential
python3 -m pip install --upgrade pip
python3 -m pip install --upgrade pwntools
However, some of the features (assembling/disassembling foreign architectures) require non-Python dependencies.  For more information, see the .ContributionSee Contact and CommunityIf you have any questions not worthy of a , join the Discord server at https://discord.gg/96VA2zvjCB"
https://github.com/coursera-dl/coursera-dl,Script for downloading Coursera.org videos and naming them.,"Coursera DownloaderIntroduction is arguably the leader in massive open online courses (MOOC)with a selection of more than 300 classes from 62 different institutions . Generous contributions by educators and institutions aremaking excellent education available to many who could not afford it otherwise.There are even non-profits with ""feet on the ground"" in remote areas of theworld who are helping spread the wealth (see the feedback below from ).This script makes it easier to batch download lecture resources (e.g., videos, ppt,etc) for Coursera classes.  Given one or more class names and account credentials,it obtains week and class names from the lectures page, and then downloadsthe related materials into appropriately named files and directories.Why is this helpful?  A utility like  can work, but has thefollowing limitations:Browser extensions like DownloadThemAll is another possibility, but provides more features such as appropriately named files.This work was originally inspired in part by  by whichI've downloaded many other good videos such as those from Khan Academy.FeaturesDisclaimer is meant to be used only for your material that Coursera givesyou access to download.We do not encourage any use that violates their . Arelevant excerpt:Installation instructions requires Python 2 or Python 3 and a free Coursera accountenrolled in the class of interest. (As of February of 2020, we testautomatically the execution of the program with Python versions 2.7, Pypy,3.6, 3.7, 3.8, and 3.9).Note: We strongly recommend that you use a Python 3 interpreter (3.9or later).On any operating system, ensure that the Python executable location is addedto your  environment variable and, once you have the dependenciesinstalled (see next section), for a basic usage, you will need to invokethe script from the main directory of the project and prepend it with theword .  You can also use more advanced features of the program bylooking at the ""Running the script"" section of this document.Note: You must already have (manually) agreed to the Honor of Code of theparticular courses that you want to use with .Recommended installation method for all Operating SystemsFrom a command line (preferably, from a virtual environment), simply issuethe command:pip install coursera-dl
This will download  of the program from the along with all the necessarydependencies. At this point, you should be ready to start using it.If this does not work, because your Python 2 version is too old (e.g. 2.7.5on Ubuntu 14.4), try:apt-get install python3 python3-pip
pip3 install coursera-dl
instead.Note 1: We strongly recommend that you don't install the packageglobally on your machine (i.e., with root/administrator privileges), as theinstalled modules may conflict with other Python applications that you haveinstalled in your system (or they can interfere with ).  Preferto use the option  to , if you need can.Note 2: As already mentioned, we strongly recommend that you use a newPython 3 interpreter (e.g., 3.9 or later), since Python 3 has better supportfor SSL/TLS (for secure connections) than earlier versions.If you must use Python 2, be sure that you have at least Python 2.7.9 (laterversions are OK).Otherwise, you can still use , but you will have to install theextra package , which may involve compilation (at least onLinux systems).Alternative ways of installing missing dependenciesWe strongly recommend that you consider installing Python packages with, as in it is the current , unless directedotherwise by one of the project members (for instance, when testing ordebugging a new feature or using the source code directly from our gitrepository).  If you are using , you can directly install all thedependencies from the requirements file using .Alternative installation method for Unix systemsWe strongly recommend that you install  and all itsdependencies in a way that does not interfere with the rest of your Pythoninstallation. This is accomplished by the creation of a virtual, or ""virtualenv"".For the initial setup, in a Unix-like operating system, please use thefollowing steps (create/adapt first the directory):cd /directory/where/I/want/my/courses
virtualenv my-coursera
cd my-coursera
source bin/activate
git clone https://github.com/coursera-dl/coursera-dl
cd coursera-dl
pip install -r requirements.txt
./coursera-dl ...
To further download new videos from your classes, simply perform:cd /directory/where/I/want/my/courses/my-coursera
source bin/activate
cd coursera-dl
./coursera-dl ...
We are working on streamlining this whole process so that it is as simple aspossible, but to support older versions of Python and to cope with Courseradisabling SSLv3, we have to take a few extra steps.  In any case, it ishighly recommended that you always install the latest version of thePython interpreter that you can.ArchLinuxAUR package: Installing dependencies on your ownWarning: This method is not recommended unless you have experienceworking with multiple Python environments.You can use the  program to install the dependencies on your own.  Theyare all listed in the  file (and the extra dependenciesneeded for development are listed in the  file).To use this method, you would proceed as:pip install -r requirements.txt
pip install -r requirements-dev.txt
The second line above should only be needed if you intend to help withdevelopment (and help is always welcome) or if a maintainer of the projectasks you to install extra packages for debugging purposes.Once again, before filing bug reports, if you installed the dependencies onyour own, please check that the versions of your modules are at least thoselisted in the  file (and,  file, ifapplicable).DockerIf you prefer you can run this software inside Docker:docker run --rm -it -v \
    ""$(pwd):/courses"" \
    courseradl/courseradl -u <USER> -p <PASSWORD>
Or using netrc file:docker run --rm -it \
    -v ""$(pwd):/courses"" -v ""$HOME/.netrc:/netrc"" \
    courseradl/courseradl -n /netrc
The actual working dir for coursera-dl is /courses, all courses will bedownloaded there if you don't specify otherwise.WindowsBe sure that the Python install path is added to the PATH system environmentvariables. This can be found in Control Panel > System > Advanced SystemSettings > Environment Variables.Example:
C:\Python39\Scripts\;C:\Python39\;
Or if you have restricted installation permissions and you've installed Pythonunder AppData, add this to your PATH.Example:
C:\Users\<user>\AppData\Local\Programs\Python\Python39-32\Scripts;C:\Users\<user>\AppData\Local\Programs\Python\Python39-32;
Coursera-dl can now be run from commandline or powershell.Create an account with CourseraIf you don't already have one, create a  account and enroll ina class. See https://www.coursera.org/courses for the list of classes.Running the scriptRefer to  for a complete, up-to-date reference on the runtime optionssupported by this utility.Run the script to download the materials by providing your Coursera accountcredentials (e.g. email address and password or a  file), theclass names, as well as any additional parameters:    General:                     coursera-dl -u <user> -p <pass> modelthinking-004

    With CAUTH parameter:	 coursera-dl -ca 'some-ca-value-from-browser' modelthinking-004
If you don't want to type your password in command line as plain text, you can use thescript without  option. In this case you will be prompted for password  once thescript is run.Here are some examples of how to invoke  from the command line:    Without -p field:            coursera-dl -u <user> modelthinking-004
    Multiple classes:            coursera-dl -u <user> -p <pass> saas historyofrock1-001 algo-2012-002
    Filter by section name:      coursera-dl -u <user> -p <pass> -sf ""Chapter_Four"" crypto-004
    Filter by lecture name:      coursera-dl -u <user> -p <pass> -lf ""3.1_"" ml-2012-002
    Download only ppt files:     coursera-dl -u <user> -p <pass> -f ""ppt"" qcomp-2012-001
    Use a ~/.netrc file:         coursera-dl -n -- matrix-001
    Get the preview classes:     coursera-dl -n -b ni-001
	Download videos at 720p:     coursera-dl -n --video-resolution 720p ni-001
    Specify download path:       coursera-dl -n --path=C:\Coursera\Classes\ comnetworks-002
    Display help:                coursera-dl --help

    Maintain a list of classes in a dir:
      Initialize:              mkdir -p CURRENT/{class1,class2,..classN}
      Update:                  coursera-dl -n --path CURRENT `\ls CURRENT`
Note: If your  command is aliased to display a colorized output, youmay experience problems.  Be sure to escape the  command (use ) toassure that no special characters get sent to the script.Note that we do support the New Platform (""on-demand"") courses.By default, videos are downloaded at 540p resolution. For on-demand courses, the flag accepts 360p, 540p, and 720p values.To download just the  and/or  subtitle files instead of the videos,use  or whatever format the videosare encoded in and desired languages for subtitles.On nix platforms, the use of a  file is a good alternative tospecifying both your username (i.e., your email address) and password everytime on the command line. To use it, simply add a line like the one below toa file named  in your home directory (or the , if youare using Windows) with contents like:    machine coursera-dl login <user> password <pass>
Create the file if it doesn't exist yet.  From then on, you can switch fromusing  and  to simply call  with the option instead.  This is especially convenient, as typing usernames (emailaddresses) and passwords directly on the command line can get tiresome (evenmore if you happened to choose a ""strong"" password).Alternatively, if you want to store your preferred parameters (which mightalso include your username and password), create a file named where the script is supposed to be executed, with the following format:    --username <user>
    --password <pass>
    --subtitle-language en,zh-CN|zh-TW
    --download-quizzes
    #--mathjax-cdn https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js
    # more other parameters
Parameters which are specified in the file will be overriden if they areprovided again on the commandline.Note: In , all the parameters should not be wrappedwith quotes.Resuming downloadsIn default mode when you interrupt the download process by pressingCTRL+C, partially downloaded files will be deleted from your disk andyou have to start the download process from the beginning. If yourdownload was interrupted by something other than KeyboardInterrupt(CTRL+C) like sudden system crash, partially downloaded files willremain on your disk and the next time you start the process again,these files will be discarded from download list!, therefore it's yourjob to delete them manually before next start. For this reason weadded an option called  which continues your downloads fromwhere they stopped:coursera-dl -u <user> -p <pass> --resume sdn1-001
This option can also be used with external downloaders:coursera-dl --wget -u <user> -p <pass> --resume sdn1-001
Note 1: Some external downloaders use their own built-in resume featurewhich may not be compatible with others, so use them at your own risk.Note 2: Remember that in resume mode, interrupted files WON'T be deleted fromyour disk.NOTE: If your password contains punctuation, quotes or other ""funnycharacters"" (e.g., , , , ,  and so on), then you may have toescape them from your shell. With bash or other Bourne-shell clones (andprobably with many other shells) one of the better ways to do so is toenclose your password in single quotes, so that you don't run intoproblems.  See  for more information.TroubleshootingIf you have problems when downloading class materials, please try to see ifone of the following actions solve your problem:China issuesIf you are from China and you're having problems downloading videos,adding ""52.84.167.78 d3c33hcgiwev3.cloudfront.net"" in the hosts file(/etc/hosts) and freshing DNS with ""ipconfig/flushdns"" may work(see https://github.com/googlehosts/hosts for more info).Found 0 sections and 0 lectures on this pageFirst of all, make sure you are enrolled to the course you want to download.Many old courses have already closed enrollment so often it's not anoption. In this case, try downloading with  option. Somecourses allow to download lecture materials without enrolling, butit's not common and is not guaranteed to work for every course.Finally, you can download the videos if you have, at least, the indexfile that lists all the course materials. Maybe your friend who is enrolledcould save that course page for you. In that case use the option.Alternatively you may want to try this various browser extensions designed forthis problem.If none of the above works for you, there is nothing we can do.Download timeoutsCoursera-dl supports external downloaders but note that they are only used todownload materials after the syllabus has been parsed, e.g. videos, PDFs, somehandouts and additional files (syllabus is always downloaded using the internaldownloader). If you experience problems with downloading such materials, you maywant to start using external downloader and configure its timeout values. Forexample, you can use aria2c downloader by passing  option:coursera-dl -n --path . --aria2  <course-name>
And put this into aria2c's configuration file  to reducetimeouts:connect-timeout=2
timeout=2
bt-stop-timeout=1
Timeout configuration for internal downloader is not supported.Windows: proxy supportIf you're on Windows behind a proxy, set up the environment variablesbefore running the script as follows:set HTTP_PROXY=http://host:port
set HTTPS_PROXY=http://host:port
Related discussion: Windows: Failed to create processIn or wherever Python installed (above is default for Windows)edit below file in idle: (right click on script name and select 'edit with idle in menu)coursera-dl-script
from#!c:\users\<user>\appdata\local\programs\python\python39-32\python.exe
to#""!c:\users\<user>\appdata\local\programs\python\python39-32\python.exe""
(add quotes). This is a known pip bug.Source:  SSLError: [Errno 1] _ssl.c:504: error:14094410:SSL routines:SSL3_READ_BYTES:sslv3 alert handshake failureThis is a known error, please do not report about this error message! The problem is in YOUR environment. To fix it, do the following:sudo apt-get install build-essential python-dev libssl-dev libffi-dev
pip install --user urllib3 pyasn1 ndg-httpsclient pyOpenSSL
If the error remains, try installing coursera-dl from github following this instruction:https://github.com/coursera-dl/coursera-dl#alternative-installation-method-for-unix-systemsIf you still have the problem, please read the following issues for more ideas on how to fix it:This is also worth reading:https://urllib3.readthedocs.io/en/latest/security.html#insecureplatformwarningAlternative CDN for When saving a course page, we enabled  rendering for math equations, byinjecting  in the header. The script is using a cdn service providedby . However, thaturl is not accessible in some countries/regions, you can provide a parameter to specify the  file that isaccessible in your region.Reporting issuesBefore reporting any issue please follow the steps below:Filing an issue/Reporting a bugWhen reporting bugs against , please don't forget to includeenough information so that you can help us help you:FeedbackI enjoy getting feedback. Here are a few of the comments I've received:ContactPlease, post bugs and issues on . Please, DON'T send supportrequests privately to the maintainers! We are quite swamped with day-to-dayactivities. If you have problems, PLEASE, file them on the issue tracker."
https://github.com/hankcs/HanLP,中文分词 词性标注 命名实体识别 依存句法分析 成分句法分析 语义依存分析 语义角色标注 指代消解 风格转换 语义相似度 新词发现 关键词短语提取 自动摘要 文本分类聚类 拼音简繁转换 自然语言处理,"面向生产环境的多语种自然语言处理工具包，基于PyTorch和TensorFlow 2.x双引擎，目标是普及落地最前沿的NLP技术。HanLP具备功能完善、精度准确、性能高效、语料时新、架构清晰、可自定义的特点。借助世界上最大的多语种语料库，HanLP2.1支持包括简繁中英日俄法德在内的上的10种联合任务以及多种单任务。HanLP预训练了十几种任务上的数十个模型并且正在持续迭代语料库与模型：| 功能                                                         | RESTful                                                      | 多任务                                                       | 单任务                                                       | 模型                                                         | 标注标准                                                     || ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ ||               |  |  |  |  | 、 ||           |  |  |  |  | 、、 ||       |  |  |  |  | 、、 ||       |  |  |  |  | 、、 ||       |  |  |  |  |  ||       |  |  |  |  |  ||       |  |  |  |  |  ||       |  | 暂无                                                         |  |  |  ||           |  | 暂无                                                         | 暂无                                                         | 暂无                                                         | OntoNotes                                                    ||     |  | 暂无                                                         |  |  | 暂无                                                         ||       |  | 暂无                                                         | 暂无                                                         | 暂无                                                         | 暂无                                                         ||  |  | 暂无                                                         | 暂无                                                         | 暂无                                                         | 暂无                                                         ||   |  | 暂无                                                         | 暂无                                                         | 暂无                                                         | 暂无                                                         ||   |  | 暂无                                                         | 暂无                                                         | 暂无                                                         | 暂无                                                         ||       |  | 暂无                                                         | 暂无                                                         | 暂无                                                         | 暂无                                                         ||  |  | 暂无                                                         | 暂无                                                         | 暂无                                                         | 暂无                                                         ||     |  | 暂无                                                         | 暂无                                                         | 暂无                                                         |                                                     ||  |  | 暂无                                                         |  | 暂无                                                         |  |量体裁衣，HanLP提供RESTful和native两种API，分别面向轻量级和海量级两种场景。无论何种API何种语言，HanLP接口在语义上保持一致，在代码上坚持开源。如果您在研究中使用了HanLP，请引用我们的。轻量级RESTful API仅数KB，适合敏捷开发、移动APP等场景。简单易用，无需GPU配环境，秒速安装。语料更多、模型更大、精度更高，强烈推荐。服务器GPU算力有限，匿名用户配额较少，。Pythonpip install hanlp_restful
创建客户端，填入服务器地址和秘钥：from hanlp_restful import HanLPClient
HanLP = HanLPClient('https://www.hanlp.com/api', auth=None, language='zh') # auth不填则匿名，zh中文，mul多语种
Golang安装  ，创建客户端，填入服务器地址和秘钥：HanLP := hanlp.HanLPClient(hanlp.WithAuth(""""),hanlp.WithLanguage(""zh"")) // auth不填则匿名，zh中文，mul多语种
Java在中添加依赖：<dependency>
    <groupId>com.hankcs.hanlp.restful</groupId>
    <artifactId>hanlp-restful</artifactId>
    <version>0.0.12</version>
</dependency>
创建客户端，填入服务器地址和秘钥：HanLPClient HanLP = new HanLPClient(""https://www.hanlp.com/api"", null, ""zh""); // auth不填则匿名，zh中文，mul多语种
快速上手无论何种开发语言，调用接口，传入一篇文章，得到HanLP精准的分析结果。HanLP.parse(""2021年HanLPv2.1为生产环境带来次世代最先进的多语种NLP技术。阿婆主来到北京立方庭参观自然语义科技公司。"")
更多功能包括语义相似度、风格转换、指代消解等，请参考和。海量级native API依赖PyTorch、TensorFlow等深度学习技术，适合专业NLP工程师、研究者以及本地海量数据场景。要求Python 3.6至3.10，支持Windows，推荐*nix。可以在CPU上运行，推荐GPU/TPU。安装PyTorch版：pip install hanlp
HanLP发布的模型分为多任务和单任务两种，多任务速度快省显存，单任务精度高更灵活。多任务模型HanLP的工作流程为加载模型然后将其当作函数调用，例如下列联合多任务模型：import hanlp
HanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH) # 世界最大中文语料库
HanLP(['2021年HanLPv2.1为生产环境带来次世代最先进的多语种NLP技术。', '阿婆主来到北京立方庭参观自然语义科技公司。'])
Native API的输入单位为句子，需使用或先行分句。RESTful和native两种API的语义设计完全一致，用户可以无缝互换。简洁的接口也支持灵活的参数，常用的技巧有：单任务模型根据我们的，多任务学习的优势在于速度和显存，然而精度往往不如单任务模型。所以，HanLP预训练了许多单任务模型并设计了优雅的将其组装起来。import hanlp
HanLP = hanlp.pipeline() \
    .append(hanlp.utils.rules.split_sentence, output_key='sentences') \
    .append(hanlp.load('FINE_ELECTRA_SMALL_ZH'), output_key='tok') \
    .append(hanlp.load('CTB9_POS_ELECTRA_SMALL'), output_key='pos') \
    .append(hanlp.load('MSRA_NER_ELECTRA_SMALL_ZH'), output_key='ner', input_key='tok') \
    .append(hanlp.load('CTB9_DEP_ELECTRA_SMALL', conll=0), output_key='dep', input_key='tok')\
    .append(hanlp.load('CTB9_CON_ELECTRA_SMALL'), output_key='con', input_key='tok')
HanLP('2021年HanLPv2.1为生产环境带来次世代最先进的多语种NLP技术。阿婆主来到北京立方庭参观自然语义科技公司。')
更多功能，请参考和了解更多模型与用法。输出格式无论何种API何种开发语言何种自然语言，HanLP的输出统一为格式兼容的:{
  ""tok/fine"": [
    [""2021年"", ""HanLPv2.1"", ""为"", ""生产"", ""环境"", ""带来"", ""次"", ""世代"", ""最"", ""先进"", ""的"", ""多"", ""语种"", ""NLP"", ""技术"", ""。""],
    [""阿婆主"", ""来到"", ""北京"", ""立方庭"", ""参观"", ""自然"", ""语义"", ""科技"", ""公司"", ""。""]
  ],
  ""tok/coarse"": [
    [""2021年"", ""HanLPv2.1"", ""为"", ""生产"", ""环境"", ""带来"", ""次世代"", ""最"", ""先进"", ""的"", ""多语种"", ""NLP"", ""技术"", ""。""],
    [""阿婆主"", ""来到"", ""北京立方庭"", ""参观"", ""自然语义科技公司"", ""。""]
  ],
  ""pos/ctb"": [
    [""NT"", ""NR"", ""P"", ""NN"", ""NN"", ""VV"", ""JJ"", ""NN"", ""AD"", ""JJ"", ""DEG"", ""CD"", ""NN"", ""NR"", ""NN"", ""PU""],
    [""NN"", ""VV"", ""NR"", ""NR"", ""VV"", ""NN"", ""NN"", ""NN"", ""NN"", ""PU""]
  ],
  ""pos/pku"": [
    [""t"", ""nx"", ""p"", ""vn"", ""n"", ""v"", ""b"", ""n"", ""d"", ""a"", ""u"", ""a"", ""n"", ""nx"", ""n"", ""w""],
    [""n"", ""v"", ""ns"", ""ns"", ""v"", ""n"", ""n"", ""n"", ""n"", ""w""]
  ],
  ""pos/863"": [
    [""nt"", ""w"", ""p"", ""v"", ""n"", ""v"", ""a"", ""nt"", ""d"", ""a"", ""u"", ""a"", ""n"", ""ws"", ""n"", ""w""],
    [""n"", ""v"", ""ns"", ""n"", ""v"", ""n"", ""n"", ""n"", ""n"", ""w""]
  ],
  ""ner/pku"": [
    [],
    [[""北京立方庭"", ""ns"", 2, 4], [""自然语义科技公司"", ""nt"", 5, 9]]
  ],
  ""ner/msra"": [
    [[""2021年"", ""DATE"", 0, 1], [""HanLPv2.1"", ""ORGANIZATION"", 1, 2]],
    [[""北京"", ""LOCATION"", 2, 3], [""立方庭"", ""LOCATION"", 3, 4], [""自然语义科技公司"", ""ORGANIZATION"", 5, 9]]
  ],
  ""ner/ontonotes"": [
    [[""2021年"", ""DATE"", 0, 1], [""HanLPv2.1"", ""ORG"", 1, 2]],
    [[""北京立方庭"", ""FAC"", 2, 4], [""自然语义科技公司"", ""ORG"", 5, 9]]
  ],
  ""srl"": [
    [[[""2021年"", ""ARGM-TMP"", 0, 1], [""HanLPv2.1"", ""ARG0"", 1, 2], [""为生产环境"", ""ARG2"", 2, 5], [""带来"", ""PRED"", 5, 6], [""次世代最先进的多语种NLP技术"", ""ARG1"", 6, 15]], [[""最"", ""ARGM-ADV"", 8, 9], [""先进"", ""PRED"", 9, 10], [""技术"", ""ARG0"", 14, 15]]],
    [[[""阿婆主"", ""ARG0"", 0, 1], [""来到"", ""PRED"", 1, 2], [""北京立方庭"", ""ARG1"", 2, 4]], [[""阿婆主"", ""ARG0"", 0, 1], [""参观"", ""PRED"", 4, 5], [""自然语义科技公司"", ""ARG1"", 5, 9]]]
  ],
  ""dep"": [
    [[6, ""tmod""], [6, ""nsubj""], [6, ""prep""], [5, ""nn""], [3, ""pobj""], [0, ""root""], [8, ""amod""], [15, ""nn""], [10, ""advmod""], [15, ""rcmod""], [10, ""assm""], [13, ""nummod""], [15, ""nn""], [15, ""nn""], [6, ""dobj""], [6, ""punct""]],
    [[2, ""nsubj""], [0, ""root""], [4, ""nn""], [2, ""dobj""], [2, ""conj""], [9, ""nn""], [9, ""nn""], [9, ""nn""], [5, ""dobj""], [2, ""punct""]]
  ],
  ""sdp"": [
    [[[6, ""Time""]], [[6, ""Exp""]], [[5, ""mPrep""]], [[5, ""Desc""]], [[6, ""Datv""]], [[13, ""dDesc""]], [[0, ""Root""], [8, ""Desc""], [13, ""Desc""]], [[15, ""Time""]], [[10, ""mDegr""]], [[15, ""Desc""]], [[10, ""mAux""]], [[8, ""Quan""], [13, ""Quan""]], [[15, ""Desc""]], [[15, ""Nmod""]], [[6, ""Pat""]], [[6, ""mPunc""]]],
    [[[2, ""Agt""], [5, ""Agt""]], [[0, ""Root""]], [[4, ""Loc""]], [[2, ""Lfin""]], [[2, ""ePurp""]], [[8, ""Nmod""]], [[9, ""Nmod""]], [[9, ""Nmod""]], [[5, ""Datv""]], [[5, ""mPunc""]]]
  ],
  ""con"": [
    [""TOP"", [[""IP"", [[""NP"", [[""NT"", [""2021年""]]]], [""NP"", [[""NR"", [""HanLPv2.1""]]]], [""VP"", [[""PP"", [[""P"", [""为""]], [""NP"", [[""NN"", [""生产""]], [""NN"", [""环境""]]]]]], [""VP"", [[""VV"", [""带来""]], [""NP"", [[""ADJP"", [[""NP"", [[""ADJP"", [[""JJ"", [""次""]]]], [""NP"", [[""NN"", [""世代""]]]]]], [""ADVP"", [[""AD"", [""最""]]]], [""VP"", [[""JJ"", [""先进""]]]]]], [""DEG"", [""的""]], [""NP"", [[""QP"", [[""CD"", [""多""]]]], [""NP"", [[""NN"", [""语种""]]]]]], [""NP"", [[""NR"", [""NLP""]], [""NN"", [""技术""]]]]]]]]]], [""PU"", [""。""]]]]]],
    [""TOP"", [[""IP"", [[""NP"", [[""NN"", [""阿婆主""]]]], [""VP"", [[""VP"", [[""VV"", [""来到""]], [""NP"", [[""NR"", [""北京""]], [""NR"", [""立方庭""]]]]]], [""VP"", [[""VV"", [""参观""]], [""NP"", [[""NN"", [""自然""]], [""NN"", [""语义""]], [""NN"", [""科技""]], [""NN"", [""公司""]]]]]]]], [""PU"", [""。""]]]]]]
  ]
}
特别地，Python RESTful和native API支持基于等宽字体的，能够直接将语言学结构在控制台内可视化出来：HanLP(['2021年HanLPv2.1为生产环境带来次世代最先进的多语种NLP技术。', '阿婆主来到北京立方庭参观自然语义科技公司。']).pretty_print()

Dep Tree    	Token    	Relati	PoS	Tok      	NER Type        	Tok      	SRL PA1     	Tok      	SRL PA2     	Tok      	PoS    3       4       5       6       7       8       9 
────────────	─────────	──────	───	─────────	────────────────	─────────	────────────	─────────	────────────	─────────	─────────────────────────────────────────────────────────
 ┌─────────►	2021年    	tmod  	NT 	2021年    	───►DATE        	2021年    	───►ARGM-TMP	2021年    	            	2021年    	NT ───────────────────────────────────────────►NP ───┐   
 │┌────────►	HanLPv2.1	nsubj 	NR 	HanLPv2.1	───►ORGANIZATION	HanLPv2.1	───►ARG0    	HanLPv2.1	            	HanLPv2.1	NR ───────────────────────────────────────────►NP────┤   
 ││┌─►┌─────	为        	prep  	P  	为        	                	为        	◄─┐         	为        	            	为        	P ───────────┐                                       │   
 │││  │  ┌─►	生产       	nn    	NN 	生产       	                	生产       	  ├►ARG2    	生产       	            	生产       	NN ──┐       ├────────────────────────►PP ───┐       │   
 │││  └─►└──	环境       	pobj  	NN 	环境       	                	环境       	◄─┘         	环境       	            	环境       	NN ──┴►NP ───┘                               │       │   
┌┼┴┴────────	带来       	root  	VV 	带来       	                	带来       	╟──►PRED    	带来       	            	带来       	VV ──────────────────────────────────┐       │       │   
││       ┌─►	次        	amod  	JJ 	次        	                	次        	◄─┐         	次        	            	次        	JJ ───►ADJP──┐                       │       ├►VP────┤   
││  ┌───►└──	世代       	nn    	NN 	世代       	                	世代       	  │         	世代       	            	世代       	NN ───►NP ───┴►NP ───┐               │       │       │   
││  │    ┌─►	最        	advmod	AD 	最        	                	最        	  │         	最        	───►ARGM-ADV	最        	AD ───────────►ADVP──┼►ADJP──┐       ├►VP ───┘       ├►IP
││  │┌──►├──	先进       	rcmod 	JJ 	先进       	                	先进       	  │         	先进       	╟──►PRED    	先进       	JJ ───────────►VP ───┘       │       │               │   
││  ││   └─►	的        	assm  	DEG	的        	                	的        	  ├►ARG1    	的        	            	的        	DEG──────────────────────────┤       │               │   
││  ││   ┌─►	多        	nummod	CD 	多        	                	多        	  │         	多        	            	多        	CD ───►QP ───┐               ├►NP ───┘               │   
││  ││┌─►└──	语种       	nn    	NN 	语种       	                	语种       	  │         	语种       	            	语种       	NN ───►NP ───┴────────►NP────┤                       │   
││  │││  ┌─►	NLP      	nn    	NR 	NLP      	                	NLP      	  │         	NLP      	            	NLP      	NR ──┐                       │                       │   
│└─►└┴┴──┴──	技术       	dobj  	NN 	技术       	                	技术       	◄─┘         	技术       	───►ARG0    	技术       	NN ──┴────────────────►NP ───┘                       │   
└──────────►	。        	punct 	PU 	。        	                	。        	            	。        	            	。        	PU ──────────────────────────────────────────────────┘   

Dep Tree    	Tok	Relat	Po	Tok	NER Type        	Tok	SRL PA1 	Tok	SRL PA2 	Tok	Po    3       4       5       6 
────────────	───	─────	──	───	────────────────	───	────────	───	────────	───	────────────────────────────────
         ┌─►	阿婆主	nsubj	NN	阿婆主	                	阿婆主	───►ARG0	阿婆主	───►ARG0	阿婆主	NN───────────────────►NP ───┐   
┌┬────┬──┴──	来到 	root 	VV	来到 	                	来到 	╟──►PRED	来到 	        	来到 	VV──────────┐               │   
││    │  ┌─►	北京 	nn   	NR	北京 	───►LOCATION    	北京 	◄─┐     	北京 	        	北京 	NR──┐       ├►VP ───┐       │   
││    └─►└──	立方庭	dobj 	NR	立方庭	───►LOCATION    	立方庭	◄─┴►ARG1	立方庭	        	立方庭	NR──┴►NP ───┘       │       │   
│└─►┌───────	参观 	conj 	VV	参观 	                	参观 	        	参观 	╟──►PRED	参观 	VV──────────┐       ├►VP────┤   
│   │  ┌───►	自然 	nn   	NN	自然 	◄─┐             	自然 	        	自然 	◄─┐     	自然 	NN──┐       │       │       ├►IP
│   │  │┌──►	语义 	nn   	NN	语义 	  │             	语义 	        	语义 	  │     	语义 	NN  │       ├►VP ───┘       │   
│   │  ││┌─►	科技 	nn   	NN	科技 	  ├►ORGANIZATION	科技 	        	科技 	  ├►ARG1	科技 	NN  ├►NP ───┘               │   
│   └─►└┴┴──	公司 	dobj 	NN	公司 	◄─┘             	公司 	        	公司 	◄─┘     	公司 	NN──┘                       │   
└──────────►	。  	punct	PU	。  	                	。  	        	。  	        	。  	PU──────────────────────────┘   
关于标注集含义，请参考及。我们购买、标注或采用了世界上量级最大、种类最多的语料库用于联合多语种多任务学习，所以HanLP的标注集也是覆盖面最广的。训练你自己的领域模型写深度学习模型一点都不难，难的是复现较高的准确率。下列展示了如何在sighan2005 PKU语料库上花6分钟训练一个超越学术界state-of-the-art的中文分词模型。tokenizer = TransformerTaggingTokenizer()
save_dir = 'data/model/cws/sighan2005_pku_bert_base_96.73'
tokenizer.fit(
    SIGHAN2005_PKU_TRAIN_ALL,
    SIGHAN2005_PKU_TEST,  # Conventionally, no devset is used. See Tian et al. (2020).
    save_dir,
    'bert-base-chinese',
    max_seq_len=300,
    char_level=True,
    hard_constraint=True,
    sampler_builder=SortingSamplerBuilder(batch_size=32),
    epochs=3,
    adam_epsilon=1e-6,
    warmup_steps=0.1,
    weight_decay=0.01,
    word_dropout=0.1,
    seed=1660853059,
)
tokenizer.evaluate(SIGHAN2005_PKU_TEST, save_dir)
其中，由于指定了随机数种子，结果一定是。不同于那些虚假宣传的学术论文或商业项目，HanLP保证所有结果可复现。如果你有任何质疑，我们将当作最高优先级的致命性bug第一时间排查问题。请参考了解更多训练脚本。性能HanLP采用的数据预处理与拆分比例与流行方法未必相同，比如HanLP采用了，而非大众使用的阉割版；HanLP使用了语法覆盖更广的，而非学术界沿用的Zhang and Clark (2008)标准；HanLP提出了，而不采用学术界不均匀且遗漏了51个黄金文件的方法。HanLP开源了，力图推动中文NLP的透明化。总之，HanLP只做我们认为正确、先进的事情，而不一定是流行、权威的事情。引用如果你在研究中使用了HanLP，请按如下格式引用：@inproceedings{he-choi-2021-stem,
    title = ""The Stem Cell Hypothesis: Dilemma behind Multi-Task Learning with Transformer Encoders"",
    author = ""He, Han and Choi, Jinho D."",
    booktitle = ""Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"",
    month = nov,
    year = ""2021"",
    address = ""Online and Punta Cana, Dominican Republic"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.emnlp-main.451"",
    pages = ""5555--5577"",
    abstract = ""Multi-task learning with transformer encoders (MTL) has emerged as a powerful technique to improve performance on closely-related tasks for both accuracy and efficiency while a question still remains whether or not it would perform as well on tasks that are distinct in nature. We first present MTL results on five NLP tasks, POS, NER, DEP, CON, and SRL, and depict its deficiency over single-task learning. We then conduct an extensive pruning analysis to show that a certain set of attention heads get claimed by most tasks during MTL, who interfere with one another to fine-tune those heads for their own objectives. Based on this finding, we propose the Stem Cell Hypothesis to reveal the existence of attention heads naturally talented for many tasks that cannot be jointly trained to create adequate embeddings for all of those tasks. Finally, we design novel parameter-free probes to justify our hypothesis and demonstrate how attention heads are transformed across the five tasks during MTL through label analysis."",
}
License源代码HanLP源代码的授权协议为 Apache License 2.0，可免费用做商业用途。请在产品说明中附加HanLP的链接和授权协议。HanLP受版权法保护，侵权必究。自然语义（青岛）科技有限公司HanLP从v1.7版起独立运作，由自然语义（青岛）科技有限公司作为项目主体，主导后续版本的开发，并拥有后续版本的版权。大快搜索HanLP v1.3~v1.65版由大快搜索主导开发，继续完全开源，大快搜索拥有相关版权。上海林原公司HanLP 早期得到了上海林原公司的大力支持，并拥有1.28及前序版本的版权，相关版本也曾在上海林原公司网站发布。预训练模型机器学习模型的授权在法律上没有定论，但本着尊重开源语料库原始授权的精神，如不特别说明，HanLP的多语种模型授权沿用，中文模型授权为仅供研究与教学使用。Referenceshttps://hanlp.hankcs.com/docs/references.html"
https://github.com/bup/bup,"Very efficient backup system based on the git packfile format, providing fast incremental saves and global deduplication (among and within files, including virtual machine images). Please post problems or patches to the mailing list for discussion (see the end of the README below).","bup: It backs things upbup is a program that backs things up.  It's short for ""backup."" Can youbelieve that nobody else has named an open source program ""bup"" after allthis time?  Me neither.Despite its unassuming name, bup is pretty cool.  To give you an idea ofjust how cool it is, I wrote you this poem:                         Bup is teh awesome
                      What rhymes with awesome?
                        I guess maybe possum
                       But that's irrelevant.
		
Hmm.  Did that help?  Maybe prose is more useful after all.Reasons bup is awesomebup has a few advantages over other backup software:Reasons you might want to avoid bupNotable changes introduced by a releaseTest status| main ||--------||  |Getting startedFrom sourceFrom binary packagesBinary packages of bup are known to be built for the following OSes:Using bupThat's all there is to it!Platform specific informationNotes on Debian (and likely, on derivatives like Ubuntu)If your distribution is recent enough, or includes new enough sources, this may be sufficient (run as root):apt-get build-dep bup
Otherwise try this:apt-get install python3-dev python3-fuse
apt-get install python3-pyxattr python3-pytest
apt-get install python3-distutils
apt-get install pkg-config linux-libc-dev libacl1-dev
apt-get install gcc make acl attr rsync
apt-get isntall python3-pytest-xdist # optional (parallel tests)
apt-get install par2 # optional (error correction)
apt-get install libreadline-dev # optional (bup ftp)
apt-get install python3-tornado # optional (bup web)
Notes on FreeBSDNotes on NetBSD/pkgsrcNotes on CygwinNotes on OS XHow it worksBasic storage:bup stores its data in a git-formatted repository.  Unfortunately, gititself doesn't actually behave very well for bup's use case (huge numbers offiles, files with huge sizes, retaining file permissions/ownership areimportant), so we mostly don't use git's code except for a few helperprograms.  For example, bup has its own git packfile writer written inpython.Basically, 'bup split' reads the data on stdin (or from files specified onthe command line), breaks it into chunks using a rolling checksum (similar torsync), and saves those chunks into a new git packfile.  There is at least onegit packfile per backup.When deciding whether to write a particular chunk into the new packfile, bupfirst checks all the other packfiles that exist to see if they already have thatchunk.  If they do, the chunk is skipped.git packs come in two parts: the pack itself (.pack) and the index (.idx).The index is pretty small, and contains a list of all the objects in thepack.  Thus, when generating a remote backup, we don't have to have a copyof the packfiles from the remote server: the local end just downloads a copyof the server's index files, and compares objects against those whengenerating the new pack, which it sends directly to the server.The ""-n"" option to 'bup split' and 'bup save' is the name of the backup youwant to create, but it's actually implemented as a git branch.  So you cando cute things like checkout a particular branch using git, and receive abunch of chunk files corresponding to the file you split.If you use '-b' or '-t' or '-c' instead of '-n', bup split will output alist of blobs, a tree containing that list of blobs, or a commit containingthat tree, respectively, to stdout.  You can use this to construct your ownscripts that do something with those values.The bup index:'bup index' walks through your filesystem and updates a file (whose name is,by default, ~/.bup/bupindex) to contain the name, attributes, and anoptional git SHA1 (blob id) of each file and directory.'bup save' basically just runs the equivalent of 'bup split' a whole bunchof times, once per file in the index, and assembles a git treethat contains all the resulting objects.  Among other things, that makes'git diff' much more useful (compared to splitting a tarball, which isessentially a big binary blob).  However, since bup splits large files intosmaller chunks, the resulting tree structure doesn't exactly correspond towhat git itself would have stored.  Also, the tree format used by 'bup save'will probably change in the future to support storing file ownership, morecomplex file permissions, and so on.If a file has previously been written by 'bup save', then its git blob/treeid is stored in the index.  This lets 'bup save' avoid reading that file toproduce future incremental backups, which means it can go very fast unlessa lot of files have changed.Things that are stupid for now but which we'll fix laterHelp with any of these problems, or others, is very welcome.  Join themailing list (see below) if you'd like to help.More Documentationbup has an extensive set of man pages.  Try using 'bup help' to getstarted, or use 'bup help SUBCOMMAND' for any bup subcommand (like split,join, index, save, etc.) to get details on that command.For further technical details, please see ./DESIGN.How you can helpbup is a work in progress and there are many ways it can still be improved.If you'd like to contribute patches, ideas, or bug reports, please join thebup mailing list:You can find the mailing list archives here:http://groups.google.com/group/bup-list
and you can subscribe by sending a message to:bup-list+subscribe@googlegroups.com
You can also reach us via thebup IRC channel at ircs://irc.libera.chat:6697/bupon the  network or via this.Please see ./HACKING foradditional information, i.e. how to submit patches (hint - no pullrequests), how we handle branches, etc.Have fun,Avery"
https://github.com/spec-first/connexion,Connexion is a modern Python web framework that makes spec-first and api-first development easy.,"Connexion.. image:: https://badges.gitter.im/zalando/connexion.svg:alt: Join the chat at https://gitter.im/zalando/connexion:target: https://gitter.im/zalando/connexion?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge.. image:: https://github.com/zalando/connexion/actions/workflows/pipeline.yml/badge.svg:alt: Build status:target: https://github.com/zalando/connexion/actions/workflows/pipeline.yml.. image:: https://coveralls.io/repos/github/zalando/connexion/badge.svg?branch=main:target: https://coveralls.io/github/zalando/connexion?branch=main:alt: Coveralls status.. image:: https://img.shields.io/pypi/v/connexion.svg:target: https://pypi.python.org/pypi/connexion:alt: Latest Version.. image:: https://img.shields.io/pypi/status/connexion.svg:target: https://pypi.python.org/pypi/connexion:alt: Development Status.. image:: https://img.shields.io/pypi/pyversions/connexion.svg:target: https://pypi.python.org/pypi/connexion:alt: Python Versions.. image:: https://img.shields.io/pypi/l/connexion.svg:target: https://github.com/zalando/connexion/blob/main/LICENSE.txt:alt: LicenseConnexion is a framework that automagically handles HTTP requests based on _(formerly known as Swagger Spec) of your API described in _. Connexion allows you towrite an OpenAPI specification, then maps the endpoints to your Python functions; this makes itunique, as many tools generate the specification based on your Python code. You can describe yourREST API in as much detail as you want; then Connexion guarantees that it will work as youspecified.We built Connexion this way in order to:Connexion Features:Why ConnexionWith Connexion, you write the spec first. Connexion then calls your Pythoncode, handling the mapping from the specification to the code. Thisincentivizes you to write the specification so that all of yourdevelopers can understand what your API does, even before you write asingle line of code.If multiple teams depend on your APIs, you can use Connexion to easily send them the documentation of your API. This guarantees that your API will follow the specification that you wrote. This is a different process from that offered by frameworks such as Hug_, which generates a specification after you've written the code. Some disadvantages of generating specifications based on code is that they often end up lacking details or mix your documentation with the code logic of your application.Other Sources/MentionsNew in Connexion 2.0:How to UsePrerequisitesPython 3.6+Installing ItIn your command line, type:.. code-block:: bash$ pip install connexion
Running ItPlace your API YAML inside a folder in the rootpath of your application (e.g ). Then run:.. code-block:: pythonimport connexion

app = connexion.App(__name__, specification_dir='swagger/')
app.add_api('my_api.yaml')
app.run(port=8080)
See the _ for a samplespecification.Now you're able to run and use Connexion!OAuth 2 Authentication and AuthorizationConnexion supports one of the three OAuth 2 handling methods. (See""TODO"" below.) With Connexion, the API security definition mustinclude a 'x-tokenInfoUrl' or 'x-tokenInfoFunc (or set or  env var respectively). 'x-tokenInfoUrl' must contain anURL to validate and get the _ and 'x-tokenInfoFunc mustcontain a reference to a function used to obtain the token info. When both 'x-tokenInfoUrl'and 'x-tokenInfoFunc' are used, Connexion will prioritize the function method. Connexion expects toreceive the OAuth token in the  header field in theformat described in _ section 2.1. This aspectrepresents a significant difference from the usual OAuth flow.Dynamic Rendering of Your SpecificationConnexion uses Jinja2_ to allow specification parameterization through the  parameter. You can define specification arguments for the application either globally (via the  constructor) or for each specific API (via the  method):.. code-block:: pythonapp = connexion.App(__name__, specification_dir='swagger/',
                    arguments={'global': 'global_value'})
app.add_api('my_api.yaml', arguments={'api_local': 'local_value'})
app.run(port=8080)
When a value is provided both globally and on the API, the API value will take precedence.Endpoint Routing to Your Python ViewsConnexion uses the  from each _ toidentify which Python function should handle each URL.Explicit Routing:.. code-block:: yamlpaths:
  /hello_world:
    post:
      operationId: myapp.api.hello_world
If you provide this path in your specification POST requests to, it will be handled by the function in the  module. Optionally, you can include (or ) in youroperation definition, making  relative:.. code-block:: yamlpaths:
  /hello_world:
    post:
      x-swagger-router-controller: myapp.api
      operationId: hello_world
Keep in mind that Connexion follows how _ and therefore HEAD requests will be handled by the  specified under GET in the specification. If both methods are supported,  can be used to determine which request was made.Automatic RoutingTo customize this behavior, Connexion can use alternative--for example, . The will compose an  based on the path and HTTP method ofthe endpoints in your specification:.. code-block:: pythonfrom connexion.resolver import RestyResolver

app = connexion.App(__name__)
app.add_api('swagger.yaml', resolver=RestyResolver('api'))
.. code-block:: yamlpaths:/:get:# Implied operationId: api.get/foo:get:# Implied operationId: api.foo.searchpost:# Implied operationId: api.foo.post '/foo/{id}':
   get:
      # Implied operationId: api.foo.get
   put:
      # Implied operationId: api.foo.put
   copy:
      # Implied operationId: api.foo.copy
   delete:
      # Implied operationId: api.foo.delete
 will give precedence to any  encountered in the specification. It will also respect. You can import and extend  to implement your own (and function) resolution algorithm.Automatic Parameter HandlingConnexion automatically maps the parameters defined in your endpoint specification to arguments of your Python views as named parameters, and, whenever possible, with value casting. Simply define the endpoint's parameters with the same names as your views arguments.As an example, say you have an endpoint specified as:.. code-block:: yamlpaths:
  /foo:
    get:
      operationId: api.foo_get
      parameters:
        - name: message
          description: Some message.
          in: query
          type: string
          required: true
And the view function:.. code-block:: python# api.py file

def foo_get(message):
    # do something
    return 'You send the message: {}'.format(message), 200
In this example, Connexion automatically recognizes that your viewfunction expects an argument named  and assigns the valueof the endpoint parameter  to your view function... note:: In the OpenAPI 3.x.x spec, the requestBody does not have a name.By default it will be passed in as 'body'. You can optionallyprovide the x-body-name parameter in your requestBody(or legacy position within the requestBody schema)to override the name of the parameter that will be passed to yourhandler function... code-block:: yaml/path
  post:
    requestBody:
      x-body-name: body
      content:
        application/json:
          schema:
            # legacy location here should be ignored because the preferred location for x-body-name is at the requestBody level above
            x-body-name: this_should_be_ignored
            $ref: '#/components/schemas/someComponent'
.. warning:: When you define a parameter at your endpoint as not required, andthis argument does not have default value in your Python view, you will geta ""missing positional argument"" exception whenever you call this endpointWITHOUT the parameter. Provide a default value for a named argument or use dict.Type casting^^^^^^^^^^^^Whenever possible, Connexion will try to parse your argument values anddo type casting to related Python native values. The currentavailable type castings are:+--------------+-------------+| OpenAPI Type | Python Type |+==============+=============+| integer      | int         |+--------------+-------------+| string       | str         |+--------------+-------------+| number       | float       |+--------------+-------------+| boolean      | bool        |+--------------+-------------+| array        | list        |+--------------+-------------+| null         | None        |+--------------+-------------+| object       | dict        |+--------------+-------------+If you use the  type In the Swagger definition, you can define the so that it won't be recognized. Connexion currentlysupports collection formats ""pipes"" and ""csv"". The default format is ""csv"".Connexion is opinionated about how the URI is parsed for  types.The default behavior for query parameters that have been defined multipletimes is to use the right-most value. For example, if you provide a URI withthe query string , connexion will set.You can override this behavior by specifying the URI parser in the app orapi options... code-block:: pythonfrom connexion.decorators.uri_parsing import AlwaysMultiURIParseroptions = {'uri_parser_class': AlwaysMultiURIParser}app = connexion.App(name, specification_dir='swagger/', options=options)You can implement your own URI parsing behavior by inheriting from.There are a handful of URI parsers included with connection.+----------------------+---------------------------------------------------------------------------+| OpenAPIURIParser     | This parser adheres to the OpenAPI 3.x.x spec, and uses the      || default: OpenAPI 3.0 | parameter. Query parameters are parsed from left to right, so if a query  ||                      | parameter is defined twice, then the right-most definition will take      ||                      | precedence. For example, if you provided a URI with the query string      ||                      | , and , then connexion   ||                      | will set . For additional information see    ||                      | _.                                              |+----------------------+---------------------------------------------------------------------------+| Swagger2URIParser    | This parser adheres to the Swagger 2.0 spec, and will only join together  || default: OpenAPI 2.0 | multiple instance of the same query parameter if the  ||                      | is set to . Query parameters are parsed from left to right, so   ||                      | if a query parameter is defined twice, then the right-most definition     ||                      | wins. For example, if you provided a URI with the query string            ||                      | , and , then     ||                      | connexion will set                           |+----------------------+---------------------------------------------------------------------------+| FirstValueURIParser  | This parser behaves like the Swagger2URIParser, except that it prefers    ||                      | the first defined value. For example, if you provided a URI with the query||                      | string  and      ||                      | hen connexion will set                       |+----------------------+---------------------------------------------------------------------------+| AlwaysMultiURIParser | This parser is backwards compatible with Connexion 1.x. It joins together ||                      | multiple instances of the same query parameter.                           |+----------------------+---------------------------------------------------------------------------+Parameter validation^^^^^^^^^^^^^^^^^^^^Connexion can apply strict parameter validation for query and form dataparameters.  When this is enabled, requests that include parameters not definedin the swagger spec return a 400 error.  You can enable it when adding the APIto your application:.. code-block:: pythonapp.add_api('my_apy.yaml', strict_validation=True)
API Versioning and basePathSetting a base path is useful for versioned APIs. An example ofa base path would be the  in .If you are using OpenAPI 3.x.x, you set your base URL path in theservers block of the specification. You can either specify a fullURL, or just a relative path... code-block:: yamlservers:
  - url: https://MYHOST/1.0
    description: full url example
  - url: /1.0
    description: relative path example

paths:
  ...
If you are using OpenAPI 2.0, you can define a  on the top levelof your OpenAPI 2.0 specification... code-block:: yamlbasePath: /1.0

paths:
  ...
If you don't want to include the base path in your specification, youcan provide it when adding the API to your application:.. code-block:: pythonapp.add_api('my_api.yaml', base_path='/1.0')
Swagger JSONConnexion makes the OpenAPI/Swagger specification in JSON formatavailable from either  (for OpenAPI 2.0) or (for OpenAPI 3.x.x) at the base path of the API.For example, if your base path was , then your spec would beavailable at .You can disable serving the spec JSON at the application level:.. code-block:: pythonoptions = {""serve_spec"": False}
app = connexion.App(__name__, specification_dir='openapi/',
                    options=options)
app.add_api('my_api.yaml')
You can also disable it at the API level:.. code-block:: pythonoptions = {""serve_spec"": False}
app = connexion.App(__name__, specification_dir='openapi/')
app.add_api('my_api.yaml', options=options)
HTTPS SupportWhen specifying HTTPS as the scheme in the API YAML file, all the URIsin the served Swagger UI are HTTPS endpoints. The problem: The defaultserver that runs is a ""normal"" HTTP server. This means that theSwagger UI cannot be used to play with the API. What is the correctway to start a HTTPS server when using Connexion?One way, _, looks like this:.. code-block:: pythonfrom OpenSSL import SSLcontext = SSL.Context(SSL.SSLv23_METHOD)context.use_privatekey_file('yourserver.key')context.use_certificate_file('yourserver.crt')app.run(host='127.0.0.1', port='12344',debug=False/True, ssl_context=context)However, Connexion doesn't provide an ssl_context parameter. This isbecause Flask doesn't, either--but it uses  to send theparameters to the underlying _ server.The Swagger UI ConsoleThe Swagger UI for an API is available through pip extras.You can install it with .It will be served up at  where  is thebase path of the API.You can disable the Swagger UI at the application level:.. code-block:: pythonapp = connexion.App(__name__, specification_dir='openapi/',
                    options={""swagger_ui"": False})
app.add_api('my_api.yaml')
You can also disable it at the API level:.. code-block:: pythonapp = connexion.App(__name__, specification_dir='openapi/')
app.add_api('my_api.yaml', options={""swagger_ui"": False})
If necessary, you can explicitly specify the path to the directory withswagger-ui to not use the connexion[swagger-ui] distro.In order to do this, you should specify the following option:.. code-block:: pythonoptions = {'swagger_path': '/path/to/swagger_ui/'}app = connexion.App(name, specification_dir='openapi/', options=options)If you wish to provide your own swagger-ui distro, note that connexionexpects a jinja2 file called  in order to load thecorrect  by default. Your  file can use the jinja variable for this purpose:.. code-block::const ui = SwaggerUIBundle({ url: ""{{ openapi_spec_url }}""})
Additionally, if you wish to use swagger-ui-3.x.x, it is also provided byinstalling connexion[swagger-ui], and can be enabled like this:.. code-block:: pythonfrom swagger_ui_bundle import swagger_ui_3_pathoptions = {'swagger_path': swagger_ui_3_path}app = connexion.App(name, specification_dir='swagger/', options=options)Server BackendBy default Connexion uses the Flask_ server. For asynchronousapplications, you can also use Tornado_ as the HTTP server. To dothis, set your server to :.. code-block:: pythonimport connexion

app = connexion.App(__name__, specification_dir='swagger/')
app.run(server='tornado', port=8080)
You can use the Flask WSGI app with any WSGI container, e.g. _ (this is common):.. code-block:: pythonapp = connexion.App(__name__, specification_dir='swagger/')
application = app.app # expose global WSGI application object
You can use the  framework as server backend as well:.. code-block:: pythonimport connexion

app = connexion.AioHttpApp(__name__, specification_dir='swagger/')
app.run(port=8080)
.. note:: Also check aiohttp handler examples_.Set up and run the installation code:.. code-block:: bash$ sudo pip3 install uwsgi
$ uwsgi --http :8080 -w app -p 16  # use 16 worker processes
See the _ for more information... _using Flask with uWSGI: http://flask.pocoo.org/docs/latest/deploying/uwsgi/.. _uWSGI documentation: https://uwsgi-docs.readthedocs.org/.. _examples: https://docs.aiohttp.org/en/stable/web.html#handlerDocumentationAdditional information is available at _.ChangesA full changelog is maintained on the _... _GitHub releases page: https://github.com/zalando/connexion/releasesContributing to Connexion/TODOsWe welcome your ideas, issues, and pull requests. Just follow theusual/standard GitHub practices.For easy development, install connexion using poetry with all extras, andinstall the pre-commit hooks to automatically run black formatting and static analysis checks... code-block:: bashpoetry install --all-extras
pre-commit install
You can find out more about how Connexion works and where to apply your changes by having a lookat our _.Unless you explicitly state otherwise in advance, any non trivialcontribution intentionally submitted for inclusion in this project by youto the steward of this repository (Zalando SE, Berlin) shall be under theterms and conditions of Apache License 2.0 written below, without anyadditional copyright information, terms or conditions.TODOsIf you'd like to become a more consistent contributor to Connexion, we'd love your help working onthese we have a list of _.ThanksWe'd like to thank all of Connexion's contributors for working on thisproject, and to Swagger/OpenAPI for their support.LicenseCopyright 2015 Zalando SELicensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0.Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License... _Flask: http://flask.pocoo.org/.. _issues waffle board: https://waffle.io/zalando/connexion.. _API First: https://opensource.zalando.com/restful-api-guidelines/#api-first.. _Hug: https://github.com/timothycrosley/hug.. _Swagger: http://swagger.io/open-source-integrations/.. _Jinja2: http://jinja.pocoo.org/.. _rfc6750: https://tools.ietf.org/html/rfc6750.. _OpenAPI Specification: https://www.openapis.org/.. _OpenAPI 3.0 Style Values: https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.2.md#style-values.. _Operation Object: https://github.com/swagger-api/swagger-spec/blob/master/versions/2.0.md#operation-object.. _swager.spec.security_definition: https://github.com/swagger-api/swagger-spec/blob/master/versions/2.0.md#security-definitions-object.. _swager.spec.security_requirement: https://github.com/swagger-api/swagger-spec/blob/master/versions/2.0.md#security-requirement-object.. _YAML format: https://github.com/OAI/OpenAPI-Specification/blob/master/versions/2.0.md#format.. _token information: https://tools.ietf.org/html/rfc6749.. _Tornado: http://www.tornadoweb.org/en/stable/.. _Connexion Pet Store Example Application: https://github.com/hjacobs/connexion-example.. _described by Flask: http://flask.pocoo.org/snippets/111/.. _werkzeug: http://werkzeug.pocoo.org/.. _Connexion's Documentation Page: http://connexion.readthedocs.org/en/latest/.. _Crafting effective Microservices in Python: https://jobs.zalando.com/tech/blog/crafting-effective-microservices-in-python/.. _issues where we are looking for contributions: https://github.com/zalando/connexion/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22.. _HTTP Methods work in Flask: http://flask.pocoo.org/docs/1.0/quickstart/#http-methods"
https://github.com/AlessandroZ/LaZagne,Credentials recovery project,"The LaZagne Project !!!DescriptionThe LaZagne project is an open source application used to retrieve lots of passwords stored on a local computer.Each software stores its passwords using different techniques (plaintext, APIs, custom algorithms, databases, etc.). This tool has been developed for the purpose of finding these passwords for the most commonly-used software. This project has been added to  as a post-exploitation module. Python code will be interpreted in memory without touching the disk and it works on Windows and Linux host.StandalonesStandalones are now available here: https://github.com/AlessandroZ/LaZagne/releases/Installationpip install -r requirements.txt
UsagelaZagne.exe all
laZagne.exe browsers
laZagne.exe browsers -firefox
laZagne.exe all -oN
laZagne.exe all -oA -output C:\Users\test\Desktop
laZagne.exe -h
laZagne.exe browsers -h
laZagne.exe all -vv
laZagne.exe all -quiet -oA
laZagne.exe all -password ZapataVive
Note: For wifi passwords \ Windows Secrets, launch it with administrator privileges (UAC Authentication / sudo)Mac OSNote: In Mac OS System, without the user password it is very difficult to retrieve passwords stored on the computer.So, I recommend using one of these optionslaZagne all --password SuperSecurePassword
laZagne all -i
Supported software|  | Windows    | Linux  | Mac || -- | -- | -- | -- || Browsers | 7Star Amigo BlackHawk Brave Centbrowser Chedot Chrome Canary Chromium Coccoc Comodo Dragon Comodo IceDragon Cyberfox Elements Browser Epic Privacy Browser Firefox Google Chrome Icecat K-Meleon Kometa Microsoft Edge Opera Orbitum Sputnik Torch Uran Vivaldi | Brave Chromium Dissenter-Browser Firefox Google Chrome IceCat Microsoft Edge Opera SlimJet Vivaldi | Chrome Firefox || Chats | Pidgin Psi Skype| Pidgin Psi |  || Databases | DBVisualizer Postgresql Robomongo Squirrel SQLdevelopper | DBVisualizer Squirrel SQLdevelopper  |  || Games | GalconFusion Kalypsomedia RogueTale Turba |  |  || Git | Git for Windows |  |  || Mails | Outlook Thunderbird  | Clawsmail Thunderbird |  || Maven | Maven Apache |  |  || Dumps from memory | Keepass Mimikatz method | System Password |  || Multimedia | EyeCON |  |  || PHP | Composer |  |  || SVN | Tortoise  | | || Sysadmin | Apache Directory Studio CoreFTP CyberDuck FileZilla FileZilla Server FTPNavigator OpenSSH OpenVPN KeePass Configuration Files (KeePass1, KeePass2) PuttyCMRcloneRDPManager VNC WinSCP Windows Subsystem for Linux | Apache Directory Studio AWS  Docker Environnement variable FileZilla gFTP History files Shares  SSH private keys  KeePass Configuration Files (KeePassX, KeePass2)  Grub  Rclone |  || Wifi | Wireless Network | Network Manager WPA Supplicant |  || Internal mechanism passwords storage | Autologon MSCache Credential Files Credman  DPAPI Hash  Hashdump (LM/NT) LSA secret Vault Files | GNOME Keyring Kwallet Hashdump | Keychains Hashdump |For developersPlease refer to the wiki before opening an issue to understand how to compile the project or to develop a new module.https://github.com/AlessandroZ/LaZagne/wikiDonationIf you want to support my work doing a donation, I will appreciate a lot:Special thanks"
https://github.com/noamraph/tqdm,Add a progress meter to your loops in a second,"tqdmInstantly make your loops show a progress meter - just wrap any iterator with ""tqdm(iterator)"", and you're done!Note: an actively developed version is here: https://github.com/tqdm/tqdmtqdm (read taqadum, تقدّم) means ""progress"" in arabic.You can also use trange(N) as a shortcut for tqdm(xrange(N))Here's the doc:def tqdm(iterable, desc='', total=None, leave=False, mininterval=0.5, miniters=1):
    """"""
    Get an iterable object, and return an iterator which acts exactly like the
    iterable, but prints a progress meter and updates it every time a value is
    requested.
    'desc' can contain a short string, describing the progress, that is added
    in the beginning of the line.
    'total' can give the number of expected iterations. If not given,
    len(iterable) is used if it is defined.
    If leave is False, tqdm deletes its traces from screen after it has finished
    iterating over all elements.
    If less than mininterval seconds or miniters iterations have passed since
    the last progress meter update, it is not updated again.
    """"""

def trange(*args, **kwargs):
    """"""A shortcut for writing tqdm(xrange)""""""
    return tqdm(xrange(*args), **kwargs)
"
https://github.com/bndr/pipreqs,pipreqs - Generate pip requirements.txt file based on imports of any project. Looking for maintainers to move this project forward.,"============================================================================= - Generate requirements.txt file for any project based on imports.. image:: https://img.shields.io/travis/bndr/pipreqs.svg:target: https://travis-ci.org/bndr/pipreqs.. image:: https://img.shields.io/pypi/v/pipreqs.svg:target: https://pypi.python.org/pypi/pipreqs.. image:: https://codecov.io/gh/bndr/pipreqs/branch/master/graph/badge.svg?token=0rfPfUZEAX:target: https://codecov.io/gh/bndr/pipreqs.. image:: https://img.shields.io/pypi/l/pipreqs.svg:target: https://pypi.python.org/pypi/pipreqsInstallation::pip install pipreqs
Usage::Usage:
    pipreqs [options] [<path>]

Arguments:
    <path>                The path to the directory containing the application files for which a requirements file
                          should be generated (defaults to the current working directory)

Options:
    --use-local           Use ONLY local package info instead of querying PyPI
    --pypi-server <url>   Use custom PyPi server
    --proxy <url>         Use Proxy, parameter will be passed to requests library. You can also just set the
                          environments parameter in your terminal:
                          $ export HTTP_PROXY=""http://10.10.1.10:3128""
                          $ export HTTPS_PROXY=""https://10.10.1.10:1080""
    --debug               Print debug information
    --ignore <dirs>...    Ignore extra directories, each separated by a comma
    --no-follow-links     Do not follow symbolic links in the project
    --encoding <charset>  Use encoding parameter for file open
    --savepath <file>     Save the list of requirements in the given file
    --print               Output the list of requirements in the standard output
    --force               Overwrite existing requirements.txt
    --diff <file>         Compare modules in requirements.txt to project imports
    --clean <file>        Clean up requirements.txt by removing modules that are not imported in project
    --mode <scheme>       Enables dynamic versioning with <compat>, <gt> or <non-pin> schemes
                          <compat> | e.g. Flask~=1.1.2
                          <gt>     | e.g. Flask>=1.1.2
                          <no-pin> | e.g. Flask
Example::$ pipreqs /home/project/location
Successfully saved requirements file in /home/project/location/requirements.txt
Contents of requirements.txt::wheel==0.23.0
Yarg==0.1.9
docopt==0.6.2
Why not pip freeze?"
https://github.com/cowrie/cowrie,Cowrie SSH/Telnet Honeypot https://cowrie.readthedocs.io,"CowrieWelcome to the Cowrie GitHub repositoryThis is the official repository for the Cowrie SSH and TelnetHoneypot effort.What is CowrieCowrie is a medium to high interaction SSH and Telnet honeypotdesigned to log brute force attacks and the shell interactionperformed by the attacker. In medium interaction mode (shell) itemulates a UNIX system in Python, in high interaction mode (proxy)it functions as an SSH and telnet proxy to observe attacker behaviorto another system._ is maintained by Michel Oosterhof.DocumentationThe Documentation can be found _.SlackYou can join the Cowrie community at the following _.FeaturesFor both settings:DockerDocker versions are available.Cowrie in Docker can be configured using environment variables. Thevariables start with COWRIE_ then have the section name in capitals,followed by the stanza in capitals. An example is below to enabletelnet support::COWRIE_TELNET_ENABLED=yes
Alternatively, Cowrie in Docker can use an  volume to storeconfiguration data.  Create  inside the etc volumewith the following contents to enable telnet in your Cowrie Honeypotin Docker::[telnet]
enabled = yes
RequirementsSoftware required to run locally:For Python dependencies, see _.Files of interest:ContributorsMany people have contributed to Cowrie over the years. Special thanks to:"
https://github.com/pudo/dataset,"Easy-to-use data handling for SQL data stores with support for implicit table creation, bulk loading, and transactions.","dataset: databases for lazy peopleIn short, dataset makes reading and writing data in databases as simple as reading and writing JSON files.To install dataset, fetch it with :$ pip install dataset
Note: as of version 1.0, dataset is split into two packages, with thedata export features now extracted into a stand-alone package, datafreeze.See the relevant repository ."
https://github.com/mwaskom/seaborn,Statistical data visualization in Python,"seaborn: statistical data visualizationSeaborn is a Python visualization library based on matplotlib. It provides a high-level interface for drawing attractive statistical graphics.DocumentationOnline documentation is available at .The docs include a , , , , and other useful information.To build the documentation locally, please refer to .DependenciesSeaborn supports Python 3.8+.Installation requires , , and . Some advanced statistical functionality requires  and/or .InstallationThe latest stable release (and required dependencies) can be installed from PyPI:pip install seaborn
It is also possible to include optional statistical dependencies:pip install seaborn[stats]
Seaborn can also be installed with conda:conda install seaborn
Note that the main anaconda repository lags PyPI in adding new releases, but conda-forge () typically updates quickly.CitingA paper describing seaborn has been published in the . The paper provides an introduction to the key features of the library, and it can be used as a citation if seaborn proves integral to a scientific publication.TestingTesting seaborn requires installing additional dependencies; they can be installed with the  extra (e.g., ).To test the code, run  in the source directory. This will exercise the unit tests (using ) and generate a coverage report.Code style is enforced with  using the settings in the  file. Run  to check. Alternately, you can use  to automatically run lint checks on any files you are committing: just run  to set it up, and then commit as usual going forward.DevelopmentSeaborn development takes place on Github: https://github.com/mwaskom/seabornPlease submit bugs that you encounter to the  with a reproducible example demonstrating the problem. Questions about usage are more at home on StackOverflow, where there is a ."
https://github.com/trustedsec/social-engineer-toolkit,The Social-Engineer Toolkit (SET) repository from TrustedSec - All new versions of SET will be deployed here.,"The Social-Engineer Toolkit (SET)DescriptionThe Social-Engineer Toolkit is an open-source penetration testing framework designed for social engineering. SET has a number of custom attack vectors that allow you to make a believable attack quickly. SET is a product of TrustedSec, LLC – an information security consulting firm located in Cleveland, Ohio.DISCLAIMER: This is only for testing purposes and can only be used where strict consent has been given. Do not use this for illegal purposes, period.Please read the LICENSE under readme/LICENSE for the licensing of SET. Supported platforms:InstallationInstall via requirements.txtpip3 install -r requirements.txt
python3 setup.py 
Install SET=======InstallationWindows 10 WSL/WSL2 Kali Linuxsudo apt install set -y
Kali Linux on Windows 10 is a minimal installation so it doesn't have any tools installed.You can easily install Social Engineer Toolkit on WSL/WSL2 without needing pip using the above command.Linuxgit clone https://github.com/trustedsec/social-engineer-toolkit/ setoolkit/
cd setoolkit
pip3 install -r requirements.txt
python setup.py
SET TutorialFor a full document on how to use SET, .Bugs and enhancementsFor bug reports or enhancements, please open an  here."
https://github.com/apenwarr/sshuttle,Wrong project!  You should head over to http://github.com/sshuttle/sshuttle,"sshuttle: where transparent proxy meets VPN meets sshAs far as I know, sshuttle is the only program that solves the followingcommon case:Obtaining sshuttleDocumentationThe documentation for the stable version is available at:http://sshuttle.readthedocs.org/The documentation for the latest development version is available at:http://sshuttle.readthedocs.org/en/latest/"
https://github.com/python-websockets/websockets,Library for building WebSocket servers and clients in Python,".. image:: logo/horizontal.svg:width: 480px:alt: websockets|licence| |version| |pyversions| |tests| |docs| |openssf|.. |licence| image:: https://img.shields.io/pypi/l/websockets.svg:target: https://pypi.python.org/pypi/websockets.. |version| image:: https://img.shields.io/pypi/v/websockets.svg:target: https://pypi.python.org/pypi/websockets.. |pyversions| image:: https://img.shields.io/pypi/pyversions/websockets.svg:target: https://pypi.python.org/pypi/websockets.. |tests| image:: https://img.shields.io/github/checks-status/python-websockets/websockets/main?label=tests:target: https://github.com/python-websockets/websockets/actions/workflows/tests.yml.. |docs| image:: https://img.shields.io/readthedocs/websockets.svg:target: https://websockets.readthedocs.io/.. |openssf| image:: https://bestpractices.coreinfrastructure.org/projects/6475/badge:target: https://bestpractices.coreinfrastructure.org/projects/6475What is ?websockets is a library for building WebSocket_ servers and clients in Pythonwith a focus on correctness, simplicity, robustness, and performance... _WebSocket: https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_APIBuilt on top of , Python's standard asynchronous I/O framework, thedefault implementation provides an elegant coroutine-based API.An implementation on top of  and a Sans-I/O implementation are alsoavailable._.. copy-pasted because GitHub doesn't support the include directiveHere's an echo server with the  API:.. code:: python#!/usr/bin/env python

import asyncio
from websockets.server import serve

async def echo(websocket):
    async for message in websocket:
        await websocket.send(message)

async def main():
    async with serve(echo, ""localhost"", 8765):
        await asyncio.Future()  # run forever

asyncio.run(main())
Here's how a client sends and receives messages with the  API:.. code:: python#!/usr/bin/env python

from websockets.sync.client import connect

def hello():
    with connect(""ws://localhost:8765"") as websocket:
        websocket.send(""Hello world!"")
        message = websocket.recv()
        print(f""Received: {message}"")

hello()
Does that look good?_.. raw:: html<hr>
<img align=""left"" height=""150"" width=""150"" src=""https://raw.githubusercontent.com/python-websockets/websockets/main/logo/tidelift.png"">
<h3 align=""center""><i>websockets for enterprise</i></h3>
<p align=""center""><i>Available as part of the Tidelift Subscription</i></p>
<p align=""center""><i>The maintainers of websockets and thousands of other packages are working with Tidelift to deliver commercial support and maintenance for the open source dependencies you use to build your applications. Save time, reduce risk, and improve code health, while paying the maintainers of the exact dependencies you use. <a href=""https://tidelift.com/subscription/pkg/pypi-websockets?utm_source=pypi-websockets&utm_medium=referral&utm_campaign=readme"">Learn more.</a></i></p>
<hr>
<p>(If you contribute to <code>websockets</code> and would like to become an official support provider, <a href=""https://fractalideas.com/"">let me know</a>.)</p>
Why should I use ?The development of  is shaped by four principles:Documentation is a first class concern in the project. Head over to _ and see for yourself... _Read the Docs: https://websockets.readthedocs.io/.. _handle backpressure correctly: https://vorpus.org/blog/some-thoughts-on-asynchronous-api-design-in-a-post-asyncawait-world/#websocket-serversWhy shouldn't I use ?.. _Sanic: https://sanicframework.org/en/What else?Bug reports, patches and suggestions are welcome!To report a security vulnerability, please use the _. Tidelift will coordinate the fix and disclosure... _Tidelift security contact: https://tidelift.com/securityFor anything else, please open an issue_ or send a _... _issue: https://github.com/python-websockets/websockets/issues/new.. _pull request: https://github.com/python-websockets/websockets/compare/Participants must uphold the _... _Contributor Covenant code of conduct: https://github.com/python-websockets/websockets/blob/main/CODE_OF_CONDUCT.md is released under the _... _BSD license: https://github.com/python-websockets/websockets/blob/main/LICENSE"
https://github.com/rougier/numpy-100,100 numpy exercises (with solutions),"100 numpy exercisesThis is a collection of numpy exercises from numpy mailing list, stack overflow, and numpy documentation. I've also created some problems myself to reach the 100 limit. The goal of this collection is to offer a quick reference for both old and new users but also to provide a set of exercises for those who teach. For extended exercises, make sure to read .→ →   Note: markdown and ipython notebook are created programmatically from the source data in .To modify the content of these files, please change the text in the source and run the  module with a pythoninterpreter with the libraries under  installed.The keyed text format () is a minimal human readable key-values to store text (markdown or others) indexed by keys. This work is licensed under the MIT license.Variants in Other Languages"
https://github.com/minimaxir/big-list-of-naughty-strings,The Big List of Naughty Strings is a list of strings which have a high probability of causing issues when used as user-input data.,"Big List of Naughty StringsThe Big List of Naughty Strings is an evolving list of strings which have a high probability of causing issues when used as user-input data. This is intended for use in helping both automated and manual QA testing; useful for whenever your QA engineer .Why Test Naughty Strings?Even multi-billion dollar companies with huge amounts of automated testing can't find every bad input. For example, look at what happens when you try to Tweet a  (U+200B) on Twitter:Although this is not a malicious error, and typical users aren't Tweeting weird unicode, an ""internal server error"" for unexpected input is never a positive experience for the user, and may in fact be a symptom of deeper string-validation issues. The Big List of Naughty Strings is intended to help reveal such issues.Usage consists of newline-delimited strings and comments which are preceded with . The comments divide the strings into sections for easy manual reading and copy/pasting into input forms. For those who want to access the strings programmatically, a  file is provided containing an array with all the comments stripped out (the  folder contains a Python script used to generate the ).ContributionsFeel free to send a pull request to add more strings, or additional sections. However, please do not send pull requests with very-long strings (255+ characters), as that makes the list much more difficult to view.Likewise, please do not send pull requests which compromise manual usability of the file. This includes the , which can cause the file to be flagged by antivirus scanners, and files which alter the encoding of . Also, do not send a null character (U+0000) string, as it  and renders it unreadable in pull requests. Finally, when adding or removing a string please update all files when you perform a pull request.DisclaimerThe Big List of Naughty Strings is intended to be used for software you own and manage. Some of the Naughty Strings can indicate security vulnerabilities, and as a result using such strings with third-party software may be a crime. The maintainer is not responsible for any negative actions that result from the use of the list.Additionally, the Big List of Naughty Strings is not a fully-comprehensive substitute for formal security/penetration testing for your service.Library / PackagesVarious implementations of the Big List of Naughty Strings have made it to various package managers.  Those are maintained by outside parties, but can be found here:| Library | Link || ------- | ---- || Node | https://www.npmjs.com/package/blns || Node | https://www.npmjs.com/package/big-list-of-naughty-strings || .NET | https://github.com/SimonCropp/NaughtyStrings || PHP | https://github.com/mattsparks/blns-php || C++  | https://github.com/eliabieri/blnscpp |Please open a PR to list others.Maintainer/CreatorMax Woolf ()Social Media DiscussionsLicenseMIT"
https://github.com/isnowfy/snownlp,Python library for processing Chinese text,"SnowNLP: Simplified Chinese Text ProcessingSnowNLP是一个python写的类库，可以方便的处理中文文本内容，是受到了的启发而写的，由于现在大部分的自然语言处理库基本都是针对英文的，于是写了一个方便处理中文的类库，并且和TextBlob不同的是，这里没有用NLTK，所有的算法都是自己实现的，并且自带了一些训练好的字典。注意本程序都是处理的unicode编码，所以使用时请自行decode成unicode。from snownlp import SnowNLP

s = SnowNLP(u'这个东西真心很赞')

s.words         # [u'这个', u'东西', u'真心',
                #  u'很', u'赞']

s.tags          # [(u'这个', u'r'), (u'东西', u'n'),
                #  (u'真心', u'd'), (u'很', u'd'),
                #  (u'赞', u'Vg')]

s.sentiments    # 0.9769663402895832 positive的概率

s.pinyin        # [u'zhe', u'ge', u'dong', u'xi',
                #  u'zhen', u'xin', u'hen', u'zan']

s = SnowNLP(u'「繁體字」「繁體中文」的叫法在臺灣亦很常見。')

s.han           # u'「繁体字」「繁体中文」的叫法
                # 在台湾亦很常见。'

text = u'''
自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。
它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。
自然语言处理是一门融语言学、计算机科学、数学于一体的科学。
因此，这一领域的研究将涉及自然语言，即人们日常使用的语言，
所以它与语言学的研究有着密切的联系，但又有重要的区别。
自然语言处理并不是一般地研究自然语言，
而在于研制能有效地实现自然语言通信的计算机系统，
特别是其中的软件系统。因而它是计算机科学的一部分。
'''

s = SnowNLP(text)

s.keywords(3)	# [u'语言', u'自然', u'计算机']

s.summary(3)	# [u'因而它是计算机科学的一部分',
                #  u'自然语言处理是一门融语言学、计算机科学、
				#	 数学于一体的科学',
				#  u'自然语言处理是计算机科学领域与人工智能
				#	 领域中的一个重要方向']
s.sentences

s = SnowNLP([[u'这篇', u'文章'],
             [u'那篇', u'论文'],
             [u'这个']])
s.tf
s.idf
s.sim([u'文章'])# [0.3756070762985226, 0, 0]
FeaturesGet It now$ pip install snownlp
关于训练现在提供训练的包括分词，词性标注，情感分析，而且都提供了我用来训练的原始文件以分词为例分词在目录下from snownlp import seg
seg.train('data.txt')
seg.save('seg.marshal')
# from snownlp import tag
# tag.train('199801.txt')
# tag.save('tag.marshal')
# from snownlp import sentiment
# sentiment.train('neg.txt', 'pos.txt')
# sentiment.save('sentiment.marshal')
这样训练好的文件就存储为了，之后修改里的指向刚训练好的文件即可LicenseMIT licensed."
https://github.com/MycroftAI/mycroft-core,"Mycroft Core, the Mycroft Artificial Intelligence platform. ","MycroftMycroft is a hackable open source voice assistant.Table of ContentsGetting StartedFirst, get the code on your system!  The simplest method is via git ():This script sets up dependencies and a .  If running in an environment besides Ubuntu/Debian, Arch or Fedora you may need to manually install packages as instructed by dev_setup.sh.NOTE: The default branch for this repository is 'dev', which should be considered a work-in-progress. If you want to clone a more stable version, switch over to the 'master' branch.Running MycroftMycroft provides  to perform common tasks. This script uses a virtualenv created by .  Assuming you installed mycroft-core in your home directory run:The ""debug"" command will start the background services (microphone listener, skill, messagebus, and audio subsystems) as well as bringing up a text-based Command Line Interface (CLI) you can use to interact with Mycroft and see the contents of the various logs. Alternatively you can run  to begin the services without the command line interface.  Later you can bring up the CLI using .The background services can be stopped as a group with:Using MycroftHome Device and Account ManagerMycroft AI, Inc. maintains a device and account management system known as Mycroft Home. Developers may sign up at: https://home.mycroft.aiBy default, mycroft-core  is configured to use Home. By saying ""Hey Mycroft, pair my device"" (or any other request verbal request) you will be informed that your device needs to be paired. Mycroft will speak a 6-digit code which you can enter into the pairing page within the .Once paired, your unit will use Mycroft API keys for services such as Speech-to-Text (STT), weather and various other skills.SkillsMycroft is nothing without skills.  There are a handful of default skills that are downloaded automatically to your  directory, but most need to be installed explicitly.  See the  to discover skills made by others.  Please share your own interesting work!Behind the scenesPairing InformationPairing information generated by registering with Home is stored in: <-- DO NOT SHARE THIS WITH OTHERS!ConfigurationMycroft's configuration consists of 4 possible locations:When the configuration loader starts, it looks in these locations in this order, and loads ALL configurations. Keys that exist in multiple configuration files will be overridden by the last file to contain the value. This process results in a minimal amount being written for a specific device and user, without modifying default distribution files.Using Mycroft Without HomeIf you do not wish to use the Mycroft Home service, before starting Mycroft for the first time, create  with the following contents:{
  ""skills"": {
    ""blacklisted_skills"": [
      ""mycroft-configuration.mycroftai"",
      ""mycroft-pairing.mycroftai""
    ]
  }
}
API Key ServicesThe Mycroft backend provides access to a range of API keys for specific services. Without pairing with the Mycroft backend, you will need to add your own API keys, install a different Skill or Plugin to perform that function, or not have access to that functionality.These are the keys currently used in Mycroft Core through the Mycroft backend:Using Mycroft behind a proxyMany schools, universities and workplaces run a  on their network. If you need to type in a username and password to access the external internet, then you are likely behind a .If you plan to use Mycroft behind a proxy, then you will need to do an additional configuration step.NOTE: In order to complete this step, you will need to know the Using Mycroft behind a proxy without authenticationIf you are using Mycroft behind a proxy without authentication, add the following environment variables, changing the  and  for the values for your network. These commands are executed from the Linux command line interface (CLI).$ export http_proxy=http://proxy_hostname.com:proxy_port
$ export https_port=http://proxy_hostname.com:proxy_port
$ export no_proxy=""localhost,127.0.0.1,localaddress,.localdomain.com,0.0.0.0,::1""
Using Mycroft behind an authenticated proxyIf  you are behind a proxy which requires authentication, add the following environment variables, changing the  and  for the values for your network. These commands are executed from the Linux command line interface (CLI).$ export http_proxy=http://user:password@proxy_hostname.com:proxy_port
$ export https_port=http://user:password@proxy_hostname.com:proxy_port
$ export no_proxy=""localhost,127.0.0.1,localaddress,.localdomain.com,0.0.0.0,::1""
Getting InvolvedThis is an open source project. We would love your help. We have prepared a  guide to help you get started.If this is your first PR, or you're not sure where to get started,say hi in  and a team member would be happy to mentor you.Join the  for questions and answers.Links"
https://github.com/nvbn/thefuck,Magnificent app which corrects your previous console command.,"The Fuck    The Fuck is a magnificent app, inspired by a ,that corrects errors in previous console commands.Is The Fuck too slow? More examples:➜ apt-get install vim
E: Could not open lock file /var/lib/dpkg/lock - open (13: Permission denied)
E: Unable to lock the administration directory (/var/lib/dpkg/), are you root?

➜ fuck
sudo apt-get install vim [enter/↑/↓/ctrl+c]
[sudo] password for nvbn:
Reading package lists... Done
...
➜ git push
fatal: The current branch master has no upstream branch.
To push the current branch and set the remote as upstream, use

    git push --set-upstream origin master


➜ fuck
git push --set-upstream origin master [enter/↑/↓/ctrl+c]
Counting objects: 9, done.
...
➜ puthon
No command 'puthon' found, did you mean:
 Command 'python' from package 'python-minimal' (main)
 Command 'python' from package 'python3' (main)
zsh: command not found: puthon

➜ fuck
python [enter/↑/↓/ctrl+c]
Python 3.4.2 (default, Oct  8 2014, 13:08:17)
...
➜ git brnch
git: 'brnch' is not a git command. See 'git --help'.

Did you mean this?
    branch

➜ fuck
git branch [enter/↑/↓/ctrl+c]
* master
➜ lein rpl
'rpl' is not a task. See 'lein help'.

Did you mean this?
         repl

➜ fuck
lein repl [enter/↑/↓/ctrl+c]
nREPL server started on port 54848 on host 127.0.0.1 - nrepl://127.0.0.1:54848
REPL-y 0.3.1
...
If you're not afraid of blindly running corrected commands, the  option can be disabled:➜ apt-get install vim
E: Could not open lock file /var/lib/dpkg/lock - open (13: Permission denied)
E: Unable to lock the administration directory (/var/lib/dpkg/), are you root?

➜ fuck
sudo apt-get install vim
[sudo] password for nvbn:
Reading package lists... Done
...
ContentsRequirementsInstallationOn macOS or Linux, you can install The Fuck via :brew install thefuck
On Ubuntu / Mint, install The Fuck with the following commands:sudo apt update
sudo apt install python3-dev python3-pip python3-setuptools
pip3 install thefuck --user
On FreeBSD, install The Fuck with the following commands:pkg install thefuck
On ChromeOS, install The Fuck using  with the following command:crew install thefuck
On Arch based systems, install The Fuck with the following command:sudo pacman -S thefuck
On other systems, install The Fuck  by using :pip install thefuck
#It is recommended that you place this command in your ,,  or other startup script:eval $(thefuck --alias)
# You can use whatever you want as an alias, like for Mondays:
eval $(thefuck --alias FUCK)
Changes are only available in a new shell session. To make changes immediatelyavailable, run  (or your shell config file like ).To run fixed commands without confirmation, use the  option (or just  for short, or  if you're especially frustrated):fuck --yeah
To fix commands recursively until succeeding, use the  option:fuck -r
Updatingpip3 install thefuck --upgrade
Note: Alias functionality was changed in v1.34 of UninstallTo remove The Fuck, reverse the installation process:How it worksThe Fuck attempts to match the previous command with a rule. If a match isfound, a new command is created using the matched rule and executed. Thefollowing rules are enabled by default:The following rules are enabled by default on specific platforms only:The following commands are bundled with The Fuck, but are not enabled bydefault:Creating your own rulesTo add your own rule, create a file named in . The rule file must contain two functions:match(command: Command) -> bool
get_new_command(command: Command) -> str | list[str]
Additionally, rules can contain optional functions:side_effect(old_command: Command, fixed_command: str) -> None
Rules can also contain the optional variables ,  and . has three attributes: ,  and .Your rule should not change .Rules api changed in 3.0: To access a rule's settings, import it with is a special object assembled from ,and values from env ().A simple example rule for running a script with :def match(command):
    return ('permission denied' in command.output.lower()
            or 'EACCES' in command.output)


def get_new_command(command):
    return 'sudo {}'.format(command.script)

# Optional:
enabled_by_default = True

def side_effect(command, fixed_command):
    subprocess.call('chmod 777 .', shell=True)

priority = 1000  # Lower first, default is 1000

requires_output = True
,,.SettingsSeveral The Fuck parameters can be changed in the file ( defaults to ):An example of :rules = ['sudo', 'no_command']
exclude_rules = ['git_push']
require_confirmation = True
wait_command = 10
no_colors = False
priority = {'sudo': 100, 'no_command': 9999}
debug = False
history_limit = 9999
wait_slow_command = 20
slow_commands = ['react-native', 'gradle']
num_close_matches = 5
Or via environment variables:For example:export THEFUCK_RULES='sudo:no_command'
export THEFUCK_EXCLUDE_RULES='git_pull:git_push'
export THEFUCK_REQUIRE_CONFIRMATION='true'
export THEFUCK_WAIT_COMMAND=10
export THEFUCK_NO_COLORS='false'
export THEFUCK_PRIORITY='no_command=9999:apt_get=100'
export THEFUCK_HISTORY_LIMIT='2000'
export THEFUCK_NUM_CLOSE_MATCHES='5'
Third-party packages with rulesIf you'd like to make a specific set of non-public rules, but would still liketo share them with others, create a package named  withthe following structure:thefuck_contrib_foo
  thefuck_contrib_foo
    rules
      __init__.py
      *third-party rules*
    __init__.py
    *third-party-utils*
  setup.py
The Fuck will find rules located in the  module.Experimental instant modeThe default behavior of The Fuck requires time to re-run previous commands.When in instant mode, The Fuck saves time by logging output with ,then reading the log.Currently, instant mode only supports Python 3 with bash or zsh. zsh's autocorrect function also needs to be disabled in order for thefuck to work properly.To enable instant mode, add to the alias initialization in ,  or .For example:eval $(thefuck --alias --enable-experimental-instant-mode)
DevelopingSee License MITProject License can be found ."
https://github.com/plotly/dash,Data Apps & Dashboards for Python. No JavaScript Required.,"DashDash is the most downloaded, trusted Python framework for building ML & data science web apps.Built on top of ,  and , Dash ties modern UI elements like dropdowns, sliders, and graphs directly to your analytical Python code. Read  (proudly crafted ❤️ with Dash itself).Dash App Examples| Dash App | Description ||--- | :---: || | Here’s a simple example of a Dash App that ties a Dropdown to a Plotly Graph. As the user selects a value in the Dropdown, the application code dynamically exports data from Google Finance into a Pandas DataFrame. This app was written in just 43 lines of code (). |||Dash app code is declarative and reactive, which makes it easy to build complex apps that contain many interactive elements. Here’s an example with 5 inputs, 3 outputs, and cross filtering. This app was composed in just 160 lines of code, all of which were Python.||| Dash uses  for charting. About 50 chart types are supported, including maps. ||| Dash isn't just for dashboards. You have full control over the look and feel of your applications. Here's a Dash App that's styled to look like a PDF report. |To learn more about Dash, read the  or .Dash OSS & Dash EnterpriseWith Dash Open Source, Dash apps run on your local laptop or workstation, but cannot be easily accessed by others in your organization.Scale up with Dash Enterprise when your Dash app is ready for department or company-wide consumption. Or, launch your initiative with Dash Enterprise from the start to unlock developer productivity gains and hands-on acceleration from Plotly's team.ML Ops Features: A one-stop shop for ML Ops: Horizontally scalable hosting, deployment, and authentication for your Dash apps. No IT or DevOps required.Low-Code Features: Low-code Dash app capabilities that supercharge developer productivity.Enterprise AI Features: Everything that your data science team needs to rapidly deliver AI/ML research and business initiatives.See  to get in touch."
https://github.com/joeyespo/grip,Preview GitHub README.md files locally before committing them.,"Grip -- GitHub Readme Instant PreviewRender local readme files before sending off to GitHub.Grip is a command-line server application written in Python that uses the to render a local readme file. The stylesand rendering come directly from GitHub, so you'll know exactly how it will appear.Changes you make to the Readme will be instantly reflected in the browser withoutrequiring a page refresh.MotivationSometimes you just want to see the exact readmeresult before committing and pushing to GitHub.Especially when doing .InstallationTo install grip, simply:$ pip install grip
On OS X, you can also install with Homebrew:$ brew install grip
UsageTo render the readme of a repository:$ cd myrepo
$ grip
 * Running on http://localhost:6419/
Now open a browser and visit .Or run with  and Grip will open a new browser tab for you.You can also specify a port:$ grip 80
 * Running on http://localhost:80/
Or an explicit file:$ grip AUTHORS.md
 * Running on http://localhost:6419/
Alternatively, you could just run  and visit since grip supports relative URLs.You can combine the previous examples. Or specify a hostname instead of a port. Or provide both.$ grip AUTHORS.md 80
 * Running on http://localhost:80/
$ grip CHANGES.md 0.0.0.0
 * Running on http://0.0.0.0:6419/
$ grip . 0.0.0.0:80
 * Running on http://0.0.0.0:80/
You can even bypass the server and export to a single HTML file, with all the styles and assets inlined:$ grip --export
Exporting to README.html
Control the output name with the second argument:$ grip README.md --export index.html
Exporting to index.html
If you're exporting a bunch of files, you can prevent styles from being inlining to save space with :$ grip README.md --export --no-inline introduction.html
Exporting to introduction.html
Reading and writing from stdin and stdout is also supported, allowing you to use Grip with other programs:$ cat README.md | grip -
 * Running on http://localhost:6419/
$ grip AUTHORS.md --export - | bcat
$ cat README.md | grip --export - | less
This allows you to quickly test how things look by entering Markdown directly in your terminal:$ grip -
Hello **world**!
^D
 * Running on http://localhost:6419/
Note: Rendering as user-content like comments and issues is also supported, with an optional repository context for linking to issues:$ grip --user-content --context=joeyespo/grip
 * Running on http://localhost:6419/
For more details and additional options, see the help:$ grip -h
AccessGrip strives to be as close to GitHub as possible. To accomplish this, gripuses  so that changes to their renderingengine are reflected immediately without requiring you to upgrade grip.However, because of this you may hit the API's hourly rate limit. If thishappens, grip offers a way to access the API using your credentialsto unlock a much higher rate limit.$ grip --user <your-username> --pass <your-password>
Or use a  with an empty scope (note that a token isrequired if your GitHub account is set up with two-factor authentication):$ grip --pass <token>
You can persist these options .For security purposes, it's highly recommended that you use an access token. (You could also keep your password safe by configuringGrip to .)There's also a  to provideoffline rendering. Once this resembles GitHub more precisely, it'llbe exposed in the CLI, and will ultimately be used as a seamless fallbackengine for when the API can't be accessed.Grip always accesses GitHub over HTTPS,so your README and credentials are protected.TipsHere's how others from the community are using Grip.Want to share your own?  or .Create a local mirror of a Github Wiki$ git clone https://github.com/YOUR_USERNAME/YOUR_REPOSITORY.wiki.git
$ cd YOUR_REPOSITORY.wiki
$ grip
By Generate HTML documentation from a collection of linked README filesYou can optionally compress the set of HTML files to  with:$ tar -czvf docs.tgz `ls | grep [\.]html$` assets
Looking for a cross platform solution? Here's an equivalent .By ConfigurationTo customize Grip, create , then add one or more of the following variables:Note that this is a Python file. If you see  errors, youmay have overlooked some quotes. For example:USERNAME = 'your-username'
PASSWORD = 'your-personal-access-token'
Environment variablesAdvancedThis file is a normal Python script, so you can add more advanced configuration.For example, to read a setting from the environment and provide a default valuewhen it's not set:PORT = os.environ.get('GRIP_PORT', 8080)
APIYou can access the API directly with Python, using it in your own projects:from grip import serve

serve(port=8080)
 * Running on http://localhost:8080/
Run main directly:from grip import main

main(argv=['-b', '8080'])
 * Running on http://localhost:8080/
Or access the underlying Flask application for even more flexibility:from grip import create_app

grip_app = create_app(user_content=True)
# Use in your own app
DocumentationserveRuns a local server and renders the Readme file locatedat  when visited in the browser.serve(path=None, host=None, port=None, user_content=False, context=None, username=None, password=None, render_offline=False, render_wide=False, render_inline=False, api_url=None, title=None, autorefresh=True, browser=False, grip_class=None)
exportWrites the specified Readme file to an HTML file with styles and assets inlined.export(path=None, user_content=False, context=None, username=None, password=None, render_offline=False, render_wide=False, render_inline=True, out_filename=None, api_url=None, title=None, quiet=None, theme='light', grip_class=None)
create_appCreates a Flask application you can use to render and serve the Readme files.This is the same app used by  and  and initializes the cache,using the cached styles when available.create_app(path=None, user_content=False, context=None, username=None, password=None, render_offline=False, render_wide=False, render_inline=False, api_url=None, title=None, text=None, grip_class=None)
render_appRenders the application created by  and returns the HTML that wouldnormally appear when visiting that route.render_app(app, route='/')
render_contentRenders the specified markdown text without caching.render_content(text, user_content=False, context=None, username=None, password=None, render_offline=False, api_url=None, title=None)
render_pageRenders the markdown from the specified path or text, without caching,and returns an HTML page that resembles the GitHub Readme view.render_page(path=None, user_content=False, context=None, username=None, password=None, render_offline=False, render_wide=False, render_inline=False, api_url=None, title=None, text=None, quiet=None, theme='light', grip_class=None)
clear_cacheClears the cached styles and assets.clear_cache(grip_class=None)
mainRuns Grip with the specified arguments.main(argv=None, force_utf8=True)
Classesclass Grip(Flask)A Flask application that can serve a file or directory containing a README.Grip(source=None, auth=None, renderer=None, assets=None, render_wide=None, render_inline=None, title=None, autorefresh=None, quiet=None, theme='light', grip_url=None, static_url_path=None, instance_path=None, **kwargs)
default_rendererReturns the default renderer using the current config. This is only used ifrenderer is set to None in the constructor.Grip.default_renderer()
default_asset_managerReturns the default asset manager using the current config. This is only usedif asset_manager is set to None in the constructor.Grip.default_asset_manager()
add_content_typesAdds the application/x-font-woff and application/octet-stream content types ifthey are missing. Override to add additional content types on initialization.Grip.add_content_types()
clear_cacheClears the downloaded assets.Grip.clear_cache()
renderRenders the application and returns the HTML unicode that would normally appearwhen visiting in the browser.Grip.render(route=None)
runStarts a server to render the README. This calls  internally.Grip.run(host=None, port=None, debug=None, use_reloader=None, open_browser=False)
class AlreadyRunningError(RuntimeError)Raised when  is called while the server is already running.AlreadyRunningError()
class ReadmeNotFoundError(NotFoundError or IOError)Raised when the specified Readme could not be found.ReadmeNotFoundError(path=None, message=None)
class ReadmeAssetManager(object)Manages the style and font assets rendered with Readme pages. This is anabstract base class.ReadmeAssetManager(cache_path, style_urls=None)
class GitHubAssetManager(ReadmeAssetManager)Manages the style and font assets rendered with Readme pages. Set cache_path toNone to disable caching.class ReadmeReader(object)Reads Readme content from a URL subpath. This is an abstract base class.ReadmeReader()
class DirectoryReader(ReadmeReader)Reads Readme files from URL subpaths.DirectoryReader(path=None, silent=False)
class TextReader(ReadmeReader)Reads Readme content from the provided unicode string.TextReader(text, display_filename=None)
class StdinReader(TextReader)Reads Readme text from STDIN.StdinReader(display_filename=None)
class ReadmeRenderer(object)Renders the Readme. This is an abstract base class.ReadmeRenderer(user_content=None, context=None)
class GitHubRenderer(ReadmeRenderer)Renders the specified Readme using the GitHub Markdown API.GitHubRenderer(user_content=None, context=None, api_url=None, raw=None)
class OfflineRenderer(ReadmeRenderer)Renders the specified Readme locally using pure Python. Note: This is currentlyan incomplete feature.OfflineRenderer(user_content=None, context=None)
ConstantsSUPPORTED_TITLESThe common Markdown file titles on GitHub.SUPPORTED_TITLES = ['README', 'Home']
SUPPORTED_EXTENSIONSThe supported extensions, as defined by .SUPPORTED_EXTENSIONS = ['.md', '.markdown']
DEFAULT_FILENAMESThis constant contains the names Grip looks for when no file is provided.DEFAULT_FILENAMES = [title + ext
                     for title in SUPPORTED_TITLES
                     for ext in SUPPORTED_EXTENSIONS]
DEFAULT_FILENAMEThis constant contains the default Readme filename, namely:DEFAULT_FILENAME = DEFAULT_FILENAMES[0]  # README.md
DEFAULT_GRIPHOMEThis constant points to the default value if the  is not specified.DEFAULT_GRIPHOME = '~/.grip'
DEFAULT_GRIPURLThe default URL of the Grip server and all its assets:DEFAULT_GRIPURL = '/__/grip'
DEFAULT_API_URLThe default app_url value:DEFAULT_API_URL = 'https://api.github.com'
TestingInstall the package and test requirements:$ pip install -e .[tests]
Run tests with :$ pytest
Or to re-run tests as you make changes, use :$ ptw
External assumption testsIf you're experiencing a problem with Grip, it's likely that an assumption madeabout the GitHub API has been broken. To verify this, run:$ pytest -m assumption
Since the external assumptions rely on an internet connection, you may want to skipthem when developing locally. Tighten the cycle even further by stopping on thefirst failure with :$ pytest -xm ""not assumption""
Or with :$ ptw -- -xm ""not assumption""
ContributingIf your PR has been waiting a while, feel free to .Use this software often? :smiley:"
https://github.com/andresriancho/w3af,"w3af: web application attack and audit framework, the open source web vulnerability scanner.","w3af - Web Application Attack and Audit Framework is an web application security scanner which helps developers and penetration testersidentify and exploit vulnerabilities in their web applications.The scanner is able to identify ,including , and.ContributingPull requests are always welcome! If you're not sure where to start, please takea look at the document in our wiki. All contributions, no matter how small, are welcome.Links and documentationSponsors sponsors the project and uses as part of their amazing .Found this project useful? Donations are accepted via  at "
https://github.com/amoffat/sh,Python process launching,".. image:: https://raw.githubusercontent.com/amoffat/sh/master/images/logo-230.png:target: https://amoffat.github.com/sh:alt: Logo[<marko.inline.RawText object at 0x000001592FE33508>], please see MIGRATION.md**|.. image:: https://img.shields.io/pypi/v/sh.svg?style=flat-square:target: https://pypi.python.org/pypi/sh:alt: Version.. image:: https://img.shields.io/pypi/dm/sh.svg?style=flat-square:target: https://pypi.python.org/pypi/sh:alt: Downloads Status.. image:: https://img.shields.io/pypi/pyversions/sh.svg?style=flat-square:target: https://pypi.python.org/pypi/sh:alt: Python Versions.. image:: https://img.shields.io/coveralls/amoffat/sh.svg?style=flat-square:target: https://coveralls.io/r/amoffat/sh?branch=master:alt: Coverage Status|sh is a full-fledged subprocess replacement for Python 3.8 - 3.11, and PyPythat allows you to call any program as if it were a function:.. code:: pythonfrom sh import ifconfig
print(ifconfig(""eth0""))
sh is not a collection of system commands implemented in Python.sh relies on various Unix system calls and only works on Unix-like operatingsystems - Linux, macOS, BSDs etc. Specifically, Windows is not supported._Installation::$> pip install sh
SupportDevelopersTestingTests are run in a docker container against all supported Python versions. To run, make the following target::$> make test
To run a single test::$> make test='FunctionalTests.test_background' test_one
CoverageFirst run all of the tests::$> SH_TESTS_RUNNING=1 coverage run --source=sh -m pytest
This will aggregate a .  You may then visualize the report with::$> coverage report
Or generate visual html files with::$> coverage html
Which will create  that you may open in a web browser."
https://github.com/tartley/colorama,Simple cross-platform colored terminal text in Python,".. image:: https://img.shields.io/pypi/v/colorama.svg:target: https://pypi.org/project/colorama/:alt: Latest Version.. image:: https://img.shields.io/pypi/pyversions/colorama.svg:target: https://pypi.org/project/colorama/:alt: Supported Python versions.. image:: https://github.com/tartley/colorama/actions/workflows/test.yml/badge.svg:target: https://github.com/tartley/colorama/actions/workflows/test.yml:alt: Build StatusColoramaMakes ANSI escape character sequences (for producing colored terminal text andcursor positioning) work under MS Windows... |donate| image:: https://www.paypalobjects.com/en_US/i/btn/btn_donate_SM.gif:target: https://www.paypal.com/cgi-bin/webscr?cmd=_donations&business=2MZ9D2GMLYCUJ&item_name=Colorama&currency_code=USD:alt: Donate with Paypal_ |_ |_If you find Colorama useful, please |donate| to the authors. Thank you!InstallationTested on CPython 2.7, 3.7, 3.8, 3.9, 3.10 and 3.11 and PyPy 2.7 and 3.8.No requirements other than the standard library... code-block:: bashpip install colorama
# or
conda install -c anaconda colorama
DescriptionANSI escape character sequences have long been used to produce colored terminaltext and cursor positioning on Unix and Macs. Colorama makes this work onWindows, too, by wrapping , stripping ANSI sequences it finds (whichwould appear as gobbledygook in the output), and converting them into theappropriate win32 calls to modify the state of the terminal. On other platforms,Colorama does nothing.This has the upshot of providing a simple cross-platform API for printingcolored terminal text from Python, and has the happy side-effect that existingapplications or libraries which use ANSI sequences to produce colored output onLinux or Macs can now also work on Windows, simply by calling (since v0.4.6) or (all versions, but may have other side-effects – see below).An alternative approach is to install  on Windows machines, whichprovides the same behaviour for all applications running in terminals. Coloramais intended for situations where that isn't easy (e.g., maybe your app doesn'thave an installer.)Demo scripts in the source code repository print some colored text usingANSI sequences. Compare their output under Gnome-terminal's built in ANSIhandling, versus on Windows Command-Prompt using Colorama:.. image:: https://github.com/tartley/colorama/raw/master/screenshots/ubuntu-demo.png:width: 661:height: 357:alt: ANSI sequences on Ubuntu under gnome-terminal... image:: https://github.com/tartley/colorama/raw/master/screenshots/windows-demo.png:width: 668:height: 325:alt: Same ANSI sequences on Windows, using Colorama.These screenshots show that, on Windows, Colorama does not support ANSI 'dimtext'; it looks the same as 'normal text'.UsageInitialisation..............If the only thing you want from Colorama is to get ANSI escapes to work onWindows, then run:.. code-block:: pythonfrom colorama import just_fix_windows_console
just_fix_windows_console()
If you're on a recent version of Windows 10 or better, and your stdout/stderrare pointing to a Windows console, then this will flip the magic configurationswitch to enable Windows' built-in ANSI support.If you're on an older version of Windows, and your stdout/stderr are pointing toa Windows console, then this will wrap  and/or  in amagic file object that intercepts ANSI escape sequences and issues theappropriate Win32 calls to emulate them.In all other circumstances, it does nothing whatsoever. Basically the idea isthat this makes Windows act like Unix with respect to ANSI escape handling.It's safe to call this function multiple times. It's safe to call this functionon non-Windows platforms, but it won't do anything. It's safe to call thisfunction when one or both of your stdout/stderr are redirected to a file – itwon't do anything to those streams.Alternatively, you can use the older interface with more features (but also morepotential footguns):.. code-block:: pythonfrom colorama import init
init()
This does the same thing as , except for thefollowing differences:To stop using Colorama before your program exits, simply call .This will restore  and  to their original values, so thatColorama is disabled. To resume using Colorama again, call ; it ischeaper than calling  again (but does the same thing).Most users should depend on , and use. The old  interface will be supportedindefinitely for backwards compatibility, but we don't plan to fix any issueswith it, also for backwards compatibility.Colored Output..............Cross-platform printing of colored text can then be done using Colorama'sconstant shorthand for ANSI escape sequences. These are deliberatelyrudimentary, see below... code-block:: pythonfrom colorama import Fore, Back, Style
print(Fore.RED + 'some red text')
print(Back.GREEN + 'and with a green background')
print(Style.DIM + 'and in dim text')
print(Style.RESET_ALL)
print('back to normal now')
...or simply by manually printing ANSI sequences from your own code:.. code-block:: pythonprint('\033[31m' + 'some red text')
print('\033[39m') # and reset to default color
...or, Colorama can be used in conjunction with existing ANSI librariessuch as the venerable _the fabulous ,.If you wish Colorama's Fore, Back and Style constants were more capable,then consider using one of the above highly capable libraries to generatecolors, etc, and use Colorama just for its primary purpose: to convertthose ANSI sequences to also work on Windows:SIMILARLY, do not send PRs adding the generation of new ANSI types to Colorama.We are only interested in converting ANSI codes to win32 API calls, notshortcuts like the above to generate ANSI characters... code-block:: pythonfrom colorama import just_fix_windows_console
from termcolor import colored

# use Colorama to make Termcolor work on Windows too
just_fix_windows_console()

# then use Termcolor for all colored text output
print(colored('Hello, World!', 'green', 'on_red'))
Available formatting constants are::Fore: BLACK, RED, GREEN, YELLOW, BLUE, MAGENTA, CYAN, WHITE, RESET.
Back: BLACK, RED, GREEN, YELLOW, BLUE, MAGENTA, CYAN, WHITE, RESET.
Style: DIM, NORMAL, BRIGHT, RESET_ALL
 resets foreground, background, and brightness. Colorama willperform this reset automatically on program exit.These are fairly well supported, but not part of the standard::Fore: LIGHTBLACK_EX, LIGHTRED_EX, LIGHTGREEN_EX, LIGHTYELLOW_EX, LIGHTBLUE_EX, LIGHTMAGENTA_EX, LIGHTCYAN_EX, LIGHTWHITE_EX
Back: LIGHTBLACK_EX, LIGHTRED_EX, LIGHTGREEN_EX, LIGHTYELLOW_EX, LIGHTBLUE_EX, LIGHTMAGENTA_EX, LIGHTCYAN_EX, LIGHTWHITE_EX
Cursor Positioning..................ANSI codes to reposition the cursor are supported. See  foran example of how to generate them.Init Keyword Args................. accepts some  to override default behaviour.init(autoreset=False):If you find yourself repeatedly sending reset sequences to turn off colorchanges at the end of every print, then  willautomate that:.. code-block:: python

    from colorama import init
    init(autoreset=True)
    print(Fore.RED + 'some red text')
    print('automatically back to default color again')
init(strip=None):Pass  or  to override whether ANSI codes should bestripped from the output. The default behaviour is to strip if on Windowsor if output is redirected (not a tty).init(convert=None):Pass  or  to override whether to convert ANSI codes in theoutput into win32 calls. The default behaviour is to convert if on Windowsand output is to a tty (terminal).init(wrap=True):On Windows, Colorama works by replacing  and with proxy objects, which override the  method to do their work.If this wrapping causes you problems, then this can be disabled by passing. The default behaviour is to wrap if  or or  are True.When wrapping is disabled, colored printing on non-Windows platforms will
continue to work as normal. To do cross-platform colored output, you can
use Colorama's ``AnsiToWin32`` proxy directly:

.. code-block:: python

    import sys
    from colorama import init, AnsiToWin32
    init(wrap=False)
    stream = AnsiToWin32(sys.stderr).stream

    # Python 2
    print >>stream, Fore.BLUE + 'blue text on stderr'

    # Python 3
    print(Fore.BLUE + 'blue text on stderr', file=stream)
Recognised ANSI Sequences.........................ANSI sequences generally take the form::ESC [ <param> ; <param> ... <command>
Where  is an integer, and  is a single letter. Zero ormore params are passed to a . If no params are passed, it isgenerally synonymous with passing a single zero. No spaces exist in thesequence; they have been inserted here simply to read more easily.The only ANSI sequences that Colorama converts into win32 calls are::ESC [ 0 m       # reset all (colors and brightness)
ESC [ 1 m       # bright
ESC [ 2 m       # dim (looks same as normal brightness)
ESC [ 22 m      # normal brightness

# FOREGROUND:
ESC [ 30 m      # black
ESC [ 31 m      # red
ESC [ 32 m      # green
ESC [ 33 m      # yellow
ESC [ 34 m      # blue
ESC [ 35 m      # magenta
ESC [ 36 m      # cyan
ESC [ 37 m      # white
ESC [ 39 m      # reset

# BACKGROUND
ESC [ 40 m      # black
ESC [ 41 m      # red
ESC [ 42 m      # green
ESC [ 43 m      # yellow
ESC [ 44 m      # blue
ESC [ 45 m      # magenta
ESC [ 46 m      # cyan
ESC [ 47 m      # white
ESC [ 49 m      # reset

# cursor positioning
ESC [ y;x H     # position cursor at x across, y down
ESC [ y;x f     # position cursor at x across, y down
ESC [ n A       # move cursor n lines up
ESC [ n B       # move cursor n lines down
ESC [ n C       # move cursor n characters forward
ESC [ n D       # move cursor n characters backward

# clear the screen
ESC [ mode J    # clear the screen

# clear the line
ESC [ mode K    # clear the line
Multiple numeric params to the  command can be combined into a singlesequence::ESC [ 36 ; 45 ; 1 m     # bright cyan text on magenta background
All other ANSI sequences of the form are silently stripped from the output on Windows.Any other form of ANSI sequence, such as single-character codes or alternativeinitial characters, are not recognised or stripped. It would be cool to addthem though. Let me know if it would be useful for you, via the Issues onGitHub.Status & Known ProblemsI've personally only tested it on Windows XP (CMD, Console2), Ubuntu(gnome-terminal, xterm), and OS X.Some valid ANSI sequences aren't recognised.If you're hacking on the code, see _. ESPECIALLY, see theexplanation there of why we do not want PRs that allow Colorama to generate newtypes of ANSI codes.See outstanding issues and wish-list:https://github.com/tartley/colorama/issuesIf anything doesn't work for you, or doesn't do what you expected or hoped for,I'd love to hear about it on that issues list, would be delighted by patches,and would be happy to grant commit access to anyone who submits a working patchor two... _README-hacking.md: README-hacking.mdLicenseCopyright Jonathan Hartley & Arnon Yaari, 2013-2020. BSD 3-Clause license; seeLICENSE file.Professional support.. |tideliftlogo| image:: https://cdn2.hubspot.net/hubfs/4008838/website/logos/logos_for_download/Tidelift_primary-shorthand-logo.png:alt: Tidelift:target: https://tidelift.com/subscription/pkg/pypi-colorama?utm_source=pypi-colorama&utm_medium=referral&utm_campaign=readme.. list-table:::widths: 10 100.. _Tidelift Subscription: https://tidelift.com/subscription/pkg/pypi-colorama?utm_source=pypi-colorama&utm_medium=referral&utm_campaign=readmeThanksSee the CHANGELOG for more thanks!"
https://github.com/chatopera/Synonyms,:herb: 中文近义词：聊天机器人，智能问答工具包,"     SynonymsChinese Synonyms for Natural Language Processing and Understanding.更好的中文近义词：聊天机器人、智能问答工具包。可以用于自然语言理解的很多任务：文本对齐，推荐算法，相似度计算，语义偏移，关键字提取，概念提取，自动摘要，搜索引擎等。为提供稳定、可靠、长期优化的服务，Synonyms 改为使用  并针对机器学习模型的下载进行收费，详见。之前的贡献者（突出贡献的代码贡献者），可与我们联系，讨论收费问题。--  @ Oct. 2023Table of Content:WelcomeFollow steps below to install and activate packages.1/2 Install Sourcecodes Packagepip install -U synonyms
python -c ""import synonyms"" # download word vectors file
兼容 py2 和 py3，当前稳定版本 。2/2 Install Model PackageSynonyms's machine learning model package(s) requires a License from , first purchase a License and get the  from Licenses page on Chatopera License Store(：在证书商店，证书详情页，点击【复制证书标识】).Secondly, set environment variable in your terminal or shell scripts as below.# Linux / macOS
export SYNONYMS_DL_LICENSE=YOUR_LICENSE
## e.g. if your license id is `FOOBAR`, run `export SYNONYMS_DL_LICENSE=FOOBAR`

# Windows
set SYNONYMS_DL_LICENSE=YOUR_LICENSE
Last, download the model package by command or script -python -c ""import synonyms""
提示：安装后初次使用会下载词向量文件，下载速度取决于网络情况。其它环境变量介绍见下文，本文档的配置和接口说明面向 python 工具包。Usage支持使用环境变量配置分词词表和 word2vec 词向量文件。| 环境变量                            | 描述                                                                                                                                                                                               || ----------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- || SYNONYMS_WORD2VEC_BIN_MODEL_ZH_CN | 使用 word2vec 训练的词向量文件，二进制格式。                                                                                                                                                       || SYNONYMS_WORDSEG_DICT             | 中文分词，格式和使用 || SYNONYMS_DEBUG                    | [""TRUE""""FALSE""], 是否输出调试日志，设置为 “TRUE” 输出，默认为 “FALSE”                                                                                                                            |synonyms#nearby(word [, size = 10])import synonyms
print(""人脸: "", synonyms.nearby(""人脸""))
print(""识别: "", synonyms.nearby(""识别""))
print(""NOT_EXIST: "", synonyms.nearby(""NOT_EXIST""))
返回一个元组，元组中包含两项：，是 WORD 的近义词们，也以 list 的方式存储，并且按照距离的长度由近及远排列，是中对应位置的词的距离的分数，分数在(0-1)区间内，越接近于 1，代表越相近； 是返回词汇数量，默认 10。比如:synonyms.nearby(人脸, 10) = (
    [""图片"", ""图像"", ""通过观察"", ""数字图像"", ""几何图形"", ""脸部"", ""图象"", ""放大镜"", ""面孔"", ""Mii""],
    [0.597284, 0.580373, 0.568486, 0.535674, 0.531835, 0.530
095, 0.525344, 0.524009, 0.523101, 0.516046])
在 OOV 的情况下，返回 ，目前的字典大小: 435,729。synonyms#compare(sen1, sen2 [, seg=True])两个句子的相似度比较    sen1 = ""发生历史性变革""
    sen2 = ""发生历史性变革""
    r = synonyms.compare(sen1, sen2, seg=True)
其中，参数 seg 表示 synonyms.compare 是否对 sen1 和 sen2 进行分词，默认为 True。返回值：[0-1]，并且越接近于 1 代表两个句子越相似。旗帜引领方向 vs 道路决定命运: 0.429
旗帜引领方向 vs 旗帜指引道路: 0.93
发生历史性变革 vs 发生历史性变革: 1.0
synonyms#display(word [, size = 10])以友好的方式打印近义词，方便调试，调用了  方法。>>> synonyms.display(""飞机"")
'飞机'近义词：
  1. 飞机:1.0
  2. 直升机:0.8423391
  3. 客机:0.8393003
  4. 滑翔机:0.7872388
  5. 军用飞机:0.7832081
  6. 水上飞机:0.77857226
  7. 运输机:0.7724742
  8. 航机:0.7664748
  9. 航空器:0.76592904
  10. 民航机:0.74209654
 是打印词汇表的数量，默认 10。synonyms#describe()打印当前包的描述信息：>>> synonyms.describe()
Vocab size in vector model: 435729
model_path: /Users/hain/chatopera/Synonyms/synonyms/data/words.vector.gz
version: 3.18.0
{'vocab_size': 435729, 'version': '3.18.0', 'model_path': '/chatopera/Synonyms/synonyms/data/words.vector.gz'}
synonyms#v(word)获得一个词语的向量，该向量为 numpy 的 array，当该词语是未登录词时，抛出 KeyError 异常。>>> synonyms.v(""飞机"")
array([-2.412167  ,  2.2628384 , -7.0214124 ,  3.9381874 ,  0.8219283 ,
       -3.2809453 ,  3.8747153 , -5.217062  , -2.2786229 , -1.2572327 ],
      dtype=float32)
synonyms#sv(sentence, ignore=False)获得一个分词后句子的向量，向量以 BoW 方式组成    sentence: 句子是分词后通过空格联合起来
    ignore: 是否忽略OOV，False时，随机生成一个向量
synonyms#seg(sentence)中文分词synonyms.seg(""中文近义词工具包"")
分词结果，由两个 list 组成的元组，分别是单词和对应的词性。(['中文', '近义词', '工具包'], ['nz', 'n', 'n'])
该分词不去停用词和标点。synonyms#keywords(sentence [, topK=5, withWeight=False])提取关键词，默认按照重要程度提取关键词。keywords = synonyms.keywords(""9月15日以来，台积电、高通、三星等华为的重要合作伙伴，只要没有美国的相关许可证，都无法供应芯片给华为，而中芯国际等国产芯片企业，也因采用美国技术，而无法供货给华为。目前华为部分型号的手机产品出现货少的现象，若该形势持续下去，华为手机业务将遭受重创。"")
ContributionGet more logs for debugging, set environment variable.SYNONYMS_DEBUG=TRUE
PCA以“人脸”为例主要成分分析：Quick Get Start$ pip install -r Requirements.txt
$ python demo.py
Change logs更新情况。Voice of Users用户怎么说：Datadata is built based on .Valuation同义词词林《同义词词林》是梅家驹等人于 1983 年编纂而成，现在使用广泛的是哈工大社会计算与信息检索研究中心维护的《同义词词林扩展版》，它精细的将中文词汇划分成大类和小类，梳理了词汇间的关系，同义词词林扩展版包含词语 7 万余条，其中 3 万余条被以开放数据形式共享。知网, HowNetHowNet，也被称为知网，它并不只是一个语义字典，而是一个知识系统，词汇之间的关系是其一个基本使用场景。知网包含词语 8 余条。国际上对词语相似度算法的评价标准普遍采用 Miller&Charles 发布的英语词对集的人工判定值。该词对集由十对高度相关、十对中度相关、十对低度相关共 30 个英语词对组成,然后让 38 个受试者对这 30 对进行语义相关度判断，最后取他们的平均值作为人工判定标准。然后不同近义词工具也对这些词汇进行相似度评分，与人工判定标准做比较，比如使用皮尔森相关系数。在中文领域，使用这个词表的翻译版进行中文近义词比较也是常用的办法。对比Synonyms 的词表容量是 435,729，下面选择一些在同义词词林、知网和 Synonyms 都存在的几个词，给出其近似度的对比：注：同义词林及知网数据、分数。Synonyms 也在不断优化中，新的分数可能和上图不一致。更多。Used byBenchmarkTest with py3, MacBook Pro.python benchmark.py
++++++++++ OS Name and version ++++++++++Platform: DarwinKernel: 16.7.0Architecture: ('64bit', '')++++++++++ CPU Cores ++++++++++Cores: 4CPU Load: 60++++++++++ System Memory ++++++++++meminfo 8GBLive SharingStatement发布证书 MIT。数据和程序可用于研究和商业产品，必须注明引用和地址，比如发布的任何媒体、期刊、杂志或博客等内容。@online{Synonyms:hain2017,
  author = {Hai Liang Wang, Hu Ying Xi},
  title = {中文近义词工具包Synonyms},
  year = 2017,
  url = {https://github.com/chatopera/Synonyms},
  urldate = {2017-09-27}
}
ReferencesFrequently Asked Questions (FAQ)不支持，欲了解更多请看 Google 发布的，该库由 C 语言编写，内存使用效率高，训练速度快。gensim 可以加载 word2vec 输出的模型文件。Authors自然语言处理推荐入门&工具书本书由  作者参与著作。 这本书是服务于准备入门机器学习和自然语言处理的学生和软件工程师的，在理论上介绍了很多原理、算法，同时也提供很多示例程序增加实践性，这些程序被汇总到示例程序代码库，这些程序主要是帮助大家理解原理和算法的，欢迎大家下载和执行。代码库的地址是：Give credits toLicenseProject SponsorChatopera 云服务是一站式实现聊天机器人的云服务，按接口调用次数计费。Chatopera 云服务是 的软件即服务实例。在云计算基础上，Chatopera 云服务属于聊天机器人即服务的云服务。Chatopera 机器人平台包括知识库、多轮对话、意图识别和语音识别等组件，标准化聊天机器人开发，支持企业 OA 智能问答、HR 智能问答、智能客服和网络营销等场景。企业 IT 部门、业务部门借助 Chatopera 云服务快速让聊天机器人上线！"
https://github.com/facebookresearch/DrQA,Reading Wikipedia to Answer Open-Domain Questions,"DrQAThis is a PyTorch implementation of the DrQA system described in the ACL 2017 paper .Quick LinksMachine Reading at ScaleDrQA is a system for reading comprehension applied to open-domain question answering. In particular, DrQA is targeted at the task of ""machine reading at scale"" (MRS). In this setting, we are searching for an answer to a question in a potentially very large corpus of unstructured documents (that may not be redundant). Thus the system has to combine the challenges of document retrieval (finding the relevant documents) with that of machine comprehension of text (identifying the answers from those documents).Our experiments with DrQA focus on answering factoid questions while using Wikipedia as the unique knowledge source for documents. Wikipedia is a well-suited source of large-scale, rich, detailed information. In order to answer any question, one must first retrieve the few potentially relevant articles among more than 5 million, and then scan them carefully to identify the answer.Note that DrQA treats Wikipedia as a generic collection of articles and does not rely on its internal graph structure. As a result, [<marko.inline.RawText object at 0x000001592FDA56C8>], as described in the retriever .This repository includes code, data, and pre-trained models for processing and querying Wikipedia as described in the paper -- see . We also list several different datasets for evaluation, see . Note that this work is a refactored and more efficient version of the original code. Reproduction numbers are very similar but not exact.Quick Start: Demo DrQA and  our models to start asking open-domain questions!Run  to drop into an interactive session. For each question, the top span and the Wikipedia paragraph it came from are returned.>>> process('What is question answering?')

Top Predictions:
+------+----------------------------------------------------------------------------------------------------------+--------------------+--------------+-----------+
| Rank |                                                  Answer                                                  |        Doc         | Answer Score | Doc Score |
+------+----------------------------------------------------------------------------------------------------------+--------------------+--------------+-----------+
|  1   | a computer science discipline within the fields of information retrieval and natural language processing | Question answering |    1917.8    |   327.89  |
+------+----------------------------------------------------------------------------------------------------------+--------------------+--------------+-----------+

Contexts:
[ Doc = Question answering ]
Question Answering (QA) is a computer science discipline within the fields of
information retrieval and natural language processing (NLP), which is
concerned with building systems that automatically answer questions posed by
humans in a natural language.
>>> process('What is the answer to life, the universe, and everything?')

Top Predictions:
+------+--------+---------------------------------------------------+--------------+-----------+
| Rank | Answer |                        Doc                        | Answer Score | Doc Score |
+------+--------+---------------------------------------------------+--------------+-----------+
|  1   |   42   | Phrases from The Hitchhiker's Guide to the Galaxy |    47242     |   141.26  |
+------+--------+---------------------------------------------------+--------------+-----------+

Contexts:
[ Doc = Phrases from The Hitchhiker's Guide to the Galaxy ]
The number 42 and the phrase, ""Life, the universe, and everything"" have
attained cult status on the Internet. ""Life, the universe, and everything"" is
a common name for the off-topic section of an Internet forum and the phrase is
invoked in similar ways to mean ""anything at all"". Many chatbots, when asked
about the meaning of life, will answer ""42"". Several online calculators are
also programmed with the Question. Google Calculator will give the result to
""the answer to life the universe and everything"" as 42, as will Wolfram's
Computational Knowledge Engine. Similarly, DuckDuckGo also gives the result of
""the answer to the ultimate question of life, the universe and everything"" as
42. In the online community Second Life, there is a section on a sim called
43. ""42nd Life."" It is devoted to this concept in the book series, and several
attempts at recreating Milliways, the Restaurant at the End of the Universe, were made.
>>> process('Who was the winning pitcher in the 1956 World Series?')

Top Predictions:
+------+------------+------------------+--------------+-----------+
| Rank |   Answer   |       Doc        | Answer Score | Doc Score |
+------+------------+------------------+--------------+-----------+
|  1   | Don Larsen | New York Yankees |  4.5059e+06  |   278.06  |
+------+------------+------------------+--------------+-----------+

Contexts:
[ Doc = New York Yankees ]
In 1954, the Yankees won over 100 games, but the Indians took the pennant with
an AL record 111 wins; 1954 was famously referred to as ""The Year the Yankees
Lost the Pennant"". In , the Dodgers finally beat the Yankees in the World
Series, after five previous Series losses to them, but the Yankees came back
strong the next year. On October 8, 1956, in Game Five of the 1956 World
Series against the Dodgers, pitcher Don Larsen threw the only perfect game in
World Series history, which remains the only perfect game in postseason play
and was the only no-hitter of any kind to be pitched in postseason play until
Roy Halladay pitched a no-hitter on October 6, 2010.
Try some of your own! Of course, DrQA might provide alternative facts, so enjoy the ride.Installing DrQASetting up DrQA is easy!DrQA requires Linux/OSX and Python 3.5 or higher. It also requires installing  version 1.0. Its other dependencies are listed in requirements.txt. CUDA is strongly recommended for speed, but not necessary.Run the following commands to clone the repository and install DrQA:git clone https://github.com/facebookresearch/DrQA.git
cd DrQA; pip install -r requirements.txt; python setup.py develop
Note: requirements.txt includes a subset of all the possible required packages. Depending on what you want to run, you might need to install an extra package (e.g. spacy).If you use the CoreNLPTokenizer or SpacyTokenizer you also need to download the Stanford CoreNLP jars and spaCy  model, respectively. If you use Stanford CoreNLP, have the jars in your java  environment variable, or set the path programmatically with:import drqa.tokenizers
drqa.tokenizers.set_default('corenlp_classpath', '/your/corenlp/classpath/*')
IMPORTANT: The default Ex: .If you do not already have a CoreNLP  you can run:./install_corenlp.sh
Verify that it runs:from drqa.tokenizers import CoreNLPTokenizer
tok = CoreNLPTokenizer()
tok.tokenize('hello world').words()  # Should complete immediately
For convenience, the Document Reader, Retriever, and Pipeline modules will try to load default models if no model argument is given. See below for downloading these models.Trained Models and DataTo download all provided trained models and data for Wikipedia question answering, run:./download.sh
Warning: this downloads a 7.5GB tarball (25GB untarred) and will take some time.This stores the data in  at the file paths specified in the various modules' defaults. This top-level directory can be modified by setting a  environment variable to point to somewhere else.Default directory structure (see  for more info on additional downloads for training):DrQA
├── data (or $DRQA_DATA)
    ├── datasets
    │   ├── SQuAD-v1.1-<train/dev>.<txt/json>
    │   ├── WebQuestions-<train/test>.txt
    │   ├── freebase-entities.txt
    │   ├── CuratedTrec-<train/test>.txt
    │   └── WikiMovies-<train/test/entities>.txt
    ├── reader
    │   ├── multitask.mdl
    │   └── single.mdl
    └── wikipedia
        ├── docs.db
        └── docs-tfidf-ngram=2-hash=16777216-tokenizer=simple.npz
Default model paths for the different modules can also be modified programmatically in the code, e.g.:import drqa.reader
drqa.reader.set_default('model', '/path/to/model')
reader = drqa.reader.Predictor()  # Default model loaded for prediction
Document RetrieverTF-IDF model using Wikipedia (unigrams and bigrams, 2^24 bins, simple tokenization), evaluated on multiple datasets (test sets, dev set for SQuAD):| Model | SQuAD P@5 | CuratedTREC P@5 | WebQuestions P@5 | WikiMovies P@5 | Size || :---: | :-------: | :-------------: | :--------------: | :------------: | :---: ||  | 78.0 | 87.6 | 75.0 | 69.8 | ~13GB |P@5 here is defined as the % of questions for which the answer segment appears in one of the top 5 documents.Document ReaderModel trained only on SQuAD, evaluated in the SQuAD setting:| Model | SQuAD Dev EM | SQuAD Dev F1 | Size || :---: | :-----------:| :----------: | :--: ||  | 69.4 | 78.9 | ~130MB |Model trained with distant supervision without NER/POS/lemma features, evaluated on multiple datasets (test sets, dev set for SQuAD) in the full Wikipedia setting:| Model | SQuAD EM | CuratedTREC EM | WebQuestions EM | WikiMovies EM | Size || :---: | :------: | :------------: | :-------------: | :-----------: | :--:|  | 29.5 | 27.2 | 18.5 | 36.9 | ~270MB |WikipediaOur full-scale experiments were conducted on the 2016-12-21 dump of English Wikipedia. The dump was processed with the  and filtered for internal disambiguation, list, index, and outline pages (pages that are typically just links). We store the documents in an sqlite database for which  provides an interface.| Database | Num. Documents | Size || :------: | :------------: | :-----------------: ||  | 5,075,182 | ~13GB |QA DatasetsThe datasets used for DrQA training and evaluation can be found here:Format AThe , , and  scripts expect the datasets as a  file where each line is a JSON encoded QA pair, like so:'{""question"": ""q1"", ""answer"": [""a11"", ..., ""a1i""]}'
...
'{""question"": ""qN"", ""answer"": [""aN1"", ..., ""aNi""]}'
Scripts to convert SQuAD and WebQuestions to this format are included in . This is automatically done in .Format BThe  directory scripts expect the datasets as a  file where the data is arranged like SQuAD:file.json
├── ""data""
│   └── [i]
│       ├── ""paragraphs""
│       │   └── [j]
│       │       ├── ""context"": ""paragraph text""
│       │       └── ""qas""
│       │           └── [k]
│       │               ├── ""answers""
│       │               │   └── [l]
│       │               │       ├── ""answer_start"": N
│       │               │       └── ""text"": ""answer""
│       │               ├── ""id"": ""<uuid>""
│       │               └── ""question"": ""paragraph question?""
│       └── ""title"": ""document id""
└── ""version"": 1.1
Entity listsSome datasets have (potentially large) candidate lists for selecting answers. For example, WikiMovies' answers are OMDb entries while WebQuestions is based on Freebase. If we have known candidates, we can impose that all predicted answers must be in this list by discarding any higher scoring spans that are not.DrQA ComponentsDocument RetrieverDrQA is not tied to any specific type of retrieval system -- as long as it effectively narrows the search space and focuses on relevant documents.Following classical QA systems, we include an efficient (non-machine learning) document retrieval system based on sparse, TF-IDF weighted bag-of-word vectors. We use bags of hashed n-grams (here, unigrams and bigrams).To see how to build your own such model on new documents, see the retriever .To interactively query Wikipedia:python scripts/retriever/interactive.py --model /path/to/model
If  is left out our  will be used (assuming it was ).To evaluate the retriever accuracy (% match in top 5) on a dataset:python scripts/retriever/eval.py /path/to/format/A/dataset.txt --model /path/to/model
Document ReaderDrQA's Document Reader is a multi-layer recurrent neural network machine comprehension model trained to do extractive question answering. That is, the model tries to find the answer to any question as a text span in one of the returned documents.The Document Reader was inspired by, and primarily trained on, the  dataset. It can also be used standalone on such SQuAD-like tasks where a specific context is supplied with the question, the answer to which is contained in the context.To see how to train the Document Reader on SQuAD, see the reader .To interactively ask questions about text with a trained model:python scripts/reader/interactive.py --model /path/to/model
Again, here  is optional; a  will be used if it is left out.To run model predictions on a dataset:python scripts/reader/predict.py /path/to/format/B/dataset.json --model /path/to/model
DrQA PipelineThe full system is linked together in .To interactively ask questions using the full DrQA:python scripts/pipeline/interactive.py
Optional arguments:--reader-model    Path to trained Document Reader model.
--retriever-model Path to Document Retriever model (tfidf).
--doc-db          Path to Document DB.
--tokenizer      String option specifying tokenizer type to use (e.g. 'corenlp').
--candidate-file  List of candidates to restrict predictions to, one candidate per line.
--no-cuda         Use CPU only.
--gpu             Specify GPU device id to use.
To run predictions on a dataset:python scripts/pipeline/predict.py /path/to/format/A/dataset.txt
Optional arguments:--out-dir             Directory to write prediction file to (<dataset>-<model>-pipeline.preds).
--reader-model        Path to trained Document Reader model.
--retriever-model     Path to Document Retriever model (tfidf).
--doc-db              Path to Document DB.
--embedding-file      Expand dictionary to use all pretrained embeddings in this file (e.g. all glove vectors to minimize UNKs at test time).
--candidate-file      List of candidates to restrict predictions to, one candidate per line.
--n-docs              Number of docs to retrieve per query.
--top-n               Number of predictions to make per query.
--tokenizer           String option specifying tokenizer type to use (e.g. 'corenlp').
--no-cuda             Use CPU only.
--gpu                 Specify GPU device id to use.
--parallel            Use data parallel (split across GPU devices).
--num-workers         Number of CPU processes (for tokenizing, etc).
--batch-size          Document paragraph batching size (Reduce in case of GPU OOM).
--predict-batch-size  Question batching size (Reduce in case of CPU OOM).
Distant Supervision (DS)DrQA's performance improves significantly in the full-setting when provided with distantly supervised data from additional datasets. Given question-answer pairs but no supporting context, we can use string matching heuristics to automatically associate paragraphs to these training examples.The  directory contains code to generate and inspect such distantly supervised data. More information can be found in the distant supervision .TokenizersWe provide a number of different tokenizer options for convenience. Each has its own pros/cons based on how many dependencies it requires, overhead for running it, speed, and performance. For our reported experiments we used CoreNLP (but results are all similar).Available tokenizers:See the  of mappings between string option names and tokenizer classes.CitationPlease cite the ACL paper if you use DrQA in your work:@inproceedings{chen2017reading,
  title={Reading {Wikipedia} to Answer Open-Domain Questions},
  author={Chen, Danqi and Fisch, Adam and Weston, Jason and Bordes, Antoine},
  booktitle={Association for Computational Linguistics (ACL)},
  year={2017}
}
DrQA ElsewhereConnection with ParlAIThis implementation of the DrQA Document Reader is closely related to the one found in . Here, however, the work is extended to interact with the Document Retriever in the open-domain setting. On the other hand, the implementation in ParlAI is more general, and follows the appropriate API to work in more QA/Dialog settings.Web UI has provided a wrapper for a .LicenseDrQA is BSD-licensed."
https://github.com/google-deepmind/dm_control,"Google DeepMind's software stack for physics-based simulation and Reinforcement Learning environments, using MuJoCo.",": Google DeepMind Infrastructure for Physics-Based Simulation.Google DeepMind's software stack for physics-based simulation and ReinforcementLearning environments, using MuJoCo physics.An introductory tutorial for this package is available as a Colaboratorynotebook:OverviewThis package consists of the following ""core"" components:Additionally, the following components are available for the creation of morecomplex control tasks:If you use this package, please cite our accompanying :@article{tunyasuvunakool2020,
         title = {dm_control: Software and tasks for continuous control},
         journal = {Software Impacts},
         volume = {6},
         pages = {100022},
         year = {2020},
         issn = {2665-9638},
         doi = {https://doi.org/10.1016/j.simpa.2020.100022},
         url = {https://www.sciencedirect.com/science/article/pii/S2665963820300099},
         author = {Saran Tunyasuvunakool and Alistair Muldal and Yotam Doron and
                   Siqi Liu and Steven Bohez and Josh Merel and Tom Erez and
                   Timothy Lillicrap and Nicolas Heess and Yuval Tassa},
}
InstallationInstall  from PyPI by runningpip install dm_control
VersioningStarting from version 1.0.0, we adopt semantic versioning.Prior to version 1.0.0, the  Python package was versioned ,where  was an internal revision number that increased by an arbitrary amountat every single Git commit.If you want to install an unreleased version of  directly from ourrepository, you can do so by running .RenderingThe MuJoCo Python bindings support three different OpenGL rendering backends:EGL (headless, hardware-accelerated), GLFW (windowed, hardware-accelerated), andOSMesa (purely software-based). At least one of these three backends must beavailable in order render through .By default,  will attempt to use GLFW first, then EGL, then OSMesa.You can also specify a particular backend to use by setting the environment variable to , , or , respectively. Whenrendering with EGL, you can also specify which GPU to use for rendering bysetting the environment variable  to the target GPU ID.Additional instructions for Homebrew users on macOS"
https://github.com/pytorch/captum,Model interpretability and understanding for PyTorch,"Captum is a model interpretability and understanding library for PyTorch.Captum means comprehension in Latin and contains general purpose implementationsof integrated gradients, saliency maps, smoothgrad, vargrad and others forPyTorch models. It has quick integration for models built with domain-specificlibraries such as torchvision, torchtext, and others.Captum is currently in beta and under active development!About CaptumWith the increase in model complexity and the resulting lack of transparency, model interpretability methods have become increasingly important. Model understanding is both an active area of research as well as an area of focus for practical applications across industries using machine learning. Captum provides state-of-the-art algorithms such as Integrated Gradients, Testing with Concept Activaton Vectors (TCAV), TracIn influence functions, just to name a few, that provide researchers and developers with an easy way to understand which features, training examples or concepts contribute to a models' predictions and in general what and how the model learns. In addition to that, Captum also provides adversarial attacks and minimal input perturbation capabilities that can be used both for generating counterfactual explanations and adversarial perturbations.Captum helps ML researchers more easily implement interpretability algorithms that can interact with PyTorch models. Captum also allows researchers to quickly benchmark their work against other existing algorithms available in the library.Target AudienceThe primary audiences for Captum are model developers who are looking to improve their models and understand which concepts, features or training examples are important and interpretability researchers focused on identifying algorithms that can better interpret many types of models.Captum can also be used by application engineers who are using trained models in production. Captum provides easier troubleshooting through improved model interpretability, and the potential for delivering better explanations to end users on why they’re seeing a specific piece of content, such as a movie recommendation.InstallationInstallation RequirementsInstalling the latest releaseThe latest release of Captum is easily installed either via (recommended) or via .with You can install captum from any of the following supported conda channels:With pip install captum
Manual / Dev installIf you'd like to try our bleeding edge features (and don't mind potentiallyrunning into the occasional bug here or there), you can install the latestmaster directly from GitHub. For a basic install, run:git clone https://github.com/pytorch/captum.git
cd captum
pip install -e .
To customize the installation, you can also run the following variants of theabove:To execute unit tests from a manual install, run:# running a single unit test
python -m unittest -v tests.attr.test_saliency
# running all unit tests
pytest -ra
Getting StartedCaptum helps you interpret and understand predictions of PyTorch models byexploring features that contribute to a prediction the model makes.It also helps understand which neurons and layers are important formodel predictions.Let's apply some of those algorithms to a toy model we have created fordemonstration purposes.For simplicity, we will use the following architecture, but users are welcometo use any PyTorch model of their choice.import numpy as np

import torch
import torch.nn as nn

from captum.attr import (
    GradientShap,
    DeepLift,
    DeepLiftShap,
    IntegratedGradients,
    LayerConductance,
    NeuronConductance,
    NoiseTunnel,
)

class ToyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.lin1 = nn.Linear(3, 3)
        self.relu = nn.ReLU()
        self.lin2 = nn.Linear(3, 2)

        # initialize weights and biases
        self.lin1.weight = nn.Parameter(torch.arange(-4.0, 5.0).view(3, 3))
        self.lin1.bias = nn.Parameter(torch.zeros(1,3))
        self.lin2.weight = nn.Parameter(torch.arange(-3.0, 3.0).view(2, 3))
        self.lin2.bias = nn.Parameter(torch.ones(1,2))

    def forward(self, input):
        return self.lin2(self.relu(self.lin1(input)))
Let's create an instance of our model and set it to eval mode.model = ToyModel()
model.eval()
Next, we need to define simple input and baseline tensors.Baselines belong to the input space and often carry no predictive signal.Zero tensor can serve as a baseline for many tasks.Some interpretability algorithms such as ,  and  are designed to attribute the changebetween the input and baseline to a predictive class or a value that the neuralnetwork outputs.We will apply model interpretability algorithms on the networkmentioned above in order to understand the importance of individualneurons/layers and the parts of the input that play an important role in thefinal prediction.To make computations deterministic, let's fix random seeds.torch.manual_seed(123)
np.random.seed(123)
Let's define our input and baseline tensors. Baselines are used in someinterpretability algorithms such as  and.input = torch.rand(2, 3)
baseline = torch.zeros(2, 3)
Next we will use  algorithms to assign attributionscores to each input feature with respect to the first target output.ig = IntegratedGradients(model)
attributions, delta = ig.attribute(input, baseline, target=0, return_convergence_delta=True)
print('IG Attributions:', attributions)
print('Convergence Delta:', delta)
Output:IG Attributions: tensor([[-0.5922, -1.5497, -1.0067],
                         [ 0.0000, -0.2219, -5.1991]])
Convergence Delta: tensor([2.3842e-07, -4.7684e-07])
The algorithm outputs an attribution score for each input element and aconvergence delta. The lower the absolute value of the convergence delta the betteris the approximation. If we choose not to return delta,we can simply not provide the  inputargument. The absolute value of the returned deltas can be interpreted as anapproximation error for each input sample.It can also serve as a proxy of how accurate the integral approximation for giveninputs and baselines is.If the approximation error is large, we can try a larger number of integralapproximation steps by setting  to a larger value. Not all algorithmsreturn approximation error. Those which do, though, compute it based on thecompleteness property of the algorithms.Positive attribution score means that the input in that particular positionpositively contributed to the final prediction and negative means the opposite.The magnitude of the attribution score signifies the strength of the contribution.Zero attribution score means no contribution from that particular feature.Similarly, we can apply ,  and other attribution algorithms to the model. first chooses a random baseline from baselines' distribution, thenadds gaussian noise with std=0.09 to each input example  times.Afterwards, it chooses a random point between each example-baseline pair andcomputes the gradients with respect to target class (in this case target=0). Resultingattribution is the mean of gradients * (inputs - baselines)gs = GradientShap(model)

# We define a distribution of baselines and draw `n_samples` from that
# distribution in order to estimate the expectations of gradients across all baselines
baseline_dist = torch.randn(10, 3) * 0.001
attributions, delta = gs.attribute(input, stdevs=0.09, n_samples=4, baselines=baseline_dist,
                                   target=0, return_convergence_delta=True)
print('GradientShap Attributions:', attributions)
print('Convergence Delta:', delta)
OutputGradientShap Attributions: tensor([[-0.1542, -1.6229, -1.5835],
                                   [-0.3916, -0.2836, -4.6851]])
Convergence Delta: tensor([ 0.0000, -0.0005, -0.0029, -0.0084, -0.0087, -0.0405,  0.0000, -0.0084])

Deltas are computed for each  example. The user can,for instance, average them:deltas_per_example = torch.mean(delta.reshape(input.shape[0], -1), dim=1)
in order to get per example average delta.Below is an example of how we can apply  and  on the described above. The current implementation of DeepLift supports only the rule.For more details on alternative implementations, please see the .dl = DeepLift(model)
attributions, delta = dl.attribute(input, baseline, target=0, return_convergence_delta=True)
print('DeepLift Attributions:', attributions)
print('Convergence Delta:', delta)
OutputDeepLift Attributions: tensor([[-0.5922, -1.5497, -1.0067],
                               [ 0.0000, -0.2219, -5.1991])
Convergence Delta: tensor([0., 0.])
 assigns similar attribution scores as  to inputs,however it has lower execution time. Another important thing to remember aboutDeepLift is that it currently doesn't support all non-linear activation types.For more details on limitations of the current implementation, please see the.Similar to integrated gradients, DeepLift returns a convergence delta scoreper input example. The approximation error is then the absolutevalue of the convergence deltas and can serve as a proxy of how accurate thealgorithm's approximation is.Now let's look into . Similar to ,  usesbaseline distribution. In the example below, we use the same baseline distributionas for .dl = DeepLiftShap(model)
attributions, delta = dl.attribute(input, baseline_dist, target=0, return_convergence_delta=True)
print('DeepLiftSHAP Attributions:', attributions)
print('Convergence Delta:', delta)
OutputDeepLiftShap Attributions: tensor([[-5.9169e-01, -1.5491e+00, -1.0076e+00],
                                   [-4.7101e-03, -2.2300e-01, -5.1926e+00]], grad_fn=<MeanBackward1>)
Convergence Delta: tensor([-4.6120e-03, -1.6267e-03, -5.1045e-04, -1.4184e-03, -6.8886e-03,
                           -2.2224e-02,  0.0000e+00, -2.8790e-02, -4.1285e-03, -2.7295e-02,
                           -3.2349e-03, -1.6265e-03, -4.7684e-07, -1.4191e-03, -6.8889e-03,
                           -2.2224e-02,  0.0000e+00, -2.4792e-02, -4.1289e-03, -2.7296e-02])
 uses  to compute attribution score for eachinput-baseline pair and averages it for each input across all baselines.It computes deltas for each input example-baseline pair, thus resulting to delta values.Similar to GradientShap in order to compute example-based deltas we can average them per example:deltas_per_example = torch.mean(delta.reshape(input.shape[0], -1), dim=1)
In order to smooth and improve the quality of the attributions we can run and other attribution methods through a . allows us to use ,  and  techniquesto smoothen the attributions by aggregating them for multiple noisysamples that were generated by adding gaussian noise.Here is an example of how we can use  with .ig = IntegratedGradients(model)
nt = NoiseTunnel(ig)
attributions, delta = nt.attribute(input, nt_type='smoothgrad', stdevs=0.02, nt_samples=4,
      baselines=baseline, target=0, return_convergence_delta=True)
print('IG + SmoothGrad Attributions:', attributions)
print('Convergence Delta:', delta)
OutputIG + SmoothGrad Attributions: tensor([[-0.4574, -1.5493, -1.0893],
                                      [ 0.0000, -0.2647, -5.1619]])
Convergence Delta: tensor([ 0.0000e+00,  2.3842e-07,  0.0000e+00, -2.3842e-07,  0.0000e+00,
        -4.7684e-07,  0.0000e+00, -4.7684e-07])

The number of elements in the  tensor is equal to: In order to get an example-wise delta, we can, for example, average them:deltas_per_example = torch.mean(delta.reshape(input.shape[0], -1), dim=1)
Let's look into the internals of our network and understand which layersand neurons are important for the predictions.We will start with the .  helps us to identifyinput features that are important for a particular neuron in a givenlayer. It decomposes the computation of integrated gradients via the chain rule bydefining the importance of a neuron as path integral of the derivative of the outputwith respect to the neuron times the derivatives of the neuron with respect to theinputs of the model.In this case, we choose to analyze the first neuron in the linear layer.nc = NeuronConductance(model, model.lin1)
attributions = nc.attribute(input, neuron_selector=1, target=0)
print('Neuron Attributions:', attributions)
OutputNeuron Attributions: tensor([[ 0.0000,  0.0000,  0.0000],
                             [ 1.3358,  0.0000, -1.6811]])
Layer conductance shows the importance of neurons for a layer and given input.It is an extension of path integrated gradients for hidden layers and holds thecompleteness property as well.It doesn't attribute the contribution scores to the input featuresbut shows the importance of each neuron in the selected layer.lc = LayerConductance(model, model.lin1)
attributions, delta = lc.attribute(input, baselines=baseline, target=0, return_convergence_delta=True)
print('Layer Attributions:', attributions)
print('Convergence Delta:', delta)
OutputsLayer Attributions: tensor([[ 0.0000,  0.0000, -3.0856],
                            [ 0.0000, -0.3488, -4.9638]], grad_fn=<SumBackward1>)
Convergence Delta: tensor([0.0630, 0.1084])
Similar to other attribution algorithms that return convergence delta, returns the deltas for each example. The approximation error is then the absolutevalue of the convergence deltas and can serve as a proxy of how accurate integralapproximation for given inputs and baselines is.More details on the list of supported algorithms and how to applyCaptum on different types of models can be found in our tutorials.Captum InsightsCaptum provides a web interface called Insights for easy visualization andaccess to a number of our interpretability algorithms.To analyze a sample model on CIFAR10 via Captum Insights runpython -m captum.insights.example
and navigate to the URL specified in the output.To build Insights you will need  >= 8.xand  >= 1.5.To build and launch from a checkout in a conda environment runconda install -c conda-forge yarn
BUILD_INSIGHTS=1 python setup.py develop
python captum/insights/example.py
Captum Insights Jupyter WidgetCaptum Insights also has a Jupyter widget providing the same user interface as the web app.To install and enable the widget, runjupyter nbextension install --py --symlink --sys-prefix captum.insights.attr_vis.widget
jupyter nbextension enable captum.insights.attr_vis.widget --py --sys-prefix
To build the widget from a checkout in a conda environment runconda install -c conda-forge yarn
BUILD_INSIGHTS=1 python setup.py develop
FAQIf you have questions about using Captum methods, please check this , which addresses many common issues.ContributingSee the  file for how to help out.Talks and PapersNeurIPS 2019:The slides of our presentation  can be found KDD 2020:The slides of our presentation from KDD 2020 tutorial can be found .You can watch the recorded talk GTC 2020:Opening Up the Black Box: Model Understanding with Captum and PyTorch.You can watch the recorded talk XAI Summit 2020:Using Captum and Fiddler to Improve Model Understanding with Explainable AI.You can watch the recorded talk PyTorch Developer Day 2020Model Interpretability.You can watch the recorded talk NAACL 2021Tutorial on Fine-grained Interpretation and Causation Analysis in Deep NLP Models.You can watch the recorded talk ICLR 2021 workshop on Responsible AI:Summer school on medical imaging at University of Lyon. A class on model explainability (link to the video)https://www.youtube.com/watch?v=vn-jLzY67V0References of AlgorithmsMore details about the above mentioned  and their pros and cons can be found on our .LicenseCaptum is BSD licensed, as found in the  file."
https://github.com/PrefectHQ/prefect,"Prefect is a workflow orchestration tool empowering developers to build, observe, and react to data pipelines","PrefectPrefect is an orchestrator for data-intensive workflows. It's the simplest way to transform any Python function into a unit of work that can be observed and orchestrated. With Prefect, you can build resilient, dynamic workflows that react to the world around them and recover from unexpected changes. With just a few decorators, Prefect supercharges your code with features like automatic retries, distributed execution, scheduling, caching, and much more. Every activity is tracked and can be monitored with the Prefect server or Prefect Cloud dashboard.from prefect import flow, task
from typing import List
import httpx


@task(retries=3)
def get_stars(repo: str):
    url = f""https://api.github.com/repos/{repo}""
    count = httpx.get(url).json()[""stargazers_count""]
    print(f""{repo} has {count} stars!"")


@flow(name=""GitHub Stars"")
def github_stars(repos: List[str]):
    for repo in repos:
        get_stars(repo)


# run the flow!
github_stars([""PrefectHQ/Prefect""])
After running some flows, fire up the Prefect UI to see what happened:prefect server start
From here, you can continue to use Prefect interactively or  to remote environments, running on a scheduled or event-driven basis.Getting StartedPrefect requires Python 3.8 or later. To , run the following command in a shell or terminal session:pip install prefect
Start by then exploring the , then follow one of our  to learn by example.Join the communityPrefect is made possible by the fastest growing community of thousands of friendly data engineers. Join us in building a new kind of workflow system. The  is a fantastic place to learn more about Prefect, ask questions, or get help with workflow design. The  is a community-driven knowledge base to find answers to your Prefect-related questions. All community forums, including code contributions, issue discussions, and slack messages are subject to our .ContributeSee our .Thanks for being part of the mission to build a new kind of workflow system and, of course, happy engineering!"
https://github.com/zq1997/deepin-wine,【deepin源移植】Debian/Ubuntu上最快的QQ/微信安装方式,deepin-wine跳转查看快速开始常见问题没有应用图标重新登入即可，可注销或重启。微信/QQ等无法启动最新版本可能会遇到这个问题，与环境变量有关，参考。容器被重置问题每次启动应用后发现之前的wine配置或者app内配置都丢失了？尤其是TIM，发现聊天文件啥的都没了。建议参考。QQ/微信托盘小图标显示异常这和桌面环境有关，Linux发行版桌众多，面布局千奇百怪，并不是每一个都具有与【Windows系统托盘】对应的控件。QQ头像无法加载可能是IPv6问题，见。没办法进行QQ远程/视频通话视频相关的功能对硬件和底层驱动的依赖很大，Wine毕竟不是Windows，100%完美模拟是不可能的。字体相关问题新版本的deepin-wine似乎已经能比较好地解决字体问题了，一般装上去就能用了。讨论的是以前的版本，谨慎参考。安装依赖问题这说明系统试图安装但是无法装上去，这一般是你已有的软件源配置问题、或者安装过了一些有冲突的东西。那么，你应该试着安装，执行（不需sudo，只是模拟，放心测试），它一般又会接着告诉你，说明更底层的错误出在了，不断尝试，找到罪魁祸首，可以尝试先解决这个罪魁祸首。更多问题卸载清理卸载与清理按照层次从浅到深可以分为如下四个层级。如果只是想清除APP账户配置啥的那么请按照清理；如果你发现程序奔溃之类的，请按照清理；如果需要卸载APP，按照清理；如果你想把一切回到最初的起点，执行清理。高级文档如果你是资深Linux用户，可以了解一下这部分。移植原理Deepin把QQ/微信之类的deepin-wine应用打包放在了deepin仓库中，因此先提取出这些应用及依赖的软件包，再减去Debian/Ubuntu等发行版官方仓库中固有的软件包，就可以打包成一个移植于对应发行版的“差量仓库”，然后把这个差量仓库的索引发布出来即可，其中的可以直接重定向到Deepin官方仓库地址去。配置过程详解环境配置其实就是添加我自行构建的软件仓库为源，具体包括以下几步。版权与致谢这个git仓库中的代码只包括了移植版软件仓库的构建工具，最后仓库中软件包的下载地址会被301重定向到deepin的官方仓库（或者镜像）中去，其版权由  所有。本项目受  项目启发，改进了一下安装方式，因此兼容原项目，已经按照deepin-wine-ubuntu项目安装好后，依然可以再按此项目进行配置，可以更方便地进行后续更新。
https://github.com/NVIDIA/Megatron-LM,Ongoing research training transformer models at scale,"Megatron (, , and ) is a large, powerful transformer developed by the Applied Deep Learning Research team at NVIDIA. This repository is for ongoing research on training large transformer language models at scale. We developed efficient, model-parallel (, , and ), and multi-node pre-training of transformer based models such as , , and  using mixed precision.Below are some of the projects where we have directly used Megatron:Megatron is also used in , a framework to help enterprises overcome the challenges of building and training sophisticated natural language processing models with billions and trillions of parameters.Our codebase is capable of efficiently training very large (hundreds of billions of parameters) language models with both model and data parallelism. To demonstrate how the code scales with multiple GPUs and model sizes, we consider GPT models from 1 billion all the way to 1 trillion parameters. All models use a vocabulary size of 51,200 and a sequence length of 2048. We vary hidden size, number of attention heads, and number of layers to arrive at a specific model size. As the model size increases, we also modestly increase the batch size. We leverage  to perform scaling studies and use up to 3072  GPUs for the largest model. Each cluster node has 8 NVIDIA 80GB A100 GPUs. The graph below shows that we scale nearly linear up to 1 trillion parameter models running on 3072 GPUs. Note that these results are from benchmark runs and these models were not trained to convergence; however, the FLOPs are measured for end-to-end training, i.e., includes all operations including data loading, optimization, and even logging.The following table shows both model (MFU) and hardware (HFU) FLOPs utilization for select configurations up to 1T parameters (see  for a description of how these are calculated). As the model size increases, we achieve better GPU utilization and for the one trillion parameter model, we reach a MFU and HFU of 56.3% and 57.0%, respectively. Note that these numbers are also measured on benchmark runs and in this case are measured using a data parallel size of one. Data parallelism introduces some overhead due to the gradient all-reduce required between the data parallel groups. However, for large transformer models, this overhead is not large and can almost entirely eliminated by overlapping the gradient all-reduce with backpropagation.| Model Size | Model FLOPs Utilization | Hardware FLOPs Utilization || :---: | :---: | :---: || 22B   | 41.5% | 43.7% || 175B  | 51.4% | 52.8% || 530B  | 56.0% | 57.0% || 1T    | 56.3% | 57.0% |ContentsSetupWe strongly recommend using the latest release of  with DGX nodes. If you can't use this for some reason, use the latest pytorch, cuda, nccl, and NVIDIA  releases.  Data preprocessing requires , though this is not required for training, evaluation, or downstream tasks.You can launch an instance of the PyTorch container and mount Megatron, your dataset, and checkpoints with the following Docker commands:docker pull nvcr.io/nvidia/pytorch:xx.xx-py3
docker run --gpus all -it --rm -v /path/to/megatron:/workspace/megatron -v /path/to/dataset:/workspace/dataset -v /path/to/checkpoints:/workspace/checkpoints nvcr.io/nvidia/pytorch:xx.xx-py3
Downloading CheckpointsWe have provided pretrained  and  checkpoints for use to evaluate or finetuning downstream tasks. To access these checkpoints, first  for and  the NVIDIA GPU Cloud (NGC) Registry CLI. Further documentation for downloading models can be found in the .Alternatively, you can directly download the checkpoints using:The models require vocabulary files to run. The BERT  WordPiece vocab file can be extracted from Google's pretrained BERT models: , . The GPT  and  can be downloaded directly.UsageAfter installation, there are several possible workflows. The most comprehensive is:However, steps 1 and 2 can be replaced by using one of the pretrained models mentioned above.We've provided several scripts for pretraining both BERT and GPT in  directory, as well as scripts for both zero-shot and fine-tuned downstream tasks including MNLI, RACE, WikiText103, and LAMBADA evaluation. There is also a script for GPT interactive text generation.TrainingData PreprocessingThe training data requires preprocessing. First, place your training data in a loose json format, with one json containing a text sample per line. For example:The name of the  field of the json can be changed by using the  flag in  The other metadata are optional and are not used in training.The loose json is then processed into a binary format for training. To convert the json into mmap format use . An example script to prepare data for BERT training is:The output will be two files named, in this case,  and . The  specified in later BERT training is the full path and new filename, but without the file extension.For T5 use the same preprocessing as BERT, perhaps renaming it to:Some minor modifications are required for GPT data preprocessing, namely, the addition of a merge table, an end-of-document token, removal of sentence splitting, and a change to the tokenizer type:Here the output files are named  and . As before, in GPT training, use the longer name without the extension as .Further command line arguments are described in the source file .BERT PretrainingThe  script runs single GPU 345M parameter BERT pretraining. Debugging is the primary use for single GPU training, as the code base and command line arguments are optimized for highly distributed training. Most of the arguments are fairly self-explanatory. By default, the learning rate decays linearly over the training iterations starting at  to a minimum set by  over  iterations. The fraction of training iterations used for warmup is set by . While this is single GPU training, the batch size specified by  is a single forward-backward path batch-size and the code will perform gradient accumulation steps until it reaches  which is the batch size per iteration. The data is partitioned into a 949:50:1 ratio for training/validation/test sets (default is 969:30:1). This partitioning happens on the fly, but is consistent across runs with the same random seed (1234 by default, or specified manually with ). We use  as the training iterations requested. Alternatively, one can provide  which is total number of samples to train on. If this option is present, then instead of providing , one will need to provide .The logging, checkpoint-saving, and evaluation intervals are specified. Checkpointing the activations facilitates the training of larger models and/or batches. Note that the  now includes the additional  suffix added in preprocessing, but does not include the file extensions.Further command line arguments are described in the source file .To run , make any desired modifications including setting the environment variables for , , and . Make sure to set these variables to their paths in the container. Then launch the container with Megatron and necessary paths mounted (as explained in ) and run the example script.GPT PretrainingThe  script runs single GPU 345M parameter GPT pretraining. As mentioned above, single GPU training is primarily intended for debugging purposes, as the code is optimized for distributed training.It follows largely the same format as the previous BERT script with a few notable differences: the tokenization scheme used is BPE (which requires a merge table and a  vocabulary file) instead of WordPiece, the model architecture allows for longer sequences (note that the max position embedding must be greater than or equal to the maximum sequence length), and the  has been set to cosine decay.  Note that the  now includes the additional  suffix added in preprocessing, but does not include the file extensions.Further command line arguments are described in the source file . can be launched the same way as described for BERT. Set the env vars and make any other modifications, launch the container with appropriate mounts, and run the script.T5 PretrainingVery similar to BERT and GPT, the  script runs single GPU ""base"" (~220M parameter) T5 pretraining. The primary difference from BERT and GPT is the addition of the following arguments to accommodate the T5 architecture:All of the other arguments remain as they were for BERT and GPT pretraining. Run this example with the same steps described above for the other scripts.Distributed PretrainingThe  scripts use the PyTorch distributed launcher for distributed training. As such, multi-node training can be achieved by properly setting environment variables. See the official PyTorch  for further description of these . By default, multi-node training uses the  distributed backend. A simple set of additional arguments and the use of the PyTorch distributed module with the  elastic launcher (equivalent to ) are the only additional requirements to adopt distributed training. See any of  for more details.We use two types of parallelism: data and model parallelism. We facilitate two distributed data parallel implementations: a simple one of our own that performs gradient all-reduce at the end of back propagation step, and Torch's distributed data parallel wrapper that overlaps gradient reduction with back propagation computation. To switch between these two options use  or , respectively. As expected, Torch distributed data parallelism is more efficient at larger model sizes. For example, for the 8.3 billion parameters model running on 512 GPUs, the scaling increases from 60% to 76% when Torch's distributed data parallel is used. However, the overlapping method requires more memory and for some configurations (e.g., 2.5 billion parameters using 2-way model parallel and 1.2 billion parameters with no model parallel) can make the overall training slower as a result. We empirically found that using a smaller model in those cases improves the training time.Second, we developed a simple and efficient two-dimensional model-parallel approach. To use tensor model parallelism (splitting execution of a single transformer module over multiple GPUs, see Section 3 of ), add the  flag to specify the number of GPUs among which to split the model, along with the arguments passed to the distributed launcher as mentioned above. To use sequence parallelism specify , which requires tensor model parallel as it split among the same GPUs (more details in Section 4.2.2 of ).To use pipeline model parallelism (sharding the transformer modules into stages with an equal number of transformer modules on each stage, and then pipelining execution by breaking the batch into smaller microbatches, see Section 2.2 of ), use the  flag to specify the number of stages to split the model into (e.g., splitting a model with 24 transformer layers across 4 stages would mean each stage gets 6 transformer layers each).We have examples of how to use these two different forms of model parallelism the example scripts ending in :Other than these minor changes, the distributed training is identical to the training on a single GPU.The interleaved pipelining schedule (more details in Section 2.2.2 of ) can be enabled using the  argument, which controls the number of transformer layers in a virtual stage (by default with the non-interleaved schedule, each GPU will execute a single virtual stage with  transformer layers). The total number of layers in the transformer model should be divisible by this argument value. Additionally, the number of microbatches in the pipeline (computed as ) should be divisible by the  when using this schedule (this condition is checked in an assertion in the code). The interleaved schedule is not supported for pipelines with 2 stages ().Activation Checkpointing and RecomputationTo reduce GPU memory usage so deploy a large model to a training system, we support activation checkpointing and recomputation. We support two levels of recompute granularity:  and . Selective recomputation is the default and recommended in almost all cases. It saves the activations that take less space and are expensive to recompute and recomputes activations that take a lot of space but are relatively cheap to recompute (see  for details). To enable selective activation recompute simply use .For cases where memory is very tight,  checkpointing saves just the inputs to a transformer layer, or a block of transformer layers, and recomputes everything else. To turn on full activation recompute use . When using full activation recomputation, there are two methods:  and , chosen using the  argument.Distributed OptimizerUsage: . Compatible with all model and data types.The distributed optimizer is a memory savings technique, whereby the optimizer state is evenly distributed across data parallel ranks (versus the traditional method of replicating the optimizer state across data parallel ranks). As described in , our implementation distributes all optimizer state that does not overlap with the model state. For example, when using fp16 model params, the distributed optimizer maintains its own separate copy of fp32 main params & grads, which are distributed across DP ranks. When using bf16 model params, however, the distributed optimizer's fp32 main grads are the same as the model's fp32 grads, and so the grads in this case are not distributed (although the fp32 main params are still distributed, as they are separate from the bf16 model params).Theoretical memory savings vary depending on the combination of the model's param dtype and grad dtype. In our implementation, the theoretical number of bytes per parameter is (where 'd' is the data parallel size):| | Non-distributed optim | Distributed optim ||-|-|-|| fp16 param, fp16 grads | 20 | 4 + 16/d || bf16 param, fp32 grads | 18 | 6 + 12/d || fp32 param, fp32 grads | 16 | 8 + 8/d |FlashAttentionUsage: . Support attention head dimensions at most 128. is a fast andmemory-efficient algorithm to compute exact attention. It speeds up modeltraining and reduces memory requirement.To install FlashAttention:pip install flash-attn
GPT-3 ExampleIn  we have provided an example of how to configure Megatron to run  with 175 billion parameters on 1024 GPUs. The script is designed for  with  plugin but can be easily adopted to any other scheduler. It uses 8-way and 16-way tensor and pipeline parallelism, respectively. With options  and , the training will start with global batch size 16 and linearly increase the global batch size to 1536 over 5,859,375 samples with incremental steps 16. The training dataset can be either a single set or a multiple datasets combined with a set of weights.With full global batch size of 1536 on 1024 A100 GPUs, each iteration takes around 32 seconds resulting in 138 teraFLOPs per GPU which is 44% of the theoretical peak FLOPs.RetroSee:Retro is a retrieval-enhanced model that is based on GPT. As described in , Retro retrieves from a database of document chunks by performing locality search using a sample's tokens. The retrieval database can be large -- often billions or even trillions of tokens -- and provides a more efficient storage mechanism of factual knowledge, when compared to storing factual knowledge implicitly within the network's parameters.Using Retro requires two steps: 1) preprocessing the retrieval database and pretraining neighbors, and 2) pretraining a model using this data. Please see  for a detailed overview.Evaluation and TasksWe provide several command line arguments, detailed in the scripts listed below, to handle various zero-shot and fine-tuned downstream tasks. However, you can also finetune your model from a pretrained checkpoint on other corpora as desired. To do so, simply add the  flag and adjust the input files and training parameters within the original training script. The iteration count will be reset to zero, and the optimizer and internal state will be reinitialized. If the fine-tuning is interrupted for any reason, be sure to remove the  flag before continuing, otherwise the training will start again from the beginning.Because evaluation requires substantially less memory than training, it may be advantageous to merge a model trained in parallel for use on fewer GPUs in downstream tasks. The following script accomplishes this. This example reads in a GPT model with 4-way tensor and 4-way pipeline model parallelism and writes out a model with 2-way tensor and 2-way pipeline model parallelism.Several downstream tasks are described for both GPT and BERT models below. They can be run in distributed and model parallel modes with the same changes used in the training scripts.GPT Text GenerationWe have included a simple REST server to use for text generation in . You run it much like you would start a pretraining job, specifying an appropriate pretrained checkpoint. There are also few optional parameters: , and . See  or the source file for more information. See  for an example of how to run the server.Once the server is running you can use  to query it, it takes one argument which is the host the server is running on.You can also use CURL or any other tools to query the server directly:See  for more API options.Detoxify GPT via Self-generationWe include an example in  to detoxify language models by leveraging the generative power of language models.See  for step-by-step tutorials on how to perform domain-adaptive training and detoxify LM using self-generated corpus.GPT EvaluationWe include example scripts for GPT evaluation on WikiText perplexity evaluation and LAMBADA Cloze accuracy.WikiText Perplexity EvaluationFor even comparison with prior works, we evaluate perplexity on the word-level , and appropriately compute perplexity given the change in tokens when using our subword tokenizer.We use the following command to run WikiText-103 evaluation on a 345M parameter model.LAMBADA Cloze AccuracyTo compute LAMBADA cloze accuracy (the accuracy of predicting the last token given the preceding tokens) we utilize a detokenized, processed version of the .We use the following command to run LAMBADA evaluation on a 345M parameter model. Note that the  flag should be used to require whole word matching. Make that  is part of the file path.Further command line arguments are described in the source file BERT Task EvaluationRACE EvaluationThe following script finetunes the BERT model for evaluation on the . The  and  directory contain the RACE dataset as separate  files. Note that for RACE, the batch size is the number of RACE query's to evaluate. Since each RACE query has four samples, the effective batch size passed through the model will be four times the batch size specified on the command line.MNLI EvaluationThe following script finetunes the BERT model for evaluation with the . Because the matching tasks are quite similar, the script can be quickly tweaked to work with the  (QQP) dataset as well.Llama-2 Inference and FinetuningThe Llama-2  are an open-source set of pretrained & finetuned (for chat) models that have achieved strong results across a wide set of benchmarks. At the time of release, Llama-2 models achieved among the best results for open-source models, and were competitive with the closed-source GPT-3.5 model (see https://arxiv.org/pdf/2307.09288.pdf).The Llama-2 checkpoints can be loaded into Megatron for inference and finetuning. See documentation .DatasetsWe do not host any datasets for GPT or BERT training, however, we detail their collection so that our results may be reproduced.Collecting Wikipedia Training DataWe recommend following the Wikipedia data extraction process specified by Google research: ""the recommended pre-processing is to download , extract the text with , and then apply any necessary cleanup to convert it into plain text.""We recommend using the  argument when using WikiExtractor, which will dump the Wikipedia data into loose json format (one json per line), making it more manageable on the file system and also readily consumable by our codebase. We recommend further preprocessing this json dataset by nltk punctuation standardization. For BERT training, use the  flag to  as described  to include sentence breaks in the produced index. If you'd like to use Wikipedia data for GPT training you should still clean it with nltk/spacy/ftfy, but do not use the  flag.Collecting GPT Webtext DataWe utilize the publicly available  library from  and  work to download urls. We then filtered, cleaned, and deduplicated all downloaded content according to the procedure described in our  directory. For reddit URLs corresponding to content up to October 2018 we arrived at approximately 37GB of content.ReproducibilityMegatron training is intended to be bitwise reproducible. This means that the same training config run twice in the same HW and SW environment should produce identical model checkpoints, losses and accuracy metric values (iteration time metrics may vary).There are currently two known Megatron optimizations that break reproducibility whilst still producing almost identical training runs. The following workarounds should be applied in cases where reproducibility is required:These sources of non-determinism are under active investigation. If you observe non-determinism in Megatron training under other circumstances please open an issue."
https://github.com/KevinMusgrave/pytorch-metric-learning,"The easiest way to use deep metric learning in your application. Modular, flexible, and extensible. Written in PyTorch.","NewsJuly 25: v2.3.0June 18: v2.2.0DocumentationGoogle Colab ExamplesSee the  for notebooks you can download or run on Google Colab.PyTorch Metric Learning OverviewThis library contains 9 modules, each of which can be used independently within your existing codebase, or combined together for a complete train/test workflow.How loss functions workUsing losses and miners in your training loopLet’s initialize a plain :from pytorch_metric_learning import losses
loss_func = losses.TripletMarginLoss()
To compute the loss in your training loop, pass in the embeddings computed by your model, and the corresponding labels. The embeddings should have size (N, embedding_size), and the labels should have size (N), where N is the batch size.# your training loop
for i, (data, labels) in enumerate(dataloader):
	optimizer.zero_grad()
	embeddings = model(data)
	loss = loss_func(embeddings, labels)
	loss.backward()
	optimizer.step()
The TripletMarginLoss computes all possible triplets within the batch, based on the labels you pass into it. Anchor-positive pairs are formed by embeddings that share the same label, and anchor-negative pairs are formed by embeddings that have different labels. Sometimes it can help to add a mining function:from pytorch_metric_learning import miners, losses
miner = miners.MultiSimilarityMiner()
loss_func = losses.TripletMarginLoss()

# your training loop
for i, (data, labels) in enumerate(dataloader):
	optimizer.zero_grad()
	embeddings = model(data)
	hard_pairs = miner(embeddings, labels)
	loss = loss_func(embeddings, labels, hard_pairs)
	loss.backward()
	optimizer.step()
In the above code, the miner finds positive and negative pairs that it thinks are particularly difficult. Note that even though the TripletMarginLoss operates on triplets, it’s still possible to pass in pairs. This is because the library automatically converts pairs to triplets and triplets to pairs, when necessary.Customizing loss functionsLoss functions can be customized using , , and . In the diagram below, a miner finds the indices of hard pairs within a batch. These are used to index into the distance matrix, computed by the distance object. For this diagram, the loss function is pair-based, so it computes a loss per pair. In addition, a regularizer has been supplied, so a regularization loss is computed for each embedding in the batch. The per-pair and per-element losses are passed to the reducer, which (in this diagram) only keeps losses with a high value. The averages are computed for the high-valued pair and element losses, and are then added together to obtain the final loss.Now here's an example of a customized TripletMarginLoss:from pytorch_metric_learning.distances import CosineSimilarity
from pytorch_metric_learning.reducers import ThresholdReducer
from pytorch_metric_learning.regularizers import LpRegularizer
from pytorch_metric_learning import losses
loss_func = losses.TripletMarginLoss(distance = CosineSimilarity(), 
				     reducer = ThresholdReducer(high=0.3), 
			 	     embedding_regularizer = LpRegularizer())
This customized triplet loss has the following properties:Using loss functions for unsupervised / self-supervised learningA  wrapper is provided for self-supervised learning:from pytorch_metric_learning.losses import SelfSupervisedLoss
loss_func = SelfSupervisedLoss(TripletMarginLoss())

# your training for-loop
for i, data in enumerate(dataloader):
	optimizer.zero_grad()
	embeddings = your_model(data)
	augmented = your_model(your_augmentation(data))
	loss = loss_func(embeddings, augmented)
	loss.backward()
	optimizer.step()
If you're interested in -style self-supervision, take a look at the  notebook. It uses CrossBatchMemory to implement the momentum encoder queue, which means you can use any tuple loss, and any tuple miner to extract hard samples from the queue.Highlights of the rest of the libraryIf you're short of time and want a complete train/test workflow, check out the .To learn more about all of the above, . InstallationRequired PyTorch versionOther dependencies: Pippip install pytorch-metric-learning
To get the latest dev version:pip install pytorch-metric-learning --pre
To install on Windows:pip install torch===1.6.0 torchvision===0.7.0 -f https://download.pytorch.org/whl/torch_stable.html
pip install pytorch-metric-learning
To install with evaluation and logging capabilities(This will install the unofficial pypi version of faiss-gpu, plus record-keeper and tensorboard):pip install pytorch-metric-learning[with-hooks]
To install with evaluation and logging capabilities (CPU)(This will install the unofficial pypi version of faiss-cpu, plus record-keeper and tensorboard):pip install pytorch-metric-learning[with-hooks-cpu]
Condaconda install -c conda-forge pytorch-metric-learning
To use the testing module, you'll need faiss, which can be installed via conda as well. See the Benchmark resultsSee  to view benchmark results and to use the benchmarking tool.DevelopmentDevelopment is done on the  branch:git checkout dev
Unit tests can be run with the default  library:python -m unittest discover
You can specify the test datatypes and test device as environment variables. For example, to test using float32 and float64 on the CPU:TEST_DTYPES=float32,float64 TEST_DEVICE=cpu python -m unittest discover
To run a single test file instead of the entire test suite, specify the file name:python -m unittest tests/losses/test_angular_loss.py
Code is formatted using  and :pip install black isort
./format_code.sh
AcknowledgementsContributorsThanks to the contributors who made pull requests!| Contributor | Highlights || -- | -- ||| -   -   - | | - Made the  work on any combination of query and reference sets  - Made  work with arbitrary label comparisons || | -   -   - Added mean reciprocal rank accuracy to   - BaseLossWrapper||| -   -   -   -  ||  |  ||  |  ||  |  ||  |  ||  |  ||  |  ||  | optimized  ||  |  in  ||  |  ||  | Made  accept datasets ||  |  in  ||  | Added  to  ||  |  ||  | Helped add  and  to the distributed wrappers. ||  | Fixed an edge case in ArcFaceLoss. ||  | Improved documentation for NTXentLoss ||  | ||  | ||  | ||  | ||  | ||  | ||  | ||  | ||  | ||  | ||  | ||  | ||  | ||  | |Facebook AIThank you to  at , and my research advisor, . This project began during my internship at Facebook AI where I received valuable feedback from Ser-Nam, and his team of computer vision and machine learning engineers and research scientists. In particular, thanks to  and  for reviewing my code during its early stages of development.Open-source reposThis library contains code that has been adapted and modified from the following great open-source repos:LogoThanks to  for designing the logo.Citing this libraryIf you'd like to cite pytorch-metric-learning in your paper, you can use this bibtex:@article{Musgrave2020PyTorchML,
  title={PyTorch Metric Learning},
  author={Kevin Musgrave and Serge J. Belongie and Ser-Nam Lim},
  journal={ArXiv},
  year={2020},
  volume={abs/2008.09164}
}
"
https://github.com/naturomics/CapsNet-Tensorflow,A Tensorflow implementation of CapsNet(Capsules Net) in paper Dynamic Routing Between Capsules,"CapsNet-TensorflowA Tensorflow implementation of CapsNet based on Geoffrey Hinton's paper RequirementsUsageStep 1. Download this repository with  or click the  button.$ git clone https://github.com/naturomics/CapsNet-Tensorflow.git
$ cd CapsNet-Tensorflow
Step 2. Download  or  dataset. In this step, you have two choices:$ python download_data.py   (for mnist dataset)
$ python download_data.py --dataset fashion-mnist --save_to data/fashion-mnist (for fashion-mnist dataset)
$ mkdir -p data/mnist
$ wget -c -P data/mnist http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
$ wget -c -P data/mnist http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
$ wget -c -P data/mnist http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
$ wget -c -P data/mnist http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
$ gunzip data/mnist/*.gz
Step 3. Start the training(Using the MNIST dataset by default):$ python main.py
$ # or training for fashion-mnist dataset
$ python main.py --dataset fashion-mnist
$ # If you need to monitor the training process, open tensorboard with this command
$ tensorboard --logdir=logdir
$ # or use `tail` command on linux system
$ tail -f results/val_acc.csv
Step 4. Calculate test accuracy$ python main.py --is_training=False
$ # for fashion-mnist dataset
$ python main.py --dataset fashion-mnist --is_training=False
ResultsThe pictures here are plotted by tensorboard and my tool Here are the models I trained and my talk and something else:(password:ahjs)Routing iteration | 1 | 3 | 4 |:-----|:----:|:----:|:------|val error | 0.36 | 0.36 | 0.41 |Paper | 0.29 | 0.25 | - |My weChat:Reference"
https://github.com/jisungk/deepjazz,Deep learning driven jazz generation using Keras & Theano!,"Note: deepjazz is no longer being actively developed. It may be refactored at some point in the future. Goodbye and thank you for your interest 😢Using Keras & Theano for deep learning driven jazz generationI built  in 36 hours at a hackathon. It uses Keras & Theano, two deep learning libraries, to generate jazz music. Specifically, it builds a two-layer , learning from the given MIDI file. It uses deep learning, the AI tech that powers  and , to make music -- something that's considered as deeply human.Check out deepjazz's music on [<marko.inline.RawText object at 0x000001592FE0D208>]!DependenciesInstructionsRun on CPU with command:  python generator.py [# of epochs]
Run on GPU with command:  THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python generator.py [# of epochs]
Note: running Keras/Theano on GPU is formally supported for only NVIDIA cards (CUDA backend).Note:  must be modified to work with other MIDI files (the relevant ""melody"" MIDI part needs to be selected). The ability to handle this natively is a planned feature.AuthorPrinceton University, Department of Computer Sciencehello (at) jisungkim.com  CitationsThis project develops a lot of preprocessing code (with permission) from Evan Chow's . Thank you ! Public examples from the  were also referenced.Code License, Media CopyrightCode is licensed under the Apache License 2.0Images and other media are copyrighted (Ji-Sung Kim)"
https://github.com/ticarpi/jwt_tool,":snake: A toolkit for testing, tweaking and cracking JSON Web Tokens","The JSON Web Token Toolkit v2 Its functionality includes:AudienceThis tool is written for pentesters, who need to check the strength of the tokens in use, and their susceptibility to known attacks. A range of tampering, signing and verifying options are available to help delve deeper into the potential weaknesses present in some JWT libraries.It has also been successful for CTF challengers - as CTFs seem keen on JWTs at present.It may also be useful for developers who are using JWTs in projects, but would like to test for stability and for known vulnerabilities when using forged tokens.RequirementsThis tool is written natively in Python 3 (version 3.6+) using the common libraries, however various cryptographic funtions (and general prettiness/readability) do require the installation of a few common Python libraries.(An older Python 2.x version of this tool is available on the legacy branch for those who need it, although this is no longer be supported or updated)InstallationInstallation is just a case of downloading the  file (or  the repo).( the file too if you want to add it to your $PATH and call it from anywhere.)  On first run the tool will generate a config file, some utility files, logfile, and a set of Public and Private keys in various formats.  Custom ConfigsColour bug in WindowsTo fix broken colours in Windows cmd/Powershell: uncomment the below two lines in  (remove the ""# "" from the beginning of each line)You will also need to install colorama: # import colorama
# colorama.init()
UsageThe first argument should be the JWT itself (unless providing this in a header or cookie value). Providing no additional arguments will show you the decoded token values for review.  The toolkit will validate the token and list the header and payload values.  Additional argumentsThe many additional arguments will take you straight to the appropriate function and return you a token ready to use in your tests.For example, to tamper the existing token run the following:  Many options need additional values to set options.For example, to run a particular type of exploit you need to choose the eXploit (-X) option and select the vulnerability (here using ""a"" for the alg:none exploit):Extra parametersSome options such as Verifying tokens require additional parameters/files to be provided (here providing the Public Key in PEM format):  Sending tokens to a web applicationAll modes now allow for sending the token directly to an application.You need to specify:  Various responses from the request are displayed:  Common WorkflowHere is a quick run-through of a basic assessment of a JWT implementation. If no success with these options then dig deeper into other modes and options to hunt for new vulnerabilities (or zero-days!).  Recon:Read the token value to get a feel for the claims/values expected in the application:  Scanning:Run a [<marko.inline.RawText object at 0x000001592FE19108>] using the provided token directly against the application to hunt for common misconfigurations:  Exploitation:If any successful vulnerabilities are found change any relevant claims to try to exploit it (here using the Inject JWKS exploit and injecting a new username): Fuzzing:Dig deeper by testing for unexpected values and claims to identify unexpected app behaviours, or run attacks on programming logic or token processing:  Review:Review any successful exploitation by querying the logs to read more data about the request and :   HelpFor a list of options call the usage function:Some options such as Verifying tokens require additional parameters/files to be provided:A more detailed user guide can be found on the JWT Attack Playbook - new wiki content!Head over to the  for a detailed run-though of what JWTs are, what they do, and a full workflow of how to thoroughly test them for vulnerabilities, common weaknesses and unintended coding errors.TipsRegex for finding JWTs in Burp Search(make sure 'Case sensitive' and 'Regex' options are ticked) - url-safe JWT version - all JWT versions (higher possibility of false positives)Further Reading"
https://github.com/EnableSecurity/wafw00f,WAFW00F allows one to identify and fingerprint Web Application Firewall (WAF) products protecting a website.,"How does it work?To do its magic, WAFW00F does the following:For further details, check out the source code on our .What does it detect?WAFW00F can detect a number of firewalls, a list of which is as below:$ wafw00f -l

                   ______
                  /      \
                 (  Woof! )
                  \  ____/                      )
                  ,,                           ) (_
             .-. -    _______                 ( |__|
            ()``; |==|_______)                .)|__|
            / ('        /|\                  (  |__|
        (  /  )        / | \                  . |__|
         \(_)_))      /  |  \                   |__|

                    ~ WAFW00F : v2.2.0 ~
    The Web Application Firewall Fingerprinting Toolkit

[+] Can test for these WAFs:

  WAF Name                        Manufacturer
  --------                        ------------

  ACE XML Gateway                  Cisco
  aeSecure                         aeSecure
  AireeCDN                         Airee
  Airlock                          Phion/Ergon
  Alert Logic                      Alert Logic
  AliYunDun                        Alibaba Cloud Computing
  Anquanbao                        Anquanbao
  AnYu                             AnYu Technologies
  Approach                         Approach
  AppWall                          Radware
  Armor Defense                    Armor
  ArvanCloud                       ArvanCloud
  ASP.NET Generic                  Microsoft
  ASPA Firewall                    ASPA Engineering Co.
  Astra                            Czar Securities
  AWS Elastic Load Balancer        Amazon
  AzionCDN                         AzionCDN
  Azure Front Door                 Microsoft
  Barikode                         Ethic Ninja
  Barracuda                        Barracuda Networks
  Bekchy                           Faydata Technologies Inc.
  Beluga CDN                       Beluga
  BIG-IP Local Traffic Manager     F5 Networks
  BinarySec                        BinarySec
  BitNinja                         BitNinja
  BlockDoS                         BlockDoS
  Bluedon                          Bluedon IST
  BulletProof Security Pro         AITpro Security
  CacheWall                        Varnish
  CacheFly CDN                     CacheFly
  Comodo cWatch                    Comodo CyberSecurity
  CdnNS Application Gateway        CdnNs/WdidcNet
  ChinaCache Load Balancer         ChinaCache
  Chuang Yu Shield                 Yunaq
  Cloudbric                        Penta Security
  Cloudflare                       Cloudflare Inc.
  Cloudfloor                       Cloudfloor DNS
  Cloudfront                       Amazon
  CrawlProtect                     Jean-Denis Brun
  DataPower                        IBM
  DenyALL                          Rohde & Schwarz CyberSecurity
  Distil                           Distil Networks
  DOSarrest                        DOSarrest Internet Security
  DotDefender                      Applicure Technologies
  DynamicWeb Injection Check       DynamicWeb
  Edgecast                         Verizon Digital Media
  Eisoo Cloud Firewall             Eisoo
  Expression Engine                EllisLab
  BIG-IP AppSec Manager            F5 Networks
  BIG-IP AP Manager                F5 Networks
  Fastly                           Fastly CDN
  FirePass                         F5 Networks
  FortiWeb                         Fortinet
  GoDaddy Website Protection       GoDaddy
  Greywizard                       Grey Wizard
  Huawei Cloud Firewall            Huawei
  HyperGuard                       Art of Defense
  Imunify360                       CloudLinux
  Incapsula                        Imperva Inc.
  IndusGuard                       Indusface
  Instart DX                       Instart Logic
  ISA Server                       Microsoft
  Janusec Application Gateway      Janusec
  Jiasule                          Jiasule
  Kona SiteDefender                Akamai
  KS-WAF                           KnownSec
  KeyCDN                           KeyCDN
  LimeLight CDN                    LimeLight
  LiteSpeed                        LiteSpeed Technologies
  Open-Resty Lua Nginx             FLOSS
  Oracle Cloud                     Oracle
  Malcare                          Inactiv
  MaxCDN                           MaxCDN
  Mission Control Shield           Mission Control
  ModSecurity                      SpiderLabs
  NAXSI                            NBS Systems
  Nemesida                         PentestIt
  NevisProxy                       AdNovum
  NetContinuum                     Barracuda Networks
  NetScaler AppFirewall            Citrix Systems
  Newdefend                        NewDefend
  NexusGuard Firewall              NexusGuard
  NinjaFirewall                    NinTechNet
  NullDDoS Protection              NullDDoS
  NSFocus                          NSFocus Global Inc.
  OnMessage Shield                 BlackBaud
  Palo Alto Next Gen Firewall      Palo Alto Networks
  PerimeterX                       PerimeterX
  PentaWAF                         Global Network Services
  pkSecurity IDS                   pkSec
  PT Application Firewall          Positive Technologies
  PowerCDN                         PowerCDN
  Profense                         ArmorLogic
  Puhui                            Puhui
  Qcloud                           Tencent Cloud
  Qiniu                            Qiniu CDN
  Qrator                           Qrator
  Reblaze                          Reblaze
  RSFirewall                       RSJoomla!
  RequestValidationMode            Microsoft
  Sabre Firewall                   Sabre
  Safe3 Web Firewall               Safe3
  Safedog                          SafeDog
  Safeline                         Chaitin Tech.
  SecKing                          SecKing
  eEye SecureIIS                   BeyondTrust
  SecuPress WP Security            SecuPress
  SecureSphere                     Imperva Inc.
  Secure Entry                     United Security Providers
  SEnginx                          Neusoft
  ServerDefender VP                Port80 Software
  Shield Security                  One Dollar Plugin
  Shadow Daemon                    Zecure
  SiteGround                       SiteGround
  SiteGuard                        Sakura Inc.
  Sitelock                         TrueShield
  SonicWall                        Dell
  UTM Web Protection               Sophos
  Squarespace                      Squarespace
  SquidProxy IDS                   SquidProxy
  StackPath                        StackPath
  Sucuri CloudProxy                Sucuri Inc.
  Tencent Cloud Firewall           Tencent Technologies
  Teros                            Citrix Systems
  Trafficshield                    F5 Networks
  TransIP Web Firewall             TransIP
  URLMaster SecurityCheck          iFinity/DotNetNuke
  URLScan                          Microsoft
  UEWaf                            UCloud
  Varnish                          OWASP
  Viettel                          Cloudrity
  VirusDie                         VirusDie LLC
  Wallarm                          Wallarm Inc.
  WatchGuard                       WatchGuard Technologies
  WebARX                           WebARX Security Solutions
  WebKnight                        AQTRONIX
  WebLand                          WebLand
  RayWAF                           WebRay Solutions
  WebSEAL                          IBM
  WebTotem                         WebTotem
  West263 CDN                      West263CDN
  Wordfence                        Defiant
  WP Cerber Security               Cerber Tech
  WTS-WAF                          WTS
  360WangZhanBao                   360 Technologies
  XLabs Security WAF               XLabs
  Xuanwudun                        Xuanwudun
  Yundun                           Yundun
  Yunsuo                           Yunsuo
  Yunjiasu                         Baidu Cloud Computing
  YXLink                           YxLink Technologies
  Zenedge                          Zenedge
  ZScaler                          Accenture
How do I use it?First, install the tools as described .For help you can make use of the  option. The basic usage is to passan URL as an argument. Example:$   wafw00f https://example.org

                   ______
                  /      \
                 (  Woof! )
                  \  ____/                      )
                  ,,                           ) (_
             .-. -    _______                 ( |__|
            ()``; |==|_______)                .)|__|
            / ('        /|\                  (  |__|
        (  /  )        / | \                  . |__|
         \(_)_))      /  |  \                   |__|

                    ~ WAFW00F : v2.2.0 ~
    The Web Application Firewall Fingerprinting Toolkit

[*] Checking https://example.org
[+] The site https://example.org is behind Edgecast (Verizon Digital Media) WAF.
[~] Number of requests: 2
How do I install it?The following should do the trick:python setup.py install
It is also possible to run it within a docker container. Clone this repository first and build the Docker image using .Now you can run Final WordsQuestions? Pull up an  or contact .,  are highly welcome. If you wish to see how WAFW00F is being developed, check out the .Some useful links:Presently being developed and maintained by:"
https://github.com/cupy/cupy,NumPy & SciPy for GPU,"CuPy : NumPy & SciPy for GPU| | | | | | CuPy is a NumPy/SciPy-compatible array library for GPU-accelerated computing with Python.CuPy acts as a  to run existing NumPy/SciPy code on NVIDIA CUDA or AMD ROCm platforms.>>> import cupy as cp
>>> x = cp.arange(6).reshape(2, 3).astype('f')
>>> x
array([[ 0.,  1.,  2.],
       [ 3.,  4.,  5.]], dtype=float32)
>>> x.sum(axis=1)
array([  3.,  12.], dtype=float32)
CuPy also provides access to low-level CUDA features.You can pass  to existing CUDA C/C++ programs via , use  for performance, or even call  directly.InstallationPipBinary packages (wheels) are available for Linux and Windows on .Choose the right package for your platform.| Platform              | Architecture      | Command                                                       || --------------------- | ----------------- | ------------------------------------------------------------- || CUDA 10.2             | x86_64 / aarch64  |                                     || CUDA 11.0             | x86_64            |                                     || CUDA 11.1             | x86_64            |                                     || CUDA 11.2 ~ 11.8      | x86_64 / aarch64  |                                     || CUDA 12.x             | x86_64 / aarch64  |                                     || ROCm 4.3 ([<marko.inline.RawText object at 0x000001592FF18888>])          | x86_64            |                                    || ROCm 5.0 ([<marko.inline.RawText object at 0x000001592FF187C8>])          | x86_64            |                                    |CondaBinary packages are also available for Linux and Windows on .| Platform              | Architecture                | Command                                                       || --------------------- | --------------------------- | ------------------------------------------------------------- || CUDA                  | x86_64 / aarch64 / ppc64le  |                            |If you need to use a particular CUDA version (say 11.8), you can do .DockerUse  to run .$ docker run --gpus all -it cupy/cupy
ResourcesLicenseMIT License (see  file).CuPy is designed based on NumPy's API and SciPy's API (see  file).CuPy is being developed and maintained by  and .ReferenceRyosuke Okuta, Yuya Unno, Daisuke Nishino, Shohei Hido and Crissman Loomis.CuPy: A NumPy-Compatible Library for NVIDIA GPU Calculations.Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Thirty-first Annual Conference on Neural Information Processing Systems (NIPS), (2017).[]@inproceedings{cupy_learningsys2017,
  author       = ""Okuta, Ryosuke and Unno, Yuya and Nishino, Daisuke and Hido, Shohei and Loomis, Crissman"",
  title        = ""CuPy: A NumPy-Compatible Library for NVIDIA GPU Calculations"",
  booktitle    = ""Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Thirty-first Annual Conference on Neural Information Processing Systems (NIPS)"",
  year         = ""2017"",
  url          = ""http://learningsys.org/nips17/assets/papers/paper_16.pdf""
}
"
https://github.com/brightmart/text_classification,all kinds of text classification models and more with deep learning,"Text ClassificationThe purpose of this repository is to explore text classification methods in NLP with deep learning.Update:Customize an NLP API in three minutes, for free: NLP API DemoLanguage Understanding Evaluation benchmark for Chinese(CLUE benchmark): run 10 tasks & 9 baselines with one line of code, performance comparision with details.Releasing Pre-trained Model of ALBERT_Chinese Training with 30G+ Raw Chinese Corpus, xxlarge, xlarge and more, Target to match State of the Art performance in Chinese, 2019-Oct-7, During the National Day of China!Large Amount of Chinese Corpus for NLP Available!Google's BERT achieved new state of art result on more than 10 tasks in NLP using pre-train in language model then fine-tuning. Pre-train TexCNN: idea from BERT for language understanding with running code and data setIntroductionit has all kinds of baseline models for text classification.it also support for multi-label classification where multi labels associate with an sentence or document.although many of these models are simple, and may not get you to top level of the task. but some of these models are very classic, so they may be good to serve as baseline models. each model has a test function under model class. you can run it to performance toy task first. the model is independent from data set.check here for formal report of large scale multi-label text classification with deep learningseveral models here can also be used for modelling question answering (with or without context), or to do sequences generating. we explore two seq2seq model(seq2seq with attention,transformer-attention is all you need) to do text classification. and these two models can also be used for sequences generating and other tasks. if your task is a multi-label classification, you can cast the problem to sequences generating.we implement two memory network. one is dynamic memory network. previously it reached state of art in question answering, sentiment analysis and sequence generating tasks. it is so called one model to do several different tasks, and reach high performance. it has four modules. the key component is episodic memory module. it use gate mechanism to performance attention, and use gated-gru to update episode memory, then it has another gru( in a vertical direction) to performance hidden state update. it has ability to do transitive inference.the second memory network we implemented is recurrent entity network: tracking state of the world. it has blocks of key-value pairs as memory, run in parallel, which achieve new state of art. it can be used for modelling question answering with contexts(or history). for example, you can let the model to read some sentences(as context), and ask a question(as query), then ask the model to predict an answer; if you feed story same as query, then it can do classification task. To discuss ML/DL/NLP problems and get tech support from each other, you can join QQ group: 836811304Models:and other models:Performance(mulit-label label prediction task,ask to prediction top5, 3 million training data,full score:0.5)Model   | fastText|TextCNN|TextRNN| RCNN | HierAtteNet|Seq2seqAttn|EntityNet|DynamicMemory|Transformer---     | ---     | ---   | ---   |---   |---         |---        |---      |---          |----Score   | 0.362   |  0.405| 0.358 | 0.395| 0.398      |0.322      |0.400    |0.392        |0.322Training| 10m     |  2h   |10h    | 2h   | 2h         |3h         |3h       |5h           |7hBert model achieves 0.368 after first 9 epoch from validation set.Ensemble of TextCNN,EntityNet,DynamicMemory: 0.411Ensemble EntityNet,DynamicMemory: 0.403Notice:  stand for minutes;  stand for hours; means Hierarchical Attention Networkk; means Seq2seq with attention; means DynamicMemoryNetwork; stand for model from 'Attention Is All You Need'.Usage:Each model has a test method under the model class. you can run the test method first to check whether the model can work properly.Environment:python 2.7+ tensorflow 1.8 (tensorflow 1.1 to 1.13 should also works; most of models should also work fine in other tensorflow version, since we use very few features bond to certain version.if you use python3, it will be fine as long as you change print/try catch function in case you meet any error.TextCNN model is already transfomed to python 3.6Sample data: cached file of baidu or Google Drive:send me an emailto help you run this repository, currently we re-generate training/validation/test data and vocabulary/labels, and saved them as cache file using h5py. we suggest you to download it from above link.it contain everything you need to run this repository: data is pre-processed, you can start to train the model in a minute.it's a zip file about 1.8G, contains 3 million training data. although after unzip it's quite big, but with the help of hdf5, it only need a normal size of memory of computer(e.g.8 G or less) during training.we use jupyter notebook: pre-processing.ipynb to pre-process data. you can have a better understanding of this task and data by taking a look of it. you can also generate data by yourself in the way your want, just change few lines of code using this jupyter notebook.If you want to try a model now, you can dowload cached file from above, then go to folder 'a02_TextCNN', run  python  p7_TextCNN_train.py 
it will use data from cached files to train the model, and print loss and F1 score periodically.old sample data source:if you need some sample data and word embedding per-trained on word2vec, you can find it in closed issues, such as: issue 3. you can also find some sample data at folder ""data"". it contains two files:'sample_single_label.txt', contains 50k data with single label; 'sample_multiple_label.txt', contains 20k data with multiple labels. input and label of is separate by ""   label"".if you want to know more detail about data set of text classification or task these models can be used, one of choose is below:https://biendata.com/competition/zhihu/Road MapOne way you can use this repository:step 1: you can read through this article. you will get a general idea of various classic models used to do text classification.step 2: pre-process data and/or download cached file.  a. take a look a look of jupyter notebook('pre-processing.ipynb'), where you can familiar with this text 

       classification task and data set. you will also know how we pre-process data and generate training/validation/test 
       
       set. there are a list of things you can try at the end of this jupyter.

   b. download zip file that contains cached files, so you will have all necessary data, and can start to train models.
step 3: run some of models list here, and change some codes and configurations as you want, to get a good performance.  record performances, and things you done that works, and things that are not.

  for example, you can take this sequence to explore: 
  
  1) fasttext---> 2)TextCNN---> 3)Transformer---> 4)BERT
additionally, write your article about this topic, you can follow paper's style to write. you may need to read some papers   on the way, many of these papers list in the # Reference at the end of this article; or join  a machine learning 
   
   competition, and apply it with what you've learned. 
   
Use Your Own Data:replace data in 'data/sample_multiple_label.txt', and make sure format as below:'word1 word2 word3 __label__l1 __label__l2 __label__l3'where part1: 'word1 word2 word3' is input(X), part2: '__label__l1 __label__l2 __label__l3' representing there are three labels: [l1,l2,l3]. between part1 and part2 there should be a empty string: ' '.for example: each line (multiple labels) like: 'w5466 w138990 w1638 w4301 w6 w470 w202 c1834 c1400 c134 c57 c73 c699 c317 c184 __label__5626661657638885119 __label__4921793805334628695 __label__8904735555009151318'where '5626661657638885119','4921793805334628695'，‘8904735555009151318’ are three labels associate with this input string 'w5466 w138990...c699 c317 c184'Notice:Some util function is in data_util.py;  check load_data_multilabel() of data_util for how process input and labels from raw data.there is a function to load and assign pretrained word embedding to the model,where word embedding is pretrained in word2vec or fastText. Pretrain Work Embedding:if word2vec.load not works, you may load pretrained word embedding, especially for chinese word embedding use following lines:import gensimfrom gensim.models import KeyedVectorsword2vec_model = KeyedVectors.load_word2vec_format(word2vec_model_path, binary=True, unicode_errors='ignore')  #or you can turn off use pretrain word embedding flag to false to disable loading word embedding.Models Detail:1.fastText:implmentation of Bag of Tricks for Efficient Text Classificationafter embed each word in the sentence, this word representations are then averaged into a text representation, which is in turn fed to a linear classifier.it use softmax function to compute the probability distribution over the predefined classes. then cross entropy is used to compute loss. bag of word representation does not consider word order. in order to take account of word order, n-gram features is used to capture some partial information about the local word order; when the number of classes is large, computing the linear classifier is computational expensive. so it usehierarchical softmax to speed training process.result: performance is as good as paper, speed also very fast.check: p5_fastTextB_model.py2.TextCNN:Implementation of  Convolutional Neural Networks for Sentence Classification Structure:embedding--->conv--->max pooling--->fully connected layer-------->softmaxCheck: p7_TextCNN_model.pyIn order to get very good result with TextCNN, you also need to read carefully about this paper A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification: it give you some insights of things that can affect performance. although you need to  change some settings according to your specific task.Convolutional Neural Network is main building box for solve problems of computer vision. Now we will show how CNN can be used for NLP, in in particular, text classification. Sentence length will be different from one to another. So we will use pad to get fixed length, n. For each token in the sentence, we will use word embedding to get a fixed dimension vector, d. So our input is a 2-dimension matrix:(n,d). This is similar with image for CNN. Firstly, we will do convolutional operation to our input. It is a element-wise multiply between filter and part of input. We use k number of filters, each filter size is a 2-dimension matrix (f,d). Now the output will be k number of lists. Each list has a length of n-f+1. each element is a scalar. Notice that the second dimension will be always the dimension of word embedding. We are using different size of filters to get rich features from text inputs. And this is something similar with n-gram features. Secondly, we will do max pooling for the output of convolutional operation. For k number of lists, we will get k number of scalars. Thirdly, we will concatenate scalars to form final features. It is a fixed-size vector. And it is independent from the size of filters we use.Finally, we will use linear layer to project these features to per-defined labels.3.BERT:Pre-training of Deep Bidirectional Transformers for Language UnderstandingBERT currently achieve state of art results on more than 10 NLP tasks. the key ideas behind this model is that we can pre-train the model by using one kind of language model with huge amount of raw data, where you can find it easily.as most of parameters of the model is pre-trained, only last layer for classifier need to be need for different tasks.as a result, this model is generic and very powerful. you can just fine-tuning based on the pre-trained model withina short period of time.however, this model is quite big. with sequence length 128, you may only able to train with a batch size of 32; for longdocument such as sequence length 512, it can only train a batch size 4 for a normal GPU(with 11G); and very few peoplecan pre-train this model from scratch, as it takes many days or weeks to train, and a normal GPU's memory is too small for this model.Specially, the backbone model is Transformer, where you can find it in Attention Is All You Need. it use two kind of tasks to pre-train the model.Masked Languge Modelgenerally speaking, given a sentence, some percentage of words are masked, you will need to predict the masked wordsbased on this masked sentence. masked words are chosed randomly.we feed the input through a deep Transformer encoder and then use the final hidden states corresponding to the masked positions to predict what word was masked, exactly like we would train a language model.source_file each line is a sequence of token, can be a sentence.

Input Sequence  : The man went to [MASK] store with [MASK] dog
Target Sequence :                  the                his
     
Next Sentence Predictionmany language understanding task, like question answering, inference, need understand relationshipbetween sentence. however, language model is only able to understand without a sentence. next sentenceprediction is a sample task to help model understand better in these kinds of task.50% of chance the second sentence is tbe next sentence of the first one, 50% of not the next one.given two sentence, the model is asked to predict whether the second sentence is real next sentence of the first one.Input : [CLS] the man went to the store [SEP] he bought a gallon of milk [SEP]
Label : IsNext

Input = [CLS] the man heading to the store [SEP] penguin [MASK] are flight ##less birds [SEP]
Label = NotNext
How to use BERT?basically, you can download pre-trained model, can just fine-tuning on your task with your own data.for classification task, you can add processor to define the format you want to let input and labels from source data.Use BERT for multi-label classification?run the following command under folder a00_Bert:  python  train_bert_multi-label.py
It achieve 0.368 after 9 epoch.or you can run multi-label classification with downloadable data using BERT from sentiment_analysis_fine_grain with BERTUse BERT for online predictionyou can use session and feed style to restore model and feed data, then get logits to make a online prediction.online prediction with BERToriginally, it train or evaluate model based on file, not for online.How to get better model for BERT?firstly, you can use pre-trained model download from google. run a few epoch on you dataset, and find a suitable sequence length.secondly, you can pre-train the base model in your own data as long as  you can find a dataset that is related to your task, then fine-tuning on your specific task.thirdly, you can change loss function and last layer to better suit for your task.additionally, you can add define some pre-trained tasks that will help the model understand your task much better.as experienced we got from experiments, pre-trained task is independent from model and pre-train is not limit to the tasks above.4.TextRNNStructure v1:embedding--->bi-directional lstm--->concat output--->average----->softmax layercheck: p8_TextRNN_model.pyStructure v2:embedding-->bi-directional lstm---->dropout-->concat ouput--->lstm--->droput-->FC layer-->softmax layercheck: p8_TextRNN_model_multilayer.py5.BiLstmTextRelationStructure same as TextRNN. but input is special designed. e.g.input:""how much is the computer? EOS price of laptop"". where 'EOS' is a specialtoken spilted question1 and question2.check:p9_BiLstmTextRelation_model.py6.twoCNNTextRelationStructure: first use two different convolutional to extract feature of two sentences. then concat two features. use lineartransform layer to out projection to target label, then softmax.check: p9_twoCNNTextRelation_model.py7.BiLstmTextRelationTwoRNNStructure: one bi-directional lstm for one sentence(get output1), another bi-directional lstm for another sentence(get output2). then:softmax(output1Moutput2)check:p9_BiLstmTextRelationTwoRNN_model.pyfor more detail you can go to: Deep Learning for Chatbots, Part 2 – Implementing a Retrieval-Based Model in Tensorflow8.RCNN:Recurrent convolutional neural network for text classificationimplementation of  Recurrent Convolutional Neural Network for Text Classification structure:1)recurrent structure (convolutional layer) 2)max pooling 3) fully connected layer+softmaxit learn represenation of each word in the sentence or document with left side context and right side context:representation current word=[left_side_context_vector,current_word_embedding,right_side_context_vecotor].for left side context, it use a recurrent structure, a no-linearity transfrom of previous word and left side previous context; similarly to right side context.check: p71_TextRCNN_model.py9.Hierarchical Attention Network:Implementation of Hierarchical Attention Networks for Document ClassificationStructure:In NLP, text classification can be done for single sentence, but it can also be used for multiple sentences. we may call it document classification. Words are form to sentence. And sentence are form to document. In this circumstance, there may exists a intrinsic structure. So how can we model this kinds of task? Does all parts of document are equally relevant? And how we determine which part are more important than another?It has two unique features: 1)it has a hierarchical structure that reflect the hierarchical structure of documents; 2)it has two levels of attention mechanisms used at the word and sentence-level. it enable the model to capture important information in different levels.Word Encoder:For each words in a sentence, it is embedded into word vector in distribution vector space. It use a bidirectional GRU to encode the sentence. By concatenate vector from two direction, it now can form a representation of the sentence, which also capture contextual information.Word Attention:Same words are more important than another for the sentence. So attention mechanism is used. It first use one layer MLP to get uit hidden representation of the sentence, then measure the importance of the word as the similarity of uit with a word level context vector uw and get a normalized importance through a softmax function. Sentence Encoder:for sentence vectors, bidirectional GRU is used to encode it. Similarly to word encoder.Sentence Attention:sentence level vector is used to measure importance among sentences. Similarly to word attention.Input of data: Generally speaking, input of this model should have serveral sentences instead of sinle sentence. shape is:[None,sentence_lenght]. where None means the batch_size.In my training data, for each example, i have four parts. each part has same length. i concat four parts to form one single sentence. the model will split the sentence into four parts, to form a tensor with shape:[None,num_sentence,sentence_length]. where num_sentence is number of sentences(equal to 4, in my setting).check:p1_HierarchicalAttention_model.pyfor attentive attention you can check attentive attention10.Seq2seq with attentionImplementation seq2seq with attention derived from NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATEI.Structure:1)embedding 2)bi-GRU too get rich representation from source sentences(forward & backward). 3)decoder with attention.II.Input of data:there are two kinds of three kinds of inputs:1)encoder inputs, which is a sentence; 2)decoder inputs, it is labels list with fixed length;3)target labels, it is also a list of labels.for example, labels is:""L1 L2 L3 L4"", then decoder inputs will be:[_GO,L1,L2,L2,L3,_PAD]; target label will be:[L1,L2,L3,L3,_END,_PAD]. length is fixed to 6, any exceed labels will be trancated, will pad if label is not enough to fill.III.Attention Mechanism:IV.How Vanilla Encoder Decoder Works:the source sentence will be encoded using RNN as fixed size vector (""thought vector""). then during decoder:V.Notices:11.Transformer(""Attention Is All You Need"")Status: it was able to do task classification. and able to generate reverse order of its sequences in toy task. you can check it by running test function in the model. check: a2_train_classification.py(train) or a2_transformer_classification.py(model)we do it in parallell style.layer normalization,residual connection, and mask are also used in the model. For every building blocks, we include a test function in the each file below, and we've test each small piece successfully.Sequence to sequence with attention is a typical model to solve sequence generation problem, such as translate, dialogue system. most of time, it use RNN as buidling block to do these tasks. util recently, people also apply convolutional Neural Network for sequence to sequence problem. Transformer, however, it perform these tasks solely on attention mechansim. it is fast and achieve new state-of-art result.It also has two main parts: encoder and decoder. below is desc from paper:Encoder:6 layers.each layers has two sub-layers.the first is multi-head self-attention mechanism;the second is position-wise fully connected feed-forward network.for each sublayer. use LayerNorm(x+Sublayer(x)). all dimension=512.Decoder:Main Take away from this model:Use this model to do task classification:Here we only use encode part for task classification, removed resdiual connection, used only 1 layer.no need to use mask. we use multi-head attention and postionwise feed forward to extract features of input sentence, then use linear layer to project it to get logits.for detail of the model, please check: a2_transformer_classification.py12.Recurrent Entity NetworkInput:1. story: it is multi-sentences, as context. 2.query: a sentence, which is a question, 3. ansewr: a single label.Model Structure:a. compute gate by using 'similarity' of keys,values with input of story. b. get candidate hidden state by transform each key,value and input.c. combine gate and candidate hidden state to update current hidden state.b. get weighted sum of hidden state using possibility distribution.c. non-linearity transform of query and hidden state to get predict label.Main take away from this model:for detail of the model, please check: a3_entity_network.pyunder this model, it has a test function, which ask this model to count numbers both for story(context) and query(question). but weights of story is smaller than query.13.Dynamic Memory NetworkOutlook of Model:1.Input Module: encode raw texts into vector representation2.Question Module: encode question into vector representation3.Episodic Memory Module: with inputs,it chooses which parts of inputs to focus on through the attention mechanism, taking into account of question and previous memory====>it poduce a 'memory' vecotr.4.Answer Module:generate an answer from the final memory vector.Detail:1.Input Module:a.single sentence: use gru to get hidden stateb.list of sentences: use gru to get the hidden states for each sentence. e.g. [hidden states 1,hidden states 2, hidden states...,hidden state n]2.Question Module:use gru to get hidden state3.Episodic Memory Module:use an attention mechanism and recurrent network to updates its memory. a. gate as attention mechanism: two-layer feed forward nueral network.input is candidate fact c,previous memory m and question q. features get by take: element-wise,matmul and absolute distance of q with c, and q with m.
 
b.memory update mechanism: take candidate sentence, gate and previous hidden state, it use gated-gru to update hidden state. like: h=f(c,h_previous,g). the final hidden state is the input for answer module.c.need for multiple episodes===>transitive inference. e.g. ask where is the football? it will attend to sentence of ""john put down the football""), then in second pass, it need to attend location of john.4.Answer Module:take the final epsoidic memory, question, it update hidden state of answer module.TODO1.Character-level Convolutional Networks for Text Classification2.Convolutional Neural Networks for Text Categorization:Shallow Word-level vs. Deep Character-level3.Very Deep Convolutional Networks for Text Classification4.Adversarial Training Methods For Semi-supervised Text Classification5.Ensemble ModelsConclusion:During the process of doing large scale of multi-label classification, serveral lessons has been learned, and some list as below:Reference:1.Bag of Tricks for Efficient Text Classification2.Convolutional Neural Networks for Sentence Classification3.A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification4.Deep Learning for Chatbots, Part 2 – Implementing a Retrieval-Based Model in Tensorflow, from www.wildml.com5.Recurrent Convolutional Neural Network for Text Classification6.Hierarchical Attention Networks for Document Classification7.Neural Machine Translation by Jointly Learning to Align and Translate8.Attention Is All You Need9.Ask Me Anything:Dynamic Memory Networks for Natural Language Processing10.Tracking the state of world with recurrent entity networks11.Ensemble Selection from Libraries of Models12.BERT:Pre-training of Deep Bidirectional Transformers for Language Understanding13.google-research/bertto be continued. for any problem, concat brightmart@hotmail.com"
https://github.com/testerSunshine/12306,12306智能刷票，订票,12306 购票小助手python版本已有功能依赖库项目使用说明目录对应说明思路图项目声明：日志列子使用帮助(一些安装问题和使用反馈较多的问题)：感谢一下小伙伴对本项目提供的帮助更新日志
https://github.com/iterative/dvc,🦉 Data Version Control | Git for Data & Models | ML Experiments Management,"|Banner|_• _• _• _• _• _• _• _• _• _|CI| |Python Version| |Coverage| |VS Code| |DOI||PyPI| |PyPI Downloads| |Packages| |Brew| |Conda| |Choco| |Snap||Data Version Control or DVC is a command line tool and _ to help you develop reproducible machine learning projects:#. Version your data and models.Store them in your cloud storage but keep their version info in your Git repo.#. Iterate fast with lightweight pipelines.When you make changes, only run the steps impacted by those changes.#. Track experiments in your local Git repo (no servers needed).#. Compare any data, code, parameters, model, or performance plots.#. Share experiments and automatically reproduce anyone's experiment.Quick startPlease read our `Command Reference <https://dvc.org/doc/command-reference>`_ for a complete list.
A common CLI workflow includes:+-----------------------------------+----------------------------------------------------------------------------------------------------+| Task                              | Terminal                                                                                           |+===================================+====================================================================================================+| Track data                        | |                                                                ||                                   | |                                                                             |+-----------------------------------+----------------------------------------------------------------------------------------------------+| Connect code and data             | |                      ||                                   | |  |+-----------------------------------+----------------------------------------------------------------------------------------------------+| Make changes and experiment       | |                                                                 ||                                   | |                                                                                 ||                                   | |                                                              |+-----------------------------------+----------------------------------------------------------------------------------------------------+| Compare and select experiments    | |                                                                                ||                                   | |                                                                  |+-----------------------------------+----------------------------------------------------------------------------------------------------+| Share code                        | |                                                                                   ||                                   | |                                                          ||                                   | |                                                                                    |+-----------------------------------+----------------------------------------------------------------------------------------------------+| Share data and ML models          | |                                          ||                                   | |                                                                                    |+-----------------------------------+----------------------------------------------------------------------------------------------------+How DVC worksWe encourage you to read our `Get Started
<https://dvc.org/doc/get-started>`_ docs to better understand what DVC
does and how it can fit your scenarios.
The closest analogies to describe the main DVC features are these:#. Git for data: Store and share data artifacts (like Git-LFS but without a server) and models, connecting them with a Git repository. Data management meets GitOps!#. Makefiles for ML: Describes how data or model artifacts are built from other data and code in a standard format. Now you can version your data pipelines with Git.#. Local experiment tracking: Turn your machine into an ML experiment management platform, and collaborate with others using existing Git hosting (Github, Gitlab, etc.).Git is employed as usual to store and version code (including DVC meta-files as placeholders for data).DVC _ seamlessly in a cache outside of Git, while preserving almost the same user experience as if they were in the repo.To share and back up the data cache, DVC supports multiple remote storage platforms - any cloud (S3, Azure, Google Cloud, etc.) or on-premise network storage (via SSH, for example).|Flowchart|_ (computational graphs) connect code and data together.They specify all steps required to produce a model: input dependencies including code, data, commands to run; and output information to be saved.Last but not least, _ lets you prepare and run a large number of experiments.Their results can be filtered and compared based on hyperparameters and metrics, and visualized with multiple plots... _:Visual Studio Code Extension|VS Code|To use DVC as a GUI right from your VS Code IDE, install the _ from the Marketplace.It currently features experiment tracking and data management, and more features (data pipeline support, etc.) are coming soon!|VS Code Extension Overview|Note: You'll have to install core DVC on your system separately (as detailed
below). The Extension will guide you if needed.
InstallationThere are several ways to install DVC: in VS Code; using , , , , ; or with an OS-specific package.Full instructions are _.Snapcraft (Linux)|Snap|.. code-block:: bashsnap install dvc --classicThis corresponds to the latest tagged release.Add  for the latest tagged release candidate, or  for the latest  version.Chocolatey (Windows)|Choco|.. code-block:: bashchoco install dvcBrew (mac OS)|Brew|.. code-block:: bashbrew install dvcAnaconda (Any platform)|Conda|.. code-block:: bashconda install -c conda-forge mamba # installs much faster than condamamba install -c conda-forge dvcDepending on the remote storage type you plan to use to keep and share your data, you might need to install optional dependencies: , , , , , .PyPI (Python)|PyPI|.. code-block:: bashpip install dvcDepending on the remote storage type you plan to use to keep and share your data, you might need to specify one of the optional dependencies: , , , , . Or  to include them all.The command should look like this:  (in this case AWS S3 dependencies such as  will be installed automatically).To install the development version, run:.. code-block:: bashpip install git+git://github.com/iterative/dvcPackage (Platform-specific)|Packages|Self-contained packages for Linux, Windows, and Mac are available.The latest version of the packages can be found on the GitHub _.Ubuntu / Debian (deb)^^^^^^^^^^^^^^^^^^^^^.. code-block:: bashsudo wget https://dvc.org/deb/dvc.list -O /etc/apt/sources.list.d/dvc.listwget -qO - https://dvc.org/deb/iterative.asc | sudo apt-key add -sudo apt updatesudo apt install dvcFedora / CentOS (rpm)^^^^^^^^^^^^^^^^^^^^^.. code-block:: bashsudo wget https://dvc.org/rpm/dvc.repo -O /etc/yum.repos.d/dvc.reposudo rpm --import https://dvc.org/rpm/iterative.ascsudo yum updatesudo yum install dvcContributing|Maintainability|Contributions are welcome!Please see our _ for more details.Thanks to all our contributors!|Contribs|Community and SupportCopyrightThis project is distributed under the Apache license version 2.0 (see the LICENSE file in the project root).By submitting a pull request to this project, you agree to license your contribution under the Apache license version 2.0 to this project.Citation|DOI|Iterative, DVC: Data Version Control - Git for Data & Models (2020)_.Barrak, A., Eghan, E.E. and Adams, B. _ , in Proceedings of the 28th IEEE International Conference on Software Analysis, Evolution, and Reengineering, SANER 2021. Hawaii, USA... |Banner| image:: https://dvc.org/img/logo-github-readme.png:target: https://dvc.org:alt: DVC logo.. |VS Code Extension Overview| image:: https://raw.githubusercontent.com/iterative/vscode-dvc/main/extension/docs/overview.gif:alt: DVC Extension for VS Code.. |CI| image:: https://github.com/iterative/dvc/workflows/Tests/badge.svg?branch=main:target: https://github.com/iterative/dvc/actions:alt: GHA Tests.. |Maintainability| image:: https://codeclimate.com/github/iterative/dvc/badges/gpa.svg:target: https://codeclimate.com/github/iterative/dvc:alt: Code Climate.. |Python Version| image:: https://img.shields.io/pypi/pyversions/dvc:target: https://pypi.org/project/dvc:alt: Python Version.. |Coverage| image:: https://codecov.io/gh/iterative/dvc/branch/main/graph/badge.svg:target: https://codecov.io/gh/iterative/dvc:alt: Codecov.. |Snap| image:: https://img.shields.io/badge/snap-install-82BEA0.svg?logo=snapcraft:target: https://snapcraft.io/dvc:alt: Snapcraft.. |Choco| image:: https://img.shields.io/chocolatey/v/dvc?label=choco:target: https://chocolatey.org/packages/dvc:alt: Chocolatey.. |Brew| image:: https://img.shields.io/homebrew/v/dvc?label=brew:target: https://formulae.brew.sh/formula/dvc:alt: Homebrew.. |Conda| image:: https://img.shields.io/conda/v/conda-forge/dvc.svg?label=conda&logo=conda-forge:target: https://anaconda.org/conda-forge/dvc:alt: Conda-forge.. |PyPI| image:: https://img.shields.io/pypi/v/dvc.svg?label=pip&logo=PyPI&logoColor=white:target: https://pypi.org/project/dvc:alt: PyPI.. |PyPI Downloads| image:: https://img.shields.io/pypi/dm/dvc.svg?color=blue&label=Downloads&logo=pypi&logoColor=gold:target: https://pypi.org/project/dvc:alt: PyPI Downloads.. |Packages| image:: https://img.shields.io/badge/deb|pkg|rpm|exe-blue:target: https://dvc.org/doc/install:alt: deb|pkg|rpm|exe.. |DOI| image:: https://img.shields.io/badge/DOI-10.5281/zenodo.3677553-blue.svg:target: https://doi.org/10.5281/zenodo.3677553:alt: DOI.. |Flowchart| image:: https://dvc.org/img/flow.gif:target: https://dvc.org/img/flow.gif:alt: how_dvc_works.. |Contribs| image:: https://contrib.rocks/image?repo=iterative/dvc:target: https://github.com/iterative/dvc/graphs/contributors:alt: Contributors.. |VS Code| image:: https://img.shields.io/visual-studio-marketplace/v/Iterative.dvc?color=blue&label=VSCode&logo=visualstudiocode&logoColor=blue:target: https://marketplace.visualstudio.com/items?itemName=Iterative.dvc:alt: VS Code Extension"
https://github.com/jazzband/pip-tools,A set of tools to keep your pinned Python dependencies fresh.,"pip-tools = pip-compile + pip-syncA set of command line tools to help you keep your -based packages fresh,even when you've pinned them. You do pin them, right? (In building your Python application and its dependencies for production, you want to make sure that your builds are predictable and deterministic.)InstallationSimilar to ,  must be installed in each of your project's:$ source /path/to/venv/bin/activate
(venv) $ python -m pip install pip-tools
Note: all of the remaining example commands assume you've activated yourproject's virtual environment.Example usage for The  command lets you compile a  file fromyour dependencies, specified in either , ,, or .Run it with  or  (or if  was installed with theappropriate Python version). If you use multiple Python versions, you can alsorun  on Windows and on other systems. should be run from the same virtual environment as yourproject so conditional dependencies that require a specific Python version,or other environment markers, resolve relative to your project'senvironment.Note: If  finds an existing  file thatfulfils the dependencies then no changes will be made, even if updates areavailable. To compile from scratch, first delete the existing file, or seefor alternative approaches.Requirements from The  file is the for configuringpackages and applications, and is recommended for new projects. supports both installing your  as well as your. Thanks to the fact that this is anofficial standard, you can use  to pin the dependenciesin projects that use modern standards-adhering packaging tools like, or .Suppose you have a 'foobar' Python application that is packaged using ,and you want to pin it for production. You can declare the project metadata as:[build-system]
requires = [""setuptools"", ""setuptools-scm""]
build-backend = ""setuptools.build_meta""

[project]
requires-python = "">=3.9""
name = ""foobar""
dynamic = [""dependencies"", ""optional-dependencies""]

[tool.setuptools.dynamic]
dependencies = { file = [""requirements.in""] }
optional-dependencies.test = { file = [""requirements-test.txt""] }

If you have a Django application that is packaged using , and youwant to pin it for production. You also want to pin your development toolsin a separate pin file. You declare  as a dependency and create anoptional dependency  that includes :[build-system]
requires = [""hatchling""]
build-backend = ""hatchling.build""

[project]
name = ""my-cool-django-app""
version = ""42""
dependencies = [""django""]

[project.optional-dependencies]
dev = [""pytest""]
You can produce your pin files as easily as:$ pip-compile -o requirements.txt pyproject.toml
#
# This file is autogenerated by pip-compile with Python 3.10
# by the following command:
#
#    pip-compile --output-file=requirements.txt pyproject.toml
#
asgiref==3.6.0
    # via django
django==4.1.7
    # via my-cool-django-app (pyproject.toml)
sqlparse==0.4.3
    # via django

$ pip-compile --extra dev -o dev-requirements.txt pyproject.toml
#
# This file is autogenerated by pip-compile with Python 3.10
# by the following command:
#
#    pip-compile --extra=dev --output-file=dev-requirements.txt pyproject.toml
#
asgiref==3.6.0
    # via django
attrs==22.2.0
    # via pytest
django==4.1.7
    # via my-cool-django-app (pyproject.toml)
exceptiongroup==1.1.1
    # via pytest
iniconfig==2.0.0
    # via pytest
packaging==23.0
    # via pytest
pluggy==1.0.0
    # via pytest
pytest==7.2.2
    # via my-cool-django-app (pyproject.toml)
sqlparse==0.4.3
    # via django
tomli==2.0.1
    # via pytest
This is great for both pinning your applications, but also to keep the CIof your open-source Python package stable.Requirements from  and  has also full support for - and-based projects that use .Just define your dependencies and extras as usual and run as above.Requirements from You can also use plain text files for your requirements (e.g. if you don'twant your application to be a package). To use a  file todeclare the Django dependency:# requirements.in
django
Now, run :$ pip-compile requirements.in
#
# This file is autogenerated by pip-compile with Python 3.10
# by the following command:
#
#    pip-compile requirements.in
#
asgiref==3.6.0
    # via django
django==4.1.7
    # via -r requirements.in
sqlparse==0.4.3
    # via django
And it will produce your , with all the Django dependencies(and all underlying dependencies) pinned.(updating-requirements)=Updating requirements generates a  file using the latest versionsthat fulfil the dependencies you specify in the supported files.If  finds an existing  file that fulfils thedependencies then no changes will be made, even if updates are available.To force  to update all packages in an existing, run .To update a specific package to the latest or a specific version use the or  flag:# only update the django package
$ pip-compile --upgrade-package django

# update both the django and requests packages
$ pip-compile --upgrade-package django --upgrade-package requests

# update the django package to the latest, and requests to v2.0.0
$ pip-compile --upgrade-package django --upgrade-package requests==2.0.0
You can combine  and  in one command, toprovide constraints on the allowed upgrades. For example to upgrade allpackages whilst constraining requests to the latest version less than 3.0:$ pip-compile --upgrade --upgrade-package 'requests<3.0'
Using hashesIf you would like to use Hash-Checking Mode available in  sinceversion 8.0,  offers  flag:$ pip-compile --generate-hashes requirements.in
#
# This file is autogenerated by pip-compile with Python 3.10
# by the following command:
#
#    pip-compile --generate-hashes requirements.in
#
asgiref==3.6.0 \
    --hash=sha256:71e68008da809b957b7ee4b43dbccff33d1b23519fb8344e33f049897077afac \
    --hash=sha256:9567dfe7bd8d3c8c892227827c41cce860b368104c3431da67a0c5a65a949506
    # via django
django==4.1.7 \
    --hash=sha256:44f714b81c5f190d9d2ddad01a532fe502fa01c4cb8faf1d081f4264ed15dcd8 \
    --hash=sha256:f2f431e75adc40039ace496ad3b9f17227022e8b11566f4b363da44c7e44761e
    # via -r requirements.in
sqlparse==0.4.3 \
    --hash=sha256:0323c0ec29cd52bceabc1b4d9d579e311f3e4961b98d174201d5622a23b85e34 \
    --hash=sha256:69ca804846bb114d2ec380e4360a8a340db83f0ccf3afceeb1404df028f57268
    # via django
Output FileTo output the pinned requirements in a filename other than, use . This might be useful for compilingmultiple files, for example with different constraints on django to test alibrary with both versions using :$ pip-compile --upgrade-package 'django<1.0' --output-file requirements-django0x.txt
$ pip-compile --upgrade-package 'django<2.0' --output-file requirements-django1x.txt
Or to output to standard output, use :$ pip-compile --output-file=- > requirements.txt
$ pip-compile - --output-file=- < requirements.in > requirements.txt
Forwarding options to Any valid  flags or arguments may be passed on with 's option, e.g.$ pip-compile requirements.in --pip-args ""--retries 10 --timeout 30""
ConfigurationYou can define project-level defaults for  and  bywriting them to a configuration file in the same directory as your requirementsinput files (or the current working directory if piping input from stdin).By default, both  and  will look firstfor a  file and then in your . You canalso specify an alternate TOML configuration file with the  option.For example, to by default generate  hashes in the resultingrequirements file output, you can specify in a configuration file:[tool.pip-tools]
generate-hashes = true
Options to  and  that may be used more than oncemust be defined as lists in a configuration file, even if they only have onevalue. supports default values for of its subcommands. Configuration keys may contain underscores instead of dashes,so the above could also be specified in this format:[tool.pip-tools]
generate_hashes = true
You might be wrapping the  command in another script. To avoidconfusing consumers of your custom script you can override the update commandgenerated at the top of requirements files by setting the environment variable.$ CUSTOM_COMPILE_COMMAND=""./pipcompilewrapper"" pip-compile requirements.in
#
# This file is autogenerated by pip-compile with Python 3.10
# by the following command:
#
#    ./pipcompilewrapper
#
asgiref==3.6.0
    # via django
django==4.1.7
    # via -r requirements.in
sqlparse==0.4.3
    # via django
Workflow for layered requirementsIf you have different environments that you need to install different butcompatible packages for, then you can create layered requirements files and useone layer to constrain the other.For example, if you have a Django project where you want the newest release in production and when developing you want to use the Django debugtoolbar, then you can create two  files, one for each layer:# requirements.in
django<2.2
At the top of the development requirements  you use  to constrain the dev requirements to packages alreadyselected for production in .# dev-requirements.in
-c requirements.txt
django-debug-toolbar<2.2
First, compile  as usual:$ pip-compile
#
# This file is autogenerated by pip-compile with Python 3.10
# by the following command:
#
#    pip-compile
#
django==2.1.15
    # via -r requirements.in
pytz==2023.3
    # via django
Now compile the dev requirements and the  file is used asa constraint:$ pip-compile dev-requirements.in
#
# This file is autogenerated by pip-compile with Python 3.10
# by the following command:
#
#    pip-compile dev-requirements.in
#
django==2.1.15
    # via
    #   -c requirements.txt
    #   django-debug-toolbar
django-debug-toolbar==2.1
    # via -r dev-requirements.in
pytz==2023.3
    # via
    #   -c requirements.txt
    #   django
sqlparse==0.4.3
    # via django-debug-toolbar
As you can see above, even though a  release of Django is available, thedev requirements only include a  version of Django because they wereconstrained. Now both compiled requirements files can be installed safely inthe dev environment.To install requirements in production stage use:$ pip-sync
You can install requirements in development stage by:$ pip-sync requirements.txt dev-requirements.txt
Version control integrationYou might use  as a hook for the .See  for instructions.Sample :repos:
  - repo: https://github.com/jazzband/pip-tools
    rev: 7.3.0
    hooks:
      - id: pip-compile
You might want to customize  args by configuring  and/or , for example:repos:
  - repo: https://github.com/jazzband/pip-tools
    rev: 7.3.0
    hooks:
      - id: pip-compile
        files: ^requirements/production\.(in|txt)$
        args: [--index-url=https://example.com, requirements/production.in]
If you have multiple requirement files make sure you create a hook for each file.repos:
  - repo: https://github.com/jazzband/pip-tools
    rev: 7.3.0
    hooks:
      - id: pip-compile
        name: pip-compile setup.py
        files: ^(setup\.py|requirements\.txt)$
      - id: pip-compile
        name: pip-compile requirements-dev.in
        args: [requirements-dev.in]
        files: ^requirements-dev\.(in|txt)$
      - id: pip-compile
        name: pip-compile requirements-lint.in
        args: [requirements-lint.in]
        files: ^requirements-lint\.(in|txt)$
      - id: pip-compile
        name: pip-compile requirements.in
        args: [requirements.in]
        files: ^requirements\.(in|txt)$
Example usage for Now that you have a , you can use  to updateyour virtual environment to reflect exactly what's in there. This willinstall/upgrade/uninstall everything necessary to match the contents.Run it with  or . If you use multiplePython versions, you can also run  on Windows and on other systems. must be installed into and run from the same virtualenvironment as your project to identify which packages to installor upgrade.Be careful:  is meant to be used only with a generated by .$ pip-sync
Uninstalling flake8-2.4.1:
    Successfully uninstalled flake8-2.4.1
Collecting click==4.1
    Downloading click-4.1-py2.py3-none-any.whl (62kB)
    100% |................................| 65kB 1.8MB/s
    Found existing installation: click 4.0
    Uninstalling click-4.0:
        Successfully uninstalled click-4.0
Successfully installed click-4.1
To sync multiple  dependency lists, just pass them in via commandline arguments, e.g.$ pip-sync dev-requirements.txt requirements.txt
Passing in empty arguments would cause it to default to .Any valid  flags or arguments may be passed with 's option, e.g.$ pip-sync requirements.txt --pip-args ""--no-cache-dir --no-deps""
Note:  will not upgrade or uninstall packaging tools like, , or  itself. Use to upgrade those packages.Should I commit  and  to source control?Generally, yes. If you want a reproducible environment installation available from your source control,then yes, you should commit both  and  to source control.Note that if you are deploying on multiple Python environments (read the section below),then you must commit a separate output file for each Python environment.We suggest to use the  format(ex: , , etc.).Cross-environment usage of / and The dependencies of a package can change depending on the Python environment in which itis installed. Here, we define a Python environment as the combination of OperatingSystem, Python version (3.7, 3.8, etc.), and Python implementation (CPython, PyPy,etc.). For an exact definition, refer to the possible combinations of .As the resulting  can differ for each environment, users mustexecute  on each Python environment separately to generate a valid for each said environment. The same  canbe used as the source file for all environments, using asneeded, the same way it would be done for regular  cross-environment usage.If the generated  remains exactly the same for all Pythonenvironments, then it can be used across Python environments safely. But usersshould be careful as any package update can introduce environment-dependentdependencies, making any newly generated  environment-dependent too.As a general rule, it's advised that users should still always execute on each targeted Python environment to avoid issues.Other useful toolsDeprecationsThis section lists  features that are currently deprecated.A Note on ResolversYou can choose from either default backtracking resolver or the deprecated legacy resolver.The legacy resolver will occasionally fail to resolve dependencies. Thebacktracking resolver is more robust, but can take longer to run in general.You can continue using the legacy resolver with  althoughnote that it is deprecated and will be removed in a future release."
https://github.com/wangshub/Douyin-Bot,😍 Python 抖音机器人，论如何在抖音上找到漂亮小姐姐？ ,如何在抖音上找到漂亮小姐姐----抖音机器人       最近沉迷于抖音无法自拔，常常花好几个小时在抖音漂亮小姐姐身上。本着高效、直接地找到漂亮小姐姐的核心思想，我用 Python + ADB 做了一个 Python 抖音机器人 Douyin-Bot。 特性原理使用教程注意脸部截取LICENSEMIT欢迎 Star 和 Fork ~如果你有什么问题请提 Issue，或者关注我的微信公众号留言，我都会一一解答
https://github.com/eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee/eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee,eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee,eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee
https://github.com/ZiniuLu/Python-100-Days,出处：https://github.com/jackfrued/Python-100-Days.git,Python - 100天从新手到大师Python应用领域和就业形势分析简单的说，Python是一个“优雅”、“明确”、“简单”的编程语言。目前几个比较流行的领域，Python都有用武之地。作为一名Python开发者，主要的就业领域包括：下图显示了主要城市Python招聘需求量及薪资待遇排行榜（截止到2018年5月）。给初学者的几个建议（老司机的忠告）：Day01~15 - Day01 - Day02 - Day03 - Day04 - Day05 - Day06 - Day07 - Day08 - Day09 - Day10 - Day11 - Day12 - Day13 - Day14 - Day15 - Day16~Day20 - Day21~30 - Day31~35 - Day36~40 - Day41~55 - Day41 - Day42 - Day43 - Day44 - Day45 - Day46 - Day47 - Day48 - Day49 - Day50 - Day51-55 - Day56~65 - Day56 - Day57 - Day58 - Day59 - Day60 - Day61-65 - Day66~75 - Day66 - Day67 - Day68 - Day69 - Day70 - Day71 - Day72 - Day73 - Day74 - Day76~90 - Pandas的应用NumPy和SciPy的应用Matplotlib和数据可视化K最邻近分类算法(KNN)线性回归和Logistic回归支持向量机(SVM)和Kmeans聚类决策树和贝叶斯分类Tensorflow实战01Tensorflow实战02Tensorflow实战03Day91~100 - 软件项目的过程模型团队开发工具模块分割设计与单元测试用Jenkins实现持续集成部署和自动化部署性能测试和改善
https://github.com/google-research/bert,TensorFlow code and pre-trained models for BERT,"BERT*This is a release of 24 smaller BERT models (English only, uncased, trained with WordPiece masking) referenced in .We have shown that the standard BERT recipe (including model architecture and training objective) is effective on a wide range of model sizes, beyond BERT-Base and BERT-Large. The smaller BERT models are intended for environments with restricted computational resources. They can be fine-tuned in the same manner as the original BERT models. However, they are most effective in the context of knowledge distillation, where the fine-tuning labels are produced by a larger and more accurate teacher.Our goal is to enable research in institutions with fewer computational resources and encourage the community to seek directions of innovation alternative to increasing model capacity.You can download all 24 from , or individually from the table below:|   |H=128|H=256|H=512|H=768||---|:---:|:---:|:---:|:---:|| L=2  |||||| L=4  |||||| L=6  |||||| L=8  |||||| L=10 |||||| L=12 |||||Note that the BERT-Base model in this release is included for completeness only; it was re-trained under the same regime as the original model.Here are the corresponding GLUE scores on the test set:|Model|Score|CoLA|SST-2|MRPC|STS-B|QQP|MNLI-m|MNLI-mm|QNLI(v2)|RTE|WNLI|AX||---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:||BERT-Tiny|64.2|0.0|83.2|81.1/71.1|74.3/73.6|62.2/83.4|70.2|70.3|81.5|57.2|62.3|21.0||BERT-Mini|65.8|0.0|85.9|81.1/71.8|75.4/73.3|66.4/86.2|74.8|74.3|84.1|57.9|62.3|26.1||BERT-Small|71.2|27.8|89.7|83.4/76.2|78.8/77.0|68.1/87.0|77.6|77.0|86.4|61.8|62.3|28.6||BERT-Medium|73.5|38.0|89.6|86.6/81.6|80.4/78.4|69.6/87.9|80.0|79.1|87.7|62.2|62.3|30.5|For each task, we selected the best fine-tuning hyperparameters from the lists below, and trained for 4 epochs:If you use these models, please cite the following paper:@article{turc2019,
  title={Well-Read Students Learn Better: On the Importance of Pre-training Compact Models},
  author={Turc, Iulia and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1908.08962v2 },
  year={2019}
}
*This is a release of several new models which were the result of an improvementthe pre-processing code.In the original pre-processing code, we randomly select WordPiece tokens tomask. For example:The new technique is called Whole Word Masking. In this case, we always maskall of the the tokens corresponding to a word at once. The overall maskingrate remains the same.The training is identical -- we still predict each masked WordPiece tokenindependently. The improvement comes from the fact that the original predictiontask was too 'easy' for words that had been split into multiple WordPieces.This can be enabled during data generation by passing the flag to .Pre-trained models with Whole Word Masking are linked below. The data andtraining were otherwise identical, and the models have identical structure andvocab to the original models. We only include BERT-Large models. When usingthese models, please make it clear in the paper that you are using the WholeWord Masking variant of BERT-Large.Model                                    | SQUAD 1.1 F1/EM | Multi NLI Accuracy---------------------------------------- | :-------------: | :----------------:BERT-Large, Uncased (Original)           | 91.0/84.3       | 86.05BERT-Large, Uncased (Whole Word Masking) | 92.8/86.7       | 87.07BERT-Large, Cased (Original)             | 91.5/84.8       | 86.09BERT-Large, Cased (Whole Word Masking)   | 92.9/86.7       | 86.46*BERT has been uploaded to . See for an example of how to use the TF Hub module,or run an example in the browser on.*We uploaded a new multilingual model which does not perform any normalizationon the input (no lower casing, accent stripping, or Unicode normalization), andadditionally inclues Thai and Mongolian.It is recommended to use this version for developing multilingual models,This does not require any code changes, and can be downloaded here:*We released code changes to reproduce our 83% F1 SQuAD 2.0 system, which iscurrently 1st place on the leaderboard by 3%. See the SQuAD 2.0 section of theREADME for details.*NLP researchers from HuggingFace made awhich is compatible with our pre-trained checkpoints and is able to reproduceour results. Sosuke Kobayashi also made a(Thanks!) We were not involved in the creation or maintenance of the PyTorchimplementation so please direct any questions towards the authors of thatrepository.*We have made two new BERT models available:We use character-based tokenization for Chinese, and WordPiece tokenization forall other languages. Both models should work out-of-the-box without any codechanges. We did update the implementation of  in to support Chinese character tokenization, so please update ifyou forked it. However, we did not change the tokenization API.For more, see the.*IntroductionBERT, or Bidirectional Encoder Representations fromTransformers, is a new method of pre-training language representations whichobtains state-of-the-art results on a wide array of Natural Language Processing(NLP) tasks.Our academic paper which describes BERT in detail and provides full results on anumber of tasks can be found here:.To give a few numbers, here are the results on the question answeringtask:SQuAD v1.1 Leaderboard (Oct 8th 2018) | Test EM  | Test F1------------------------------------- | :------: | :------:1st Place Ensemble - BERT             | 87.4 | 93.22nd Place Ensemble - nlnet            | 86.0     | 91.71st Place Single Model - BERT         | 85.1 | 91.82nd Place Single Model - nlnet        | 83.5     | 90.1And several natural language inference tasks:System                  | MultiNLI | Question NLI | SWAG----------------------- | :------: | :----------: | :------:BERT                    | 86.7 | 91.1     | 86.3OpenAI GPT (Prev. SOTA) | 82.2     | 88.1         | 75.0Plus many other tasks.Moreover, these results were all obtained with almost no task-specific neuralnetwork architecture design.If you already know what BERT is and you just want to get started, you can and in only a fewminutes.What is BERT?BERT is a method of pre-training language representations, meaning that we traina general-purpose ""language understanding"" model on a large text corpus (likeWikipedia), and then use that model for downstream NLP tasks that we care about(like question answering). BERT outperforms previous methods because it is thefirst unsupervised, deeply bidirectional system for pre-training NLP.Unsupervised means that BERT was trained using only a plain text corpus, whichis important because an enormous amount of plain text data is publicly availableon the web in many languages.Pre-trained representations can also either be context-free or contextual,and contextual representations can further be unidirectional orbidirectional. Context-free models such as or generate a single ""wordembedding"" representation for each word in the vocabulary, so  would havethe same representation in  and . Contextual modelsinstead generate a representation of each word that is based on the other wordsin the sentence.BERT was built upon recent work in pre-training contextual representations —including ,,, and— but crucially these models are all unidirectional or shallowly. This means that each word is only contextualized using the wordsto its left (or right). For example, in the sentence  theunidirectional representation of  is only based on  but not. Some previous work does combine the representations from separateleft-context and right-context models, but only in a ""shallow"" manner. BERTrepresents ""bank"" using both its left and right context — — starting from the very bottom of a deep neural network, so it is deeply.BERT uses a simple approach for this: We mask out 15% of the words in the input,run the entire sequence through a deep bidirectional encoder, and then predict onlythe masked words. For example:Input: the man went to the [MASK1] . he bought a [MASK2] of milk.
Labels: [MASK1] = store; [MASK2] = gallon
In order to learn relationships between sentences, we also train on a simpletask which can be generated from any monolingual corpus: Given two sentences and , is  the actual next sentence that comes after , or just a randomsentence from the corpus?Sentence A: the man went to the store .
Sentence B: he bought a gallon of milk .
Label: IsNextSentence
Sentence A: the man went to the store .
Sentence B: penguins are flightless .
Label: NotNextSentence
We then train a large model (12-layer to 24-layer Transformer) on a large corpus(Wikipedia + ) for a long time (1Mupdate steps), and that's BERT.Using BERT has two stages: Pre-training and fine-tuning.Pre-training is fairly expensive (four days on 4 to 16 Cloud TPUs), but is aone-time procedure for each language (current models are English-only, butmultilingual models will be released in the near future). We are releasing anumber of pre-trained models from the paper which were pre-trained at Google.Most NLP researchers will never need to pre-train their own model from scratch.Fine-tuning is inexpensive. All of the results in the paper can bereplicated in at most 1 hour on a single Cloud TPU, or a few hours on a GPU,starting from the exact same pre-trained model. SQuAD, for example, can betrained in around 30 minutes on a single Cloud TPU to achieve a Dev F1 score of91.0%, which is the single system state-of-the-art.The other important aspect of BERT is that it can be adapted to many types ofNLP tasks very easily. In the paper, we demonstrate state-of-the-art results onsentence-level (e.g., SST-2), sentence-pair-level (e.g., MultiNLI), word-level(e.g., NER), and span-level (e.g., SQuAD) tasks with almost no task-specificmodifications.What has been released in this repository?We are releasing the following:All of the code in this repository works out-of-the-box with CPU, GPU, and CloudTPU.Pre-trained modelsWe are releasing the  and  models from the paper. means that the text has been lowercased before WordPiece tokenization,e.g.,  becomes . The  model also strips out anyaccent markers.  means that the true case and accent markers arepreserved. Typically, the  model is better unless you know that caseinformation is important for your task (e.g., Named Entity Recognition orPart-of-Speech tagging).These models are all released under the same license as the source code (Apache2.0).For information about the Multilingual and Chinese model, see the.When using a cased model, make sure to pass The links to the models are here (right-click, 'Save link as...' on the name):Each .zip file contains three items:Fine-tuning with BERTImportant: All results on the paper were fine-tuned on a single Cloud TPU,which has 64GB of RAM. It is currently not possible to re-produce most of the results on the paper using a GPU with 12GB - 16GB of RAM, becausethe maximum batch size that can fit in memory is too small. We are working onadding code to this repository which allows for much larger effective batch sizeon the GPU. See the section on  formore details.This code was tested with TensorFlow 1.11.0. It was tested with Python2 andPython3 (but more thoroughly with Python2, since this is what's used internallyin Google).The fine-tuning examples which use  should be able to run on a GPUthat has at least 12GB of RAM using the hyperparameters given.Fine-tuning with Cloud TPUsMost of the examples below assumes that you will be running training/evaluationon your local machine, using a GPU like a Titan X or GTX 1080.However, if you have access to a Cloud TPU that you want to train on, just addthe following flags to  or :  --use_tpu=True \
  --tpu_name=$TPU_NAME
Please see thefor how to use Cloud TPUs. Alternatively, you can use the Google Colab notebook"""".On Cloud TPUs, the pretrained model and the output directory will need to be onGoogle Cloud Storage. For example, if you have a bucket named , youmight use the following flags instead:  --output_dir=gs://some_bucket/my_output_dir/
The unzipped pre-trained model files can also be found in the Google CloudStorage folder . For example:export BERT_BASE_DIR=gs://bert_models/2018_10_18/uncased_L-12_H-768_A-12
Sentence (and sentence-pair) classification tasksBefore running this example you must download the by runningand unpack it to some directory . Next, download the checkpoint and unzip it to some directory .This example code fine-tunes  on the Microsoft Research ParaphraseCorpus (MRPC) corpus, which only contains 3,600 examples and can fine-tune in afew minutes on most GPUs.export BERT_BASE_DIR=/path/to/bert/uncased_L-12_H-768_A-12
export GLUE_DIR=/path/to/glue

python run_classifier.py \
  --task_name=MRPC \
  --do_train=true \
  --do_eval=true \
  --data_dir=$GLUE_DIR/MRPC \
  --vocab_file=$BERT_BASE_DIR/vocab.txt \
  --bert_config_file=$BERT_BASE_DIR/bert_config.json \
  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \
  --max_seq_length=128 \
  --train_batch_size=32 \
  --learning_rate=2e-5 \
  --num_train_epochs=3.0 \
  --output_dir=/tmp/mrpc_output/
You should see output like this:***** Eval results *****
  eval_accuracy = 0.845588
  eval_loss = 0.505248
  global_step = 343
  loss = 0.505248
This means that the Dev set accuracy was 84.55%. Small sets like MRPC have ahigh variance in the Dev set accuracy, even when starting from the samepre-training checkpoint. If you re-run multiple times (making sure to point todifferent ), you should see results between 84% and 88%.A few other pre-trained models are implemented off-the-shelf in, so it should be straightforward to follow those examples touse BERT for any single-sentence or sentence-pair classification task.Note: You might see a message . This really just meansthat it's running on something other than a Cloud TPU, which includes a GPU.Prediction from classifierOnce you have trained your classifier you can use it in inference mode by usingthe --do_predict=true command. You need to have a file named test.tsv in theinput folder. Output will be created in file called test_results.tsv in theoutput folder. Each line will contain output for each sample, columns are theclass probabilities.export BERT_BASE_DIR=/path/to/bert/uncased_L-12_H-768_A-12
export GLUE_DIR=/path/to/glue
export TRAINED_CLASSIFIER=/path/to/fine/tuned/classifier

python run_classifier.py \
  --task_name=MRPC \
  --do_predict=true \
  --data_dir=$GLUE_DIR/MRPC \
  --vocab_file=$BERT_BASE_DIR/vocab.txt \
  --bert_config_file=$BERT_BASE_DIR/bert_config.json \
  --init_checkpoint=$TRAINED_CLASSIFIER \
  --max_seq_length=128 \
  --output_dir=/tmp/mrpc_output/
SQuAD 1.1The Stanford Question Answering Dataset (SQuAD) is a popular question answeringbenchmark dataset. BERT (at the time of the release) obtains state-of-the-artresults on SQuAD with almost no task-specific network architecture modificationsor data augmentation. However, it does require semi-complex data pre-processingand post-processing to deal with (a) the variable-length nature of SQuAD contextparagraphs, and (b) the character-level answer annotations which are used forSQuAD training. This processing is implemented and documented in .To run on SQuAD, you will first need to download the dataset. The does not seem tolink to the v1.1 datasets any longer, but the necessary files can be found here:Download these to some directory .The state-of-the-art SQuAD results from the paper currently cannot be reproducedon a 12GB-16GB GPU due to memory constraints (in fact, even batch size 1 doesnot seem to fit on a 12GB GPU using ). However, a reasonably strong model can be trained on the GPU with these hyperparameters:python run_squad.py \
  --vocab_file=$BERT_BASE_DIR/vocab.txt \
  --bert_config_file=$BERT_BASE_DIR/bert_config.json \
  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \
  --do_train=True \
  --train_file=$SQUAD_DIR/train-v1.1.json \
  --do_predict=True \
  --predict_file=$SQUAD_DIR/dev-v1.1.json \
  --train_batch_size=12 \
  --learning_rate=3e-5 \
  --num_train_epochs=2.0 \
  --max_seq_length=384 \
  --doc_stride=128 \
  --output_dir=/tmp/squad_base/
The dev set predictions will be saved into a file called  inthe :python $SQUAD_DIR/evaluate-v1.1.py $SQUAD_DIR/dev-v1.1.json ./squad/predictions.json
Which should produce an output like this:{""f1"": 88.41249612335034, ""exact_match"": 81.2488174077578}
You should see a result similar to the 88.5% reported in the paper for.If you have access to a Cloud TPU, you can train with . Here is aset of hyperparameters (slightly different than the paper) which consistentlyobtain around 90.5%-91.0% F1 single-system trained only on SQuAD:python run_squad.py \
  --vocab_file=$BERT_LARGE_DIR/vocab.txt \
  --bert_config_file=$BERT_LARGE_DIR/bert_config.json \
  --init_checkpoint=$BERT_LARGE_DIR/bert_model.ckpt \
  --do_train=True \
  --train_file=$SQUAD_DIR/train-v1.1.json \
  --do_predict=True \
  --predict_file=$SQUAD_DIR/dev-v1.1.json \
  --train_batch_size=24 \
  --learning_rate=3e-5 \
  --num_train_epochs=2.0 \
  --max_seq_length=384 \
  --doc_stride=128 \
  --output_dir=gs://some_bucket/squad_large/ \
  --use_tpu=True \
  --tpu_name=$TPU_NAME
For example, one random run with these parameters produces the following Devscores:{""f1"": 90.87081895814865, ""exact_match"": 84.38978240302744}
If you fine-tune for one epoch on before this the results willbe even better, but you will need to convert TriviaQA into the SQuAD jsonformat.SQuAD 2.0This model is also implemented and documented in .To run on SQuAD 2.0, you will first need to download the dataset. The necessaryfiles can be found here:Download these to some directory .On Cloud TPU you can run with BERT-Large as follows:python run_squad.py \
  --vocab_file=$BERT_LARGE_DIR/vocab.txt \
  --bert_config_file=$BERT_LARGE_DIR/bert_config.json \
  --init_checkpoint=$BERT_LARGE_DIR/bert_model.ckpt \
  --do_train=True \
  --train_file=$SQUAD_DIR/train-v2.0.json \
  --do_predict=True \
  --predict_file=$SQUAD_DIR/dev-v2.0.json \
  --train_batch_size=24 \
  --learning_rate=3e-5 \
  --num_train_epochs=2.0 \
  --max_seq_length=384 \
  --doc_stride=128 \
  --output_dir=gs://some_bucket/squad_large/ \
  --use_tpu=True \
  --tpu_name=$TPU_NAME \
  --version_2_with_negative=True
We assume you have copied everything from the output directory to a localdirectory called ./squad/. The initial dev set predictions will be at./squad/predictions.json and the differences between the score of no answer ("""")and the best non-null answer for each question will be in the file./squad/null_odds.jsonRun this script to tune a threshold for predicting null versus non-null answers:python $SQUAD_DIR/evaluate-v2.0.py $SQUAD_DIR/dev-v2.0.json./squad/predictions.json --na-prob-file ./squad/null_odds.jsonAssume the script outputs ""best_f1_thresh"" THRESH. (Typical values are between-1.0 and -5.0). You can now re-run the model to generate predictions with thederived threshold or alternatively you can extract the appropriate answers from./squad/nbest_predictions.json.python run_squad.py \
  --vocab_file=$BERT_LARGE_DIR/vocab.txt \
  --bert_config_file=$BERT_LARGE_DIR/bert_config.json \
  --init_checkpoint=$BERT_LARGE_DIR/bert_model.ckpt \
  --do_train=False \
  --train_file=$SQUAD_DIR/train-v2.0.json \
  --do_predict=True \
  --predict_file=$SQUAD_DIR/dev-v2.0.json \
  --train_batch_size=24 \
  --learning_rate=3e-5 \
  --num_train_epochs=2.0 \
  --max_seq_length=384 \
  --doc_stride=128 \
  --output_dir=gs://some_bucket/squad_large/ \
  --use_tpu=True \
  --tpu_name=$TPU_NAME \
  --version_2_with_negative=True \
  --null_score_diff_threshold=$THRESH
Out-of-memory issuesAll experiments in the paper were fine-tuned on a Cloud TPU, which has 64GB ofdevice RAM. Therefore, when using a GPU with 12GB - 16GB of RAM, you are likelyto encounter out-of-memory issues if you use the same hyperparameters describedin the paper.The factors that affect memory usage are:Using the default training scripts ( and ), webenchmarked the maximum batch size on single Titan X GPU (12GB RAM) withTensorFlow 1.11.0:System       | Seq Length | Max Batch Size------------ | ---------- | --------------  | 64         | 64...          | 128        | 32...          | 256        | 16...          | 320        | 14...          | 384        | 12...          | 512        | 6 | 64         | 12...          | 128        | 6...          | 256        | 2...          | 320        | 1...          | 384        | 0...          | 512        | 0Unfortunately, these max batch sizes for  are so small that theywill actually harm the model accuracy, regardless of the learning rate used. Weare working on adding code to this repository which will allow much largereffective batch sizes to be used on the GPU. The code will be based on one (orboth) of the following techniques:However, this is not implemented in the current release.Using BERT to extract fixed feature vectors (like ELMo)In certain cases, rather than fine-tuning the entire pre-trained modelend-to-end, it can be beneficial to obtained pre-trained contextual, which are fixed contextual representations of each input tokengenerated from the hidden layers of the pre-trained model. This should alsomitigate most of the out-of-memory issues.As an example, we include the script  which can be usedlike this:# Sentence A and Sentence B are separated by the ||| delimiter for sentence
# pair tasks like question answering and entailment.
# For single sentence inputs, put one sentence per line and DON'T use the
# delimiter.
echo 'Who was Jim Henson ? ||| Jim Henson was a puppeteer' > /tmp/input.txt

python extract_features.py \
  --input_file=/tmp/input.txt \
  --output_file=/tmp/output.jsonl \
  --vocab_file=$BERT_BASE_DIR/vocab.txt \
  --bert_config_file=$BERT_BASE_DIR/bert_config.json \
  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \
  --layers=-1,-2,-3,-4 \
  --max_seq_length=128 \
  --batch_size=8
This will create a JSON file (one line per line of input) containing the BERTactivations from each Transformer layer specified by  (-1 is the finalhidden layer of the Transformer, etc.)Note that this script will produce very large output files (by default, around15kb for every input token).If you need to maintain alignment between the original and tokenized words (forprojecting training labels), see the  sectionbelow.Note: You may see a message like  This message is expected, itjust means that we are using the  API rather than thesaved model API. If you don't specify a checkpoint or specify an invalidcheckpoint, this script will complain.TokenizationFor sentence-level tasks (or sentence-pair) tasks, tokenization is very simple.Just follow the example code in  and .The basic procedure for sentence-level tasks is:Word-level and span-level tasks (e.g., SQuAD and NER) are more complex, sinceyou need to maintain alignment between your input text and output text so thatyou can project your training labels. SQuAD is a particularly complex examplebecause the input labels are character-based, and SQuAD paragraphs are oftenlonger than our maximum sequence length. See the code in  to showhow we handle this.Before we describe the general recipe for handling word-level tasks, it'simportant to understand what exactly our tokenizer is doing. It has three mainsteps:The advantage of this scheme is that it is ""compatible"" with most existingEnglish tokenizers. For example, imagine that you have a part-of-speech taggingtask which looks like this:Input:  John Johanson 's   house
Labels: NNP  NNP      POS NN
The tokenized output will look like this:Tokens: john johan ##son ' s house
Crucially, this would be the same output as if the raw text were  (with no space before the ).If you have a pre-tokenized representation with word-level annotations, you cansimply tokenize each input word independently, and deterministically maintain anoriginal-to-tokenized alignment:### Input
orig_tokens = [""John"", ""Johanson"", ""'s"",  ""house""]
labels      = [""NNP"",  ""NNP"",      ""POS"", ""NN""]

### Output
bert_tokens = []

# Token map will be an int -> int mapping between the `orig_tokens` index and
# the `bert_tokens` index.
orig_to_tok_map = []

tokenizer = tokenization.FullTokenizer(
    vocab_file=vocab_file, do_lower_case=True)

bert_tokens.append(""[CLS]"")
for orig_token in orig_tokens:
  orig_to_tok_map.append(len(bert_tokens))
  bert_tokens.extend(tokenizer.tokenize(orig_token))
bert_tokens.append(""[SEP]"")

# bert_tokens == [""[CLS]"", ""john"", ""johan"", ""##son"", ""'"", ""s"", ""house"", ""[SEP]""]
# orig_to_tok_map == [1, 2, 4, 6]
Now  can be used to project  to the tokenizedrepresentation.There are common English tokenization schemes which will cause a slight mismatchbetween how BERT was pre-trained. For example, if your input tokenization splitsoff contractions like , this will cause a mismatch. If it is possible todo so, you should pre-process your data to convert these back to raw-lookingtext, but if it's not possible, this mismatch is likely not a big deal.Pre-training with BERTWe are releasing code to do ""masked LM"" and ""next sentence prediction"" on anarbitrary text corpus. Note that this is not the exact code that was used forthe paper (the original code was written in C++, and had some additionalcomplexity), but this code does generate pre-training data as described in thepaper.Here's how to run the data generation. The input is a plain text file, with onesentence per line. (It is important that these be actual sentences for the ""nextsentence prediction"" task). Documents are delimited by empty lines. The outputis a set of s serialized into  file format.You can perform sentence segmentation with an off-the-shelf NLP toolkit such as. The  script willconcatenate segments until they reach the maximum sequence length to minimizecomputational waste from padding (see the script for more details). However, youmay want to intentionally add a slight amount of noise to your input data (e.g.,randomly truncate 2% of input segments) to make it more robust to non-sententialinput during fine-tuning.This script stores all of the examples for the entire input file in memory, sofor large data files you should shard the input file and call the scriptmultiple times. (You can pass in a file glob to , e.g.,.)The  is the maximum number of masked LM predictions persequence. You should set this to around  *  (thescript doesn't do that automatically because the exact value needs to be passedto both scripts).python create_pretraining_data.py \
  --input_file=./sample_text.txt \
  --output_file=/tmp/tf_examples.tfrecord \
  --vocab_file=$BERT_BASE_DIR/vocab.txt \
  --do_lower_case=True \
  --max_seq_length=128 \
  --max_predictions_per_seq=20 \
  --masked_lm_prob=0.15 \
  --random_seed=12345 \
  --dupe_factor=5
Here's how to run the pre-training. Do not include  if you arepre-training from scratch. The model configuration (including vocab size) isspecified in . This demo code only pre-trains for a smallnumber of steps (20), but in practice you will probably want to set to 10000 steps or more. The  and parameters passed to  must be thesame as .python run_pretraining.py \
  --input_file=/tmp/tf_examples.tfrecord \
  --output_dir=/tmp/pretraining_output \
  --do_train=True \
  --do_eval=True \
  --bert_config_file=$BERT_BASE_DIR/bert_config.json \
  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \
  --train_batch_size=32 \
  --max_seq_length=128 \
  --max_predictions_per_seq=20 \
  --num_train_steps=20 \
  --num_warmup_steps=10 \
  --learning_rate=2e-5
This will produce an output like this:***** Eval results *****
  global_step = 20
  loss = 0.0979674
  masked_lm_accuracy = 0.985479
  masked_lm_loss = 0.0979328
  next_sentence_accuracy = 1.0
  next_sentence_loss = 3.45724e-05
Note that since our  file is very small, this example trainingwill overfit that data in only a few steps and produce unrealistically highaccuracy numbers.Pre-training tips and caveatsPre-training dataWe will not be able to release the pre-processed datasets used in the paper.For Wikipedia, the recommended pre-processing is to download,extract the text with, and then applyany necessary cleanup to convert it into plain text.Unfortunately the researchers who collected the no longer have it available forpublic download. Theis a somewhat smaller (200M word) collection of older books that are publicdomain. is another very large collection oftext, but you will likely have to do substantial pre-processing and cleanup toextract a usable corpus for pre-training BERT.Learning a new WordPiece vocabularyThis repository does not include code for learning a new WordPiece vocabulary.The reason is that the code used in the paper was implemented in C++ withdependencies on Google's internal libraries. For English, it is almost alwaysbetter to just start with our vocabulary and pre-trained models. For learningvocabularies of other languages, there are a number of open source optionsavailable. However, keep in mind that these are not compatible with our library:Using BERT in ColabIf you want to use BERT with , you canget started with the notebook"""".At the time of this writing (October 31st, 2018), Colab users can access a Note: One per user, availability limited,requires a Google Cloud Platform account with storage (although storage may bepurchased with free credit for signing up with GCP), and this capability may notlonger be available in the future. Click on the BERT Colab that was just linkedfor more information.FAQIs this code compatible with Cloud TPUs? What about GPUs?Yes, all of the code in this repository works out-of-the-box with CPU, GPU, andCloud TPU. However, GPU training is single-GPU only.I am getting out-of-memory errors, what is wrong?See the section on  for moreinformation.Is there a PyTorch version available?There is no official PyTorch implementation. However, NLP researchers fromHuggingFace made awhich is compatible with our pre-trained checkpoints and is able to reproduceour results. We were not involved in the creation or maintenance of the PyTorchimplementation so please direct any questions towards the authors of thatrepository.Is there a Chainer version available?There is no official Chainer implementation. However, Sosuke Kobayashi made awhich is compatible with our pre-trained checkpoints and is able to reproduceour results. We were not involved in the creation or maintenance of the Chainerimplementation so please direct any questions towards the authors of thatrepository.Will models in other languages be released?Yes, we plan to release a multi-lingual BERT model in the near future. We cannotmake promises about exactly which languages will be included, but it will likelybe a single model which includes most of the languages which have asignificantly-sized Wikipedia.Will models larger than  be released?So far we have not attempted to train anything larger than . It ispossible that we will release larger models if we are able to obtain significantimprovements.What license is this library released under?All code and models are released under the Apache 2.0 license. See the file for more information.How do I cite BERT?For now, cite :@article{devlin2018bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
If we submit the paper to a conference or journal, we will update the BibTeX.DisclaimerThis is not an official Google product.Contact informationFor help or issues using BERT, please submit a GitHub issue.For personal communication related to BERT, please contact Jacob Devlin(), Ming-Wei Chang (), orKenton Lee ()."
https://github.com/pydantic/pydantic,Data validation using Python type hints,"PydanticData validation using Python type hints.Fast and extensible, Pydantic plays nicely with your linters/IDE/brain.Define how data should be in pure, canonical Python 3.7+; validate it with Pydantic.Pydantic Company :rocket:We've started a company based on the principles that I believe have led to Pydantic's success.Learning more from the .Pydantic V1.10 vs. V2Pydantic V2 is a ground-up rewrite that offers many new features, performance improvements, and some breaking changes compared to Pydantic V1.If you're using Pydantic V1 you may want to look at the or,. Pydantic V2 also ships with the latest version of Pydantic V1 built in so that you can incrementally upgrade your code base and projects: .HelpSee  for more details.InstallationInstall using  or .For more installation options to make Pydantic even faster,see the  section in the documentation.A Simple Examplefrom datetime import datetime
from typing import List, Optional
from pydantic import BaseModel

class User(BaseModel):
    id: int
    name: str = 'John Doe'
    signup_ts: Optional[datetime] = None
    friends: List[int] = []

external_data = {'id': '123', 'signup_ts': '2017-06-01 12:22', 'friends': [1, '2', b'3']}
user = User(**external_data)
print(user)
#> User id=123 name='John Doe' signup_ts=datetime.datetime(2017, 6, 1, 12, 22) friends=[1, 2, 3]
print(user.id)
#> 123
ContributingFor guidance on setting up a development environment and how to make acontribution to Pydantic, see.Reporting a Security VulnerabilitySee our ."
https://github.com/miloyip/game-programmer,A Study Path for Game Programmer,DisclaimerBuildAcknowledgement
https://github.com/google-deepmind/sonnet,TensorFlow-based neural network library,"Sonnet | Sonnet is a library built on top of designed to provide simple, composable abstractions for machine learningresearch.IntroductionSonnet has been designed and built by researchers at DeepMind. It can be used toconstruct neural networks for many different purposes (un/supervised learning,reinforcement learning, ...). We find it is a successful abstraction for ourorganization, you might too!More specifically, Sonnet provides a simple but powerful programming modelcentered around a single concept: . Modules can hold references toparameters, other modules and methods that apply some function on the userinput. Sonnet ships with many predefined modules (e.g. ,, ) and some predefined networks of modules (e.g.) but users are also encouraged to build their own modules.Unlike many frameworks Sonnet is extremely unopinionated about how you willuse your modules. Modules are designed to be self contained and entirelydecoupled from one another. Sonnet does not ship with a training framework andusers are encouraged to build their own or adopt those built by others.Sonnet is also designed to be simple to understand, our code is (hopefully!)clear and focussed. Where we have picked defaults (e.g. defaults for initialparameter values) we try to point out why.Getting StartedExamplesThe easiest way to try Sonnet is to use Google Colab which offers a free Pythonnotebook attached to a GPU or TPU.InstallationTo get started install TensorFlow 2.0 and Sonnet 2:$ pip install tensorflow tensorflow-probability
$ pip install dm-sonnet
You can run the following to verify things installed correctly:import tensorflow as tf
import sonnet as snt

print(""TensorFlow version {}"".format(tf.__version__))
print(""Sonnet version {}"".format(snt.__version__))
Using existing modulesSonnet ships with a number of built in modules that you can trivially use. Forexample to define an MLP we can use the  module to call asequence of modules, passing the output of a given module as the input for thenext module. We can use  and  to actually define ourcomputation:mlp = snt.Sequential([
    snt.Linear(1024),
    tf.nn.relu,
    snt.Linear(10),
])
To use our module we need to ""call"" it. The  module (and mostmodules) define a  method that means you can call them by name:logits = mlp(tf.random.normal([batch_size, input_size]))
It is also very common to request all the parameters for your module. Mostmodules in Sonnet create their parameters the first time they are called withsome input (since in most cases the shape of the parameters is a function ofthe input). Sonnet modules provide two properties for accessing parameters.The  property returns all s that are referenced bythe given module:all_variables = mlp.variables
It is worth noting that s are not just used for parameters of yourmodel. For example they are used to hold state in metrics used in. In most cases users retrieve the module variables to pass themto an optimizer to be updated. In this case non-trainable variables shouldtypically not be in that list as they are updated via a different mechanism.TensorFlow has a built in mechanism to mark variables as ""trainable"" (parametersof your model) vs. non-trainable (other variables). Sonnet provides a mechanismto gather all trainable variables from your module which is probably what youwant to pass to an optimizer:model_parameters = mlp.trainable_variables
Building your own moduleSonnet strongly encourages users to subclass  to define their ownmodules. Let's start by creating a simple  layer called :class MyLinear(snt.Module):

  def __init__(self, output_size, name=None):
    super(MyLinear, self).__init__(name=name)
    self.output_size = output_size

  @snt.once
  def _initialize(self, x):
    initial_w = tf.random.normal([x.shape[1], self.output_size])
    self.w = tf.Variable(initial_w, name=""w"")
    self.b = tf.Variable(tf.zeros([self.output_size]), name=""b"")

  def __call__(self, x):
    self._initialize(x)
    return tf.matmul(x, self.w) + self.b
Using this module is trivial:mod = MyLinear(32)
mod(tf.ones([batch_size, input_size]))
By subclassing  you get many nice properties for free. For examplea default implementation of  which shows constructor arguments (veryuseful for debugging and introspection):>>> print(repr(mod))
MyLinear(output_size=10)
You also get the  and  properties:>>> mod.variables
(<tf.Variable 'my_linear/b:0' shape=(10,) ...)>,
 <tf.Variable 'my_linear/w:0' shape=(1, 10) ...)>)
You may notice the  prefix on the variables above. This is becauseSonnet modules also enter the modules name scope whenever methods are called.By entering the module name scope we provide a much more useful graph for toolslike TensorBoard to consume (e.g. all operations that occur inside my_linearwill be in a group called my_linear).Additionally your module will now support TensorFlow checkpointing and savedmodel which are advanced features covered later.SerializationSonnet supports multiple serialization formats. The simplest format we supportis Python's , and all built in modules are tested to make sure they canbe saved/loaded via pickle in the same Python process. In general we discouragethe use of pickle, it is not well supported by many parts of TensorFlow and inour experience can be quite brittle.TensorFlow CheckpointingReference: https://www.tensorflow.org/alpha/guide/checkpointsTensorFlow checkpointing can be used to save the value of parametersperiodically during training. This can be useful to save the progress oftraining in case your program crashes or is stopped. Sonnet is designed to workcleanly with TensorFlow checkpointing:checkpoint_root = ""/tmp/checkpoints""
checkpoint_name = ""example""
save_prefix = os.path.join(checkpoint_root, checkpoint_name)

my_module = create_my_sonnet_module()  # Can be anything extending snt.Module.

# A `Checkpoint` object manages checkpointing of the TensorFlow state associated
# with the objects passed to it's constructor. Note that Checkpoint supports
# restore on create, meaning that the variables of `my_module` do **not** need
# to be created before you restore from a checkpoint (their value will be
# restored when they are created).
checkpoint = tf.train.Checkpoint(module=my_module)

# Most training scripts will want to restore from a checkpoint if one exists. This
# would be the case if you interrupted your training (e.g. to use your GPU for
# something else, or in a cloud environment if your instance is preempted).
latest = tf.train.latest_checkpoint(checkpoint_root)
if latest is not None:
  checkpoint.restore(latest)

for step_num in range(num_steps):
  train(my_module)

  # During training we will occasionally save the values of weights. Note that
  # this is a blocking call and can be slow (typically we are writing to the
  # slowest storage on the machine). If you have a more reliable setup it might be
  # appropriate to save less frequently.
  if step_num and not step_num % 1000:
    checkpoint.save(save_prefix)

# Make sure to save your final values!!
checkpoint.save(save_prefix)
TensorFlow Saved ModelReference: https://www.tensorflow.org/alpha/guide/saved_modelTensorFlow saved models can be used to save a copy of your network that isdecoupled from the Python source for it. This is enabled by saving a TensorFlowgraph describing the computation and a checkpoint containing the value ofweights.The first thing to do in order to create a saved model is to create a that you want to save:my_module = snt.nets.MLP([1024, 1024, 10])
my_module(tf.ones([1, input_size]))
Next, we need to create another module describing the specific parts of ourmodel that we want to export. We advise doing this (rather than modifying theoriginal model in-place) so you have fine grained control over what is actuallyexported. This is typically important to avoid creating very large saved models,and such that you only share the parts of your model you want to (e.g. you onlywant to share the generator for a GAN but keep the discriminator private).@tf.function(input_signature=[tf.TensorSpec([None, input_size])])
def inference(x):
  return my_module(x)

to_save = snt.Module()
to_save.inference = inference
to_save.all_variables = list(my_module.variables)
tf.saved_model.save(to_save, ""/tmp/example_saved_model"")
We now have a saved model in the  folder:$ ls -lh /tmp/example_saved_model
total 24K
drwxrwsr-t 2 tomhennigan 154432098 4.0K Apr 28 00:14 assets
-rw-rw-r-- 1 tomhennigan 154432098  14K Apr 28 00:15 saved_model.pb
drwxrwsr-t 2 tomhennigan 154432098 4.0K Apr 28 00:15 variables
Loading this model is simple and can be done on a different machine without anyof the Python code that built the saved model:loaded = tf.saved_model.load(""/tmp/example_saved_model"")

# Use the inference method. Note this doesn't run the Python code from `to_save`
# but instead uses the TensorFlow Graph that is part of the saved model.
loaded.inference(tf.ones([1, input_size]))

# The all_variables property can be used to retrieve the restored variables.
assert len(loaded.all_variables) > 0
Note that the loaded object is not a Sonnet module, it is a container objectthat has the specific methods (e.g. ) and properties (e.g.) that we added in the previous block.Distributed trainingExample: https://github.com/deepmind/sonnet/blob/v2/examples/distributed_cifar10.ipynbSonnet has support for distributed training using.A key difference between Sonnet and distributed training using  isthat Sonnet modules and optimizers do not behave differently when run underdistribution strategies (e.g. we do not average your gradients or sync yourbatch norm stats). We believe that users should be in full control of theseaspects of their training and they should not be baked into the library. Thetrade off here is that you need to implement these features in your trainingscript (typically this is just 2 lines of code to all reduce your gradientsbefore applying your optimizer) or swap in modules that are explicitlydistribution aware (e.g. ).Our example walks through doing multi-GPU training with Sonnet."
https://github.com/Kr1s77/awesome-python-login-model,😮python模拟登陆一些大型网站，还有一些简单的爬虫，希望对你们有所帮助❤️，如果喜欢记得给个star哦🌟,"传送门💕Website login model一些爬虫示例程序，以及模拟登陆程序,模拟登陆基于 selenium，有些模拟登录基于 js 逆向，持续更新，有问题可以直接提交 Issues，欢迎提交 PR, 测试通过可以直接 merge，文中所有程序都是使用  编写 :-)About模拟登陆基本采用的是直接登录或者使用selenium+webdriver的方式，有的网站直接登录难度很大，比如qq空间，bilibili等如果采用selenium就相对轻松一些。虽然在登录的时候采用的是selenium,为了效率，我们可以在登录过后得到的cookie维护起来，然后调用requests或者scrapy等进行数据采集，这样数据采集的速度可以得到保证。WebDriverCompletedcatalogueTestInformationstips of pull requestProblemsAcknowledgments联系我注意："
https://github.com/xflux-gui/fluxgui,Better lighting for Linux. Open source GUI for xflux,"XFLUX DOES NOT WORK ON MOST MODERN SYSTEMSThe  program that Fluxgui traditionally used to change thescreen color hasn't worked on most modern systems since 2016, it's aclosed source program that is not part of this project, and there areno plans to fix it. Because of this, Fluxgui by default now uses to control your screen color,which should be supported on all systems.See Issue #27 for why  probably won't work on your system andhow to test if it can.f.lux indicator appletBetter lighting for your computerThe f.lux indicator applet  is an indicator applet that uses or  to make the color of your computer's displayadapt to the time of day: warm at night, and like sunlight during theday. Reducing blue light exposure in the evening can help you fallasleep at night. See https://justgetflux.com/research.html orhttp://jonls.dk/redshift/ for more details.This project -- https://github.com/xflux-gui/fluxgui -- is onlyconcerned with the  indicator applet program, not with theunderlying  or  program the indicator appletcontrols. The  or  program is responsible foractually changing the color of your screen. Seehttps://justgetflux.com/linux.html for more information about .The  program is downloaded automatically when installing. You can install  via the  package onmost Linux distros. Simply run  in your terminal afterinstallation to open the applet.  You can also easily configure theapplet to auto-start on login.Install InstructionsOnly Python 3 is SupportedThe  is only known to work with Python 3.Ubuntu PPA Package Manager Install[<marko.inline.RawText object at 0x000001592FEFE188>, <marko.inline.Link object at 0x000001592FEFE808>, <marko.inline.RawText object at 0x000001592FEFE348>, <marko.inline.Link object at 0x000001592FEFEC48>, <marko.inline.RawText object at 0x000001592FEFE0C8>]To install via apt:sudo add-apt-repository ppa:nathan-renniewaldock/flux
sudo apt-get update
sudo apt-get install fluxgui
See  for more details.Workaround for Ubuntu 20.04 LTS and aboveWhile  there is an error E: The repository 'http://ppa.launchpad.net/nathan-renniewaldock/flux/ubuntu focal Release' does not have a Release file. 
To solve:If you have trouble with the PPA version try the manual install below.Fedora Package Manager InstallThere is no Fedora package provided yet. Please use  below.Manual InstallTo install manually you first install the dependencies using your package manager, and then install  using the provided . The manual install can be done locally or system wide.Install Dependencies Using Package ManagerFor the  implementation, both plain  and the Ayatana  are supported.Ubuntu/DebianPartial list of Python 3 dependencies (after the upgrade to GTK+ 3 in PR #112. If you discover the correct deps, please submit a PR):sudo apt-get install python3-pexpect python3-distutils python3-xdg gir1.2-ayatanaappindicator3-0.1 gir1.2-gtk-3.0 redshift
Out of date Python 2 dependencies; the remaining Python 3 deps should be similar:sudo apt-get install git python-gconf python-gtk2 python-glade2 libxxf86vm1 libcanberra-gtk-module
Fedora/CentOSWARNING: these dependencies may be out of date after the upgrade to GTK+ 3 in PR #112. If you discover the correct deps, please submit a PR.sudo yum install git python-appindicator python2-pyxdg python3-pexpect gnome-python2-gconf pygtk2 pygtk2-libglade redshift
Install There are separate instructions in the code below for installing system wide and for installing locally in your user directory; choose one.# Download fluxgui
cd /tmp
git clone ""https://github.com/xflux-gui/fluxgui.git""
cd fluxgui
./download-xflux.py

# EITHER install system wide
sudo ./setup.py install --record installed.txt

# EXCLUSIVE OR, install in your home directory
#
# The fluxgui program installs
# into ~/.local/bin, so be sure to add that to your PATH if installing
# locally. In particular, autostarting fluxgui in Gnome will not work
# if the locally installed fluxgui is not on your PATH.
./setup.py install --user --record installed.txt
       
# Run flux
fluxgui
Manual UninstallIf you manually installed instead of using package manager, you can uninstallby making  tell you where it installed files and thenremoving the installed files.# EITHER uninstall globally
#
# The 'installed.txt' is generated when you install. Reinstall first if you
# as described above if you don't have an 'installed.txt' file.
sudo xargs rm -vr < installed.txt
sudo glib-compile-schemas ""$(dirname ""$(grep apps.fluxgui.gschema.xml installed.txt)"")""

# EXCLUSIVE OR uninstall in your home directory
xargs rm -vr < installed.txt
glib-compile-schemas ""$(dirname ""$(grep apps.fluxgui.gschema.xml installed.txt)"")""
LicenseThe  applet is released under the . The underlying  program that actually controls the screen color is closed source.DevelopingCoding StyleTry to stick to the same coding style that is already used in the file you are editing.In particular, don't change the style of code you're not already editing for some otherreason. Style changes create noise in the Git history and make the  outputmisleading. When reviewing a PR, the maintainers want to focus on the logical changesintroduced by your code, and extraneous style changes make that harder.Running  Without InstallingWhen working on , you can usecd <path to your fluxgui.git clone>
# You only need to download xflux once.
./download-xflux.py
glib-compile-schemas .
GSETTINGS_SCHEMA_DIR=`pwd` PATH=`pwd`:$PATH PYTHONPATH=`pwd`/src:$PYTHONPATH ./fluxgui
to test your local copy of  without installing anything.Change Logs, Versions, ReleasesNote changes in .Use version  until ready to release a version. Whenreleasing a version remove the  suffix from the version stringsand commit, copying the changelog changes for the current release intothe commit message. Then , using the commit msg forthe tag annotation, and push the version tag with . Finally, create another commit with the new  version strings and changelog entry.When releasing the version string needs to be changed in and , and the release dates needs to beadded in ."
https://github.com/fortra/impacket,Impacket is a collection of Python classes for working with network protocols.,"ImpacketFORTRA. Copyright (C) 2023 Fortra. All rights reserved.Impacket was originally created by , and now maintained by Fortra's Core Security.Impacket is a collection of Python classes for working with networkprotocols. Impacket is focused on providing low-levelprogrammatic access to the packets and for some protocols (e.g.SMB1-3 and MSRPC) the protocol implementation itself.Packets can be constructed from scratch, as well as parsed fromraw data, and the object-oriented API makes it simple to work withdeep hierarchies of protocols. The library provides a set of toolsas examples of what can be done within the context of this library.What protocols are featured?MaintainerTable of ContentsGetting ImpacketLatest versionDevelopment versionSetupQuick startIn order to grab the latest stable release run:python3 -m pipx install impacket
If you want to play with the unreleased changes, download the developmentversion from the ,extract the package, and execute the following command from thedirectory where Impacket has been unpacked:python3 -m pipx install .
Docker SupportBuild Impacket's image:  $ docker build -t ""impacket:latest"" .
Using Impacket's image:  $ docker run -it --rm ""impacket:latest""
TestingThe library leverages the  framework for organizingand marking test cases,  to automate the process ofrunning them across supported Python versions, and to obtain coverage statistics.A  is available.LicensingThis software is provided under a slightly modified version ofthe Apache Software License. See the accompanying  file formore information.SMBv1 and NetBIOS support based on Pysmb by Michael Teo.DisclaimerThe spirit of this Open Source initiative is to help security researchers,and the community, speed up research and educational activities related tothe implementation of networking protocols and stacks.The information in this repository is for research and educational purposesand not meant to be used in production environments and/or as partof commercial products.If you desire to use this code or some part of it for your own uses, werecommend applying proper security development life cycle and secure codingpractices, as well as generate and track the respective indicators ofcompromise according to your needs.Contact UsWhether you want to report a bug, send a patch, or give some suggestionson this package, reach out to us at https://www.coresecurity.com/about/contact.For security-related questions check our ."
https://github.com/Yorko/mlcourse.ai,Open Machine Learning Course,"[<marko.inline.RawText object at 0x000001592FE30488>] is an open Machine Learning course by , led by . Having both a Ph.D. degree in applied math and a Kaggle Competitions Master tier, Yury aimed at designing an ML course with a perfect balance between theory and practice. Thus, the course meets you with math formulae in lectures, and a lot of practice in a form of assignments and  Kaggle Inclass competitions. Currently, the course is in a self-paced mode. Here we guide you through the self-paced .Bonus:Additionally, you can purchase a Bonus Assignments pack with the best non-demo versions of  assignments. Select the . Refer to the details of the deal on the main page .Mirrors (:uk:-only):  (main site),  (same notebooks as Kaggle Notebooks)Self-paced passingYou are guided through 10 weeks of . For each week, from Pandas to Gradient Boosting, instructions are given on which articles to read, lectures to watch, what assignments to accomplish.ArticlesThis is the list of published articles on medium.com , habr.com . Also notebooks in Chinese are mentioned :cn: and links to Kaggle Notebooks (in English) are given. Icons are clickable.LecturesVideolectures are uploaded to  YouTube playlist.Introduction, , AssignmentsThe following are demo-assignments. Additionally, within the  you can get access to non-demo assignments.Bonus assignmentsAdditionally, you can purchase a Bonus Assignments pack with the best non-demo versions of  assignments. Select the  on Patreon or a  on Boosty (rus).mlcourse.ai is still in self-paced mode but we offer you Bonus Assignments with solutions for a contribution of $17/month. The idea is that you pay for ~1-5 months while studying the course materials, but a single contribution is still fine and opens your access to the bonus pack.Note: the first payment is charged at the moment of joining the Tier Patreon, and the next payment is charged on the 1st day of the next month, thus it's better to purchase the pack in the 1st half of the month.mlcourse.ai is never supposed to go fully monetized (it's created in the wonderful open ODS.ai community and will remain open and free) but it'd help to cover some operational costs, and Yury also put in quite some effort into assembling all the best assignments into one pack. Please note that unlike the rest of the course content, Bonus Assignments are copyrighted. Informally, Yury's fine if you share the pack with 2-3 friends but public sharing of the Bonus Assignments pack is prohibited.The bonus pack contains 10 assignments, in some of them you are challenged to beat a baseline in a Kaggle competition under thorough guidance ( and ) or implement an algorithm from scratch -- efficient stochastic gradient descent  and .Kaggle competitionsCiting mlcourse.aiIf you happen to cite  in your work, you can use this BibTeX record:@misc{mlcourse_ai,
    author = {Kashnitsky, Yury},
    title = {mlcourse.ai – Open Machine Learning Course},
    year = {2020},
    publisher = {GitHub},
    journal = {GitHub repository},
    howpublished = {\url{https://github.com/Yorko/mlcourse.ai}},
}
CommunityYou can join the  Slack community to ask questions on the course materials. The community is mostly Russian-speaking but questions in English are still welcome. "
https://github.com/JiahuiYu/generative_inpainting,"DeepFill v1/v2 with Contextual Attention and Gated Convolution, CVPR 2018, and ICCV 2019 Oral","Generative Image InpaintingAn open source framework for generative image inpainting task, with the support of  (CVPR 2018) and  (ICCV 2019 Oral).For the code of previous version (DeepFill v1), please checkout branch  |  |  |  |  |  |     Free-form image inpainting results by our system built on gated convolution. Each triad shows original image, free-form input and our result from left to right.RunPretrained models | Download the model dirs and put it under  (rename  to  because google drive automatically add ext after download). Run testing or resume training as described above. All models are trained with images of resolution 256x256 and largest hole size 128x128, above which the results may be deteriorated. We provide several example test cases. Please run:# Places2 512x680 input
python test.py --image examples/places2/case1_input.png --mask examples/places2/case1_mask.png --output examples/places2/case1_output.png --checkpoint_dir model_logs/release_places2_256
# CelebA-HQ 256x256 input
# Please visit CelebA-HQ demo at: jhyu.me/deepfill
Note: Please make sure the mask file completely cover the masks in input file. You may check it with saving a new image to visualize .TensorBoardVisualization on  for training and validation is supported. Run  to view training progress.LicenseCC 4.0 Attribution-NonCommercial InternationalThe software is for educational and academic research purposes only.Citing@article{yu2018generative,
  title={Generative Image Inpainting with Contextual Attention},
  author={Yu, Jiahui and Lin, Zhe and Yang, Jimei and Shen, Xiaohui and Lu, Xin and Huang, Thomas S},
  journal={arXiv preprint arXiv:1801.07892},
  year={2018}
}

@article{yu2018free,
  title={Free-Form Image Inpainting with Gated Convolution},
  author={Yu, Jiahui and Lin, Zhe and Yang, Jimei and Shen, Xiaohui and Lu, Xin and Huang, Thomas S},
  journal={arXiv preprint arXiv:1806.03589},
  year={2018}
}
"
https://github.com/princewen/tensorflow_practice,tensorflow实战练习，包括强化学习、推荐系统、nlp等,Tensroflow练习相关数据集下载地址：链接:https://pan.baidu.com/s/1GMv7_3qruoVZBJMvN-afGA  密码:ako7基于tf1.4目录1、基础2、自然语言相关3、强化学习相关4、推荐系统5、GAN推荐阅读1、基础2、自然语言相关3、强化学习相关4、推荐系统5、GAN
https://github.com/nccgroup/ScoutSuite,Multi-Cloud Security Auditing Tool,"DescriptionScout Suite is an open source multi-cloud security-auditing tool, which enables security posture assessment of cloud environments. Using the APIs exposed by cloud providers, Scout Suite gathers configuration data for manual inspection and highlights risk areas. Rather than going through dozens of pages on the web consoles, Scout Suite presents a clear view of the attack surface automatically.Scout Suite was designed by security consultants/auditors. It is meant to provide a point-in-time security-oriented view of the cloud account it was run in. Once the data has been gathered, all usage may be performed offline.The project team can be contacted at .Cloud Provider SupportThe following cloud providers are currently supported:InstallationRefer to the .UsageScout Suite is run through the CLI:Once this has completed, it will generate an HTML report including findings and Cloud account configuration:The above report was generated by running Scout Suite against https://github.com/nccgroup/sadcloud.Additional information can be found in the .There are also a number of handy  for automation of common tasks."
https://github.com/baowenbo/DAIN,Depth-Aware Video Frame Interpolation (CVPR 2019),"DAIN (Depth-Aware Video Frame Interpolation) | ,,,Xiaoyun Zhang,Zhiyong Gao,and IEEE Conference on Computer Vision and Pattern Recognition, Long Beach, CVPR 2019This work is developed based on our TPAMI work , where we propose the adaptive warping layer. Please also consider referring to it.Table of ContentsIntroductionWe propose the Depth-Aware video frame INterpolation (DAIN) model to explicitly detect the occlusion by exploring the depth cue.We develop a depth-aware flow projection layer to synthesize intermediate flows that preferably sample closer objects than farther ones.Our method achieves state-of-the-art performance on the Middlebury dataset.We provide videos .CitationIf you find the code and datasets useful in your research, please cite:@inproceedings{DAIN,
    author    = {Bao, Wenbo and Lai, Wei-Sheng and Ma, Chao and Zhang, Xiaoyun and Gao, Zhiyong and Yang, Ming-Hsuan}, 
    title     = {Depth-Aware Video Frame Interpolation}, 
    booktitle = {IEEE Conference on Computer Vision and Pattern Recognition},
    year      = {2019}
}
@article{MEMC-Net,
     title={MEMC-Net: Motion Estimation and Motion Compensation Driven Neural Network for Video Interpolation and Enhancement},
     author={Bao, Wenbo and Lai, Wei-Sheng, and Zhang, Xiaoyun and Gao, Zhiyong and Yang, Ming-Hsuan},
     journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
     doi={10.1109/TPAMI.2019.2941941},
     year={2018}
}
Requirements and DependenciesInstallationDownload repository:$ git clone https://github.com/baowenbo/DAIN.git
Before building Pytorch extensions, be sure you have :$ python -c ""import torch; print(torch.__version__)""
Generate our PyTorch extensions:$ cd DAIN
$ cd my_package 
$ ./build.sh
Generate the Correlation package required by :$ cd ../PWCNet/correlation_package_pytorch1_0
$ ./build.sh
Testing Pre-trained ModelsMake model weights dir and Middlebury dataset dir:$ cd DAIN
$ mkdir model_weights
$ mkdir MiddleBurySet
Download pretrained models, $ cd model_weights
$ wget http://vllab1.ucmerced.edu/~wenbobao/DAIN/best.pth
and Middlebury dataset:$ cd ../MiddleBurySet
$ wget http://vision.middlebury.edu/flow/data/comp/zip/other-color-allframes.zip
$ unzip other-color-allframes.zip
$ wget http://vision.middlebury.edu/flow/data/comp/zip/other-gt-interp.zip
$ unzip other-gt-interp.zip
$ cd ..
preinstallations:$ cd PWCNet/correlation_package_pytorch1_0
$ sh build.sh
$ cd ../my_package
$ sh build.sh
$ cd ..
We are good to go by:$ CUDA_VISIBLE_DEVICES=0 python demo_MiddleBury.py
The interpolated results are under , where the  is used to distinguish different runnings. Downloading ResultsOur DAIN model achieves the state-of-the-art performance on the UCF101, Vimeo90K, and Middlebury ( and other).Download our interpolated results with:$ wget http://vllab1.ucmerced.edu/~wenbobao/DAIN/UCF101_DAIN.zip
$ wget http://vllab1.ucmerced.edu/~wenbobao/DAIN/Vimeo90K_interp_DAIN.zip
$ wget http://vllab1.ucmerced.edu/~wenbobao/DAIN/Middlebury_eval_DAIN.zip
$ wget http://vllab1.ucmerced.edu/~wenbobao/DAIN/Middlebury_other_DAIN.zip
Slow-motion GenerationOur model is fully capable of generating slow-motion effect with minor modification on the network architecture.Run the following code by specifying  to generate x4 slow-motion effect:$ CUDA_VISIBLE_DEVICES=0 python demo_MiddleBury_slowmotion.py --netName DAIN_slowmotion --time_step 0.25
or set  to  or  as follows $ CUDA_VISIBLE_DEVICES=0 python demo_MiddleBury_slowmotion.py --netName DAIN_slowmotion --time_step 0.125
$ CUDA_VISIBLE_DEVICES=0 python demo_MiddleBury_slowmotion.py --netName DAIN_slowmotion --time_step 0.1
to generate x8 and x10 slow-motion respectively. Or if you would like to have x100 slow-motion for a little fun.$ CUDA_VISIBLE_DEVICES=0 python demo_MiddleBury_slowmotion.py --netName DAIN_slowmotion --time_step 0.01
You may also want to create gif animations by:$ cd MiddleBurySet/other-result-author/[random number]/Beanbags
$ convert -delay 1 *.png -loop 0 Beanbags.gif //1*10ms delay 
Have fun and enjoy yourself! Training New ModelsDownload the Vimeo90K triplet dataset for video frame interpolation task, also see  by .$ cd DAIN
$ mkdir /path/to/your/dataset & cd /path/to/your/dataset 
$ wget http://data.csail.mit.edu/tofu/dataset/vimeo_triplet.zip
$ unzip vimeo_triplet.zip
$ rm vimeo_triplet.zip
Download the pretrained MegaDepth and PWCNet models$ cd MegaDepth/checkpoints/test_local
$ wget http://vllab1.ucmerced.edu/~wenbobao/DAIN/best_generalization_net_G.pth
$ cd ../../../PWCNet
$ wget http://vllab1.ucmerced.edu/~wenbobao/DAIN/pwc_net.pth.tar
$ cd  ..
Run the training script:$ CUDA_VISIBLE_DEVICES=0 python train.py --datasetPath /path/to/your/dataset --batch_size 1 --save_which 1 --lr 0.0005 --rectify_lr 0.0005 --flow_lr_coe 0.01 --occ_lr_coe 0.0 --filter_lr_coe 1.0 --ctx_lr_coe 1.0 --alpha 0.0 1.0 --patience 4 --factor 0.2
The optimized models will be saved to the  directory, where [random number] is generated for different runs.Replace the pre-trained  model with the newly trained  model.Then test the new model by executing: $ CUDA_VISIBLE_DEVICES=0 python demo_MiddleBury.py
Google Colab DemoThis is a modification of DAIN that allows the usage of Google Colab and is able to do a full demo interpolation from a source video to a target video.Original Notebook File by btahir can be found .To use the Colab, follow these steps:Colab file authors:  and .Contact; LicenseSee "
https://github.com/kkroening/ffmpeg-python,Python bindings for FFmpeg - with complex filtering support,"ffmpeg-python: Python bindings for FFmpegOverviewThere are tons of Python FFmpeg wrappers out there but they seem to lack complex filter support.   works well for simple as well as complex signal graphs.QuickstartFlip a video horizontally:import ffmpeg
stream = ffmpeg.input('input.mp4')
stream = ffmpeg.hflip(stream)
stream = ffmpeg.output(stream, 'output.mp4')
ffmpeg.run(stream)
Or if you prefer a fluent interface:import ffmpeg
(
    ffmpeg
    .input('input.mp4')
    .hflip()
    .output('output.mp4')
    .run()
)
Complex filter graphsFFmpeg is extremely powerful, but its command-line interface gets really complicated rather quickly - especially when working with signal graphs and doing anything more than trivial.Take for example a signal graph that looks like this:The corresponding command-line arguments are pretty gnarly:ffmpeg -i input.mp4 -i overlay.png -filter_complex ""[0]trim=start_frame=10:end_frame=20[v0];\
    [0]trim=start_frame=30:end_frame=40[v1];[v0][v1]concat=n=2[v2];[1]hflip[v3];\
    [v2][v3]overlay=eof_action=repeat[v4];[v4]drawbox=50:50:120:120:red:t=5[v5]""\
    -map [v5] output.mp4
Maybe this looks great to you, but if you're not an FFmpeg command-line expert, it probably looks alien.If you're like me and find Python to be powerful and readable, it's easier with :import ffmpeg

in_file = ffmpeg.input('input.mp4')
overlay_file = ffmpeg.input('overlay.png')
(
    ffmpeg
    .concat(
        in_file.trim(start_frame=10, end_frame=20),
        in_file.trim(start_frame=30, end_frame=40),
    )
    .overlay(overlay_file.hflip())
    .drawbox(50, 50, 120, 120, color='red', thickness=5)
    .output('out.mp4')
    .run()
)
 takes care of running  with the command-line arguments that correspond to the above filter diagram, in familiar Python terms.Real-world signal graphs can get a heck of a lot more complex, but  handles arbitrarily large (directed-acyclic) signal graphs.InstallationInstalling The latest version of  can be acquired via a typical pip install:pip install ffmpeg-python
Or the source can be cloned and installed from locally:git clone git@github.com:kkroening/ffmpeg-python.git
pip install -e ./ffmpeg-python
Installing FFmpegBefore using , FFmpeg must be installed and accessible via the  environment variable.There are a variety of ways to install FFmpeg, such as the , or using your package manager of choice (e.g.  on Debian/Ubuntu,  on OS X, etc.).Regardless of how FFmpeg is installed, you can check if your environment path is set correctly by running the  command from the terminal, in which case the version information should appear, as in the following example (truncated for brevity):$ ffmpeg
ffmpeg version 4.2.4-1ubuntu0.1 Copyright (c) 2000-2020 the FFmpeg developers
  built with gcc 9 (Ubuntu 9.3.0-10ubuntu2)
When in doubt, take a look at the  to see if there's something that's close to whatever you're trying to do.Here are a few:See the  for additional examples.Custom FiltersDon't see the filter you're looking for?  While  includes shorthand notation for some of the most commonly used filters (such as ), all filters can be referenced via the  operator:stream = ffmpeg.input('dummy.mp4')
stream = ffmpeg.filter(stream, 'fps', fps=25, round='up')
stream = ffmpeg.output(stream, 'dummy2.mp4')
ffmpeg.run(stream)
Or fluently:(
    ffmpeg
    .input('dummy.mp4')
    .filter('fps', fps=25, round='up')
    .output('dummy2.mp4')
    .run()
)
Special option names:Arguments with special names such as  (variable bitrate),  (constant bitrate), etc. can be specified as a keyword-args dictionary as follows:(
    ffmpeg
    .input('in.mp4')
    .output('out.mp4', **{'qscale:v': 3})
    .run()
)
Multiple inputs:Filters that take multiple input streams can be used by passing the input streams as an array to :main = ffmpeg.input('main.mp4')
logo = ffmpeg.input('logo.png')
(
    ffmpeg
    .filter([main, logo], 'overlay', 10, 10)
    .output('out.mp4')
    .run()
)
Multiple outputs:Filters that produce multiple outputs can be used with :split = (
    ffmpeg
    .input('in.mp4')
    .filter_multi_output('split')  # or `.split()`
)
(
    ffmpeg
    .concat(split[0], split[1].reverse())
    .output('out.mp4')
    .run()
)
(In this particular case,  is the equivalent shorthand, but the general approach works for other multi-output filters)String expressions:Expressions to be interpreted by ffmpeg can be included as string parameters and reference any special ffmpeg variable names:(
    ffmpeg
    .input('in.mp4')
    .filter('crop', 'in_w-2*10', 'in_h-2*20')
    .input('out.mp4')
)
When in doubt, refer to the , , and/or the .Frequently asked questionsWhy do I get an import/attribute/etc. error from Make sure you ran  and [<marko.inline.RawText object at 0x000001592FDC8FC8>]  (wrong) or  (also wrong).Why did my audio stream get dropped?Some ffmpeg filters drop audio streams, and care must be taken to preserve the audio in the final output.  The  and  operators can be used to reference the audio/video portions of a stream so that they can be processed separately and then re-combined later in the pipeline.This dilemma is intrinsic to ffmpeg, and ffmpeg-python tries to stay out of the way while users may refer to the official ffmpeg documentation as to why certain filters drop audio.As usual, take a look at the  (Audio/video pipeline in particular).How can I find out the used command line arguments?You can run  before  to retrieve the command line arguments that will be passed to . You can also run  that also includes the  executable as the first argument.How do I do XYZ?Take a look at each of the links in the  section at the end of this README.  If you look everywhere and can't find what you're looking for and have a question that may be relevant to other users, you may open an issue asking how to do it, while providing a thorough explanation of what you're trying to do and what you've tried so far.Issues not directly related to  or issues asking others to write your code for you or how to do the work of solving a complex signal processing problem for you that's not relevant to other users will be closed.That said, we hope to continue improving our documentation and provide a community of support for people using  to do cool and exciting things.ContributingOne of the best things you can do to help make  better is to answer  in the issue tracker.  The questions that are answered will be tagged and incorporated into the documentation, examples, and other learning resources.If you notice things that could be better in the documentation or overall development experience, please say so in the .  And of course, feel free to report any bugs or submit feature requests.Pull requests are welcome as well, but it wouldn't hurt to touch base in the issue tracker or hop on the  first.Anyone who fixes any of the  or implements  is a hero, but changes should include passing tests.Running testsgit clone git@github.com:kkroening/ffmpeg-python.git
cd ffmpeg-python
virtualenv venv
. venv/bin/activate  # (OS X / Linux)
venv\bin\activate    # (Windows)
pip install -e .[dev]
pytest
Special thanksAdditional Resources"
https://github.com/metabrainz/picard,MusicBrainz Picard audio file tagger,"MusicBrainz Picard is a cross-platform (Linux, macOS, Windows) audio tagging application. It is the official  tagger.Picard supports the majority of audio file formats, is capable of using audio fingerprints (), performing CD lookups and  submissions, and it has excellent Unicode support. Additionally, there are several plugins available that extend Picard's features.When tagging files, Picard uses an album-oriented approach. This approach allows it to utilize the MusicBrainz data as effectively as possible and correctly tag your music. For more information,  and the .FeaturesInstallationBinary downloads are available on the .Support and issue reportingPlease report all bugs and feature requests in the . If you need support in using Picard please read the  first and have a look at the .TriviaPicard is named after  from the TV series ."
https://github.com/hack4impact/flask-base,"A simple Flask boilerplate app with SQLAlchemy, Redis, User Authentication, and more.","flask-baseA Flask application template with the boilerplate code already done for you.Documentation available at What's included?DemosHome Page:Registering User:Admin Editing Page:Admin Editing Users:Setting upCreate your own repository from this TemplateNavigate to the  and click the big, green ""Use this template"" button at the top right of the page. Give your new repository a name and save it.Clone the repository$ git clone https://github.com/YOUR_USERNAME/REPO_NAME.git
$ cd REPO_NAME
Initialize a virtual environmentWindows:$ python3 -m venv venv
$ venv\Scripts\activate.bat
Unix/MacOS:$ python3 -m venv venv
$ source venv/bin/activate
Learn more in .Note: if you are using a python before 3.3, it doesn't come with venv. Install  with pip instead.(If you're on a Mac) Make sure xcode tools are installed$ xcode-select --install
Add Environment VariablesCreate a file called  that contains environment variables. Very important: do not include the  You will manually maintain this file locally, and keep it in sync on your host.Variables declared in file have the following format: . You may also wrap values in double quotes like .Other useful variables include:| Variable        | Default   | Discussion  || --------------- |-------------| -----||    |  | email for your first admin account || |                      | password for your first admin account ||   |               | Database URL. Can be Postgres, sqlite, etc. ||  |         |  URL or any redis server url ||  |                          | API key for , a crash and performance monitoring service ||   |                       | can be , , , , , or . Most of the time you will use  or . |Install the dependencies$ pip install -r requirements.txt
Other dependencies for running locallyYou need , and . Chances are, these commands will work:Sass:$ gem install sass
Redis:Mac (using $ brew install redis
Linux:$ sudo apt-get install redis-server
You will also need to install PostgresQLMac (using homebrew):brew install postgresql
Linux (based on this sudo apt-get install libpq-dev
Create the database$ python manage.py recreate_db
Other setup (e.g. creating roles in database)$ python manage.py setup_dev
Note that this will create an admin user with email and password specified by the  and  config variables. If not specified, they are both  and  respectively.[Optional] Add fake data to the database$ python manage.py add_fake_data
Running the app$ source env/bin/activate
$ honcho start -e config.env -f Local
For Windows users having issues with binding to a redis port locally, refer to .Gettin up and running with Docker and docker-compose:Clone the repository$ git clone https://github.com/YOUR_USERNAME/REPO_NAME.git
$ cd REPO_NAME
Create and run the images:$ docker-compose up
Create database and initial data for development:$ docker-compose exec server ./init_database.sh
It will deploy 5 docker images:Formatting codeBefore you submit changes to flask-base, you may want to autoformat your code with .ContributingContributions are welcome! Please refer to our  for more information.Documentation ChangesTo make changes to the documentation refer to the  for setup.To create a new documentation page, add a file to the  directory and edit  to reference the file.When the new files are merged into  and pushed to github. Run  to update the online documentation.Relatedhttps://medium.freecodecamp.com/how-we-got-a-2-year-old-repo-trending-on-github-in-just-48-hours-12151039d78b#.se9jwnfk5License"
https://github.com/abewley/sort,"Simple, online, and realtime tracking of multiple objects in a video sequence.","SORTA simple online and realtime tracking algorithm for 2D multiple object tracking in video sequences.See an example .By Alex Bewley  IntroductionSORT is a barebones implementation of a visual multiple object tracking framework based on rudimentary data association and state estimation techniques. It is designed for online tracking applications where only past and current frames are available and the method produces object identities on the fly. While this minimalistic tracker doesn't handle occlusion or re-entering objects its purpose is to serve as a baseline and testbed for the development of future trackers.SORT was initially described in . At the time of the initial publication, SORT was ranked the best open source multiple object tracker on the .Note: A significant proportion of SORT's accuracy is attributed to the detections.For your convenience, this repo also contains Faster RCNN detections for the MOT benchmark sequences in the . To run the detector yourself please see the original  or the python reimplementation of  by Ross Girshick.Also see:A new and improved version of SORT with a Deep Association Metric implemented in tensorflow is available at  .LicenseSORT is released under the GPL License (refer to the LICENSE file for details) to promote the open use of the tracker and future improvements. If you require a permissive license contact Alex (alex@bewley.ai).Citing SORTIf you find this repo useful in your research, please consider citing:@inproceedings{Bewley2016_sort,
  author={Bewley, Alex and Ge, Zongyuan and Ott, Lionel and Ramos, Fabio and Upcroft, Ben},
  booktitle={2016 IEEE International Conference on Image Processing (ICIP)},
  title={Simple online and realtime tracking},
  year={2016},
  pages={3464-3468},
  keywords={Benchmark testing;Complexity theory;Detectors;Kalman filters;Target tracking;Visualization;Computer Vision;Data Association;Detection;Multiple Object Tracking},
  doi={10.1109/ICIP.2016.7533003}
}
Dependencies:To install required dependencies run:$ pip install -r requirements.txt
Demo:To run the tracker with the provided detections:$ cd path/to/sort
$ python sort.py
To display the results you need to:$ ln -s /path/to/MOT2015_challenge/data/2DMOT2015 mot_benchmark
$ python sort.py --display
Main ResultsUsing the  the method produces the following results (as described in the paper).Sequence       | Rcll | Prcn |  FAR | GT  MT  PT  ML|   FP    FN  IDs   FM|  MOTA  MOTP MOTAL--------------- |:----:|:----:|:----:|:-------------:|:-------------------:|:------------------:TUD-Campus     | 68.5 | 94.3 | 0.21 |  8   6   2   0|   15   113    6    9|  62.7  73.7  64.1ETH-Sunnyday   | 77.5 | 81.9 | 0.90 | 30  11  16   3|  319   418   22   54|  59.1  74.4  60.3ETH-Pedcross2  | 51.9 | 90.8 | 0.39 | 133  17  60  56|  330  3014   77  103|  45.4  74.8  46.6ADL-Rundle-8   | 44.3 | 75.8 | 1.47 | 28   6  16   6|  959  3781  103  211|  28.6  71.1  30.1Venice-2       | 42.5 | 64.8 | 2.75 | 26   7   9  10| 1650  4109   57  106|  18.6  73.4  19.3KITTI-17       | 67.1 | 92.3 | 0.26 |  9   1   8   0|   38   225    9   16|  60.2  72.3  61.3Overall      | 49.5 | 77.5 | 1.24 | 234  48 111  75| 3311 11660  274  499|  34.0  73.3  35.1Using SORT in your own projectBelow is the gist of how to instantiate and update SORT. See the  section of  for a complete example.from sort import *

#create instance of SORT
mot_tracker = Sort() 

# get detections
...

# update SORT
track_bbs_ids = mot_tracker.update(detections)

# track_bbs_ids is a np array where each row contains a valid bounding box and track_id (last column)
...
"
https://github.com/scipy-lectures/scientific-python-lectures,Tutorial material on the scientific Python ecosystem,".. image:: https://zenodo.org/badge/doi/10.5281/zenodo.594102.svg:target: https://dx.doi.org/10.5281/zenodo.594102.. image:: https://github.com/scipy-lectures/scientific-python-lectures/workflows/test/badge.svg?branch=main:target: https://github.com/scipy-lectures/scientific-python-lectures/actions?query=workflow%3A%22test%22==========================Scientific Python LecturesThis repository gathers some lectures on the scientific Pythonecosystem that can be used for a full course of scientific computing withPython.These documents are written with the rest markup language (extension) and built using _.You can view the online version at: https://lectures.scientific-python.org/Reusing and distributingAs stated in the  file, this material comes with no stringsattached. Feel free to reuse and modify for your own teaching purposes.However, we would like this reference material to be improved over time,thus we encourage people to contribute back changes. These will bereviewed and edited by the original authors and the editors.Building and contributingThe file  contains instructions to build from sourceand to contribute."
https://github.com/ReactiveX/RxPY,ReactiveX for Python,"===============================The ReactiveX for Python (RxPY).. image:: https://github.com/ReactiveX/RxPY/workflows/Python%20package/badge.svg:target: https://github.com/ReactiveX/RxPY/actions:alt: Build Status.. image:: https://img.shields.io/coveralls/ReactiveX/RxPY.svg:target: https://coveralls.io/github/ReactiveX/RxPY:alt: Coverage Status.. image:: https://img.shields.io/pypi/v/reactivex.svg:target: https://pypi.org/project/reactivex/:alt: PyPY Package Version.. image:: https://img.shields.io/readthedocs/rxpy.svg:target: https://readthedocs.org/projects/rxpy/builds/:alt: Documentation StatusA library for composing asynchronous and event-based programs using observableReactiveX for Python v4For v3.X please go to the _.ReactiveX for Python v4.x runs on _ 3.7 or above. Toinstall:.. code:: consolepip3 install reactivex
About ReactiveXReactiveX for Python (RxPY) is a library for composing asynchronous and event-basedprograms using observable sequences and pipable query operators in Python. Using Rx,developers represent asynchronous data streams with Observables, query asynchronous datastreams using operators, and parameterize concurrency in data/event streams usingSchedulers... code:: pythonimport reactivex as rx
from reactivex import operators as ops

source = rx.of(""Alpha"", ""Beta"", ""Gamma"", ""Delta"", ""Epsilon"")

composed = source.pipe(
    ops.map(lambda s: len(s)),
    ops.filter(lambda i: i >= 5)
)
composed.subscribe(lambda value: print(""Received {0}"".format(value)))
Learning ReactiveXRead the _ to learnthe principles of ReactiveX and get the complete reference of the availableoperators.If you need to migrate code from RxPY v1.x or v3.x, read the _ section.There is also a list of third party documentation available _.CommunityJoin the conversation on GitHub _! if you have any questions orsuggestions.Differences from .NET and RxJSReactiveX for Python is a fairly complete implementation of_ with more than, and. RxPYis mostly a direct port of RxJS, but also borrows a bit from Rx.NET and RxJava interms of threading and blocking operators.ReactiveX for Python follows _, soall function and method names are  i.e lowercase with words separated byunderscores as necessary to improve readability.Thus .NET code such as:.. code:: c#var group = source.GroupBy(i => i % 3);
need to be written with an  in Python:.. code:: pythongroup = source.pipe(ops.group_by(lambda i: i % 3))
With ReactiveX for Python you should use _ instead of positional arguments when anoperator has multiple optional arguments. RxPY will not try to detect which argumentsyou are giving to the operator (or not).DevelopmentThis project is managed using . Code is formatted, . Code is statically type checked using  and _.If you want to take advantage of the default VSCode integration, thenfirst configure Poetry to make its virtual environment in therepository:.. code:: consolepoetry config virtualenvs.in-project true
After cloning the repository, activate the tooling:.. code:: consolepoetry install
poetry run pre-commit install
Run unit tests:.. code:: consolepoetry run pytest
Run code checks (manually):.. code:: consolepoetry run pre-commit run --all-files
"
https://github.com/HIT-SCIR/ltp,Language Technology Platform,"| Language                             | version                                                                                                                                                                                                                                                                                                                 || ------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ||  |    ||            |                                                                                                                                                                                                                                    |LTP 4LTP（Language Technology Platform） 提供了一系列中文自然语言处理工具，用户可以使用这些工具对于中文文本进行分词、词性标注、句法分析等等工作。引用如果您在工作中使用了 LTP，您可以引用这篇论文@inproceedings{che-etal-2021-n,
    title = ""N-{LTP}: An Open-source Neural Language Technology Platform for {C}hinese"",
    author = ""Che, Wanxiang  and
      Feng, Yunlong  and
      Qin, Libo  and
      Liu, Ting"",
    booktitle = ""Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations"",
    month = nov,
    year = ""2021"",
    address = ""Online and Punta Cana, Dominican Republic"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.emnlp-demo.6"",
    doi = ""10.18653/v1/2021.emnlp-demo.6"",
    pages = ""42--49"",
    abstract = ""We introduce N-LTP, an open-source neural language technology platform supporting six fundamental Chinese NLP tasks: lexical analysis (Chinese word segmentation, part-of-speech tagging, and named entity recognition), syntactic parsing (dependency parsing), and semantic parsing (semantic dependency parsing and semantic role labeling). Unlike the existing state-of-the-art toolkits, such as Stanza, that adopt an independent model for each task, N-LTP adopts the multi-task framework by using a shared pre-trained model, which has the advantage of capturing the shared knowledge across relevant Chinese tasks. In addition, a knowledge distillation method (Clark et al., 2019) where the single-task model teaches the multi-task model is further introduced to encourage the multi-task model to surpass its single-task teacher. Finally, we provide a collection of easy-to-use APIs and a visualization tool to make users to use and view the processing results more easily and directly. To the best of our knowledge, this is the first toolkit to support six Chinese NLP fundamental tasks. Source code, documentation, and pre-trained models are available at https://github.com/HIT-SCIR/ltp."",
}
参考书：由哈工大社会计算与信息检索研究中心（HIT-SCIR）的多位学者共同编著的《》（作者：车万翔、郭江、崔一鸣；主审：刘挺）一书现已正式出版，该书重点介绍了新的基于预训练模型的自然语言处理技术，包括基础知识、预训练词向量和预训练模型三大部分，可供广大 LTP 用户学习参考。更新说明快速使用# 方法 1： 使用清华源安装 LTP
# 1. 安装 PyTorch 和 Transformers 依赖
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple torch transformers
# 2. 安装 LTP
pip install -i https://pypi.tuna.tsinghua.edu.cn/simple ltp ltp-core ltp-extension

# 方法 2： 先全局换源，再安装 LTP
# 1. 全局换 TUNA 源
pip config set global.index-url https://pypi.tuna.tsinghua.edu.cn/simple
# 2. 安装 PyTorch 和 Transformers 依赖
pip install torch transformers
# 3. 安装 LTP
pip install ltp ltp-core ltp-extension
注： 如果遇到任何错误，请尝试使用上述命令重新安装 ltp，如果依然报错，请在 Github issues 中反馈。import torch
from ltp import LTP

# 默认 huggingface 下载，可能需要代理

ltp = LTP(""LTP/small"")  # 默认加载 Small 模型
                        # 也可以传入模型的路径，ltp = LTP(""/path/to/your/model"")
                        # /path/to/your/model 应当存在 config.json 和其他模型文件

# 将模型移动到 GPU 上
if torch.cuda.is_available():
    # ltp.cuda()
    ltp.to(""cuda"")

# 自定义词表
ltp.add_word(""汤姆去"", freq=2)
ltp.add_words([""外套"", ""外衣""], freq=2)

#  分词 cws、词性 pos、命名实体标注 ner、语义角色标注 srl、依存句法分析 dep、语义依存分析树 sdp、语义依存分析图 sdpg
output = ltp.pipeline([""他叫汤姆去拿外衣。""], tasks=[""cws"", ""pos"", ""ner"", ""srl"", ""dep"", ""sdp"", ""sdpg""])
# 使用字典格式作为返回结果
print(output.cws)  # print(output[0]) / print(output['cws']) # 也可以使用下标访问
print(output.pos)
print(output.sdp)

# 使用感知机算法实现的分词、词性和命名实体识别，速度比较快，但是精度略低
ltp = LTP(""LTP/legacy"")
# cws, pos, ner = ltp.pipeline([""他叫汤姆去拿外衣。""], tasks=[""cws"", ""ner""]).to_tuple() # error: NER 需要 词性标注任务的结果
cws, pos, ner = ltp.pipeline([""他叫汤姆去拿外衣。""], tasks=[""cws"", ""pos"", ""ner""]).to_tuple()  # to tuple 可以自动转换为元组格式
# 使用元组格式作为返回结果
print(cws, pos, ner)
[<marko.inline.RawText object at 0x000001592FE76DC8>]use std::fs::File;
use itertools::multizip;
use ltp::{CWSModel, POSModel, NERModel, ModelSerde, Format, Codec};

fn main() -> Result<(), Box<dyn std::error::Error>> {
  let file = File::open(""data/legacy-models/cws_model.bin"")?;
  let cws: CWSModel = ModelSerde::load(file, Format::AVRO(Codec::Deflate))?;
  let file = File::open(""data/legacy-models/pos_model.bin"")?;
  let pos: POSModel = ModelSerde::load(file, Format::AVRO(Codec::Deflate))?;
  let file = File::open(""data/legacy-models/ner_model.bin"")?;
  let ner: NERModel = ModelSerde::load(file, Format::AVRO(Codec::Deflate))?;

  let words = cws.predict(""他叫汤姆去拿外衣。"")?;
  let pos = pos.predict(&words)?;
  let ner = ner.predict((&words, &pos))?;

  for (w, p, n) in multizip((words, pos, ner)) {
    println!(""{}/{}/{}"", w, p, n);
  }

  Ok(())
}
模型性能以及下载地址|                                深度学习模型(🤗HF/🗜 压缩包)                                 | 分词  | 词性  | 命名实体 | 语义角色 | 依存句法 | 语义依存 | 速度(句/S) || :----------------------------------------------------------------------------------------: | :---: | :---: | :------: | :------: | :------: | :------: | :--------: ||       | 98.7  | 98.5  |   95.4   |   80.6   |   89.5   |   75.2   |   39.12    ||   | 99.22 | 98.73 |  96.39   |  79.28   |  89.57   |  76.57   |   --.--    ||   | 99.18 | 98.69 |  95.97   |  79.49   |  90.19   |  76.62   |   --.--    ||   | 98.4  | 98.2  |   94.3   |   78.4   |   88.3   |   74.7   |   43.13    ||       | 96.8  | 97.1  |   91.6   |   70.9   |   83.8   |   70.1   |   53.22    ||                                 感知机算法模型(🤗HF/🗜 压缩包)                                  | 分词  | 词性  | 命名实体 | 速度(句/s) |              备注              || :--------------------------------------------------------------------------------------------: | :---: | :---: | :------: | :--------: | :----------------------------: ||   | 97.93 | 98.41 |  94.28   |  21581.48  |  |注：感知机算法速度为开启 16 线程速度如何下载对应的模型# 使用 HTTP 链接下载
# 确保已安装 git-lfs (https://git-lfs.com)
git lfs install
git clone https://huggingface.co/LTP/base

# 使用 ssh 下载
# 确保已安装 git-lfs (https://git-lfs.com)
git lfs install
git clone git@hf.co:LTP/base

# 下载压缩包
wget http://39.96.43.154/ltp/v4/base.tgz
tar -zxvf base.tgz -C base
如何使用下载的模型from ltp import LTP

# 在路径中给出模型下载或解压后的路径
# 例如：base 模型的文件夹路径为 ""path/to/base""
#      ""path/to/base"" 下应当存在 ""config.json""
ltp = LTP(""path/to/base"")
构建 Wheel 包make bdist
其他语言绑定感知机算法深度学习算法作者信息开源协议"
https://github.com/tweepy/tweepy,Twitter for Python!,"Tweepy: Twitter for Python!InstallationThe easiest way to install the latest version from PyPI is by using:pip install tweepy
To use the  subpackage, be sure to install with the extra:pip install tweepy[async]
You can also use Git to clone the repository from GitHub to install the latestdevelopment version:git clone https://github.com/tweepy/tweepy.git
cd tweepy
pip install .
Alternatively, install directly from the GitHub repository:pip install git+https://github.com/tweepy/tweepy.git
Python 3.7 - 3.11 are supported.Links"
https://github.com/kstenerud/iOS-Universal-Framework,"An XCode project template to build universal frameworks (arm7, arm7s, and simulator) for iOS / iPhone.","FINALLY!With Xcode 6, Apple has added iOS framework support to their build tools, so this repo can at last be retired!Please use Apple's framework target for all new projects, as it is less hacky and is supported by Apple themselves.iOS Universal Framework Mk 8An XCode project template to build universal (arm6, arm7, and simulator)frameworks for iOS.By Karl StenerudNotes2013-10-14:Mk 8 is now out of beta!I haven't been able to solve the problem of deeply nested projects within projects,but the new python scripts have been working in my other projects for over a year now andare quite stable for 90% of use cases.Unfortunately, I don't have the time to solve the last 10% of use cases. As a compromise,I've created a branch ""mk7"" which contains the shell script version of the build system.If Mk8 doesn't work for your unique case, give Mk7 a try.Development will continue in order to keep things working for the other 90% of use cases.If you can help, please feel free to contact me or send pull requests. All scripting is donein Python now. All template development happens within the ""devel"" directory. build.pybuilds the templates and all template source files are in ""src"".2012-06-16:Updating your project to use the new scriptsYou can now update existing projects to use the newest build scripts.Running the update_project.py script will replace your project's universalframework build script with the script in devel/src/BuildFW.py.Before upgrading, please back up your project file!Steps to Upgrade (Mk 7 or earlier):If your project was built using Mk 7 or earlier, delete the first two universalframework build scripts. The first will be right after ""Target Dependencies""and starts with the following (or something close):set -e

set +u
if [[ $UFW_MASTER_SCRIPT_RUNNING ]]
then
    # Nothing for the slave script to do
    exit 0
fi
set -u
The second script is after ""Copy Bundle Resources"" and starts with thefollowing (or something close). Note that this script may not exist in veryearly versions of the framework project:HEADERS_ROOT=$SRCROOT/$PRODUCT_NAME
FRAMEWORK_HEADERS_DIR=""$BUILT_PRODUCTS_DIR/$WRAPPER_NAME/Versions/$FRAMEWORK_VERSION/Headers""

## only header files expected at this point
PUBLIC_HEADERS=$(find $FRAMEWORK_HEADERS_DIR/. -not -type d 2> /dev/null | sed -e ""s@.*/@@g"")
The final script (the one you want to keep) will start with something similarto the first script you deleted.Now proceed with the next steps below.Steps to Upgrade (All versions)The project update script will create a backup (project.pbxproj.orig) of theold project file. To disable this behavior, use the ""-n"" switch.Selecting Framework TypeThe script now requires you to select which kind of framework (normal orembedded) you will be creating, using the config_framework_typeconfiguration variable. Only the selected framework type will be created andshown to the user.Note: Xcode requires the normal framework dir to exist, so when building anembedded framework, the script simply creates a symlink to the copy inside theembeddedframework. Be sure to tell your users not to to copy the regular""framework"" symlink by mistake!2012-06-12:New Build ProcessWhen you build normally (by selecting Build or CMD-B), the project will NO build a universal framework. It will build for the CURRENT!To build a universal framework, you must select Archive from theProduct menu. Upon completing the archive build, it will automatically openthe folder containing the fully built framework.This cuts the compilation time down by 2/3, since it no longer has to do a fullbuild process when building as a dependency.Building From Command LineSince ""archive"" is not a supported xcodebuild build action, you must specifythe env variable ""UFW_ACTION=archive"" in your xcodebuild command to build it asa universal framework.To avoid opening the destination folder when building from command line, setthe env variable ""UFW_OPEN_BUILD_DIR=False"" in your xcodebuild command.Only One ScriptThe initial beta version had 2 scripts: a clean script and a build script. Mk 7has 3 scripts. With the new build process there is only need for one script.Older stuff:Xcode Bugs and their WorkaroundsWhen Xcode creates the initial header and module file for a framework, theheader file won't be included as a member of the framework target (This is abug in Xcode; it does the same thing with Mac frameworks), so you need to dothis manually. In Build Phases under Copy Headers, click the + and addthe header, then drag it to the Public section.The Run Script build phases will have the option Show environment checked. A bug in Xcode causes it to ignore thetemplate setting and leave it checked always. This can cause issues whendiagnosing a build failure because Xcode will only show the first 200 logentries in a build phase, most of which are taken up by spitting out all ofthe environment variables! So be sure to turn it off manually.So to sum up, when starting a new framework project, always do the following:Why a Framework?Distributing libraries in a developer-friendly manner is tricky. You need toinclude not only the library itself, but also any public include files,resources, scripts etc.Apple's solution to this problem is frameworks, which are basically foldersthat follow a standard structure to include everything required to use alibrary. Unfortunately, in disallowing dynamically linked libraries in iOS,Apple also removed static iOS framework creation functionality in XCode.Xcode is still technically capable of building frameworks for iOS, and with alittle tweaking it can be re-enabled.Static frameworks are perfectly acceptable for packaging code intended for theapp store. Despite appearances, it's just a static library at the core.Kinds of FrameworksDynamic FrameworkA dynamic framework is designed to be installed in your operating system andshared by many programs. By default, Xcode only supports dynamic frameworks,and only for Mac since you can't use dynamic frameworks in iOS.Static FrameworkA static framework gets linked into your app like a static library would.However, Xcode doesn't include support for static frameworks. These templatesadd in that support. Frameworks are superior to libraries because they caninclude code as well as public headers in a single package.Embedded FrameworkAlthough frameworks are an improvement over libraries, Xcode ignores anyresources contained within frameworks. So if you have xibs, images, sounds, orother files in your framework, Xcode won't see them. An embedded framework isa way to trick Xcode into seeing the included resources. As far as Xcode isconcerned, they are simply folders, and so there are a few minor issues withembedded frameworks:Choosing Which Template System to UseIn this distribution are two template systems, each with their strengths andweaknesses. You should choose whichever one best suits your needs andconstraints (or just install both).The biggest difference is that Xcode can't build real frameworks unless youinstall the static framework xcspec file inside the Xcode app, which might bea dealbreaker for some (this applies to the PROJECT, not the frameworkitself).Short decision chart for the impatientNote: Both types will build the exact same binary. The only difference is inhow Xcode treats the project.Fake FrameworkThe fake framework is based on the well known ""relocatable object file"" bundlehack, which tricks Xcode into building something that mostly resembles aframework, but is really a bundle.The fake framework template takes this a step further, using some scripting togenerate a real static framework (based on a static library rather than arelocatable object file). However, the framework's project still definesit to be of type 'wrapper.cfbundle', which makes it a second class citizenaccording to Xcode.So while it produces a proper static framework that works just as well as a""real"" static framework, things can get tricky when you have dependencies.The problem with dependenciesIf you're just setting up a standalone project, then you're not usingdependencies, so there's no problem.If, however, you use project dependencies (such as in workspaces), Xcode won'tbe happy. The fake framework won't show up in the list when you click the '+'button under ""Link Binary With Libraries"" in your main application project.You can manually drag it from ""Products"" under your fake framework project toadd the dependency.Note: In older versions of Xcode, you'd get warnings like the following:warning: skipping file '/somewhere/MyFramework.framework'
(unexpected file type 'wrapper.cfbundle' in Frameworks & Libraries build phase)
This would be followed by linker errors for anything in your fake framework.As of Xcode 4.3.1, this doesn't seem to happen anymore.If you do encounter this issue, you can work around it by adding a ""-framework""switch with your framework's name in ""Other Linker Flags"" in the project thatuses the framework:-framework MyFramework
It won't get rid of the warning, which is annoying, but it does link properly.Real FrameworkThe real framework is real in every sense of the word. It is a true staticframework made by re-introducing specifications that Apple left out of Xcode.In order to be able to build a real framework project, you must install anxcspec file inside the Xcode installation.If you are releasing a project (rather than the built product) that buildsa real framework, anyone who wishes to build that framework must alsoinstall the xcspec file (using the install script in this distribution) sothat their Xcode can understand the target type.Note: If all you're doing is distributing the fully built framework, and notthe framework's project, then the end user doesn't need to install anything.I've submitted a report to Apple in the hopes that they'll update thespecification files in Xcode, but that could take awhile.Upgrading from previous iOS-Universal-Framework versionsIf you are upgrading from iOS-Universal-Framework Mk 6 or earlier and wereusing the Real Static Framework, and are still using Xcode 4.2.1 orearlier, please run uninstall_legacy.sh first to remove any patches thatwere previously applied to Xcode, then run install.sh and restart Xcode.If you are using Xcode 4.3 or later, just run install.sh and restartXcode.Installing the Template SystemTo install, run the install.sh script in either the ""Real Framework"" or""Fake Framework"" folder (or both).Now restart Xcode and you'll see Static iOS Framework (or Fake Static) under Framework & Library when creating a new project.To uninstall, run the uninstall.sh script and restart Xcode.Creating an iOS Framework ProjectBuilding your iOS FrameworkThere will be two folders in the build location: (your framework).frameworkand (your framework).embeddedframeworkIf your framework has only code, and no resources (like images, scripts, xibs,core data momd files, etc), you can distribute (your framework).frameworkto your users and it will just work.If you have included resources in your framework, you MUST distribute(your framework).embeddedframework.Using an iOS FrameworkiOS frameworks are basically the same as regular dynamic Mac OS X frameworks,except they are statically linked.To add a framework to your project, simply drag it into your project.When including headers from your framework, remember to use angle bracketsyntax rather than quotes.For example, with framework ""MyFramework"":#import <MyFramework/MyClass.h>
Template DevelopmentIf you're interested in tinkering with the templates themselves, I've changedthe way they are generated. There are now template metatemplates, which areused to build the Xcode templates. Inside the devel folder there is a scriptbuild.py, which rebuilds the templates under Fake Framework andReal Framework. The source files are under src:TroubleshootingHeaders Not FoundIf Xcode can't find the header files from your framework, you've likelyforgotten to make them public. See step 7 in Creating an iOS Framework ProjectNo Such Product TypeIf someone who has not installed iOS Universal Framework in their developmentenvironment attempts to build a universal framework project (for a realframework, not a fake framework), they'll get the following error:target specifies product type 'com.apple.product-type.framework.static',
but there's no such product type for the 'iphonesimulator' platform
Xcode requires some modification in order to be able to build true iOS staticframeworks (see the two diff files in the ""Real Framework"" folder of thisrepository for the gory details), so please install it on all developmentmachines that will build your real static framework projects (thisisn't needed for users of your framework, only for builders of the framework).The selected run destination is not valid for this actionSometimes Xcode gets confused and loads the wrong active settings. The firstthing to try is restarting Xcode. If it still fails, Xcode generated a badproject (this can happen with any kind of project due to a bug in Xcode 4).If this happens, you'll need to start over and create a new project.Linker WarningsThe first time you build your framework target, XCode may complain aboutmissing folders during the linking phase:ld: warning: directory not found for option
'-L/Users/myself/Library/Developer/Xcode/DerivedData/MyFramework-ccahfoccjqiognaqraesrxdyqcne/Build/Products/Debug-iphoneos'
If this happens, simply clean and rebuild the target and the warnings shouldgo away.Core Data momd not foundXcode builds managed object model files differently in a framework project thanit does in an application project. Instead of creating a .momd directorycontaining VersionInfo.plist and the .mom file, it simply creates the .mom filein the root directory.This means that when initializing your NSManagedObjectModel from a model in anembedded framework, you must specify your model URL with an extension of ""mom""rather than ""momd"":NSURL *modelURL = [[NSBundle mainBundle] URLForResource:@""MyModel"" withExtension:@""mom""];
Unknown class MyClass in Interface Builder file.Since static frameworks are statically linked, the linker strips out any codeit thinks is not being used. Unfortunately, the linker does not check xibfiles, and so if a class is referenced only in a xib, and not in objective-ccode, the linker will drop that class from the final executable. This is alinker issue, not a framework issue (it also happens when you build a staticlibrary).Apple's built-in framworks don't suffer this problem because they aredynamically loaded at runtime and the complete, unstripped dynamic libraryexists in the iOS device's firmware.There are two ways around this:In MyTextField:+ (void) forceLinkerLoad_ {}
In MyViewController:+ (void) initialize
{
    [MyTextField forceLinkerLoad_];
}
They will still need to add ""-ObjC"" to their linker settings, but won't needto force all_load.Option 2 is more work for you, but if done right it saves the end user fromhaving to disable linker optimizations (causing object file bloat) just to useyour framework.unexpected file type 'wrapper.cfbundle' in Frameworks & Libraries build phaseThis happens when you use a fake framework project as a dependency in aworkspace, or as a child project (real framework projects don't have thisissue). Even though the framework project produces a proper static framework,Xcode only looks at the project file, which says it's a bundle, and so itissues a warning during the dependency check and then skips it during thelinker phase.You can get it to link properly by manually adding a command to linkyour framework during the linker phase. Add a command to link your frameworkin ""Other Linker Flags"" in the project that depends on the framework:-framework MyFramework
You'll still get the warning, but it won't fail in the linker phase anymore.Libraries being linked or not being linked into the final frameworkUnfortunately, due to the way Xcode works, the ""Real Framework"" and""Fake Framework"" templates handle included static libraries/frameworksdifferently.The ""Real Framework"" template follows correct static library procedure, NOTlinking other static libraries/frameworks into the final product.The ""Fake Framework"" template tricks Xcode into thinking that it's building arelocatable object file, and so the linking phase treats it as if it werebuilding an executable, linking all static code sources into the final binary(although it doesn't check for missing object code). To get the""Real Framework"" behavior, you should include only the header files from thelibrary/framework in your framework project, not the static library orframework itself.Unrecognized selector in (some class with a category method)If your static library or static framework contains a module with ONLYcategory code (no full class implementations), the linker will get confused,and will leave the code out of the final binary. Since it's not present in thefinal product, you'll get ""unrecognized selector"" exceptions when any call ismade to those category methods.To get around this, put a dummy class into the module containing the categorycode. The linker, seeing a full Objective-C class, will link the module in,including the category code.I've made a header file to make this easier to do:#import ""SomeConcreteClass+MyAdditions.h""
#import ""LoadableCategory.h""


MAKE_CATEGORIES_LOADABLE(SomeConcreteClass_MyAdditions);


@implementation SomeConcreteClass (MyAdditions)

...

@end
You will also need to add ""-ObjC"" to the ""Other Linker Flags"" build setting inany project that uses the framework.Unit tests crash before executing any codeIf you make a new static framework (or static library) target with unit testsin Xcode 4.3, it will crash when you try to run the unit tests:Thread 1: EXC_BAD_ACCESS (code=2, address=0x0)
0 0x00000000
---
15 dyldbootstrap:start(...)
This is due to a bug in lldb. You can run the unit tests using GDB instead byediting your scheme, selecting ""Test"", and from the ""Info"" tab changingDebugger from LLDB to GDB.HistoryMk 1The first incarnation. It used a bunch of script hackery to cobble together afake framework. It exploited the ""bundle"" target, setting its type to arelocatable object file.Mk 2This version took advantage of the template system to do most of the workthat the script used to do. Everything (including the script) was embeddedin the template.Mk 3This version does away with the ""relocatable object"" hackery and builds a truestatic framework, with all the abilities of an OS X static framework.This solves a number of linker, unit testing, and workspace inclusion issuesthat plagued the previous hacky implementations.It also includes the concept of the embeddedframework, which allows you toinclude resources with your framework in a way that Xcode understands.Josh Weinberg also added some tweaks to make it build in the proper builddirectory with scheme-controlled configuration, and behave better as asubproject dependency.It now requires some small modifications to Xcode's specification files inorder to support true static frameworks, and thus comes with an install anduninstall script.Mk 4This version gives you the choice of installing the ""real framework"" templateor the ""fake framework"" template. Both come with an install script, but onlythe ""real framework"" installer needs to modify Xcode.This also fixes some issues that the fake framework had in Mk 2 (such asthe curious behavior of embedding the full path to the compiled files withinthe files themselves, resulting in warnings when building with that framework).Mk 5This version does away with the extra target and script. Everything is selfcontained in the framework target, and the framework under the ""Products""group is actually the universal framework (no more Debug-univesal orRelease-universal folders).You can build and clean like you would in any other project.As well, the ""Fake"" framework template now builds a proper static libraryinstead of a relocatable object file (although Xcode still doesn't believethat it's real).Mk 6This version makes the Xcode modifications for real static frameworks saferby simply adding an extra specification file instead of patching existingones.Mk 7This version was supposed to be the one that no longer modified Xcode, butalas, Xcode behaves differently depending on WHERE the xcspec file getsinstalled. Take a guess at which location doesn't work...@wtfxcodeSo instead, this version basically handles the new install location ofXcode4.3.Templates now build armv6 + armv7 by default instead of just armv7.Note: If you previously installed real framework supprt under the broken Mk 7(i.e. if you installed it on Feb 16th, or Feb 17th before 9:00 am PST, 2012),run uninstall_legacy.sh to uninstall the xcspec file from your home dir, thenreinstall.Mk 8This version replaces the bash scripts with Python scripts. This gives a LOTmore control over the build process.To build the universal version of your framework, you must now use theArchive command rather than the Build command.  Build only buildsfor the current architecture (to speed up compilation when your frameworkproject is a dependency of another project).Added a devel folder for template development.LicenseCopyright (c) 2011 Karl StenerudPermission is hereby granted, free of charge, to any person obtaining a copyof this software and associated documentation files (the ""Software""), to dealin the Software without restriction, including without limitation the rightsto use, copy, modify, merge, publish, distribute, sublicense, and/or sellcopies of the Software, and to permit persons to whom the Software isfurnished to do so, subject to the following conditions:The above copyright notice and this permission notice shall be included inthe documentation of any redistributions of the template files themselves(but not in projects built using the templates).THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS ORIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THEAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHERLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS INTHE SOFTWARE."
https://github.com/numba/numba,NumPy aware dynamic Python compiler using LLVM,"Numba.. image:: https://badges.gitter.im/numba/numba.svg:target: https://gitter.im/numba/numba?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge:alt: Gitter.. image:: https://img.shields.io/badge/discuss-on%20discourse-blue:target: https://numba.discourse.group/:alt: Discourse.. image:: https://zenodo.org/badge/3659275.svg:target: https://zenodo.org/badge/latestdoi/3659275:alt: Zenodo DOI.. image:: https://img.shields.io/pypi/v/numba.svg:target: https://pypi.python.org/pypi/numba/:alt: PyPI.. image:: https://dev.azure.com/numba/numba/_apis/build/status/numba.numba?branchName=main:target: https://dev.azure.com/numba/numba/_build/latest?definitionId=1?branchName=main:alt: Azure PipelinesA Just-In-Time Compiler for Numerical Functions in Python#########################################################Numba is an open source, NumPy-aware optimizing compiler for Python sponsoredby Anaconda, Inc.  It uses the LLVM compiler project to generate machine codefrom Python syntax.Numba can compile a large subset of numerically-focused Python, including manyNumPy functions.  Additionally, Numba has support for automaticparallelization of loops, generation of GPU-accelerated code, and creation ofufuncs and C callbacks.For more information about Numba, see the Numba homepage:https://numba.pydata.org and the online documentation:https://numba.readthedocs.io/en/stable/index.htmlInstallationPlease follow the instructions:https://numba.readthedocs.io/en/stable/user/installing.htmlDemoPlease have a look and the demo notebooks via the mybinder service:https://mybinder.org/v2/gh/numba/numba-examples/master?filepath=notebooksContactNumba has a discourse forum for discussions:"
https://github.com/openstack/nova,OpenStack Compute (Nova). Mirror of code maintained at opendev.org.,"==============OpenStack Nova.. image:: https://governance.openstack.org/tc/badges/nova.svg:target: https://governance.openstack.org/tc/reference/tags/index.html.. Change things from this point onOpenStack Nova provides a cloud computing fabric controller, supporting a widevariety of compute technologies, including: libvirt (KVM, Xen, LXC and more),Hyper-V, VMware and OpenStack Ironic.Use the following resources to learn more.APITo learn how to use Nova's API, consult the documentation available online at:For more information on OpenStack APIs, SDKs and CLIs in general, refer to:OperatorsTo learn how to deploy and configure OpenStack Nova, consult the documentationavailable online at:In the unfortunate event that bugs are discovered, they should be reported tothe appropriate bug tracker. If you obtained the software from a 3rd partyoperating system vendor, it is often wise to use their own bug tracker forreporting problems. In all other cases use the master OpenStack bug tracker,available at:DevelopersFor information on how to contribute to Nova, please see the contents of theCONTRIBUTING.rst.Any new code must follow the development guidelines detailed in the HACKING.rstfile, and pass all unit tests.Further developer focused documentation is available at:Other InformationDuring each _ and _, we agree on what the wholecommunity wants to focus on for the upcoming release. The plans for nova canbe found at:.. _Summit: https://www.openstack.org/summit/.. _Project Team Gathering: https://www.openstack.org/ptg/"
https://github.com/locustio/locust,Write scalable load tests in plain Python 🚗💨,"LocustLocust is an easy to use, scriptable and scalable performance testing tool. You define the behaviour of your users in regular Python code, instead of being constrained by a UI or domain specific language that only pretends to be real code. This makes Locust infinitely expandable and very developer friendly.To get started right away, head over to the .FeaturesWrite user test scenarios in plain old PythonIf you want your users to loop, perform some conditional behaviour or do some calculations, you just use the regular programming constructs provided by Python. Locust runs every user inside its own greenlet (a lightweight process/coroutine). This enables you to write your tests like normal (blocking) Python code instead of having to use callbacks or some other mechanism. Because your scenarios are “just python” you can use your regular IDE, and version control your tests as regular code (as opposed to some other tools that use XML or binary formats)from locust import HttpUser, task, between

class QuickstartUser(HttpUser):
    wait_time = between(1, 2)

    def on_start(self):
        self.client.post(""/login"", json={""username"":""foo"", ""password"":""bar""})

    @task
    def hello_world(self):
        self.client.get(""/hello"")
        self.client.get(""/world"")

    @task(3)
    def view_item(self):
        for item_id in range(10):
            self.client.get(f""/item?id={item_id}"", name=""/item"")
Distributed & Scalable - supports hundreds of thousands of usersLocust makes it easy to run load tests distributed over multiple machines. It is event-based (using ), which makes it possible for a single process to handle many thousands concurrent users. While there may be other tools that are capable of doing more requests per second on a given hardware, the low overhead of each Locust user makes it very suitable for testing highly concurrent workloads.Web-based UILocust has a user friendly web interface that shows the progress of your test in real-time. You can even change the load while the test is running. It can also be run without the UI, making it easy to use for CI/CD testing.   Can test any systemEven though Locust primarily works with web sites/services, it can be used to test almost any system or protocol. Just  for what you want to test, or .HackableLocust's code base is intentionally kept small and doesn't solve everything out of the box. Instead, we try to make it easy to adapt to any situation you may come across, using regular Python code. There is nothing stopping you from: LinksAuthorsLicenseOpen source licensed under the MIT license (see LICENSE file for details)."
https://github.com/hyperopt/hyperopt,Distributed Asynchronous Hyperparameter Optimization in Python,"Hyperopt: Distributed Hyperparameter Optimization is a Python library for serial and parallel optimization over awkwardsearch spaces, which may include real-valued, discrete, and conditionaldimensions.Getting startedInstall hyperopt from PyPIpip install hyperopt
to run your first example# define an objective function
def objective(args):
    case, val = args
    if case == 'case 1':
        return val
    else:
        return val ** 2

# define a search space
from hyperopt import hp
space = hp.choice('a',
    [
        ('case 1', 1 + hp.lognormal('c1', 0, 1)),
        ('case 2', hp.uniform('c2', -10, 10))
    ])

# minimize the objective over the space
from hyperopt import fmin, tpe, space_eval
best = fmin(objective, space, algo=tpe.suggest, max_evals=100)

print(best)
# -> {'a': 1, 'c2': 0.01420615366247227}
print(space_eval(space, best))
# -> ('case 2', 0.01420615366247227}
ContributingIf you're a developer and wish to contribute, please follow these steps.Setup (based on )Note that dev dependencies require python 3.6+.AlgorithmsCurrently three algorithms are implemented in hyperopt:Hyperopt has been designed to accommodate Bayesian optimization algorithms based on Gaussian processes and regression trees, but these are not currently implemented.All algorithms can be parallelized in two ways, using:Documentation, but is partly still hosted on the wiki. Here are some quick links to the most relevant pages:Related ProjectsExamplesSee  on the wiki.Announcements mailing listDiscussion mailing listCiteIf you use this software for research, please cite the paper (http://proceedings.mlr.press/v28/bergstra13.pdf) as follows:Bergstra, J., Yamins, D., Cox, D. D. (2013) Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures. TProc. of the 30th International Conference on Machine Learning (ICML 2013), June 2013, pp. I-115 to I-23.ThanksThis project has received support from"
https://github.com/vispy/vispy,Main repository for Vispy,"VisPy: interactive scientific visualization in PythonMain website: http://vispy.org|Build Status| |Coverage Status| |Zenodo Link| |Contributor Covenant|VisPy is a high-performance interactive 2D/3D data visualization. VisPy leverages the computational power of modern Graphics through the OpenGL library to display verylarge datasets. Applications of VisPy include:ReleasesSee _.AnnouncementsSee the _.Using VisPyVisPy is a young library under heavy development at this time. Ittargets two categories of users:If you're in the first category, you can already start using VisPy.VisPy offers a Pythonic, NumPy-aware, user-friendly interface for OpenGLES 2.0 called gloo. You can focus on writing your GLSL code insteadof dealing with the complicated OpenGL API - VisPy takes care of thatautomatically for you.If you're in the second category, we're starting to build experimentalhigh-level plotting interfaces. Notably, VisPy now ships a very basicand experimental OpenGL backend for matplotlib.InstallationPlease follow the detailed_on the VisPy website.Structure of VisPyCurrently, the main subpackages are:The API of all public interfaces are subject to change in the future,although app and gloo are relatively stable at this point.Code of ConductThe VisPy community requires its members to abide by the_. In this CoC you will find theexpectations of members, the penalties for violating these expectations, andhow violations can be reported to the members of the community in charge ofenforcing this Code of Conduct.GovernanceThe VisPy project maintainers make decisions about the project based on asimple consensus model. This is described in more detail on the_ of the vispywebsite as well as the_.In addition to decisions about the VisPy project, there is also a steeringcommittee for the overall VisPy organization. More information about thiscommittee can also be found on the _of the vispy website,along with the organization's _ andother related documents (linked in the charter).GenesisVisPy began when four developers with their own visualization librariesdecided to team up:__ with , with , with , with __.Now VisPy looks to build on the expertise of these developers and thebroader open-source community to build a high-performance OpenGL library.External links.. |Build Status| image:: https://github.com/vispy/vispy/workflows/CI/badge.svg:target: https://github.com/vispy/vispy/actions.. |Coverage Status| image:: https://img.shields.io/coveralls/vispy/vispy/main.svg:target: https://coveralls.io/r/vispy/vispy?branch=main.. |Zenodo Link| image:: https://zenodo.org/badge/5822/vispy/vispy.svg:target: http://dx.doi.org/10.5281/zenodo.17869.. |Contributor Covenant| image:: https://img.shields.io/badge/Contributor%20Covenant-2.0-4baaaa.svg:target: CODE_OF_CONDUCT.md"
https://github.com/pyinstaller/pyinstaller,Freeze (package) Python programs into stand-alone executables,"PyInstaller Overview.. image:: https://img.shields.io/pypi/v/pyinstaller:alt: PyPI:target: https://pypi.org/project/pyinstaller.. image:: https://img.shields.io/pypi/pyversions/pyinstaller:alt: PyPI - Python Version:target: https://pypi.org/project/pyinstaller.. image:: https://img.shields.io/readthedocs/pyinstaller/stable:alt: Read the Docs (version):target: https://pyinstaller.org.. image:: https://img.shields.io/pypi/dm/pyinstaller:alt: PyPI - Downloads:target: https://pypistats.org/packages/pyinstallerPyInstaller bundles a Python application and all its dependencies into a singlepackage. The user can run the packaged app without installing a Pythoninterpreter or any modules.:Documentation: https://pyinstaller.org/:Code:          https://github.com/pyinstaller/pyinstallerPyInstaller reads a Python script written by you. It analyzes your codeto discover every other module and library your script needs in order toexecute. Then it collects copies of all those files -- including the activePython interpreter! -- and puts them with your script in a single folder, oroptionally in a single executable file.PyInstaller is tested against Windows, macOS, and GNU/Linux.However, it is not a cross-compiler:to make a Windows app you run PyInstaller in Windows; to makea GNU/Linux app you run it in GNU/Linux, etc.PyInstaller has been used successfullywith AIX, Solaris, FreeBSD and OpenBSD,but is not tested against them as part of the continuous integration tests.Main AdvantagesInstallationPyInstaller is available on PyPI. You can install it through :.. code:: bash  pip install pyinstaller
Requirements and Tested PlatformsUsageBasic usage is very simple, just run it against your main script:.. code:: bash  pyinstaller /path/to/yourscript.py
For more details, see the _.Untested PlatformsThe following platforms have been contributed and any feedback orenhancements on these are welcome.Before using any contributed platform, you need to build the PyInstallerbootloader. This will happen automatically when you  provided that you have an appropriate C compiler (typicallyeither  or ) and zlib's development headers already installed.SupportChanges in this ReleaseYou can find a detailed list of changes in this releasein the _ section of the manual... _: https://pyinstaller.org/en/v6.1.0/.. _: https://pyinstaller.org/en/v6.1.0/CHANGES.html"
https://github.com/qutebrowser/qutebrowser,"A keyboard-driven, vim-like browser based on Python and Qt.","// SPDX-License-Identifier: GPL-3.0-or-later// If you are reading this in plaintext or on PyPi://// A rendered version is available at:// https://github.com/qutebrowser/qutebrowser/blob/main/README.asciidocqutebrowser// QUTE_WEB_HIDEimage:qutebrowser/icons/qutebrowser-64x64.png[qutebrowser logo] A keyboard-driven, vim-like browser based on Python and Qt.image:https://github.com/qutebrowser/qutebrowser/workflows/CI/badge.svg[""Build Status"", link=""https://github.com/qutebrowser/qutebrowser/actions?query=workflow%3ACI""]image:https://codecov.io/github/qutebrowser/qutebrowser/coverage.svg?branch=main[""coverage badge"",link=""https://codecov.io/github/qutebrowser/qutebrowser?branch=main""]link:https://www.qutebrowser.org[website] | link:https://blog.qutebrowser.org[blog] | https://github.com/qutebrowser/qutebrowser/blob/main/doc/faq.asciidoc[FAQ] | https://www.qutebrowser.org/doc/contributing.html[contributing] | link:https://github.com/qutebrowser/qutebrowser/releases[releases] | https://github.com/qutebrowser/qutebrowser/blob/main/doc/install.asciidoc[installing]// QUTE_WEB_HIDE_ENDqutebrowser is a keyboard-focused browser with a minimal GUI. It's basedon Python and Qt and free software, licensed under the GPL.It was inspired by other browsers/addons like dwb and Vimperator/Pentadactyl.// QUTE_WEB_HIDEqutebrowser's primary maintainer, The-Compiler, is currently working To sustain this for a longtime, your help is needed! See thehttps://github.com/sponsors/The-Compiler/[GitHub Sponsors page] orhttps://github.com/qutebrowser/qutebrowser/blob/main/README.asciidoc#donating[alternative donation methods]for more information. Depending on your sign-up date and howlong you keep a certain level, you can get qutebrowser t-shirts, stickers andmore!// QUTE_WEB_HIDE_ENDScreenshotsimage:doc/img/main.png[""screenshot 1"",width=300,link=""doc/img/main.png""]image:doc/img/downloads.png[""screenshot 2"",width=300,link=""doc/img/downloads.png""]image:doc/img/completion.png[""screenshot 3"",width=300,link=""doc/img/completion.png""]image:doc/img/hints.png[""screenshot 4"",width=300,link=""doc/img/hints.png""]DownloadsSee the https://github.com/qutebrowser/qutebrowser/releases[github releasespage] for available downloads and the link:doc/install.asciidoc[INSTALL] file fordetailed instructions on how to get qutebrowser running on various platforms.Documentation and getting helpPlease see the link:doc/help/index.asciidoc[help page] for available documentationpages and support channels.Contributions / BugsYou want to contribute to qutebrowser? Awesome! Please readlink:doc/contributing.asciidoc[the contribution guidelines] for details anduseful hints.If you found a bug or have a feature request, you can report it in severalways:Please report security bugs to security@qutebrowser.org(or if GPG encryption is desired, contact me@the-compiler.org with GPG IDhttps://www.the-compiler.org/pubkey.asc[0x916EB0C8FD55A072]).Alternatively,https://github.com/qutebrowser/qutebrowser/security/advisories/new[report a vulnerability]via GitHub'shttps://docs.github.com/en/code-security/security-advisories/guidance-on-reporting-and-writing/privately-reporting-a-security-vulnerability[private reporting feature].RequirementsThe following software and libraries are required to run qutebrowser:On Python 3.8, the following backport is also required:On macOS, the following libraries are also required:The following libraries are optional:See link:doc/install.asciidoc[the documentation] for directions on how toinstall qutebrowser and its dependencies.Donatingqutebrowser's primary maintainer, The-Compiler, is currently working To sustain this for a longtime, your help is needed! See thehttps://github.com/sponsors/The-Compiler/[GitHub Sponsors page] for moreinformation. Depending on your sign-up date and how long you keep a certainlevel, you can get qutebrowser t-shirts, stickers and more!GitHub Sponsors allows for one-time donations (using the buttons next to ""Select atier"") as well as custom amounts. For currencies other than Euro or Swiss Francs, this GitHub uses https://stripe.com/[Stripe] to acceptpayment via credit carts without any fees. Billing via PayPal is available as well, withless fees than a direct PayPal transaction.Alternatively, the following donation methods are available -- note thateligibility for swag (shirts/stickers/etc.) is handled on a case-by-case basisfor those, please mailto:mail@qutebrowser.org[get in touch] for details.SponsorsThanks a lot to https://www.macstadium.com/[MacStadium] for supportingqutebrowser with a free hosted Mac Mini via theirhttps://www.macstadium.com/opensource[Open Source Project].(They don't require including this here - I've just been very happy with theiroffer, and without them, no macOS releases or tests would exist)Thanks to the https://www.hsr.ch/[HSR Hochschule für Technik Rapperswil], whichmade it possible to work on qutebrowser extensions as a student research project.image:doc/img/sponsors/macstadium.png[""powered by MacStadium"",width=200,link=""https://www.macstadium.com/""]image:doc/img/sponsors/hsr.png[""HSR Hochschule für Technik Rapperswil"",link=""https://www.hsr.ch/""]Authorsqutebrowser's primary author is Florian Bruhin (The Compiler), but qutebrowserwouldn't be what it is without the help ofhttps://github.com/qutebrowser/qutebrowser/graphs/contributors[hundreds of contributors]!Additionally, the following people have contributed graphics:Also, thanks to everyone who contributed to one of qutebrowser'slink:doc/backers.asciidoc[crowdfunding campaigns]!Similar projectsVarious projects with a similar goal like qutebrowser exist.Many of them were inspirations for qutebrowser in some way, thanks for that!Active
* https://fanglingsu.github.io/vimb/[vimb] (C, GTK+ with WebKit2)
* https://luakit.github.io/[luakit] (C/Lua, GTK+ with WebKit2)
* https://nyxt.atlas.engineer/[Nyxt browser] (formerly ""Next browser"", Lisp, Emacs-like but also offers Vim bindings, QtWebEngine or GTK+/WebKit2 - note there was a https://jgkamat.gitlab.io/blog/next-rce.html[critical remote code execution in 2019] which was handled quite badly)
* https://vieb.dev/[Vieb] (JavaScript, Electron)
* https://surf.suckless.org/[surf] (C, GTK+ with WebKit1/WebKit2)
* https://github.com/jun7/wyeb[wyeb] (C, GTK+ with WebKit2)
* Chrome/Chromium addons:
  https://vimium.github.io/[Vimium]
* Firefox addons (based on WebExtensions):
  https://tridactyl.xyz/[Tridactyl],
  https://addons.mozilla.org/en-GB/firefox/addon/vimium-ff/[Vimium-FF]
* Addons for Firefox and Chrome:
  https://github.com/brookhong/Surfingkeys[Surfingkeys],
  https://lydell.github.io/LinkHints/[Link Hints] (hinting only),
  https://github.com/ueokande/vimmatic[Vimmatic]

Inactive
LicenseThis program is free software: you can redistribute it and/or modifyit under the terms of the GNU General Public License as published bythe Free Software Foundation, either version 3 of the License, or(at your option) any later version.This program is distributed in the hope that it will be useful,but WITHOUT ANY WARRANTY; without even the implied warranty ofMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See theGNU General Public License for more details.You should have received a copy of the GNU General Public Licensealong with this program.  If not, see .pdf.jsqutebrowser optionally uses https://github.com/mozilla/pdf.js/[pdf.js] todisplay PDF files in the browser. Windows releases come with a bundled pdf.js.pdf.js is distributed under the terms of the Apache License. You canfind a copy of the license in  (in theWindows release or after running ), or onlinehttps://www.apache.org/licenses/LICENSE-2.0.html[here]."
https://github.com/xmendez/wfuzz,Web application fuzzer,"Wfuzz - The Web FuzzerWfuzz has been created to facilitate the task in web applications assessments and it is based on a simple concept: it replaces any reference to the FUZZ keyword by the value of a given payload.A payload in Wfuzz is a source of data.This simple concept allows any input to be injected in any field of an HTTP request, allowing to perform complex web security attacks in different web application components such as: parameters, authentication, forms, directories/files, headers, etc.Wfuzz is more than a web content scanner:It was created to facilitate the task in web applications assessments, it's a tool by pentesters for pentesters ;)InstallationTo install WFuzz, simply use pip:pip install wfuzz
To run Wfuzz from a docker image, run:$ docker run -v $(pwd)/wordlist:/wordlist/ -it ghcr.io/xmendez/wfuzz wfuzz
DocumentationDocumentation is available at http://wfuzz.readthedocs.ioDownloadCheck github releases. Latest is available at https://github.com/xmendez/wfuzz/releases/latest"
https://github.com/MorvanZhou/tutorials,机器学习相关教程,"我是 周沫凡,  只是谐音, 我喜欢制作,分享所学的东西, 所以你能在这里找到很多有用的东西, 少走弯路. 你能在找到关于我的所有东西.这个 Python tutorial 的一些内容:赞助和支持这些 tutorial 都是我用业余时间写出来, 录成视频, 如果你觉得它对你很有帮助, 请你也分享给需要学习的朋友们.如果你看好我的经验分享, 也请考虑适当的 , 让我能继续分享更好的内容给大家."
https://github.com/shenweichen/DeepCTR,"Easy-to-use,Modular and Extendible package of deep-learning based CTR models .","DeepCTRDeepCTR is a Easy-to-use, Modular and Extendible package of deep-learning based CTR models along with lots ofcore components layers which can be used to easily build custom models.You can use any complex model with ，and  .Some related projects:Let's () and Models List|                 Model                  | Paper                                                                                                                                                           || :------------------------------------: | :-------------------------------------------------------------------------------------------------------------------------------------------------------------- ||  Convolutional Click Prediction Model  | [CIKM 2015]             || Factorization-supported Neural Network | [ECIR 2016]                    ||      Product-based Neural Network      | [ICDM 2016]                                                   ||              Wide & Deep               | [DLRS 2016]                                                                 ||                 DeepFM                 | [IJCAI 2017]                           ||        Piece-wise Linear Model         | [arxiv 2017]                                 ||          Deep & Cross Network          | [ADKDD 2017]                                                                   ||   Attentional Factorization Machine    | [IJCAI 2017] ||      Neural Factorization Machine      | [SIGIR 2017]                                               ||                xDeepFM                 | [KDD 2018]                         ||         Deep Interest Network          | [KDD 2018]     ||                AutoInt                 | [CIKM 2019]                              ||    Deep Interest Evolution Network     | [AAAI 2019]                                            ||                FwFM                    | [WWW 2018]                ||                  ONN                  | [arxiv 2019]                                                ||                 FGCNN                  | [WWW 2019]                             ||     Deep Session Interest Network      | [IJCAI 2019]                                                ||                FiBiNET                 | [RecSys 2019]   ||                FLEN                    | [arxiv 2019]   ||                 BST                   | [DLP-KDD 2019]                           ||                IFM                 | [IJCAI 2019]   ||                DCN V2                    | [arxiv 2020]   ||                DIFM                 | [IJCAI 2020]   ||   FEFM and DeepFEFM                    | [arxiv 2020]                                         ||              SharedBottom               | [arxiv 2017]  ||   ESMM                    | [SIGIR 2018]                       ||   MMOE                    | [KDD 2018]                   ||   PLE                    | [RecSys 2020]                   ||   EDCN                   | [KDD 2021]                   |CitationIf you find this code useful in your research, please cite it using the following BibTeX:@misc{shen2017deepctr,
  author = {Weichen Shen},
  title = {DeepCTR: Easy-to-use,Modular and Extendible package of deep-learning based CTR models},
  year = {2017},
  publisher = {GitHub},
  journal = {GitHub Repository},
  howpublished = {\url{https://github.com/shenweichen/deepctr}},
}
DisscussionGroup|公众号：浅梦学习笔记|微信：deepctrbot|学习小组  ||:--:|:--:|:--:|| | ||Main contributors()"
https://github.com/facebookresearch/ParlAI,A framework for training and evaluating AI models on a variety of openly available dialogue datasets.," (pronounced “par-lay”) is a python framework forsharing, training and testing dialogue models, from open-domain chitchat, totask-oriented dialogue, to visual question answering.Its goal is to provide researchers:ParlAI is described in the following paper:or see these .Follow us on  and check out our  to see the latestinformation about new features & updates, and the website for further docs. For an archived list of updates,check out .Interactive TutorialFor those who want to start with ParlAI now, you can try our .Installing ParlAIOperating SystemParlAI should work as inteded under Linux or macOS. We do not support Windows at this time, but many users  and issues with Python 3.9. We are happy to accept patches that improve Windows support.Python InterpreterParlAI currently requires Python3.8+.RequirementsParlAI supports  1.6 or higher.All requirements of the core modules are listed in . However, some models included (in ) have additional requirements.Virtual EnvironmentWe strongly recommend you install ParlAI in a virtual environment using  or .End User InstallationIf you want to use ParlAI without modifications, you can install it with:cd /path/to/your/parlai-app
python3.8 -m venv venv
venv/bin/pip install --upgrade pip setuptools wheel
venv/bin/pip install parlai
Developer InstallationMany users will want to modify some parts of ParlAI. To set up a developmentenvironment, run the following commands to clone the repository and installParlAI:git clone https://github.com/facebookresearch/ParlAI.git ~/ParlAI
cd ~/ParlAI
python3.8 -m venv venv
venv/bin/pip install --upgrade pip setuptools wheel
venv/bin/python setup.py develop
All needed data will be downloaded to . If you need to clear outthe space used by these files, you can safely delete these directories and anyfiles needed will be downloaded again.DocumentationExamplesA large set of scripts can be found in . Here are a few of them.Note: If any of these examples fail, check the  to see if you have missed something.Display 10 random examples from the SQuAD taskparlai display_data -t squad
Evaluate an IR baseline model on the validation set of the Personachat task:parlai eval_model -m ir_baseline -t personachat -dt valid
Train a single layer transformer on PersonaChat (requires pytorch and torchtext).Detail: embedding size 300, 4 attention heads,  2 epochs using batchsize 64, word vectors are initialized with fasttext and the other elements of the batch are used as negative during training.parlai train_model -t personachat -m transformer/ranker -mf /tmp/model_tr6 --n-layers 1 --embedding-size 300 --ffn-size 600 --n-heads 4 --num-epochs 2 -veps 0.25 -bs 64 -lr 0.001 --dropout 0.1 --embedding-type fasttext_cc --candidates batch
Code OrganizationThe code is set up into several main directories:SupportIf you have any questions, bug reports or feature requests, please don't hesitate to post on our .You may also be interested in checking out our  andour .Please remember to follow our .ContributingWe welcome PRs from the community!You can find information about contributing to ParlAI in ourdocument.The TeamParlAI is currently maintained by Moya Chen, Emily Dinan, Dexter Ju, MojtabaKomeili, Spencer Poff, Pratik Ringshia, Stephen Roller, Kurt Shuster,Eric Michael Smith, Megan Ung, Jack Urbanek, Jason Weston, Mary Williamson,and Jing Xu. Kurt Shuster is the current Tech Lead.Former major contributors and maintainers include Alexander H. Miller, MargaretLi, Will Feng, Adam Fisch, Jiasen Lu, Antoine Bordes, Devi Parikh, Dhruv Batra,Filipe de Avila Belbute Peres, Chao Pan, and Vedant Puri.CitationPlease cite the  if you use ParlAI in your work:@article{miller2017parlai,
  title={ParlAI: A Dialog Research Software Platform},
  author={{Miller}, A.~H. and {Feng}, W. and {Fisch}, A. and {Lu}, J. and {Batra}, D. and {Bordes}, A. and {Parikh}, D. and {Weston}, J.},
  journal={arXiv preprint arXiv:{1705.06476}},
  year={2017}
}
LicenseParlAI is MIT licensed. See the [<marko.inline.RawText object at 0x000001592E018A48>] file for details."
https://github.com/nyaadevs/nyaa,Bittorrent software for cats,"NyaaV2 Setting up for developmentThis project uses Python 3.7. There are features used that do not exist in 3.6, so make sure to use Python 3.7.This guide also assumes you 1) are using Linux and 2) are somewhat capable with the commandline.It's not impossible to run Nyaa on Windows, but this guide doesn't focus on that.Code Quality:Running TestsThe  folder contains tests for the the  module and the webserver. To run the tests:Setting up Pyenvpyenv eases the use of different Python versions, and as not all Linux distros offer 3.7 packages, it's right up our alley.Setting up MySQL/MariaDB databaseYou may use SQLite but the current support for it in this project is outdated and rather unsupported.Finishing upYou're now ready for simple testing and development!Continue below to learn about database migrations and enabling the advanced search engine, Elasticsearch.Database migrationsSetting up and enabling ElasticsearchInstalling ElasticsearchEnabling MySQL BinloggingSetting up ESEnable the  flag in  and (re)start the application.Elasticsearch should now be functional! The ES indices won't be updated ""live"" with the current setup, continue below for instructions on how to hook Elasticsearch up to MySQL binlog.   However, take note that binglog is not necessary for simple ES testing and development; you can simply run  from time to time to reindex all the torrents.Setting up sync_es.py keeps the Elasticsearch indices updated by reading the binlog and pushing the changes to the ES indices.You're done! The script should now be feeding updates from the database to Elasticsearch.Take note, however, that the specified ES index refresh interval is 30 seconds, which may feel like a long time on local development. Feel free to adjust it or "
https://github.com/yzhao062/anomaly-detection-resources,"Anomaly detection related books, papers, videos, and toolboxes","Anomaly Detection Learning Resources.. image:: https://img.shields.io/github/stars/yzhao062/anomaly-detection-resources.svg:target: https://github.com/yzhao062/anomaly-detection-resources/stargazers:alt: GitHub stars.. image:: https://img.shields.io/github/forks/yzhao062/anomaly-detection-resources.svg?color=blue:target: https://github.com/yzhao062/anomaly-detection-resources/network:alt: GitHub forks.. image:: https://img.shields.io/github/license/yzhao062/anomaly-detection-resources.svg?color=blue:target: https://github.com/yzhao062/anomaly-detection-resources/blob/master/LICENSE:alt: License.. image:: https://awesome.re/badge-flat2.svg:target: https://awesome.re/badge-flat2.svg:alt: Awesome.. image:: https://img.shields.io/badge/ADBench-benchmark_results-pink:target: https://github.com/Minqi824/ADBench:alt: Benchmark_(also known as Anomaly Detection) is an exciting yet challenging field,which aims to identify outlying objects that are deviant from the general data distribution.Outlier detection has been proven critical in many fields, such as credit cardfraud analytics, network intrusion detection, and mechanical unit defect detection.This repository collects:#. Books & Academic Papers#. Online Courses and Videos#. Outlier Datasets#. Open-source and Commercial Libraries/Toolkits#. Key Conferences & JournalsMore items will be added to the repository.Please feel free to suggest other key resources by opening an issue report,submitting a pull request, or dropping me an email @ (zhaoy@cmu.edu).Enjoy reading!BTW, you may find my _ and_ useful.Table of Contents1.1. Books^^^^^^^^^^_by Charu Aggarwal: Classical text book covering most of the outlier analysis techniques.A must-read for people in the field of outlier detection. __by Charu Aggarwal and Saket Sathe: Great intro book for ensemble learning in outlier analysis._by Jiawei Han and Micheline Kamber and Jian Pei: Chapter 12 discusses outlier detection with many key points. _1.2. Tutorials^^^^^^^^^^^^^^===================================================== ============================================  =====  ============================  ==========================================================================================================================================================================Tutorial Title                                        Venue                                         Year   Ref                           Materials===================================================== ============================================  =====  ============================  ==========================================================================================================================================================================Data mining for anomaly detection                     PKDD                                          2008   [#Lazarevic2008Data]_         _Outlier detection techniques                          ACM SIGKDD                                    2010   [#Kriegel2010Outlier]_        _Anomaly Detection: A Tutorial                         ICDM                                          2011   [#Chawla2011Anomaly]_         _Anomaly Detection in Networks                         KDD                                           2017   [#Mendiratta2017Anomaly]_     _Which Outlier Detector Should I use?                  ICDM                                          2018   [#Ting2018Which]_             _Deep Learning for Anomaly Detection                   KDD                                           2020   [#Wang2020Deep]_              , Deep Learning for Anomaly Detection                   WSDM                                          2021   [#Pang2021Deep]_              _===================================================== ============================================  =====  ============================  ==========================================================================================================================================================================1.3. Benchmarks^^^^^^^^^^^^^^^News: We just released a 36-page, the most comprehensive . compares 30 anomaly detection algorithms on 55 benchmark datasets.=============  =================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================Data Types     Paper Title                                                                                        Venue                         Year   Ref                           Materials=============  =================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================Time-series    Revisiting Time Series Outlier Detection: Definitions and Benchmarks                               NeurIPS                       2021   [#Lai2021Revisiting]_         , Graph          Benchmarking Node Outlier Detection on Graphs                                                      Preprint                      2022   [#Liu2022Benchmarking]_       , Tabular        ADBench: Anomaly Detection Benchmark                                                               Preprint                      2022   [#Han2022Adbench]_            , =============  =================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================Coursera Introduction to Anomaly Detection (by IBM)\ :_Get started with the Anomaly Detection API (by IBM)\ :_Coursera Real-Time Cyber Threat Detection and Mitigation partly covers the topic\ :_Coursera Machine Learning by Andrew Ng also partly covers the topic\ :Udemy Outlier Detection Algorithms in Data Mining and Data Science\ :_Stanford Data Mining for Cyber Security also covers part of anomaly detection techniques\ :_3.1. Multivariate Data^^^^^^^^^^^^^^^^^^^^^^[Python] _\ : PyOD is a comprehensive and scalable Python toolkit for detecting outlying objects in multivariate data. It contains more than 20 detection algorithms, including emerging deep learning models and outlier ensembles.[Python, GPU] _: A general GPU-accelerated framework for outlier detection.[Python] _\ : PySAD is a streaming anomaly detection framework in Python, which provides a complete set of tools for anomaly detection experiments. It currently contains more than 15 online anomaly detection algorithms and 2 different methods to integrate PyOD detectors to the streaming setting.[Python] _. It supports some popular algorithms like LOF, Isolation Forest, and One-class SVM.[Python] _\ : SUOD (Scalable Unsupervised Outlier Detection) is an acceleration framework for large-scale unsupervised outlier detector training and prediction, on top of PyOD.[Julia] _\ : OutlierDetection.jl is a Julia toolkit for detecting outlying objects, also known as anomalies.[Java] _\ :ELKI is an open source (AGPLv3) data mining software written in Java. The focus of ELKI is research in algorithms, with an emphasis on unsupervised methods in cluster analysis and outlier detection. [Java] _\ : The Anomaly Detection Extension for RapidMiner comprises the most well know unsupervised anomaly detection algorithms, assigning individual anomaly scores to data rows of example sets. It allows you to find data, which is significantly different from the normal, without the need for the data being labeled.[R] _\ : This CRAN task view contains a list of packages that can be used for anomaly detection with R.[R] _\ : A collection of some tests commonly used for identifying outliers in R.[Matlab] _\ : A collection of popular outlier detection algorithms in Matlab.3.2. Time Series Outlier Detection^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[Python] _\ : TODS is a full-stack automated machine learning system for outlier detection on multivariate time-series data.[Python] _\ : Skyline is a near real time anomaly detection system.[Python] _\ : Banpei is a Python package of the anomaly detection.[Python] _\ : A framework for using LSTMs to detect anomalies in multivariate time series data.[Python] _\ : A benchmarking pipeline for anomaly detection on time series data for multiple state-of-the-art deep learning methods.[Python] _\ : NAB is a novel benchmark for evaluating algorithms for anomaly detection in streaming, real-time applications.[Python] _\ : Anomaly detection on SQL data warehouses and databases.[Python] _\ : ML powered analytics engine for outlier/anomaly detection and root cause analysis.[R] _\ : This CRAN task view contains a list of packages that can be used for anomaly detection with R.[R] _\ : AnomalyDetection is an open-source R package to detect anomalies which is robust, from a statistical standpoint, in the presence of seasonality and an underlying trend.[R] _\ : The 'anomalize' package enables a ""tidy"" workflow for detecting anomalies in data.3.3. Graph Outlier Detection^^^^^^^^^^^^^^^^^^^^^^^^^^^^[Python] _\ : PyGOD is a Python library for graph outlier detection (anomaly detection). It includes more than 10 latest graph-based detection algorithms3.4. Real-time Elasticsearch^^^^^^^^^^^^^^^^^^^^^^^^^^^^[Open Distro] \ : A machine learning-based anomaly detection plugins for Open Distro for Elasticsearch. See .[Python] _\ : An open-source framework for real-time anomaly detection using Python, Elasticsearch and Kibana.3.5. Datasets^^^^^^^^^^^^^ELKI Outlier Datasets\ : https://elki-project.github.io/datasets/outlierOutlier Detection DataSets (ODDS)\ : http://odds.cs.stonybrook.edu/#table1Unsupervised Anomaly Detection Dataverse\ : https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/OPQMVFAnomaly Detection Meta-Analysis Benchmarks\ : https://ir.library.oregonstate.edu/concern/datasets/47429f155Skoltech Anomaly Benchmark (SKAB)\ : https://github.com/waico/skab4.1. Overview & Survey Papers^^^^^^^^^^^^^^^^^^^^^^^^^^^^^Papers are sorted by the publication year.======================================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================Paper Title                                                                                                             Venue                         Year   Ref                           Materials======================================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================A survey of outlier detection methodologies                                                                             ARTIF INTELL REV              2004   [#Hodge2004A]_                _Anomaly detection: A survey                                                                                             CSUR                          2009   [#Chandola2009Anomaly]_       _A meta-analysis of the anomaly detection problem                                                                        Preprint                      2015   [#Emmott2015A]_               _On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study                         DMKD                          2016   [#Campos2016On]_              , A comparative evaluation of unsupervised anomaly detection algorithms for multivariate data                             PLOS ONE                      2016   [#Goldstein2016A]_            _A comparative evaluation of outlier detection algorithms: Experiments and analyses                                      Pattern Recognition           2018   [#Domingues2018A]_            _Research Issues in Outlier Detection                                                                                    Book Chapter                  2019   [#Suri2019Research]_          _Quantitative comparison of unsupervised anomaly detection algorithms for intrusion detection                            SAC                           2019   [#Falcao2019Quantitative]_    _Progress in Outlier Detection Techniques: A Survey                                                                      IEEE Access                   2019   [#Wang2019Progress]_          _Deep learning for anomaly detection: A survey                                                                           Preprint                      2019   [#Chalapathy2019Deep]_        _Anomalous Instance Detection in Deep Learning: A Survey                                                                 Tech Report                   2020   [#Bulusu2020Deep]_            _Anomaly detection in univariate time-series: A survey on the state-of-the-art                                           Preprint                      2020   [#Braei2020Anomaly]_          _Deep Learning for Anomaly Detection: A Review                                                                           CSUR                          2021   [#Pang2020Deep]_              _A Comprehensive Survey on Graph Anomaly Detection with Deep Learning                                                    TKDE                          2021   [#Ma2021A]_                   _Revisiting Time Series Outlier Detection: Definitions and Benchmarks                                                    NeurIPS                       2021   [#Lai2021Revisiting]_         , A Unified Survey on Anomaly, Novelty, Open-Set, and Out-of-Distribution Detection: Solutions and Future Challenges      Preprint                      2021   [#Salehi2021A]_               _Self-Supervised Anomaly Detection: A Survey and Outlook                                                                 Preprint                      2022   [#Hojjati2022Self]_           _======================================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================4.2. Key Algorithms^^^^^^^^^^^^^^^^^^^All these algorithms are available in _.====================  =================================================================================================  =================================  =====  ===========================  ==============================================================================================================================================================================================Abbreviation          Paper Title                                                                                        Venue                              Year   Ref                          Materials====================  =================================================================================================  =================================  =====  ===========================  ==============================================================================================================================================================================================kNN                   Efficient algorithms for mining outliers from large data sets                                      ACM SIGMOD Record                  2000   [#Ramaswamy2000Efficient]_   _KNN                   Fast outlier detection in high dimensional spaces                                                  PKDD                               2002   [#Angiulli2002Fast]_         _LOF                   LOF: identifying density-based local outliers                                                      ACM SIGMOD Record                  2000   [#Breunig2000LOF]_           _IForest               Isolation forest                                                                                   ICDM                               2008   [#Liu2008Isolation]_         _OCSVM                 Estimating the support of a high-dimensional distribution                                          Neural Computation                 2001   [#Scholkopf2001Estimating]_  _AutoEncoder Ensemble  Outlier detection with autoencoder ensembles                                                       SDM                                2017   [#Chen2017Outlier]_          _COPOD                 COPOD: Copula-Based Outlier Detection                                                              ICDM                               2020   [#Li2020COPOD]_              _ECOD                  Unsupervised Outlier Detection Using Empirical Cumulative Distribution Functions                   TKDE                               2022   [#Li2021ECOD]_               _====================  =================================================================================================  =================================  =====  ===========================  ==============================================================================================================================================================================================4.3. Graph & Network Outlier Detection^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^=================================================================================================  =============================  =====  ============================  ==========================================================================================================================================================================Paper Title                                                                                        Venue                          Year   Ref                           Materials=================================================================================================  =============================  =====  ============================  ==========================================================================================================================================================================Graph based anomaly detection and description: a survey                                            DMKD                           2015   [#Akoglu2015Graph]_           _Anomaly detection in dynamic networks: a survey                                                    WIREs Computational Statistic  2015   [#Ranshous2015Anomaly]_       _Outlier detection in graphs: On the impact of multiple graph models                                ComSIS                         2019   [#Campos2019Outlier]_         _A Comprehensive Survey on Graph Anomaly Detection with Deep Learning                               TKDE                           2021   [#Ma2021A]_                   _=================================================================================================  =============================  =====  ============================  ==========================================================================================================================================================================4.4. Time Series Outlier Detection^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^=================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================Paper Title                                                                                        Venue                         Year   Ref                           Materials=================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================Outlier detection for temporal data: A survey                                                      TKDE                          2014   [#Gupta2014Outlier]_          _Detecting spacecraft anomalies using lstms and nonparametric dynamic thresholding                  KDD                           2018   [#Hundman2018Detecting]_      , Time-Series Anomaly Detection Service at Microsoft                                                 KDD                           2019   [#Ren2019Time]_               _Revisiting Time Series Outlier Detection: Definitions and Benchmarks                               NeurIPS                       2021   [#Lai2021Revisiting]_         , Graph-Augmented Normalizing Flows for Anomaly Detection of Multiple Time Series                    ICLR                          2022   [#Dai2022Graph]_               , =================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================4.5. Feature Selection in Outlier Detection^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^================================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================Paper Title                                                                                                       Venue                         Year   Ref                           Materials================================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================Unsupervised feature selection for outlier detection by modelling hierarchical value-feature couplings            ICDM                          2016   [#Pang2016Unsupervised]_      _Learning homophily couplings from non-iid data for joint feature selection and noise-resilient outlier detection  IJCAI                         2017   [#Pang2017Learning]_          _================================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================4.6. High-dimensional & Subspace Outliers^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^==================================================================================================  ============================  =====  ============================  =======================================================================================================================================================================================================Paper Title                                                                                         Venue                         Year   Ref                           Materials==================================================================================================  ============================  =====  ============================  =======================================================================================================================================================================================================A survey on unsupervised outlier detection in high-dimensional numerical data                       Stat Anal Data Min            2012   [#Zimek2012A]_                _Learning Representations of Ultrahigh-dimensional Data for Random Distance-based Outlier Detection  SIGKDD                        2018   [#Pang2018Learning]_          _Reverse Nearest Neighbors in Unsupervised Distance-Based Outlier Detection                          TKDE                          2015   [#Radovanovic2015Reverse]_    , Outlier detection for high-dimensional data                                                         Biometrika                    2015   [#Ro2015Outlier]_             _==================================================================================================  ============================  =====  ============================  =======================================================================================================================================================================================================4.7. Outlier Ensembles^^^^^^^^^^^^^^^^^^^^^^=================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================Paper Title                                                                                        Venue                         Year   Ref                           Materials=================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================Outlier ensembles: position paper                                                                  SIGKDD Explorations           2013   [#Aggarwal2013Outlier]_       _Ensembles for unsupervised outlier detection: challenges and research questions a position paper   SIGKDD Explorations           2014   [#Zimek2014Ensembles]_        _An Unsupervised Boosting Strategy for Outlier Detection Ensembles                                  PAKDD                         2018   [#Campos2018An]_              _LSCP: Locally selective combination in parallel outlier ensembles                                  SDM                           2019   [#Zhao2019LSCP]_              _Adaptive Model Pooling for Online Deep Anomaly Detection from a Complex Evolving Data Stream       KDD                           2022   [#Yoon2022ARCUS]_             , , _=================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================4.8. Outlier Detection in Evolving Data^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^==================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================Paper Title                                                                                         Venue                         Year   Ref                           Materials==================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================A Survey on Anomaly detection in Evolving Data: [with Application to Forest Fire Risk Prediction]   SIGKDD Explorations           2018   [#Salehi2018A]_               _Unsupervised real-time anomaly detection for streaming data                                         Neurocomputing                2017   [#Ahmad2017Unsupervised]_     _Outlier Detection in Feature-Evolving Data Streams                                                  SIGKDD                        2018   [#Manzoor2018Outlier]_        , Evaluating Real-Time Anomaly Detection Algorithms--The Numenta Anomaly Benchmark                    ICMLA                         2015   [#Lavin2015Evaluating]_       , MIDAS: Microcluster-Based Detector of Anomalies in Edge Streams                                     AAAI                          2020   [#Bhatia2020MIDAS]_           , NETS: Extremely Fast Outlier Detection from a Data Stream via Set-Based Processing                  VLDB                          2019   [#Yoon2019NETS]_              , , _Ultrafast Local Outlier Detection from a Data Stream with Stationary Region Skipping                KDD                           2020   [#Yoon2020STARE]_             , , _Multiple Dynamic Outlier-Detection from a Data Stream by Exploiting Duality of Data and Queries     SIGMOD                        2021   [#Yoon2021MDUAL]_             , , _Adaptive Model Pooling for Online Deep Anomaly Detection from a Complex Evolving Data Stream        KDD                           2022   [#Yoon2022ARCUS]_             , , _==================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================4.9. Representation Learning in Outlier Detection^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^==================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================Paper Title                                                                                         Venue                         Year   Ref                           Materials==================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================Learning Representations of Ultrahigh-dimensional Data for Random Distance-based Outlier Detection  SIGKDD                        2018   [#Pang2018Learning]_          _Learning representations for outlier detection on a budget                                          Preprint                      2015   [#Micenkova2015Learning]_     _XGBOD: improving supervised outlier detection with unsupervised representation learning             IJCNN                         2018   [#Zhao2018Xgbod]_             _==================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================4.10. Interpretability^^^^^^^^^^^^^^^^^^^^^^=================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================Paper Title                                                                                        Venue                         Year   Ref                           Materials=================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================Explaining Anomalies in Groups with Characterizing Subspace Rules                                  DMKD                          2018   [#Macha2018Explaining]_       _Beyond Outlier Detection: LookOut for Pictorial Explanation                                        ECML-PKDD                     2018   [#Gupta2018Beyond]_           _Contextual outlier interpretation                                                                  IJCAI                         2018   [#Liu2018Contextual]_         _Mining multidimensional contextual outliers from categorical relational data                       IDA                           2015   [#Tang2015Mining]_            _Discriminative features for identifying and interpreting outliers                                  ICDE                          2014   [#Dang2014Discriminative]_    _Sequential Feature Explanations for Anomaly Detection                                              TKDD                          2019   [#Siddiqui2019Sequential]_    _Beyond Outlier Detection: Outlier Interpretation by Attention-Guided Triplet Deviation Network     WWW                           2021   [#Xu2021Beyond]_              _=================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================4.11. Outlier Detection with Neural Networks^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^==================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================Paper Title                                                                                         Venue                         Year   Ref                           Materials==================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================Detecting spacecraft anomalies using lstms and nonparametric dynamic thresholding                   KDD                           2018   [#Hundman2018Detecting]_      , MAD-GAN: Multivariate Anomaly Detection for Time Series Data with Generative Adversarial Networks   ICANN                         2019   [#Li2019MAD]_                 , Generative Adversarial Active Learning for Unsupervised Outlier Detection                           TKDE                          2019   [#Liu2019Generative]_         , Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection                         ICLR                          2018   [#Zong2018Deep]_              , Deep Anomaly Detection with Outlier Exposure                                                        ICLR                          2019   [#Hendrycks2019Deep]_         , Unsupervised Anomaly Detection With LSTM Neural Networks                                            TNNLS                         2019   [#Ergen2019Unsupervised]_     , ,Effective End-to-end Unsupervised Outlier Detection via Inlier Priority of Discriminative Network   NeurIPS                       2019   [#Wang2019Effective]_         _ _Fascinating Supervisory Signals and Where to Find Them: Deep Anomaly Detection with Scale Learning  ICML                          2023   [#Xu2023Fascinating]_         , ==================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================4.12. Active Anomaly Detection^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^==================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================Paper Title                                                                                         Venue                         Year   Ref                           Materials==================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================Active learning for anomaly and rare-category detection                                             NeurIPS                       2005   [#Pelleg2005Active]_          _Outlier detection by active learning                                                                SIGKDD                        2006   [#Abe2006Outlier]_            _Active Anomaly Detection via Ensembles: Insights, Algorithms, and Interpretability                  Preprint                      2019   [#Das2019Active]_             _Meta-AAD: Active Anomaly Detection with Deep Reinforcement Learning                                 ICDM                          2020   [#Zha2020Meta]_               _A3: Activation Anomaly Analysis                                                                     ECML-PKDD                     2020   [#Sperl2021A3]_               , ==================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================4.13. Interactive Outlier Detection^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^=================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================Paper Title                                                                                        Venue                         Year   Ref                           Materials=================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================Learning On-the-Job to Re-rank Anomalies from Top-1 Feedback                                       SDM                           2019   [#Lamba2019Learning]_         _Interactive anomaly detection on attributed networks                                               WSDM                          2019   [#Ding2019Interactive]_       _eX2: a framework for interactive anomaly detection                                                 IUI Workshop                  2019   [#Arnaldo2019ex2]_            _Tripartite Active Learning for Interactive Anomaly Discovery                                       IEEE Access                   2019   [#Zhu2019Tripartite]_         _=================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================4.14. Outlier Detection in Other fields^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^============== =================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================Field          Paper Title                                                                                        Venue                         Year   Ref                           Materials============== =================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================Text       Outlier detection for text data                                                                    SDM                           2017   [#Kannan2017Outlier]_         _============== =================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================4.15. Outlier Detection Applications^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^========================    =================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================Field                       Paper Title                                                                                        Venue                         Year   Ref                           Materials========================    =================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================Security                A survey of distance and similarity measures used within network intrusion anomaly detection       IEEE Commun. Surv. Tutor.     2015   [#WellerFahy2015A]_           _Security                Anomaly-based network intrusion detection: Techniques, systems and challenges                      Computers & Security          2009   [#GarciaTeodoro2009Anomaly]_  _Finance                 A survey of anomaly detection techniques in financial domain                                       Future Gener Comput Syst      2016   [#Ahmed2016A]_                _Traffic                 Outlier Detection in Urban Traffic Data                                                            WIMS                          2018   [#Djenouri2018Outlier]_       _Social Media            A survey on social media anomaly detection                                                         SIGKDD Explorations           2016   [#Yu2016A]_                   _Social Media            GLAD: group anomaly detection in social media analysis                                             TKDD                          2015   [#Yu2015Glad]_                _Machine Failure         Detecting the Onset of Machine Failure Using Anomaly Detection Methods                             DAWAK                         2019   [#Riazi2019Detecting]_        _Video Surveillance      AnomalyNet: An anomaly detection network for video surveillance                                    TIFS                          2019   [#Zhou2019AnomalyNet]_        , ========================    =================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================4.16. Automated Outlier Detection^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^=================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================Paper Title                                                                                        Venue                         Year   Ref                           Materials=================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================AutoML: state of the art with a focus on anomaly detection, challenges, and research directions    Int J Data Sci Anal           2022   [#Bahri2022automl]_           _AutoOD: Automated Outlier Detection via Curiosity-guided Search and Self-imitation Learning        ICDE                          2020   [#Li2020AutoOD]_              _Automatic Unsupervised Outlier Model Selection                                                     NeurIPS                       2021   [#Zhao2020Automating]_        , =================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================4.17. Machine Learning Systems for Outlier Detection^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^This section summarizes a list of systems for outlier detection, which mayoverlap with the section of tools and libraries.=================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================Paper Title                                                                                        Venue                         Year   Ref                           Materials=================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================PyOD: A Python Toolbox for Scalable Outlier Detection                                              JMLR                          2019   [#Zhao2019PYOD]_              , SUOD: Accelerating Large-Scale Unsupervised Heterogeneous Outlier Detection                        MLSys                         2021   [#Zhao2021SUOD]_              , TOD: Tensor-based Outlier Detection                                                                Preprint                      2021   [#Zhao2021TOD]_               , =================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================4.18. Fairness and Bias in Outlier Detection^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^=================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================Paper Title                                                                                        Venue                         Year   Ref                           Materials=================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================A Framework for Determining the Fairness of Outlier Detection                                      ECAI                          2020   [#Davidson2020A]_             _FAIROD: Fairness-aware Outlier Detection                                                           AIES                          2021   [#Shekhar2021FAIROD]_         _=================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================4.19. Isolation-Based Methods^^^^^^^^^^^^^^^^^^^^^^^^^^^^^=================================================================================================  ============================  =====  =============================  ==============================================================================================================================================================================================Paper Title                                                                                        Venue                         Year   Ref                            Materials=================================================================================================  ============================  =====  =============================  ==============================================================================================================================================================================================Isolation forest                                                                                   ICDM                          2008   [#Liu2008Isolation]_           _Isolation‐based anomaly detection using nearest‐neighbor ensembles                                  Computational Intelligence    2018   [#Bandaragoda2018Isolation]_   , Extended Isolation Forest                                                                          TKDE                          2019   [#Hariri2019Extended]_         , Isolation Distributional Kernel: A New Tool for Kernel based Anomaly Detection                     KDD                           2020   [#Ting2020Isolation]_          , Deep Isolation Forest for Anomaly Detection                                                        TKDE                          2023   [#Xu2023Deep]_                 , =================================================================================================  ============================  =====  =============================  ==============================================================================================================================================================================================4.20. Emerging and Interesting Topics^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^=================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================Paper Title                                                                                        Venue                         Year   Ref                           Materials=================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================Clustering with Outlier Removal                                                                    TKDE                          2019   [#Liu2018Clustering]_         _Real-World Anomaly Detection by using Digital Twin Systems and Weakly-Supervised Learning          IEEE Trans. Ind. Informat.    2020   [#Castellani2020Siamese]_     _SSD: A Unified Framework for Self-Supervised Outlier Detection                                     ICLR                          2021   [#Sehwag2021SSD]_             , =================================================================================================  ============================  =====  ============================  ==========================================================================================================================================================================5.1. Conferences & Workshops^^^^^^^^^^^^^^^^^^^^^^^^^^^^Key data mining conference deadlines, historical acceptance rates, and morecan be found _.. ._________5.2. Journals^^^^^^^^^^^^^_____References.. [#Abe2006Outlier] Abe, N., Zadrozny, B. and Langford, J., 2006, August. Outlier detection by active learning. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 504-509, ACM... [#Aggarwal2013Outlier] Aggarwal, C.C., 2013. Outlier ensembles: position paper. ACM SIGKDD Explorations Newsletter\ , 14(2), pp.49-58... [#Ahmed2016A] Ahmed, M., Mahmood, A.N. and Islam, M.R., 2016. A survey of anomaly detection techniques in financial domain. Future Generation Computer Systems\ , 55, pp.278-288... [#Ahmad2017Unsupervised] Ahmad, S., Lavin, A., Purdy, S. and Agha, Z., 2017. Unsupervised real-time anomaly detection for streaming data. Neurocomputing, 262, pp.134-147... [#Akoglu2015Graph] Akoglu, L., Tong, H. and Koutra, D., 2015. Graph based anomaly detection and description: a survey. Data Mining and Knowledge Discovery\ , 29(3), pp.626-688... [#Angiulli2002Fast] Angiulli, F. and Pizzuti, C., 2002, August. Fast outlier detection in high dimensional spaces. In European Conference on Principles of Data Mining and Knowledge Discovery, pp. 15-27... [#Arnaldo2019ex2] Arnaldo, I., Veeramachaneni, K. and Lam, M., 2019. ex2: a framework for interactive anomaly detection. In ACM IUI Workshop on Exploratory Search and Interactive Data Analytics (ESIDA)... [#Bahri2022automl] Bahri, M., Salutari, F., Putina, A. et al. AutoML: state of the art with a focus on anomaly detection, challenges, and research directions. International Journal of Data Science and Analytics  (2022)... [#Bandaragoda2018Isolation] Bandaragoda, Tharindu R., Kai Ming Ting, David Albrecht, Fei Tony Liu, Ye Zhu, and Jonathan R. Wells. ""Isolation‐based anomaly detection using nearest‐neighbor ensembles."" Computational Intelligence 34, no. 4 (2018): 968-998... [#Bhatia2020MIDAS] Bhatia, S., Hooi, B., Yoon, M., Shin, K. and Faloutsos. C., 2020. MIDAS: Microcluster-Based Detector of Anomalies in Edge Streams. In AAAI Conference on Artificial Intelligence (AAAI)... [#Braei2020Anomaly] Braei, M. and Wagner, S., 2020. Anomaly detection in univariate time-series: A survey on the state-of-the-art. arXiv preprint arXiv:2004.00433... [#Breunig2000LOF] Breunig, M.M., Kriegel, H.P., Ng, R.T. and Sander, J., 2000, May. LOF: identifying density-based local outliers. ACM SIGMOD Record\ , 29(2), pp. 93-104... [#Bulusu2020Deep] Bulusu, S., Kailkhura, B., Li, B., Varshney, P. and Song, D., 2020. Anomalous instance detection in deep learning: A survey (No. LLNL-CONF-808677). Lawrence Livermore National Lab.(LLNL), Livermore, CA (United States)... [#Campos2016On] Campos, G.O., Zimek, A., Sander, J., Campello, R.J., Micenková, B., Schubert, E., Assent, I. and Houle, M.E., 2016. On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study. Data Mining and Knowledge Discovery\ , 30(4), pp.891-927... [#Campos2018An] Campos, G.O., Zimek, A. and Meira, W., 2018, June. An Unsupervised Boosting Strategy for Outlier Detection Ensembles. In Pacific-Asia Conference on Knowledge Discovery and Data Mining (pp. 564-576). Springer, Cham... [#Campos2019Outlier] Campos, G.O., Moreira, E., Meira Jr, W. and Zimek, A., 2019. Outlier Detection in Graphs: A Study on the Impact of Multiple Graph Models. Computer Science & Information Systems, 16(2)... [#Castellani2020Siamese] Castellani, A., Schmitt, S., Squartini, S., 2020. Real-World Anomaly Detection by using Digital Twin Systems and Weakly-Supervised Learning. In IEEE Transactions on Industrial Informatics... [#Chalapathy2019Deep] Chalapathy, R. and Chawla, S., 2019. Deep learning for anomaly detection: A survey. arXiv preprint arXiv:1901.03407... [#Chandola2009Anomaly] Chandola, V., Banerjee, A. and Kumar, V., 2009. Anomaly detection: A survey. ACM computing surveys , 41(3), p.15... [#Chawla2011Anomaly] Chawla, S. and Chandola, V., 2011, Anomaly Detection: A Tutorial. Tutorial at ICDM 2011... [#Chen2017Outlier] Chen, J., Sathe, S., Aggarwal, C. and Turaga, D., 2017, June. Outlier detection with autoencoder ensembles. SIAM International Conference on Data Mining, pp. 90-98. Society for Industrial and Applied Mathematics... [#Dai2022Graph] Dai, E. and Chen, J., 2022. Graph-Augmented Normalizing Flows for Anomaly Detection of Multiple Time Series. International Conference on Learning Representations (ICLR)... [#Dang2014Discriminative] Dang, X.H., Assent, I., Ng, R.T., Zimek, A. and Schubert, E., 2014, March. Discriminative features for identifying and interpreting outliers. In International Conference on Data Engineering (ICDE). IEEE... [#Das2019Active] Das, S., Islam, M.R., Jayakodi, N.K. and Doppa, J.R., 2019. Active Anomaly Detection via Ensembles: Insights, Algorithms, and Interpretability. arXiv preprint arXiv:1901.08930... [#Davidson2020A] Davidson, I. and Ravi, S.S., 2020. A framework for determining the fairness of outlier detection. In Proceedings of the 24th European Conference on Artificial Intelligence (ECAI2020) (Vol. 2029)... [#Ding2019Interactive] Ding, K., Li, J. and Liu, H., 2019, January. Interactive anomaly detection on attributed networks. In Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining, pp. 357-365. ACM... [#Djenouri2018Outlier] Djenouri, Y. and Zimek, A., 2018, June. Outlier detection in urban traffic data. In Proceedings of the 8th International Conference on Web Intelligence, Mining and Semantics. ACM... [#Domingues2018A] Domingues, R., Filippone, M., Michiardi, P. and Zouaoui, J., 2018. A comparative evaluation of outlier detection algorithms: Experiments and analyses. Pattern Recognition, 74, pp.406-421... [#Emmott2015A] Emmott, A., Das, S., Dietterich, T., Fern, A. and Wong, W.K., 2015. A meta-analysis of the anomaly detection problem. arXiv preprint arXiv:1503.01158... [#Ergen2019Unsupervised] Ergen, T. and Kozat, S.S., 2019. Unsupervised Anomaly Detection With LSTM Neural Networks. IEEE transactions on neural networks and learning systems... [#Falcao2019Quantitative] Falcão, F., Zoppi, T., Silva, C.B.V., Santos, A., Fonseca, B., Ceccarelli, A. and Bondavalli, A., 2019, April. Quantitative comparison of unsupervised anomaly detection algorithms for intrusion detection. In Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing, (pp. 318-327). ACM... [#GarciaTeodoro2009Anomaly] Garcia-Teodoro, P., Diaz-Verdejo, J., Maciá-Fernández, G. and Vázquez, E., 2009. Anomaly-based network intrusion detection: Techniques, systems and challenges. Computers & Security\ , 28(1-2), pp.18-28... [#Goldstein2016A] Goldstein, M. and Uchida, S., 2016. A comparative evaluation of unsupervised anomaly detection algorithms for multivariate data. PloS one\ , 11(4), p.e0152173... [#Gupta2014Outlier] Gupta, M., Gao, J., Aggarwal, C.C. and Han, J., 2014. Outlier detection for temporal data: A survey. IEEE Transactions on Knowledge and Data Engineering\ , 26(9), pp.2250-2267... [#Hariri2019Extended] Hariri, S., Kind, M.C. and Brunner, R.J., 2019. Extended Isolation Forest. IEEE Transactions on Knowledge and Data Engineering... [#Hendrycks2019Deep] Hendrycks, D., Mazeika, M. and Dietterich, T.G., 2019. Deep Anomaly Detection with Outlier Exposure. International Conference on Learning Representations (ICLR)... [#Hodge2004A] Hodge, V. and Austin, J., 2004. A survey of outlier detection methodologies. Artificial intelligence review\ , 22(2), pp.85-126... [#Hojjati2022Self] Hojjati, H., Ho, T.K.K. and Armanfard, N., 2022. Self-Supervised Anomaly Detection: A Survey and Outlook. arXiv preprint arXiv:2205.05173... [#Hundman2018Detecting] Hundman, K., Constantinou, V., Laporte, C., Colwell, I. and Soderstrom, T., 2018, July. Detecting spacecraft anomalies using lstms and nonparametric dynamic thresholding. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, (pp. 387-395). ACM... [#Kannan2017Outlier] Kannan, R., Woo, H., Aggarwal, C.C. and Park, H., 2017, June. Outlier detection for text data. In Proceedings of the 2017 SIAM International Conference on Data Mining, pp. 489-497. Society for Industrial and Applied Mathematics. .. [#Kriegel2010Outlier] Kriegel, H.P., Kröger, P. and Zimek, A., 2010. Outlier detection techniques. Tutorial at ACM SIGKDD 2010... [#Lai2021Revisiting] Lai, K.H., Zha, D., Xu, J., Zhao, Y., Wang, G. and Hu, X., 2021. Revisiting Time Series Outlier Detection: Definitions and Benchmarks. Advances in Neural Information Processing Systems (NeurIPS), Datasets and Benchmarks Track... [#Lamba2019Learning] Lamba, H. and Akoglu, L., 2019, May. Learning On-the-Job to Re-rank Anomalies from Top-1 Feedback. In Proceedings of the 2019 SIAM International Conference on Data Mining (SDM), pp. 612-620. Society for Industrial and Applied Mathematics... [#Lavin2015Evaluating] Lavin, A. and Ahmad, S., 2015, December. Evaluating Real-Time Anomaly Detection Algorithms--The Numenta Anomaly Benchmark. In 2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA) (pp. 38-44). IEEE... [#Lazarevic2008Data] Lazarevic, A., Banerjee, A., Chandola, V., Kumar, V. and Srivastava, J., 2008, September. Data mining for anomaly detection. Tutorial at ECML PKDD 2008... [#Li2019MAD] Li, D., Chen, D., Jin, B., Shi, L., Goh, J. and Ng, S.K., 2019, September. MAD-GAN: Multivariate anomaly detection for time series data with generative adversarial networks. In International Conference on Artificial Neural Networks (pp. 703-716). Springer, Cham... [#Li2020COPOD] Li, Z., Zhao, Y., Botta, N., Ionescu, C. and Hu, X. COPOD: Copula-Based Outlier Detection. IEEE International Conference on Data Mining (ICDM), 2020... [#Li2021ECOD] Li, Z., Zhao, Y., Hu, X., Botta, N., Ionescu, C. and Chen, H. G. ECOD: Unsupervised Outlier Detection Using Empirical Cumulative Distribution Functions. IEEE Transactions on Knowledge and Data Engineering (TKDE), 2022... [#Liu2008Isolation] Liu, F.T., Ting, K.M. and Zhou, Z.H., 2008, December. Isolation forest. In International Conference on Data Mining\ , pp. 413-422. IEEE... [#Liu2018Clustering] Liu, H., Li, J., Wu, Y. and Fu, Y., 2019. Clustering with outlier removal. IEEE transactions on knowledge and data engineering... [#Liu2018Contextual] Liu, N., Shin, D. and Hu, X., 2017. Contextual outlier interpretation. In International Joint Conference on Artificial Intelligence (IJCAI-18), pp.2461-2467... [#Liu2019Generative] Liu, Y., Li, Z., Zhou, C., Jiang, Y., Sun, J., Wang, M. and He, X., 2019. Generative Adversarial Active Learning for Unsupervised Outlier Detection. IEEE transactions on knowledge and data engineering... [#Li2020AutoOD] Li, Y., Chen, Z., Zha, D., Zhou, K., Jin, H., Chen, H. and Hu, X., 2020. AutoOD: Automated Outlier Detection via Curiosity-guided Search and Self-imitation Learning. ICDE... [#Liu2022Benchmarking] Liu, K., Dou, Y., Zhao, Y., Ding, X., Hu, X., Zhang, R., Ding, K., Chen, C., Peng, H., Shu, K., Sun, L., Li, J., Chen, G.H., Jia, Z., and Yu, P.S. 2022. Benchmarking Node Outlier Detection on Graphs. arXiv preprint arXiv:2206.10071... [#Ma2021A] Ma, X., Wu, J., Xue, S., Yang, J., Zhou, C., Sheng, Q.Z., Xiong, H. and Akoglu, L., 2021. A comprehensive survey on graph anomaly detection with deep learning. IEEE Transactions on Knowledge and Data Engineering... [#Macha2018Explaining] Macha, M. and Akoglu, L., 2018. Explaining anomalies in groups with characterizing subspace rules. Data Mining and Knowledge Discovery, 32(5), pp.1444-1480... [#Manzoor2018Outlier] Manzoor, E., Lamba, H. and Akoglu, L. Outlier Detection in Feature-Evolving Data Streams. In 24th ACM SIGKDD International Conference on Knowledge Discovery and Data mining (KDD). 2018... [#Mendiratta2017Anomaly] Mendiratta, B.V., 2017. Anomaly Detection in Networks. Tutorial at ACM SIGKDD 2017... [#Micenkova2015Learning] Micenková, B., McWilliams, B. and Assent, I., 2015. Learning representations for outlier detection on a budget. arXiv preprint arXiv:1507.08104... [#Gupta2018Beyond] Gupta, N., Eswaran, D., Shah, N., Akoglu, L. and Faloutsos, C., Beyond Outlier Detection: LookOut for Pictorial Explanation. ECML PKDD 2018... [#Han2022Adbench] Han, S., Hu, X., Huang, H., Jiang, M. and Zhao, Y., 2022. ADBench: Anomaly Detection Benchmark. arXiv preprint arXiv:2206.09426... [#Pang2016Unsupervised] Pang, G., Cao, L., Chen, L. and Liu, H., 2016, December. Unsupervised feature selection for outlier detection by modelling hierarchical value-feature couplings. In Data Mining (ICDM), 2016 IEEE 16th International Conference on (pp. 410-419). IEEE... [#Pang2017Learning] Pang, G., Cao, L., Chen, L. and Liu, H., 2017, August. Learning homophily couplings from non-iid data for joint feature selection and noise-resilient outlier detection. In Proceedings of the 26th International Joint Conference on Artificial Intelligence (pp. 2585-2591). AAAI Press... [#Pang2018Learning] Pang, G., Cao, L., Chen, L. and Liu, H., 2018. Learning Representations of Ultrahigh-dimensional Data for Random Distance-based Outlier Detection. In 24th ACM SIGKDD International Conference on Knowledge Discovery and Data mining (KDD). 2018... [#Pang2020Deep] Pang, G., Shen, C., Cao, L. and Hengel, A.V.D., 2021. Deep Learning for Anomaly Detection: A Review. ACM Computing Surveys (CSUR), 54(2), pp.1-38... [#Pang2021Deep] Pang, G., Cao, L. and Aggarwal, C., 2021. Deep Learning for Anomaly Detection. Tutorial at WSDM 2021... [#Pelleg2005Active] Pelleg, D. and Moore, A.W., 2005. Active learning for anomaly and rare-category detection. In Advances in neural information processing systems pp. 1073-1080... [#Radovanovic2015Reverse] Radovanović, M., Nanopoulos, A. and Ivanović, M., 2015. Reverse nearest neighbors in unsupervised distance-based outlier detection. IEEE transactions on knowledge and data engineering, 27(5), pp.1369-1382... [#Ramaswamy2000Efficient] Ramaswamy, S., Rastogi, R. and Shim, K., 2000, May. Efficient algorithms for mining outliers from large data sets. ACM SIGMOD Record\ , 29(2), pp. 427-438... [#Ranshous2015Anomaly] Ranshous, S., Shen, S., Koutra, D., Harenberg, S., Faloutsos, C. and Samatova, N.F., 2015. Anomaly detection in dynamic networks: a survey. Wiley Interdisciplinary Reviews: Computational Statistics, 7(3), pp.223-247... [#Ren2019Time] Ren, H., Xu, B., Wang, Y., Yi, C., Huang, C., Kou, X., Xing, T., Yang, M., Tong, J. and Zhang, Q., 2019. Time-Series Anomaly Detection Service at Microsoft. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM... [#Riazi2019Detecting] Riazi, M., Zaiane, O., Takeuchi, T., Maltais, A., Günther, J. and Lipsett, M., Detecting the Onset of Machine Failure Using Anomaly Detection Methods... [#Ro2015Outlier] Ro, K., Zou, C., Wang, Z. and Yin, G., 2015. Outlier detection for high-dimensional data. Biometrika, 102(3), pp.589-599... [#Salehi2018A] Salehi, Mahsa & Rashidi, Lida. (2018). A Survey on Anomaly detection in Evolving Data: [with Application to Forest Fire Risk Prediction]. ACM SIGKDD Explorations Newsletter. 20. 13-23... [#Salehi2021A] Salehi, M., Mirzaei, H., Hendrycks, D., Li, Y., Rohban, M.H., Sabokrou, M., 2021. A Unified Survey on Anomaly, Novelty, Open-Set, and Out-of-Distribution Detection: Solutions and Future Challenges. arXiv preprint arXiv:2110.14051... [#Scholkopf2001Estimating] Schölkopf, B., Platt, J.C., Shawe-Taylor, J., Smola, A.J. and Williamson, R.C., 2001. Estimating the support of a high-dimensional distribution. Neural Computation, 13(7), pp.1443-1471... [#Sehwag2021SSD] Sehwag, V., Chiang, M., Mittal, P., 2021. SSD: A Unified Framework for Self-Supervised Outlier Detection. International Conference on Learning Representations (ICLR)... [#Shekhar2021FAIROD] Shekhar, S., Shah, N. and Akoglu, L., 2021. FAIROD: Fairness-aware Outlier Detection. AAAI/ACM Conference on AI, Ethics, and Society (AIES)... [#Siddiqui2019Sequential] Siddiqui, M.A., Fern, A., Dietterich, T.G. and Wong, W.K., 2019. Sequential Feature Explanations for Anomaly Detection. ACM Transactions on Knowledge Discovery from Data (TKDD), 13(1), p.1... [#Sperl2021A3] Sperl, P., Schulze, J.-P., and Böttinger, K., 2021. Activation Anomaly Analysis. European Conference on Machine Learning and Data Mining (ECML-PKDD) 2020... [#Suri2019Research] Suri, N.R. and Athithan, G., 2019. Research Issues in Outlier Detection. In Outlier Detection: Techniques and Applications, pp. 29-51. Springer, Cham... [#Tang2015Mining] Tang, G., Pei, J., Bailey, J. and Dong, G., 2015. Mining multidimensional contextual outliers from categorical relational data. Intelligent Data Analysis, 19(5), pp.1171-1192... [#Ting2018Which] Ting, KM., Aryal, S. and Washio, T., 2018, Which Anomaly Detector should I use? Tutorial at ICDM 2018... [#Ting2020Isolation] Ting, Kai Ming, Bi-Cun Xu, Takashi Washio, and Zhi-Hua Zhou. ""Isolation Distributional Kernel: A New Tool for Kernel based Anomaly Detection."" In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 198-206. 2020... [#Wang2019Effective] Wang, S., Zeng, Y., Liu, X., Zhu, E., Yin, J., Xu, C. and Kloft, M., 2019. Effective End-to-end Unsupervised Outlier Detection via Inlier Priority of Discriminative Network. In 33rd Conference on Neural Information Processing Systems... [#Wang2019Progress] Wang, H., Bah, M.J. and Hammad, M., 2019. Progress in Outlier Detection Techniques: A Survey. IEEE Access, 7, pp.107964-108000... [#Wang2020Deep] Wang, R., Nie, K., Chang, Y. J., Gong, X., Wang, T., Yang, Y., Long, B.,  2020. Deep Learning for Anomaly Detection. Tutorial at KDD 2020... [#WellerFahy2015A] Weller-Fahy, D.J., Borghetti, B.J. and Sodemann, A.A., 2015. A survey of distance and similarity measures used within network intrusion anomaly detection. IEEE Communications Surveys & Tutorials\ , 17(1), pp.70-91... [#Xu2021Beyond] Xu, H., Wang, Y., Jian, S., Huang, Z., Wang, Y., Liu, N. and Li, F., 2021, April. Beyond Outlier Detection: Outlier Interpretation by Attention-Guided Triplet Deviation Network. In Proceedings of the Web Conference 2021 (pp. 1328-1339)... [#Xu2023Deep] Xu, H., Pang, G., Wang, Y., Wang, Y., 2023. Deep Isolation Forest for Anomaly Detection. IEEE Transactions on Knowledge and Data Engineering. .. [#Xu2023Fascinating] Xu, H., Wang, Y., Wei, J., Jian, S., Li, Y., Liu, N., 2023. Fascinating Supervisory Signals and Where to Find Them: Deep Anomaly Detection with Scale Learning. International Conference on Machine Learning (ICML)... [#Yoon2019NETS] Yoon, S., Lee, J. G., & Lee, B. S., 2019. NETS: extremely fast outlier detection from a data stream via set-based processing. Proceedings of the VLDB Endowment, 12(11), 1303-1315... [#Yoon2020STARE] Yoon, S., Lee, J. G., & Lee, B. S., 2020. Ultrafast local outlier detection from a data stream with stationary region skipping. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (pp. 1181-1191).. [#Yoon2021MDUAL] Yoon, S., Shin, Y., Lee, J. G., & Lee, B. S. (2021, June). Multiple dynamic outlier-detection from a data stream by exploiting duality of data and queries. In Proceedings of the 2021 International Conference on Management of Data (SIGMOD)... [#Yoon2022ARCUS] Yoon, S., Lee, Y., Lee, J.G. and Lee, B.S., 2022, August. Adaptive Model Pooling for Online Deep Anomaly Detection from a Complex Evolving Data Stream. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (pp. 2347-2357)... [#Yu2015Glad] Yu, R., He, X. and Liu, Y., 2015. GLAD: group anomaly detection in social media analysis. ACM Transactions on Knowledge Discovery from Data (TKDD)\ , 10(2), p.18... [#Yu2016A] Yu, R., Qiu, H., Wen, Z., Lin, C. and Liu, Y., 2016. A survey on social media anomaly detection. ACM SIGKDD Explorations Newsletter\ , 18(1), pp.1-14... [#Zha2020Meta] Zha, D., Lai, K.H., Wan, M. and Hu, X., 2020. Meta-AAD: Active Anomaly Detection with Deep Reinforcement Learning. ICDM... [#Zhao2018Xgbod] Zhao, Y. and Hryniewicki, M.K., 2018, July. XGBOD: improving supervised outlier detection with unsupervised representation learning. In 2018 International Joint Conference on Neural Networks (IJCNN). IEEE... [#Zhao2019LSCP] Zhao, Y., Nasrullah, Z., Hryniewicki, M.K. and Li, Z., 2019, May. LSCP: Locally selective combination in parallel outlier ensembles. In Proceedings of the 2019 SIAM International Conference on Data Mining (SDM), pp. 585-593. Society for Industrial and Applied Mathematics... [#Zhao2019PYOD] Zhao, Y., Nasrullah, Z. and Li, Z., PyOD: A Python Toolbox for Scalable Outlier Detection. Journal of Machine Learning Research, 20, pp.1-7... [#Zhao2020Automating] Zhao, Y., Rossi, R.A. and Akoglu, L., 2021. Automatic Unsupervised Outlier Model Selection. Advances in Neural Information Processing Systems... [#Zhao2021SUOD] Zhao, Y., Hu, X., Cheng, C., Wang, C., Wan, C., Wang, W., Yang, J., Bai, H., Li, Z., Xiao, C. and Wang, Y., 2021. SUOD: Accelerating Large-scale Unsupervised Heterogeneous Outlier Detection. Proceedings of Machine Learning and Systems (MLSys)... [#Zhao2021TOD] Zhao, Y., Chen, G.H. and Jia, Z., 2021. TOD: Tensor-based Outlier Detection. arXiv preprint arXiv:2110.14007... [#Zhou2019AnomalyNet] Zhou, J.T., Du, J., Zhu, H., Peng, X., Liu, Y. and Goh, R.S.M., 2019. AnomalyNet: An anomaly detection network for video surveillance. IEEE Transactions on Information Forensics and Security... [#Zhu2019Tripartite] Zhu, Y. and Yang, K., 2019. Tripartite Active Learning for Interactive Anomaly Discovery. IEEE Access... [#Zimek2012A] Zimek, A., Schubert, E. and Kriegel, H.P., 2012. A survey on unsupervised outlier detection in high‐dimensional numerical data. Statistical Analysis and Data Mining: The ASA Data Science Journal\ , 5(5), pp.363-387... [#Zimek2014Ensembles] Zimek, A., Campello, R.J. and Sander, J., 2014. Ensembles for unsupervised outlier detection: challenges and research questions a position paper. ACM Sigkdd Explorations Newsletter\ , 15(1), pp.11-22... [#Zong2018Deep] Zong, B., Song, Q., Min, M.R., Cheng, W., Lumezanu, C., Cho, D. and Chen, H., 2018. Deep autoencoding gaussian mixture model for unsupervised anomaly detection. International Conference on Learning Representations (ICLR)."
https://github.com/ageitgey/face_recognition,The world's simplest facial recognition api for Python and the command line,"Face RecognitionYou can also read a translated version of this file Recognize and manipulate faces from Python or from the command line withthe world's simplest face recognition library.Built using 's state-of-the-art face recognitionbuilt with deep learning. The model has an accuracy of 99.38% on the benchmark.This also provides a simple  command line tool that letsyou do face recognition on a folder of images from the command line!FeaturesFind faces in picturesFind all the faces that appear in a picture:import face_recognition
image = face_recognition.load_image_file(""your_file.jpg"")
face_locations = face_recognition.face_locations(image)
Find and manipulate facial features in picturesGet the locations and outlines of each person's eyes, nose, mouth and chin.import face_recognition
image = face_recognition.load_image_file(""your_file.jpg"")
face_landmarks_list = face_recognition.face_landmarks(image)
Finding facial features is super useful for lots of important stuff. But you can also use it for really stupid stufflike applying  (think 'Meitu'):Identify faces in picturesRecognize who appears in each photo.import face_recognition
known_image = face_recognition.load_image_file(""biden.jpg"")
unknown_image = face_recognition.load_image_file(""unknown.jpg"")

biden_encoding = face_recognition.face_encodings(known_image)[0]
unknown_encoding = face_recognition.face_encodings(unknown_image)[0]

results = face_recognition.compare_faces([biden_encoding], unknown_encoding)
You can even use this library with other Python libraries to do real-time face recognition:See  for the code.Online DemosUser-contributed shared Jupyter notebook demo (not officially supported): InstallationRequirementsInstallation Options:Installing on Mac or LinuxFirst, make sure you have dlib already installed with Python bindings:Then, make sure you have cmake installed:  Finally, install this module from pypi using  (or  for Python 2):pip3 install face_recognition
Alternatively, you can try this library with , see .If you are having trouble with installation, you can also try out a.Installing on an Nvidia Jetson Nano boardInstalling on Raspberry Pi 2+Installing on FreeBSDpkg install graphics/py-face_recognition
Installing on WindowsWhile Windows isn't officially supported, helpful users have posted instructions on how to install this library:Installing a pre-configured Virtual Machine imageUsageCommand-Line InterfaceWhen you install , you get two simple command-lineprograms: command line toolThe  command lets you recognize faces in a photograph orfolder full  for photographs.First, you need to provide a folder with one picture of each person youalready know. There should be one image file for each person with thefiles named according to who is in the picture:Next, you need a second folder with the files you want to identify:Then in you simply run the command , passing inthe folder of known people and the folder (or single image) with unknownpeople and it tells you who is in each image:$ face_recognition ./pictures_of_people_i_know/ ./unknown_pictures/

/unknown_pictures/unknown.jpg,Barack Obama
/face_recognition_test/unknown_pictures/unknown.jpg,unknown_person
There's one line in the output for each face. The data is comma-separatedwith the filename and the name of the person found.An  is a face in the image that didn't match anyone inyour folder of known people. command line toolThe  command lets you find the location (pixel coordinatates)of any faces in an image.Just run the command , passing in a folder of imagesto check (or a single image):$ face_detection  ./folder_with_pictures/

examples/image1.jpg,65,215,169,112
examples/image2.jpg,62,394,211,244
examples/image2.jpg,95,941,244,792
It prints one line for each face that was detected. The coordinatesreported are the top, right, bottom and left coordinates of the face (in pixels).Adjusting Tolerance / SensitivityIf you are getting multiple matches for the same person, it might be thatthe people in your photos look very similar and a lower tolerance valueis needed to make face comparisons more strict.You can do that with the  parameter. The default tolerancevalue is 0.6 and lower numbers make face comparisons more strict:$ face_recognition --tolerance 0.54 ./pictures_of_people_i_know/ ./unknown_pictures/

/unknown_pictures/unknown.jpg,Barack Obama
/face_recognition_test/unknown_pictures/unknown.jpg,unknown_person
If you want to see the face distance calculated for each match in orderto adjust the tolerance setting, you can use :$ face_recognition --show-distance true ./pictures_of_people_i_know/ ./unknown_pictures/

/unknown_pictures/unknown.jpg,Barack Obama,0.378542298956785
/face_recognition_test/unknown_pictures/unknown.jpg,unknown_person,None
More ExamplesIf you simply want to know the names of the people in each photograph but don'tcare about file names, you could do this:$ face_recognition ./pictures_of_people_i_know/ ./unknown_pictures/ | cut -d ',' -f2

Barack Obama
unknown_person
Speeding up Face RecognitionFace recognition can be done in parallel if you have a computer withmultiple CPU cores. For example, if your system has 4 CPU cores, you canprocess about 4 times as many images in the same amount of time by usingall your CPU cores in parallel.If you are using Python 3.4 or newer, pass in a  parameter:$ face_recognition --cpus 4 ./pictures_of_people_i_know/ ./unknown_pictures/
You can also pass in  to use all CPU cores in your system.Python ModuleYou can import the  module and then easily manipulatefaces with just a couple of lines of code. It's super easy!API Docs: .Automatically find all the faces in an imageimport face_recognition

image = face_recognition.load_image_file(""my_picture.jpg"")
face_locations = face_recognition.face_locations(image)

# face_locations is now an array listing the co-ordinates of each face!
See to try it out.You can also opt-in to a somewhat more accurate deep-learning-based face detection model.Note: GPU acceleration (via NVidia's CUDA library) is required for goodperformance with this model. You'll also want to enable CUDA supportwhen compliling .import face_recognition

image = face_recognition.load_image_file(""my_picture.jpg"")
face_locations = face_recognition.face_locations(image, model=""cnn"")

# face_locations is now an array listing the co-ordinates of each face!
See to try it out.If you have a lot of images and a GPU, you can also.Automatically locate the facial features of a person in an imageimport face_recognition

image = face_recognition.load_image_file(""my_picture.jpg"")
face_landmarks_list = face_recognition.face_landmarks(image)

# face_landmarks_list is now an array with the locations of each facial feature in each face.
# face_landmarks_list[0]['left_eye'] would be the location and outline of the first person's left eye.
See to try it out.Recognize faces in images and identify who they areimport face_recognition

picture_of_me = face_recognition.load_image_file(""me.jpg"")
my_face_encoding = face_recognition.face_encodings(picture_of_me)[0]

# my_face_encoding now contains a universal 'encoding' of my facial features that can be compared to any other picture of a face!

unknown_picture = face_recognition.load_image_file(""unknown.jpg"")
unknown_face_encoding = face_recognition.face_encodings(unknown_picture)[0]

# Now we can see the two face encodings are of the same person with `compare_faces`!

results = face_recognition.compare_faces([my_face_encoding], unknown_face_encoding)

if results[0] == True:
    print(""It's a picture of me!"")
else:
    print(""It's not a picture of me!"")
See to try it out.Python Code ExamplesAll the examples are available .Face DetectionFacial FeaturesFacial RecognitionCreating a Standalone ExecutableIf you want to create a standalone executable that can run without the need to install  or , you can use . However, it requires some custom configuration to work with this library. See  for how to do it.Articles and Guides that cover How Face Recognition WorksIf you want to learn how face location and recognition work instead ofdepending on a black box library, .CaveatsDeployment to Cloud Hosts (Heroku, AWS, etc)Since  depends on  which is written in C++, it can be tricky to deploy an appusing it to a cloud hosting provider like Heroku or AWS.To make things easier, there's an example Dockerfile in this repo that shows how to run an app built with in a  container. With that, you should be able to deployto any service that supports Docker images.You can try the Docker image locally by running: There are also Linux users with a GPU (drivers >= 384.81) and  installed can run the example on the GPU: Open the  file and uncomment the  and  lines.Having problems?If you run into problems, please read the  section of the wiki before filing a github issue.Thanks"
https://github.com/microsoft/nlp-recipes,Natural Language Processing Best Practices & Examples,"NLP Best PracticesIn recent years, natural language processing (NLP) has seen quick growth in quality and usability, and this has helped to drive business adoption of artificial intelligence (AI) solutions. In the last few years, researchers have been applying newer deep learning methods to NLP. Data scientists started moving from traditional methods to state-of-the-art (SOTA) deep neural network (DNN) algorithms which use language models pretrained on large text corpora.This repository contains examples and best practices for building NLP systems, provided as  and . The focus of the repository is on state-of-the-art methods and common scenarios that are popular among researchers and practitioners working on problems involving text and language.OverviewThe goal of this repository is to build a comprehensive set of tools and examples that leverage recent advances in NLP algorithms, neural architectures, and distributed machine learning systems.The content is based on our past and potential future engagements with customers as well as collaboration with partners, researchers, and the open source community.We hope that the tools can significantly reduce the “time to market” by simplifying the experience from defining the business problem to development of solution by orders of magnitude. In addition, the example notebooks would serve as guidelines and showcase best practices and usage of the tools in a wide variety of languages.In an era of transfer learning, transformers, and deep architectures, we believe that pretrained models provide a unified solution to many real-world problems and allow handling different tasks and languages easily. We will, therefore, prioritize such models, as they achieve state-of-the-art results on several NLP benchmarks like  and  leaderboards. The models can be used in a number of applications ranging from simple text classification to sophisticated intelligent chat bots.Note that for certain kind of NLP problems, you may not need to build your own models. Instead, pre-built or easily customizable solutions exist which do not require any custom coding or machine learning expertise. We strongly recommend evaluating if these can sufficiently solve your problem. If these solutions are not applicable, or the accuracy of these solutions is not sufficient, then resorting to more complex and time-consuming custom approaches may be necessary. The following cognitive services offer simple solutions to address common NLP tasks:  are a set of pre-trained REST APIs which can be called for Sentiment Analysis, Key phrase extraction, Language detection and Named Entity Detection and more. These APIs work out of the box and require minimal expertise in machine learning, but have limited customization capabilities. is a cloud-based API service that lets you create a conversational question-and-answer layer over your existing data. Use it to build a knowledge base by extracting questions and answers from your semi-structured content, including FAQs, manuals, and documents. is a SaaS service to train and deploy a model as a REST API given a user-provided training set. You could do Intent Classification as well as Named Entity Extraction by performing simple steps of providing example utterances and labelling them. It supports Active Learning, so your model always keeps learning and improving.Target AudienceFor this repository our target audience includes data scientists and machine learning engineers with varying levels of NLP knowledge as our content is source-only and targets custom machine learning modelling. The utilities and examples provided are intended to be solution accelerators for real-world NLP problems.Focus AreasThe repository aims to expand NLP capabilities along three separate dimensionsScenariosWe aim to have end-to-end examples of common tasks and scenarios such as text classification, named entity recognition etc.AlgorithmsWe aim to support multiple models for each of the supported scenarios. Currently, transformer-based models are supported across most scenarios. We have been working on integrating the  from  which allows users to easily load pretrained models and fine-tune them for different tasks.LanguagesWe strongly subscribe to the multi-language principles laid down by The repository aims to support non-English languages  across all the scenarios. Pre-trained models used in the repository such as BERT, FastText support 100+ languages out of the box. Our goal is to provide end-to-end examples in as many languages as possible. We encourage community contributions in this area.ContentThe following is a summary of the commonly used NLP scenarios covered in the repository. Each scenario is demonstrated in one or more  that make use of the core code base of models and repository utilities.| Scenario                              |  Models | Description|Languages||-------------------------|  ------------------- |-------|---||Text Classification                     |BERT, DistillBERT, XLNet, RoBERTa, ALBERT, XLM| Text classification is a supervised learning method of learning and predicting the category or the class of a document given its text content. |English, Chinese, Hindi, Arabic, German, French, Japanese, Spanish, Dutch||Named Entity Recognition                |BERT| Named entity recognition (NER) is the task of classifying words or key phrases of a text into predefined entities of interest. |English||Text Summarization|BERTSumExt  BERTSumAbs  UniLM (s2s-ft)  MiniLM |Text summarization is a language generation task of summarizing the input text into a shorter paragraph of text.|English|Entailment                              |BERT, XLNet, RoBERTa| Textual entailment is the task of classifying the binary relation between two natural-language texts,  text and hypothesis, to determine if the text agrees with the hypothesis or not. |English||Question Answering                      |BiDAF, BERT, XLNet| Question answering (QA) is the task of retrieving or generating a valid answer for a given query in natural language, provided with a passage related to the query. |English||Sentence Similarity                     |BERT, GenSen| Sentence similarity is the process of computing a similarity score given a pair of text documents. |English||Embeddings| Word2VecfastTextGloVe| Embedding is the process of converting a word or a piece of text to a continuous vector space of real number, usually, in low dimension.|English||Sentiment Analysis| Dependency Parser GloVe| Provides an example of train and use Aspect Based Sentiment Analysis with Azure ML and  .|English|Getting StartedWhile solving NLP problems, it is always good to start with the prebuilt . When the needs are beyond the bounds of the prebuilt cognitive service and when you want to search for custom machine learning methods,  you will find this repository  very useful. To get started, navigate to the , which lists instructions on how to setup your environment and dependencies.Azure Machine Learning Service is a cloud service used to train, deploy, automate, and manage machine learning models, all at the broad scale that the cloud provides. AzureML is presented in notebooks across different scenarios to enhance the efficiency of developing Natural Language systems at scale and for various AI model development related tasks like:To successfully run these notebooks, you will need an  or can . There may be other Azure services or products used in the notebooks. Introduction and/or reference of those will be provided in the notebooks themselves.ContributingWe hope that the open source community would contribute to the content and bring in the latest SOTA algorithm. This project welcomes contributions and suggestions. Before contributing, please see our .Blog PostsReferencesThe following is a list of related repositories that we like and think are useful for NLP tasks.|Repository|Description||---|---|||A great PyTorch library from Hugging Face with implementations of popular transformer-based models. We've been using their package extensively in this repo and greatly appreciate their effort.|||ML and deep learning examples with Azure Machine Learning.|||End-to-end recipes for pre-training and fine-tuning BERT using Azure Machine Learning service.|||MASS: Masked Sequence to Sequence Pre-training for Language Generation.|||Multi-Task Deep Neural Networks for Natural Language Understanding.|||Unified Language Model Pre-training.|||DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation|Build Status| Build | Branch | Status || --- | --- | --- || Linux CPU | master |  || Linux CPU | staging |  || Linux GPU | master |  || Linux GPU | staging |  |"
https://github.com/yahoo/TensorFlowOnSpark,TensorFlowOnSpark brings TensorFlow programs to Apache Spark clusters.,"TensorFlowOnSparkBy combining salient features from the  deep learning framework with  and , TensorFlowOnSpark enables distributeddeep learning on a cluster of GPU and CPU servers.It enables both distributed TensorFlow training andinferencing on Spark clusters, with a goal to minimize the amountof code changes required to run existing TensorFlow programs on ashared grid.  Its Spark-compatible API helps manage the TensorFlowcluster with the following steps:Table of ContentsBackgroundTensorFlowOnSpark was developed by Yahoo for large-scale distributeddeep learning on our Hadoop clusters in Yahoo's private cloud.TensorFlowOnSpark provides some important benefits (see )over alternative deep learning solutions.InstallTensorFlowOnSpark is provided as a pip package, which can be installed on single machines via:# for tensorflow>=2.0.0
pip install tensorflowonspark

# for tensorflow<2.0.0
pip install tensorflowonspark==1.4.4
For distributed clusters, please see our  for detailed documentation for specific environments, such as our getting started guides for ,  and .  Note: the Windows operating system is not currently supported due to .UsageTo use TensorFlowOnSpark with an existing TensorFlow application, you can follow our  to describe the required changes.  Additionally, our  has pointers to some presentations which provide an overview of the platform.Note: since TensorFlow 2.x breaks API compatibility with TensorFlow 1.x, the examples have been updated accordingly.  If you are using TensorFlow 1.x, you will need to checkout the API is automatically generated from the code.ContributePlease join the  for discussions and questions.  If you have a question, please review our  before posting.Contributions are always welcome.  For more information, please see our .LicenseThe use and distribution terms for this software are covered by the Apache 2.0 license.See  file for terms."
https://github.com/openai/universe,"Universe: a software platform for measuring and training an AI's general intelligence across the world's supply of games, websites and other applications.","This repository has been deprecated in favor of the Retro (https://github.com/openai/retro) library. See our Retro Contest (https://blog.openai.com/retro-contest) blog post for detalis.universe_ is a softwareplatform for measuring and training an AI's general intelligenceacross the world's supply of games, websites and otherapplications. This is the  open-source library, whichprovides a simple __interface to each Universe environment.Universe allows anyone to train and evaluate AI agents on an extremelywide range of real-time, complex environments.Universe makes it possible for any existing program to become anOpenAI Gym environment, without needing special access to theprogram's internals, source code, or APIs. It does this by packagingthe program into a Docker container, and presenting the AI with thesame interface a human uses: sending keyboard and mouse events, andreceiving screen pixels. Our initial release contains over 1,000environments in which an AI agent can take actions and gatherobservations.Additionally, some environments include a reward signal sent to theagent, to guide reinforcement learning. We've included a few hundredenvironments with reward signals. These environments also includeautomated start menu clickthroughs, allowing your agent to skip to theinteresting part of the environment.We'd like the community's _to grow the number of available environments, including integratingincreasingly large and complex games.The following classes of tasks are packaged inside ofpublicly-available Docker containers, and can be run today with nowork on your part:We've scoped out integrations for many other games, includingcompleting a high-quality GTA V integration (thanks to _ and NVIDIA), but these aren't included in today's release... contents:: Contents of this document:depth: 2Getting startedInstallationSupported systems
We currently support Linux and OSX running Python 2.7 or 3.5.

We recommend setting up a `conda environment <http://conda.pydata.org/docs/using/envs.html>`__
before getting started, to keep all your Universe-related packages in the same place.

Install Universe
~~~~~~~~~~~~~~~~
To get started, first install ``universe``:

.. code:: shell

    git clone https://github.com/openai/universe.git
    cd universe
    pip install -e .

If this errors out, you may be missing some required packages. Here's
the list of required packages we know about so far (please let us know
if you had to install any others).

On Ubuntu 16.04:

.. code:: shell

    pip install numpy
    sudo apt-get install golang libjpeg-turbo8-dev make

On Ubuntu 14.04:

.. code:: shell

    sudo add-apt-repository ppa:ubuntu-lxc/lxd-stable  # for newer golang
    sudo apt-get update
    sudo apt-get install golang libjpeg-turbo8-dev make

On OSX:

You might need to install Command Line Tools by running:

.. code:: shell

    xcode-select --install

Or ``numpy``, ``libjpeg-turbo`` and ``incremental`` packages:

.. code:: shell

    pip install numpy incremental
    brew install golang libjpeg-turbo

Install Docker
~~~~~~~~~~~~~~

The majority of the environments in Universe run inside Docker
containers, so you will need to `install Docker
<https://docs.docker.com/engine/installation/>`__ (on OSX, we
recommend `Docker for Mac
<https://docs.docker.com/docker-for-mac/>`__). You should be able to
run ``docker ps`` and get something like this:

.. code:: shell

     $ docker ps
     CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES

Alternate configuration - running the agent in docker
The above instructions result in an agent that runs as a regular python process in your OS, and launches docker containers as needed for the remotes.Alternatively, you can build a docker image for the agent and run it as a container as well.You can do this in any operating system that has a recent version of docker installed, and the git client.To get started, clone the  repo:.. code:: shellgit clone https://github.com/openai/universe.git
cd universe
Build a docker image, tag it as 'universe':.. code:: shelldocker build -t universe .
This may take a while the first time, as the docker image layers are pulled from docker hub.Once the image is built, you can do a quick run of the test cases to make sure everything is working:.. code:: shelldocker run --privileged --rm -e DOCKER_NET_HOST=172.17.0.1 -v /var/run/docker.sock:/var/run/docker.sock universe pytest
Here's a breakdown of that command:At this point, you'll see a bunch of tests run and hopefully all pass.To do some actual development work, you probably want to do another volume map from the universe repo on your host into the container, then shell in interactively:.. code:: shelldocker run --privileged --rm -it -e DOCKER_NET_HOST=172.17.0.1 -v /var/run/docker.sock:/var/run/docker.sock -v (full path to cloned repo above):/usr/local/universe universe python
As you edit the files in your cloned git repo, they will be changed in your docker container and you'll be able to run them in python.Note if you are using docker for Windows, you'll need to enable the relevant shared drive for this to work.Notes on installation
* When installing ``universe``, you may see ``warning`` messages.  These lines occur when installing numpy and are normal.
* You'll need a ``go version`` of at least 1.5. Ubuntu 14.04 has an older Go, so you'll need to `upgrade <https://golang.org/doc/install>`_ your Go installation.
* We run Python 3.5 internally, so the Python 3.5 variants will be much more thoroughly performance tested. Please let us know if you see any issues on 2.7.
* While we don't officially support Windows, we expect our code to be very close to working there. We'd be happy to take pull requests that take our Windows compatibility to 100%. In the meantime, the easiest way for Windows users to run universe is to use the alternate configuration described above.

System overview
---------------

A Universe **environment** is similar to any other Gym environment:
the agent submits actions and receives observations using the ``step()``
method.

Internally, a Universe environment consists of two pieces: a **client** and a **remote**:

* The **client** is a `VNCEnv
  <https://github.com/openai/universe/blob/master/universe/envs/vnc_env.py>`_
  instance which lives in the same process as the agent. It performs
  functions like receiving the agent's actions, proxying them to the
  **remote**, queuing up rewards for the agent, and maintaining a
  local view of the current episode state.
* The **remote** is the running environment dynamics, usually a
  program running inside of a Docker container. It can run anywhere --
  locally, on a remote server, or in the cloud. (We have a separate
  page describing how to manage `remotes <doc/remotes.rst>`__.)
* The client and the remote communicate with one another using the
  `VNC <https://en.wikipedia.org/wiki/Virtual_Network_Computing>`__
  remote desktop system, as well as over an auxiliary WebSocket
  channel for reward, diagnostic, and control messages. (For more
  information on client-remote communication, see the separate page on
  the `Universe internal communication protocols
  <doc/protocols.rst>`__.)

The code in this repository corresponds to the **client** side of the
Universe environments. Additionally, you can freely access the Docker
images for the **remotes**. We'll release the source repositories for
the remotes in the future, along with tools to enable users to
integrate new environments. Please sign up for our `beta
<https://docs.google.com/forms/d/e/1FAIpQLScAiW-kIS0mz6hdzzFZJJFlXlicDvQs1TX9XMEkipNwjV5VlA/viewform>`_
if you'd like early access.

Run your first agent
--------------------

Now that you've installed the ``universe`` library, you should make
sure it actually works. You can paste the example below into your
``python`` REPL. (You may need to press enter an extra time to make
sure the ``while`` loop is executing.)

.. code:: python

  import gym
  import universe  # register the universe environments

  env = gym.make('flashgames.DuskDrive-v0')
  env.configure(remotes=1)  # automatically creates a local docker container
  observation_n = env.reset()

  while True:
    action_n = [[('KeyEvent', 'ArrowUp', True)] for ob in observation_n]  # your agent here
    observation_n, reward_n, done_n, info = env.step(action_n)
    env.render()

The example will instantiate a client in your Python process,
automatically pull the ``quay.io/openai/universe.flashgames`` image,
and will start that image as the remote. (In our `remotes
<doc/remotes.rst>`__ documentation page, we explain other ways you can run
remotes.)

It will take a few minutes for the image to pull the first time. After that,
if all goes well, a window like the one below will soon pop up. Your
agent, which is just pressing the up arrow repeatedly, is now
playing a Flash racing game called `Dusk Drive
<http://www.kongregate.com/games/longanimals/dusk-drive>`__. Your agent
is programmatically controlling a VNC client, connected to a VNC
server running inside of a Docker container in the cloud, rendering a
headless Chrome with Flash enabled:

.. image:: https://github.com/openai/universe/blob/master/doc/dusk-drive.png?raw=true
     :width: 600px

You can even connect your own VNC client to the environment, either
just to observe or to interfere with your agent. Our ``flashgames``
and ``gym-core`` images conveniently bundle a browser-based VNC
client, which can be accessed at
``http://localhost:15900/viewer/?password=openai``. If you're on Mac,
connecting to a VNC server is as easy as running: ``open
vnc://localhost:5900``.

(If using docker-machine, you'll need to replace ""localhost"" with the
IP address of your Docker daemon, and use ``openai`` as the password.)

Breaking down the example
So we managed to run an agent, what did all the code actuallymean? We'll go line-by-line through the example... code:: pythonimport gymimport universe # register the universe environments.. code:: pythonenv = gym.make('flashgames.DuskDrive-v0').. code:: pythonenv.configure(remotes=1).. code:: pythonobservation_n = env.reset().. code:: pythonaction_n = [[('KeyEvent', 'ArrowUp', True)] for ob in observation_n].. code:: pythonobservation_n, reward_n, done_n, info = env.step(action_n)
env.render()
TestingWe are using __ for tests. You can run them via:.. code:: shellpytest
Run  for useful options, such as  (disables output capture) or  (runs only specific tests).Additional documentationMore documentation not covered in this README can be found in the__ of this repository.Getting helpIf you encounter a problem that is not addressed in this README pageor in the , then try our wiki page of  -and add to it if your solution isn't there!You can also search through the __on this repository and our __ to see if another user has postedabout the same problem or to ask for help from the community.If you still can't solve your problem after trying all of the abovesteps, please post an issue on this repository.What's next?Changelog"
https://github.com/kornia/kornia,Computer Vision and Robotics Library for AI,"English | Website •Docs •Try it Now •Tutorials •Examples •Blog •CommunityKornia is a differentiable computer vision library for .It consists of a set of routines and differentiable modules to solve generic computer vision problems. At its core, the package uses PyTorch as its main backend both for efficiency and to take advantage of the reverse-mode auto-differentiation to define and compute the gradient of complex functions.OverviewInspired by existing packages, this library is composed by a subset of packages containing operators that can be inserted within neural networks to train models to perform image transformations, epipolar geometry, depth estimation, and low-level image processing such as filtering and edge detection that operate directly on tensors.At a granular level, Kornia is a library that consists of the following components:| Component                                                                    | Description                                                                                                                       ||----------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------||                      | a Differentiable Computer Vision library, with strong GPU support                                                                     ||  | a module to perform data augmentation in the GPU                                                                                      ||                | a set of routines to perform color space conversions                                                                                  ||            | a compilation of user contrib and experimental operators                                                                              ||            | a module to perform normalization and intensity transformation                                                                        ||            | a module to perform feature detection                                                                                                 ||            | a module to perform image filtering and edge detection                                                                                ||          | a geometric computer vision library to perform image transformations, 3D linear algebra and conversions using different camera models ||              | a stack of loss functions to solve different vision tasks                                                                             ||      | a module to perform morphological operations                                                                                          ||                | image to tensor utilities and metrics for vision problems                                                                             |InstallationFrom pip:pip install kornia
pip install kornia[x]  # to get the training API !
From source:python setup.py install
From source with symbolic links:pip install -e .
From source using pip:pip install git+https://github.com/kornia/kornia
ExamplesRun our Jupyter notebooks  to learn to use the library.:triangular_flag_on_post: UpdatesCiteIf you are using kornia in your research-related documents, it is recommended that you cite the paper. See more in .@inproceedings{eriba2019kornia,
  author    = {E. Riba, D. Mishkin, D. Ponsa, E. Rublee and G. Bradski},
  title     = {Kornia: an Open Source Differentiable Computer Vision Library for PyTorch},
  booktitle = {Winter Conference on Applications of Computer Vision},
  year      = {2020},
  url       = {https://arxiv.org/pdf/1910.02190.pdf}
}
ContributingWe appreciate all contributions. If you are planning to contribute back bug-fixes, please do so without any further discussion. If you plan to contribute new features, utility functions or extensions, please first open an issue and discuss the feature with us. Please, consider reading the  notes. The participation in this open source project is subject to .CommunityMade with ."
https://github.com/google-research/arxiv-latex-cleaner,arXiv LaTeX Cleaner: Easily clean the LaTeX code of your paper to submit to arXiv,"This tool allows you to easily clean the LaTeX code of your paper to submit toarXiv. From a folder containing all your code, e.g. , itcreates a new folder , that is ready to ZIP and upload toarXiv.Example call:arxiv_latex_cleaner /path/to/latex --resize_images --im_size 500 --images_allowlist='{""images/im.png"":2000}'
Or simply from a config filearxiv_latex_cleaner /path/to/latex --config cleaner_config.yaml
Installation:pip install arxiv-latex-cleaner
| :exclamation: arxiv_latex_cleaner is only compatible with Python >=3 :exclamation: || ---------------------------------------------------------------------------------- |If using MacOS, you can install using :brew install arxiv_latex_cleaner
Alternatively, you can download the source code:git clone https://github.com/google-research/arxiv-latex-cleaner
cd arxiv-latex-cleaner/
python -m arxiv_latex_cleaner --help
And install as a command-line program directly from the source code:python setup.py install
Main features:Privacy-orientedSize-orientedThere is a 50MB limit on arXiv submissions, so to make it fit:TikZ picture source code concealmentTo prevent the upload of tikzpicture source code or raw simulation data, thisfeature:More sophisticated pattern replacement based on regex group capturesSometimes it is useful to work with a set of custom LaTeX commands when writinga paper. To get rid of them upon arXiv submission, one can simply revert them toplain LaTeX with a regular expression insertion.{
    ""pattern"" : '(?:\\figcomp{\s*)(?P<first>.*?)\s*}\s*{\s*(?P<second>.*?)\s*}\s*{\s*(?P<third>.*?)\s*}',
    ""insertion"" : '\parbox[c]{{ {second} \linewidth}} {{ \includegraphics[width= {third} \linewidth]{{figures/{first} }} }}',
    ""description"" : ""Replace figcomp""
}
The pattern above will find all  commands and replacethem with.Note that the insertion template is filled with thefrom the pattern. Note that the replacement is processed before all commands are processed and corresponding file paths arecopied, making sure all figure files are copied to the cleaned version. See also for details on how to specify thepatterns.Usage:usage: arxiv_latex_cleaner@v1.0.1 [-h] [--resize_images] [--im_size IM_SIZE]
                                   [--compress_pdf]
                                   [--pdf_im_resolution PDF_IM_RESOLUTION]
                                   [--images_allowlist IMAGES_ALLOWLIST]
                                   [--keep_bib]
                                   [--commands_to_delete COMMANDS_TO_DELETE [COMMANDS_TO_DELETE ...]]
                                   [--use_external_tikz USE_EXTERNAL_TIKZ]
                                   [--config CONFIG] [--verbose]
                                   input_folder

Clean the LaTeX code of your paper to submit to arXiv. Check the README for
more information on the use.

positional arguments:
  input_folder          Input folder containing the LaTeX code.

optional arguments:
  -h, --help            show this help message and exit
  --resize_images       Resize images.
  --im_size IM_SIZE     Size of the output images (in pixels, longest side).
                        Fine tune this to get as close to 10MB as possible.
  --compress_pdf        Compress PDF images using ghostscript (Linux and Mac
                        only).
  --pdf_im_resolution PDF_IM_RESOLUTION
                        Resolution (in dpi) to which the tool resamples the
                        PDF images.
  --images_allowlist IMAGES_ALLOWLIST
                        Images (and PDFs) that won't be resized to the default
                        resolution,but the one provided here. Value is pixel
                        for images, and dpi forPDFs, as in --im_size and
                        --pdf_im_resolution, respectively. Format is a
                        dictionary as: '{""path/to/im.jpg"": 1000}'
  --keep_bib            Avoid deleting the *.bib files.
  --commands_to_delete COMMANDS_TO_DELETE [COMMANDS_TO_DELETE ...]
                        LaTeX commands that will be deleted. Useful for e.g.
                        user-defined \todo commands. For example, to delete
                        all occurrences of \todo1{} and \todo2{}, run the tool
                        with `--commands_to_delete todo1 todo2`.Please note
                        that the positional argument `input_folder` cannot
                        come immediately after `commands_to_delete`, as the
                        parser does not have any way to know if it's another
                        command to delete.
  --commands_only_to_delete COMMANDS_ONLY_TO_DELETE [COMMANDS_ONLY_TO_DELETE ...]
                        LaTeX commands that will be deleted but the text 
                        wrapped in the commands will be retained. Useful for
                        commands that change text formats and colors, which
                        you may want to remove but keep the text within. Usages
                        are exactly the same as commands_to_delete. Note that if
                        the commands listed here duplicate that after
                        commands_to_delete, the default action will be retaining
                        the wrapped text.
  --environments_to_delete ENVIRONMENTS_TO_DELETE [ENVIRONMENTS_TO_DELETE ...]
                        LaTeX environments that will be deleted. Useful for e.g. 
                        user-defined comment environments. For example, to 
                        delete all occurrences of \begin{note} ... \end{note},
                        run the tool with `--environments_to_delete note`. 
                        Please note that the positional argument `input_folder`
                        cannot come immediately after
                        `environments_to_delete`, as the parser does not have
                        any way to know if it's another environment to delete.
  --use_external_tikz USE_EXTERNAL_TIKZ
                        Folder (relative to input folder) containing
                        externalized tikz figures in PDF format.
  --svg_inkscape [SVG_INKSCAPE]
                        Include PDF files generated by Inkscape via the
                        `\includesvg` command from the `svg` package. This is
                        done by replacing the `\includesvg` calls with
                        `\includeinkscape` calls pointing to the generated
                        `.pdf_tex` files. By default, these files and the
                        generated PDFs are located under `./svg-inkscape`
                        (relative to the input folder), but a different path
                        (relative to the input folder) can be provided in case a
                        different `inkscapepath` was set when loading the `svg`
                        package.
  --config CONFIG       Read settings from `.yaml` config file. If command
                        line arguments are provided additionally, the config
                        file parameters are updated with the command line
                        parameters.
  --verbose             Enable detailed output.
Testing:python -m unittest arxiv_latex_cleaner.tests.arxiv_latex_cleaner_test
NoteThis is not an officially supported Google product."
https://github.com/cookiecutter/cookiecutter,"A cross-platform command-line utility that creates projects from cookiecutters (project templates), e.g. Python package projects, C projects.","CookiecutterCreate projects swiftly from cookiecutters (project templates) with this command-line utility. Ideal for generating Python package projects and more.FeaturesFor UsersQuick StartThe recommended way to use Cookiecutter as a command line utility is to run it with , which can be installed with , but if you plan to use Cookiecutter programatically, please run .Use a GitHub template# You'll be prompted to enter values.
# Then it'll create your Python package in the current working directory,
# based on those values.
# For the sake of brevity, repos on GitHub can just use the 'gh' prefix
$ pipx cookiecutter gh:audreyfeldroy/cookiecutter-pypackage
Use a local template$ pipx cookiecutter cookiecutter-pypackage/
Use it from Pythonfrom cookiecutter.main import cookiecutter

# Create project from the cookiecutter-pypackage/ template
cookiecutter('cookiecutter-pypackage/')

# Create project from the cookiecutter-pypackage.git repo template
cookiecutter('gh:audreyfeldroy//cookiecutter-pypackage.git')
Detailed UsageFor Template CreatorsAvailable TemplatesDiscover a variety of ready-to-use templates on .Special TemplatesCommunityJoin the community, contribute, or seek assistance.SupportFeedbackWe value your feedback. Share your criticisms or complaints constructively to help us improve.Waiting for a Response?Code of ConductAdhere to the  during all interactions in the project's ecosystem.AcknowledgementsCreated and led by , supported by a dedicated team of maintainers and contributors."
https://github.com/lucidrains/stylegan2-pytorch,"Simplest working implementation of Stylegan2, state of the art generative adversarial network, in Pytorch. Enabling everyone to experience disentanglement","Simple StyleGan2 for PytorchSimple Pytorch implementation of Stylegan2 based on https://arxiv.org/abs/1912.04958 that can be completely trained from the command-line, no coding needed.Below are some flowers that do not exist.Neither do these handsNor these citiesNor these celebrities (trained by @yoniker)InstallYou will need a machine with a GPU and CUDA installed. Then pip install the package like this$ pip install stylegan2_pytorch
If you are using a windows machine, the following commands reportedly works.$ conda install pytorch torchvision -c python
$ pip install stylegan2_pytorch
Use$ stylegan2_pytorch --data /path/to/images
That's it. Sample images will be saved to  and models will be saved periodically to .Advanced UseYou can specify the name of your project with$ stylegan2_pytorch --data /path/to/images --name my-project-name
You can also specify the location where intermediate results and model checkpoints should be stored with$ stylegan2_pytorch --data /path/to/images --name my-project-name --results_dir /path/to/results/dir --models_dir /path/to/models/dir
You can increase the network capacity (which defaults to ) to improve generation results, at the cost of more memory.$ stylegan2_pytorch --data /path/to/images --network-capacity 256
By default, if the training gets cut off, it will automatically resume from the last checkpointed file. If you want to restart with new settings, just add a  flag$ stylegan2_pytorch --new --data /path/to/images --name my-project-name --image-size 512 --batch-size 1 --gradient-accumulate-every 16 --network-capacity 10
Once you have finished training, you can generate images from your latest checkpoint like so.$ stylegan2_pytorch  --generate
To generate a video of a interpolation through two random points in latent space.$ stylegan2_pytorch --generate-interpolation --interpolation-num-steps 100
To save each individual frame of the interpolation$ stylegan2_pytorch --generate-interpolation --save-frames
If a previous checkpoint contained a better generator, (which often happens as generators start degrading towards the end of training), you can load from a previous checkpoint with another flag$ stylegan2_pytorch --generate --load-from {checkpoint number}
A technique used in both StyleGAN and BigGAN is truncating the latent values so that their values fall close to the mean. The small the truncation value, the better the samples will appear at the cost of sample variety. You can control this with the , where values typically fall between  and . It is set at  as default$ stylegan2_pytorch --generate --trunc-psi 0.5
Multi-GPU trainingIf you have one machine with multiple GPUs, the repository offers a way to utilize all of them for training. With multiple GPUs, each batch will be divided evenly amongst the GPUs available. For example, for 2 GPUs, with a batch size of 32, each GPU will see 16 samples.You simply have to add a  flag, everyting else is taken care of. If you would like to restrict to specific GPUs, you can use the  environment variable to control what devices can be used. (ex.  only devices 0, 2, 3 are available)$ stylegan2_pytorch --data ./data --multi-gpus --batch-size 32 --gradient-accumulate-every 1
Low amounts of Training DataIn the past, GANs needed a lot of data to learn how to generate well. The faces model took 70k high quality images from Flickr, as an example.However, in the month of May 2020, researchers all across the world independently converged on a simple technique to reduce that number to as low as 1-2k. That simple idea was to differentiably augment all images, generated or real, going into the discriminator during training.If one were to augment at a low enough probability, the augmentations will not 'leak' into the generations.In the setting of low data, you can use the feature with a simple flag.# find a suitable probability between 0. -> 0.7 at maximum
$ stylegan2_pytorch --data ./data --aug-prob 0.25
By default, the augmentations used are  and . If you would like to add , you can do so with the  argument.# make sure there are no spaces between items!
$ stylegan2_pytorch --data ./data --aug-prob 0.25 --aug-types [translation,cutout,color]
You can customize it to any combination of the three you would like. The differentiable augmentation code was copied and slightly modified from here.When do I stop training?For as long as possible until the adversarial game between the two neural nets fall apart (we call this divergence). By default, the number of training steps is set to  for 128x128 images, but you will certainly want this number to be higher if the GAN doesn't diverge by the end of training, or if you are training at a higher resolution.$ stylegan2_pytorch --data ./data --image-size 512 --num-train-steps 1000000
AttentionThis framework also allows for you to add an efficient form of self-attention to the designated layers of the discriminator (and the symmetric layer of the generator), which will greatly improve results. The more attention you can afford, the better!# add self attention after the output of layer 1
$ stylegan2_pytorch --data ./data --attn-layers 1
# add self attention after the output of layers 1 and 2
# do not put a space after the comma in the list!
$ stylegan2_pytorch --data ./data --attn-layers [1,2]
BonusTraining on transparent images$ stylegan2_pytorch --data ./transparent/images/path --transparent
Memory considerationsThe more GPU memory you have, the bigger and better the image generation will be. Nvidia recommended having up to 16GB for training 1024x1024 images. If you have less than that, there are a couple settings you can play with so that the model fits.$ stylegan2_pytorch --data /path/to/data \
    --batch-size 3 \
    --gradient-accumulate-every 5 \
    --network-capacity 16
If none of this works, you can settle for 'Lightweight' GAN, which will allow you to tradeoff quality to train at greater resolutions in reasonable amount of time.Deployment on AWSBelow are some steps which may be helpful for deployment using Amazon Web Services. In order to use this, you will haveto provision a GPU-backed EC2 instance. An appropriate instance type would be from a p2 or p3 series. I (iboates) trieda p2.xlarge (the cheapest option) and it was quite slow, slower in fact than using Google Colab. More powerful instancetypes may be better but they are more expensive. You can read more about them.Setup stepssudo snap install aws-cli --classic
aws configure
You will then have to enter your AWS access keys, which you can retrieve from the management console under AWSManagement Console > Profile > My Security Credentials > Access KeysThen, run these commands, or maybe put them in a shell script and execute that:mkdir data
curl -O https://bootstrap.pypa.io/get-pip.py
sudo apt-get install python3-distutils
python3 get-pip.py
pip3 install stylegan2_pytorch
export PATH=$PATH:/home/ubuntu/.local/bin
aws s3 sync s3://<Your bucket name> ~/data
cd data
tar -xf ../train.tar.gz
Now you should be able to train by simplying calling .Notes:ResearchFID ScoresThanks to GetsEclectic, you can now calculate the FID score periodically! Again, made super simple with one extra argument, as shown below.Firstly, install the  package$ pip install pytorch-fid
Followed by$ stylegan2_pytorch --data ./data --calculate-fid-every 5000
FID results will be logged to CodingIf you would like to sample images programmatically, you can do so with the following simple  class.import torch
from torchvision.utils import save_image
from stylegan2_pytorch import ModelLoader

loader = ModelLoader(
    base_dir = '/path/to/directory',   # path to where you invoked the command line tool
    name = 'default'                   # the project name, defaults to 'default'
)

noise   = torch.randn(1, 512).cuda() # noise
styles  = loader.noise_to_styles(noise, trunc_psi = 0.7)  # pass through mapping network
images  = loader.styles_to_images(styles) # call the generator on intermediate style vectors

save_image(images, './sample.jpg') # save your images, or do whatever you desire
Logging to experiment trackerTo log the losses to an open source experiment tracker (Aim), you simply need to pass an extra flag like so.$ stylegan2_pytorch --data ./data --log
Then, you need to make sure you have Docker installed. Following the instructions at Aim, you execute the following in your terminal.$ aim up
Then open up your browser to the address and you should seeExperimentalTop-k Training for GeneratorA new paper has produced evidence that by simply zero-ing out the gradient contributions from samples that are deemed fake by the discriminator, the generator learns significantly better, achieving new state of the art.$ stylegan2_pytorch --data ./data --top-k-training
Gamma is a decay schedule that slowly decreases the topk from the full batch size to the target fraction of 50% (also modifiable hyperparameter).$ stylegan2_pytorch --data ./data --top-k-training --generate-top-k-frac 0.5 --generate-top-k-gamma 0.99
Feature QuantizationA recent paper reported improved results if intermediate representations of the discriminator are vector quantized. Although I have not noticed any dramatic changes, I have decided to add this as a feature, so other minds out there can investigate. To use, you have to specify which layer(s) you would like to vector quantize. Default dictionary size is  and is also tunable.# feature quantize layers 1 and 2, with a dictionary size of 512 each
# do not put a space after the comma in the list!
$ stylegan2_pytorch --data ./data --fq-layers [1,2] --fq-dict-size 512
Contrastive Loss RegularizationI have tried contrastive learning on the discriminator (in step with the usual GAN training) and possibly observed improved stability and quality of final results. You can turn on this experimental feature with a simple flag as shown below.$ stylegan2_pytorch --data ./data --cl-reg
Relativistic Discriminator LossThis was proposed in the Relativistic GAN paper to stabilize training. I have had mixed results, but will include the feature for those who want to experiment with it.$ stylegan2_pytorch --data ./data --rel-disc-loss
Non-constant 4x4 BlockBy default, the StyleGAN architecture styles a constant learned 4x4 block as it is progressively upsampled. This is an experimental feature that makes it so the 4x4 block is learned from the style vector  instead.$ stylegan2_pytorch --data ./data --no-const
Dual Contrastive LossA recent paper has proposed that a novel contrastive loss between the real and fake logits can improve quality over other types of losses. (The default in this repository is hinge loss, and the paper shows a slight improvement)$ stylegan2_pytorch --data ./data --dual-contrast-loss
AlternativesStylegan2 + Unet DiscriminatorI have gotten really good results with a unet discriminator, but the architecturally change was too big to fit as an option in this repository. If you are aiming for perfection, feel free to try it.If you would like me to give the royal treatment to some other GAN architecture (BigGAN), feel free to reach out at my email. Happy to hear your pitch.AppreciationThank you to Matthew Mann for his inspiring  for Tensorflow 2.0References@article{Karras2019stylegan2,
    title   = {Analyzing and Improving the Image Quality of {StyleGAN}},
    author  = {Tero Karras and Samuli Laine and Miika Aittala and Janne Hellsten and Jaakko Lehtinen and Timo Aila},
    journal = {CoRR},
    volume  = {abs/1912.04958},
    year    = {2019},
}
@misc{zhao2020feature,
    title   = {Feature Quantization Improves GAN Training},
    author  = {Yang Zhao and Chunyuan Li and Ping Yu and Jianfeng Gao and Changyou Chen},
    year    = {2020}
}
@misc{chen2020simple,
    title   = {A Simple Framework for Contrastive Learning of Visual Representations},
    author  = {Ting Chen and Simon Kornblith and Mohammad Norouzi and Geoffrey Hinton},
    year    = {2020}
}
@article{,
    title     = {Oxford 102 Flowers},
    author    = {Nilsback, M-E. and Zisserman, A., 2008},
    abstract  = {A 102 category dataset consisting of 102 flower categories, commonly occuring in the United Kingdom. Each class consists of 40 to 258 images. The images have large scale, pose and light variations.}
}
@article{afifi201911k,
    title   = {11K Hands: gender recognition and biometric identification using a large dataset of hand images},
    author  = {Afifi, Mahmoud},
    journal = {Multimedia Tools and Applications}
}
@misc{zhang2018selfattention,
    title   = {Self-Attention Generative Adversarial Networks},
    author  = {Han Zhang and Ian Goodfellow and Dimitris Metaxas and Augustus Odena},
    year    = {2018},
    eprint  = {1805.08318},
    archivePrefix = {arXiv}
}
@article{shen2019efficient,
    author    = {Zhuoran Shen and
               Mingyuan Zhang and
               Haiyu Zhao and
               Shuai Yi and
               Hongsheng Li},
    title     = {Efficient Attention: Attention with Linear Complexities},
    journal   = {CoRR},  
    year      = {2018},
    url       = {http://arxiv.org/abs/1812.01243},
}
@article{zhao2020diffaugment,
    title   = {Differentiable Augmentation for Data-Efficient GAN Training},
    author  = {Zhao, Shengyu and Liu, Zhijian and Lin, Ji and Zhu, Jun-Yan and Han, Song},
    journal = {arXiv preprint arXiv:2006.10738},
    year    = {2020}
}
@misc{zhao2020image,
    title  = {Image Augmentations for GAN Training},
    author = {Zhengli Zhao and Zizhao Zhang and Ting Chen and Sameer Singh and Han Zhang},
    year   = {2020},
    eprint = {2006.02595},
    archivePrefix = {arXiv}
}
@misc{karras2020training,
    title   = {Training Generative Adversarial Networks with Limited Data},
    author  = {Tero Karras and Miika Aittala and Janne Hellsten and Samuli Laine and Jaakko Lehtinen and Timo Aila},
    year    = {2020},
    eprint  = {2006.06676},
    archivePrefix = {arXiv},
    primaryClass = {cs.CV}
}
@misc{jolicoeurmartineau2018relativistic,
    title   = {The relativistic discriminator: a key element missing from standard GAN},
    author  = {Alexia Jolicoeur-Martineau},
    year    = {2018},
    eprint  = {1807.00734},
    archivePrefix = {arXiv},
    primaryClass = {cs.LG}
}
@misc{sinha2020topk,
    title   = {Top-k Training of GANs: Improving GAN Performance by Throwing Away Bad Samples},
    author  = {Samarth Sinha and Zhengli Zhao and Anirudh Goyal and Colin Raffel and Augustus Odena},
    year    = {2020},
    eprint  = {2002.06224},
    archivePrefix = {arXiv},
    primaryClass = {stat.ML}
}
@misc{yu2021dual,
    title   = {Dual Contrastive Loss and Attention for GANs},
    author  = {Ning Yu and Guilin Liu and Aysegul Dundar and Andrew Tao and Bryan Catanzaro and Larry Davis and Mario Fritz},
    year    = {2021},
    eprint  = {2103.16748},
    archivePrefix = {arXiv},
    primaryClass = {cs.CV}
}
"
https://github.com/autogluon/autogluon,"AutoGluon: AutoML for Image, Text, Time Series, and Tabular Data","AutoML for Image, Text, Time Series, and Tabular Data | Documentation ( | )AutoGluon automates machine learning tasks enabling you to easily achieve strong predictive performance in your applications.  With just a few lines of code, you can train and deploy high-accuracy machine learning and deep learning models on image, text, time series, and tabular data.Example# First install package from terminal:
# pip install -U pip
# pip install -U setuptools wheel
# pip install autogluon  # autogluon==0.8.2

from autogluon.tabular import TabularDataset, TabularPredictor
train_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv')
test_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv')
predictor = TabularPredictor(label='class').fit(train_data, time_limit=120)  # Fit models for 120s
leaderboard = predictor.leaderboard(test_data)
| AutoGluon Task      |                                                                                Quickstart                                                                                |                                                                                API                                                                                ||:--------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------:|| TabularPredictor    |  |                                  || MultiModalPredictor |             |  || TimeSeriesPredictor |             |  |ResourcesSee the  for documentation and instructions on:Refer to the  for details on upcoming features and releases.Scientific PublicationsArticlesHands-on TutorialsTrain/Deploy AutoGluon in the CloudContributing to AutoGluonWe are actively accepting code contributions to the AutoGluon project. If you are interested in contributing to AutoGluon, please read the  to get started.Citing AutoGluonIf you use AutoGluon in a scientific publication, please cite the following paper:Erickson, Nick, et al.  arXiv preprint arXiv:2003.06505 (2020).BibTeX entry:@article{agtabular,
  title={AutoGluon-Tabular: Robust and Accurate AutoML for Structured Data},
  author={Erickson, Nick and Mueller, Jonas and Shirkov, Alexander and Zhang, Hang and Larroy, Pedro and Li, Mu and Smola, Alexander},
  journal={arXiv preprint arXiv:2003.06505},
  year={2020}
}
If you are using AutoGluon Tabular's model distillation functionality, please cite the following paper:Fakoor, Rasool, et al.  Advances in Neural Information Processing Systems 33 (2020).BibTeX entry:@article{agtabulardistill,
  title={Fast, Accurate, and Simple Models for Tabular Data via Augmented Distillation},
  author={Fakoor, Rasool and Mueller, Jonas W and Erickson, Nick and Chaudhari, Pratik and Smola, Alexander J},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}
If you use AutoGluon's multimodal text+tabular functionality in a scientific publication, please cite the following paper:Shi, Xingjian, et al.  8th ICML Workshop on Automated Machine Learning (AutoML). 2021.BibTeX entry:@inproceedings{agmultimodaltext,
  title={Multimodal AutoML on Structured Tables with Text Fields},
  author={Shi, Xingjian and Mueller, Jonas and Erickson, Nick and Li, Mu and Smola, Alex},
  booktitle={8th ICML Workshop on Automated Machine Learning (AutoML)},
  year={2021}
}
If you use AutoGluon's time series forecasting functionality in a scientific publication, please cite the following paper:@inproceedings{agtimeseries,
  title={{AutoGluon-TimeSeries}: {AutoML} for Probabilistic Time Series Forecasting},
  author={Shchur, Oleksandr and Turkmen, Caner and Erickson, Nick and Shen, Huibin and Shirkov, Alexander and Hu, Tony and Wang, Yuyang},
  booktitle={International Conference on Automated Machine Learning},
  year={2023}
}
AutoGluon for Hyperparameter OptimizationAutoGluon's state-of-the-art tools for hyperparameter optimization, such as ASHA, Hyperband, Bayesian Optimization and BOHB have moved to the stand-alone package .To learn more, checkout our paper  arXiv preprint arXiv:2003.10865 (2020).@article{abohb,
  title={Model-based Asynchronous Hyperparameter and Neural Architecture Search},
  author={Klein, Aaron and Tiao, Louis and Lienart, Thibaut and Archambeau, Cedric and Seeger, Matthias},
  journal={arXiv preprint arXiv:2003.10865},
  year={2020}
}
LicenseThis library is licensed under the Apache 2.0 License."
https://github.com/aws/serverless-application-model,The AWS Serverless Application Model (AWS SAM) transform is a AWS CloudFormation macro that transforms SAM templates into CloudFormation templates.,"AWS SAM transformThe  (AWS SAM) transform is a  that transforms  into .To use the SAM transform, add  to the  of your CloudFormation template.Benefits of using the SAM transform include:Getting startedSave the following as :Transform: AWS::Serverless-2016-10-31
Resources:
  MyFunction:
    Type: AWS::Serverless::Function
    Properties:
      Runtime: nodejs16.x
      Handler: index.handler
      InlineCode: |
        exports.handler = async (event) => {
          console.log(event);
        }
And deploy it with the :sam sync --stack-name sam-app
The  resource will create a  function that logs  it receives.Under the hood, the template is transformed into the JSON equivalent of the following CloudFormation template:Resources:
  MyFunction:
    Type: AWS::Lambda::Function
    Properties:
      Code:
        ZipFile: |
          exports.handler = async (event) => {
            console.log(event);
          }
      Handler: index.handler
      Role: !GetAtt MyFunctionRole.Arn
      Runtime: nodejs16.x
      Tags:
        - Key: lambda:createdBy
          Value: SAM
  MyFunctionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: ""2012-10-17""
        Statement:
          - Action:
              - sts:AssumeRole
            Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Tags:
        - Key: lambda:createdBy
          Value: SAM
For a more thorough introduction, see the  in the .ContributingSetting up development environmentYou'll need to have Python 3.7+ installed.Create a :python3 -m venv .venv
source .venv/bin/activate
Set up dependencies:make init
Run tests:make pr
See  for further development instructions, and  for the contributing guidelines.Getting helpThe best way to interact with the team is through GitHub. You can either  or .You can also join the  on Slack.Learn moreWorkshops and tutorialsDocumentation"
https://github.com/Chia-Network/chia-blockchain,"Chia blockchain python implementation (full node, farmer, harvester, timelord, and wallet)","chia-blockchainChia is a modern cryptocurrency built from scratch, designed to be efficient, decentralized, and secure. Here are some of the features and benefits:Please check out the , the , and  forinformation on this project.Python 3.8.1+ is required. Make sure your default python version is >=3.8.1by typing .If you are behind a NAT, it can be difficult for peers outside your subnet toreach you when they start up. You can enableon your router or add a NAT (for IPv4 but not IPv6) and firewall rules to allowTCP port 8444 access to your peer.These methods tend to be router make/model specific.Most users should only install harvesters, farmers, plotter, full nodes, and wallets.Setting up a seeder is best left to more advanced users.Building Timelords and VDFs is for sophisticated users, in most environments.Chia Network and additional volunteers are running sufficient Timelordsfor consensus.InstallingInstall instructions are available in thesection of the.RunningOnce installed, ais available from the repository."
https://github.com/tkipf/gcn,Implementation of Graph Convolutional Networks in TensorFlow,"Graph Convolutional NetworksThis is a TensorFlow implementation of Graph Convolutional Networks for the task of (semi-supervised) classification of nodes in a graph, as described in our paper:Thomas N. Kipf, Max Welling,  (ICLR 2017)For a high-level explanation, have a look at our blog post:Thomas Kipf,  (2016)Installationpython setup.py install
RequirementsRun the democd gcn
python train.py
DataIn order to use your own data, you have to provide Have a look at the  function in  for an example.In this example, we load citation network data (Cora, Citeseer or Pubmed). The original datasets can be found here: http://www.cs.umd.edu/~sen/lbc-proj/LBC.html. In our version (see  folder) we use dataset splits provided by https://github.com/kimiyoung/planetoid (Zhilin Yang, William W. Cohen, Ruslan Salakhutdinov, , ICML 2016). You can specify a dataset as follows:python train.py --dataset citeseer
(or by editing )ModelsYou can choose between the following models: Graph classificationOur framework also supports batch-wise classification of multiple graph instances (of potentially different size) with an adjacency matrix each. It is best to concatenate respective feature matrices and build a (sparse) block-diagonal matrix where each block corresponds to the adjacency matrix of one graph instance. For pooling (in case of graph-level outputs as opposed to node-level outputs) it is best to specify a simple pooling matrix that collects features from their respective graph instances, as illustrated below:CitePlease cite our paper if you use this code in your own work:@inproceedings{kipf2017semi,
  title={Semi-Supervised Classification with Graph Convolutional Networks},
  author={Kipf, Thomas N. and Welling, Max},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2017}
}
"
https://github.com/axi0mX/ipwndfu,open-source jailbreaking tool for many iOS devices,"Open-source jailbreaking tool for many iOS devices*Read checkm8Quick start guide for checkm8FeaturesDependenciesThis tool should be compatible with Mac and Linux. It won't work in a virtual machine.TutorialThis tool can be used to downgrade or jailbreak iPhone 3GS (new bootrom) without SHSH blobs, as documented in .Exploit write-upWrite-up for alloc8 exploit can be found here:https://github.com/axi0mX/alloc8iBSSDownload iPhone 3GS iOS 4.3.5 IPSW from Apple:http://appldnld.apple.com/iPhone4/041-1965.20110721.gxUB5/iPhone2,1_4.3.5_8L1_Restore.ipswIn Terminal, extract iBSS using the following command, then move the file to ipwndfu folder:unzip -p iPhone2,1_4.3.5_8L1_Restore.ipsw Firmware/dfu/iBSS.n88ap.RELEASE.dfu > n88ap-iBSS-4.3.5.img3
Coming soon!DisclaimerThis is BETA software.Backup your data.This tool is currently in beta and could potentially brick your device. It will attempt to save a copy of data in NOR to nor-backups folder before flashing new data to NOR, and it will attempt to not overwrite critical data in NOR which your device requires to function. If something goes wrong, hopefully you will be able to restore to latest IPSW in iTunes and bring your device back to life, or use nor-backups to restore NOR to the original state, but I cannot provide any guarantees.There is NO warranty provided.THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.ToolchainYou will not need to use  or compile anything to use ipwndfu. However, if you wish to make changes to assembly code in , you will need to use an ARM toolchain and assemble the source files by running .If you are using macOS with Homebrew, you can use binutils and gcc-arm-embedded. You can install them with these commands:brew install binutils
brew cask install https://raw.githubusercontent.com/Homebrew/homebrew-cask/b88346667547cc85f8f2cacb3dfe7b754c8afc8a/Casks/gcc-arm-embedded.rb
Creditgeohot for limera1n exploitposixninja and pod2g for SHAtter exploitchronic, CPICH, ius, MuscleNerd, Planetbeing, pod2g, posixninja, et al. for 24Kpwn exploitpod2g for steaks4uce exploitwalac for pyusb"
https://github.com/pwxcoo/chinese-xinhua,:orange_book: 中华新华字典数据库。包括歇后语，成语，词语，汉字。,"chinese-xinhua中华新华字典数据库和 API 。收录包括 14032 条歇后语，16142 个汉字，264434 个词语，31648 个成语。Project Structurechinese-xinhua/
|
+- data/ <-- 数据文件夹
|  |
|  +- idiom.json <-- 成语
|  |
|  +- word.json <-- 汉字
|  |
|  +- xiehouyu.json <-- 歇后语
|  |
|  +- ci.json <-- 词语
Database Introduction成语 (idiom.json)[
    {
        ""derivation"": ""语出《法华经·法师功德品》下至阿鼻地狱。”"",
        ""example"": ""但也有少数意志薄弱的……逐步上当，终至堕入～。★《上饶集中营·炼狱杂记》"",
        ""explanation"": ""阿鼻梵语的译音，意译为无间”，即痛苦无有间断之意。常用来比喻黑暗的社会和严酷的牢狱。又比喻无法摆脱的极其痛苦的境地。"",
        ""pinyin"": ""ā bí dì yù"",
        ""word"": ""阿鼻地狱"",
        ""abbreviation"": ""abdy""
    },
    ...
]
词语 (ci.json)[
    { 
        ""ci"": ""宸纶"", 
        ""explanation"": ""1.帝王的诏书﹑制令。"" 
    },
    ...
]
汉字 (word.json)[
    {
        ""word"": ""嗄"",
        ""oldword"": ""嗄"",
        ""strokes"": ""13"",
        ""pinyin"": ""á"",
        ""radicals"": ""口"",
        ""explanation"": ""嗄〈叹〉\n\n 同啊”。表示省悟或惊奇\n\n 嗄!难道这里是没有地方官的么?--宋·佚名《新编五代史平话》\n\n 嗄á叹词。在句首，〈表〉疑问或反问～，这是什么？～，你想干什么？\""嗄\""另见shà㈠。\n\n 嗄shà\n\n ⒈声音嘶哑～声。\n\n 嗄a 1.助词。表示强调﹑肯定或辩解。 2.助词。方言。表示疑问或反诘。\n\n 嗄xià 1.见\""嗄饭\""。 2.见\""嗄程\""。"",
        ""more"": ""嗄 ga、a 部首 口 部首笔画 03 总笔画 13  嗄2\nshà\n〈形〉\n(1)\n声音嘶哑的 [hoarse]\n终日嚎而嗌不嗄。--《老子》\n(2)\n又如嗄哑,嗄嘶(嗓音嘶哑)\n嗄\nshà\n〈叹〉\n(1)\n什么 [what]--表示否定\n我要丢个干干净,看你嗄法把我治。--清·蒲松龄《聊斋俚曲集》\n(2)\n旧时仆役对主人、下级对上级的应诺声 [yes]\n带进来”。两边军士应一声嗄”,即将牛皋推至面前。--《说岳全传》\n另见á\n嗄1\ná\n〈叹〉\n同啊”(á)。表示省悟或惊奇 [ah]\n嗄!难道这里是没有地方官的么?--宋·佚名《新编五代史平话》\n另见shà\n嗄1\nshà　ㄕㄚ╝\n嗓音嘶哑。\n郑码janr，u55c4，gbke0c4\n笔画数13，部首口，笔顺编号2511325111354\n嗄2\ná　ㄚˊ\n同啊2”。\n郑码janr，u55c4，gbke0c4\n笔画数13，部首口，笔顺编号2511325111354""
    },
    ... 
]
歇后语 (xiehouyu.json)[
    {
        ""riddle"": ""飞机上聊天"",
        ""answer"": ""高谈阔论""
    },
    ...
]
ChangelogCopyright本仓库的所有的数据都是我从网上收集整理的。仓库本来的目的是因为我以前想做一个成语接龙的东西，但是苦于没有现成可用的数据库，自己就从各个网站抓取整理了一份。放在 Github 是为了方便自己的使用，同时也能方便有类似需求的人不用去做这些 trival 的工作。所有抓取数据的都在仓库里。本仓库无任何商业目的！如果有侵权行为将及时删除！"
https://github.com/tensorlayer/SRGAN,Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network,"Super Resolution ExamplesSRGAN ArchitecturePrepare Data and Pre-trained VGGRun🔥🔥🔥🔥🔥🔥 You need install  at first!🔥🔥🔥🔥🔥🔥 Please install TensorLayerX via sourcepip install git+https://github.com/tensorlayer/tensorlayerx.git 
Trainconfig.TRAIN.img_path = ""your_image_folder/""
Your directory structure should look like this:srgan/
    └── config.py
    └── srgan.py
    └── train.py
    └── vgg.py
    └── model
          └── vgg19.npy
    └── DIV2K
          └── DIV2K_train_HR
          ├── DIV2K_train_LR_bicubic
          ├── DIV2K_valid_HR
          └── DIV2K_valid_LR_bicubic

python train.py
🔥Modify a line of code in train.py, easily switch to any framework!import os
os.environ['TL_BACKEND'] = 'tensorflow'
# os.environ['TL_BACKEND'] = 'mindspore'
# os.environ['TL_BACKEND'] = 'paddle'
# os.environ['TL_BACKEND'] = 'pytorch'
🚧 We will support PyTorch as Backend soon.Evaluation.🔥 We have trained SRGAN on DIV2K dataset.🔥 Download model weights as follows.|              | SRGAN_g | SRGAN_d ||------------- |---------|---------|| TensorFlow   | ,  |,    || PaddlePaddle | ,   | ,    || MindSpore    | 🚧Coming soon!    | 🚧Coming soon!     || PyTorch      | 🚧Coming soon!    | 🚧Coming soon!     |Download weights file and put weights under the folder srgan/models/.Your directory structure should look like this:srgan/
    └── config.py
    └── srgan.py
    └── train.py
    └── vgg.py
    └── model
          └── vgg19.npy
    └── DIV2K
          ├── DIV2K_train_HR
          ├── DIV2K_train_LR_bicubic
          ├── DIV2K_valid_HR
          └── DIV2K_valid_LR_bicubic
    └── models
          ├── g.npz  # You should rename the weigths file. 
          └── d.npz  # If you set os.environ['TL_BACKEND'] = 'tensorflow',you should rename srgan-g-tensorflow.npz to g.npz .

python train.py --mode=eval
Results will be saved under the folder srgan/samples/. ResultsReferenceCitationIf you find this project useful, we would be grateful if you cite the TensorLayer paper：@article{tensorlayer2017,
author = {Dong, Hao and Supratak, Akara and Mai, Luo and Liu, Fangde and Oehmichen, Axel and Yu, Simiao and Guo, Yike},
journal = {ACM Multimedia},
title = {{TensorLayer: A Versatile Library for Efficient Deep Learning Development}},
url = {http://tensorlayer.org},
year = {2017}
}

@inproceedings{tensorlayer2021,
  title={TensorLayer 3.0: A Deep Learning Library Compatible With Multiple Backends},
  author={Lai, Cheng and Han, Jiarong and Dong, Hao},
  booktitle={2021 IEEE International Conference on Multimedia \& Expo Workshops (ICMEW)},
  pages={1--3},
  year={2021},
  organization={IEEE}
}
Other ProjectsDiscussionLicense"
https://github.com/kangvcar/InfoSpider,INFO-SPIDER 是一个集众多数据源于一身的爬虫工具箱🧰，旨在安全快捷的帮助用户拿回自己的数据，工具代码开源，流程透明。支持数据源包括GitHub、QQ邮箱、网易邮箱、阿里邮箱、新浪邮箱、Hotmail邮箱、Outlook邮箱、京东、淘宝、支付宝、中国移动、中国联通、中国电信、知乎、哔哩哔哩、网易云音乐、QQ好友、QQ群、生成朋友圈相册、浏览器浏览历史、12306、博客园、CSDN博客、开源中国博客、简书。,"开发者回忆录场景一小明一如往常打开 Chrome 浏览器逛着论坛，贴吧，一不小心点开了网页上的广告，跳转到了京东商城，下意识去关闭窗口时发现 （OS：咦？京东怎么知道我最近心心念念的宝贝呢？刚好我正需要呢！），既然打开了那就看看商品详情吧 （OS：哎哟不错哦），那就下单试试吧！场景二小白听着网易云音乐的每日推荐歌单无法自拔 （OS：哇！怎么播放列表里都是我喜欢的音乐风格？网易云音乐太棒了吧!深得我心啊！黑胶会员必须来一个！），逛着知乎里的“如何优雅的XXX?”，“XXX是怎样一种体验？”，“如何评价XXX?” （OS：咦？这个问题就是我刚好想问的，原来早已有人提问！什么？？？还有几千条回答！！进去逛逛看！）场景三小达上班时不忘充实自己，逛着各大技术论坛博客园、CSDN、开源中国、简书、掘金等等，发现首页的内容推荐太棒了（OS：这些技术博文太棒了，不用找就出来了），再打开自己的博客主页发现不知不觉地自己也坚持写博文也有三年了，自己的技术栈也越来越丰富（OS：怎么博客后台都不提供一个数据分析系统呢？我想看看我这几年来的发文数量，发文时间，想知道哪些博文比较热门，想看看我在哪些技术上花费的时间更多，想看看我过去的创作高峰期时在晚上呢？还是凌晨？我希望系统能给我更多指引数据让我更好的创作！）看到以上几个场景你可能会感叹科技在进步，技术在发展，极大地改善了我们的生活方式。但当你深入思考，你浏览的每个网站，注册的每个网站，他们都记录着你的信息你的足迹。细思恐极的背后是自己的个人数据被赤裸裸的暴露在互联网上并且被众多的公司利用用户数据获得巨额利益，如对用户的数据收集分析后进行定制的广告推送，收取高额广告费。但作为数据的生产者却没能分享属于自己的数据收益。想法如果有一个这样的工具，它能帮你拿回你的个人信息，它能帮你把分散在各种站点的个人信息聚合起来，它能帮你分析你的个人数据并给你提供建议，它能帮你把个人数据可视化让你更清楚地了解自己。基于以上，我着手开发了 [<marko.inline.RawText object at 0x000001592FE52248>] 👇👇👇What is INFO-SPIDERINFO-SPIDER 是一个集众多数据源于一身的爬虫工具箱，旨在安全快捷的帮助用户拿回自己的数据，工具代码开源，流程透明。并提供数据分析功能，基于用户数据生成图表文件，使得用户更直观、深入了解自己的信息。目前支持数据源包括GitHub、QQ邮箱、网易邮箱、阿里邮箱、新浪邮箱、Hotmail邮箱、Outlook邮箱、京东、淘宝、支付宝、中国移动、中国联通、中国电信、知乎、哔哩哔哩、网易云音乐、QQ好友、QQ群、生成朋友圈相册、浏览器浏览历史、12306、博客园、CSDN博客、开源中国博客、简书。详细使用说明参照或你可以在  与我们一起交流学习FeaturesScreenshotQuickStart依赖安装工具运行购买服务数据源数据分析计划VisitorsDevelopers want to sayContributorsSponsorsThank you to JetBrains, who provide Open Source License for PyCharm!ChangelogLicenseGPL-3.0Star History"
https://github.com/spyoungtech/grequests,Requests + Gevent = <3,"GRequests: Asynchronous RequestsGRequests allows you to use Requests with Gevent to make asynchronous HTTPRequests easily.|version| |pyversions|InstallationInstallation is easy with pip::$ pip install grequests
✨🍰✨
UsageUsage is simple:.. code-block:: pythonimport grequests

urls = [
    'http://www.heroku.com',
    'http://python-tablib.org',
    'http://httpbin.org',
    'http://python-requests.org',
    'http://fakedomain/',
    'http://kennethreitz.com'
]
Create a set of unsent Requests:.. code-block:: python>>> rs = (grequests.get(u) for u in urls)
Send them all at the same time using :.. code-block:: python>>> grequests.map(rs)
[<Response [200]>, <Response [200]>, <Response [200]>, <Response [200]>, None, <Response [200]>]
The HTTP verb methods in  (e.g., , , etc.) accept all the same keyword arguments as in the  library.Error Handling^^^^^^^^^^^^^^To handle timeouts or any other exception during the connection ofthe request, you can add an optional exception handler that will be called with the request andexception inside the main thread. The value returned by your exception handler will be used in the result list returned by ... code-block:: python>>> def exception_handler(request, exception):
...    print(""Request failed"")

>>> reqs = [
...    grequests.get('http://httpbin.org/delay/1', timeout=0.001),
...    grequests.get('http://fakedomain/'),
...    grequests.get('http://httpbin.org/status/500')]
>>> grequests.map(reqs, exception_handler=exception_handler)
Request failed
Request failed
[None, None, <Response [500]>]
imap^^^^For some speed/performance gains, you may also want to use  instead of .  returns a generator of responses. Order of these responses does not map to the order of the requests you send out. The API for  is equivalent to the API for . You can also adjust the  argument to  or  to increase the gevent pool size... code-block:: pythonfor resp in grequests.imap(reqs, size=10):
    print(resp)
There is also an enumerated version of ,  which yields the index of the request from the original request list and its associated response. However, unlike , failed requests and exception handler results that return  will also be yielded (whereas in  they are ignored). Aditionally, the  parameter for  must be a sequence. Like in , the order in which requests are sent and received should still be considered arbitrary... code-block:: python>>> rs = [grequests.get(f'https://httpbin.org/status/{code}') for code in range(200, 206)]
>>> for index, response in grequests.imap_enumerated(rs, size=5):
...     print(index, response)
1 <Response [201]>
0 <Response [200]>
4 <Response [204]>
2 <Response [202]>
5 <Response [205]>
3 <Response [203]>
gevent - when things go wrong^^^^^^^^^^^^^^^^^^^^^^^^^^^^^Because  leverages  (which in turn uses monkeypatching for enabling concurrency), you will often need to make sure  is imported before other libraries, especially , to avoid problems. See _ for additional information... code-block:: python# GOOD
import grequests
import requests

# BAD
import requests
import grequests
.. |version| image:: https://img.shields.io/pypi/v/grequests.svg?colorB=blue:target: https://pypi.org/project/grequests/.. |pyversions| image:: https://img.shields.io/pypi/pyversions/grequests.svg?:target: https://pypi.org/project/grequests/"
https://github.com/qq547276542/Agriculture_KnowledgeGraph,农业知识图谱(AgriKG)：农业领域的信息检索，命名实体识别，关系抽取，智能问答，辅助决策,"Agricultural Knowledge Graph由于工作原因，该项目已停止维护。因此项目代码仅供参考，项目中包含的数据可免费用于学术等非商业用途。项目介绍：本项目是上海市《农业信息服务平台及农业大数据综合利用研究》子课题《上海农业农村大数据共享服务平台建设和应用》的研究成果。该课题是由上海市农业委员会信息中心主持，以“致富农民、服务市民、提高行政管理效能”为目标，充分发挥大数据在农业农村发展中的重要功能和巨大潜力，重点建设上海市级农业农村大数据中心，促进信息资源的共建共享和创新应用。华东师范大学数据科学与工程学院（以下简称华师大数据学院）作为课题主要参与单位以实现智慧农业为目标，探索农业大数据分析、挖掘和综合应用。华师大课题组在前期国家重点研发计划《大数据知识工程基础理论及其应用研究》研究基础上，在本项目中，基于碎片化农业大数据，构建面向智慧农业的知识图谱及其应用系统。目录结构：.
├── MyCrawler      // scrapy爬虫项目路径(已爬好)
│   └── MyCrawler
│       ├── data
│       └── spiders
├── data\ processing    // 数据清洗(已无用)
│   └── data
├── demo     // django项目路径
│   ├── Model  // 模型层，用于封装Item类，以及neo4j和csv的读取
│   ├── demo   // 用于写页面的逻辑(View)
│   ├── label_data    // 标注训练集页面的保存路径
│   │   └── handwork
│   ├── static    // 静态资源
│   │   ├── css
│   │   ├── js
│   │   └── open-iconic
│   ├── templates   // html页面
│   └── toolkit   // 工具库，包括预加载，命名实体识别
│   └── KNN_predict   
├── KNN_predict    // KNN算法预测标签
├── dfs_tree_crawler     // 爬取互动百科农业实体树形结构的爬虫
└── wikidataSpider    //  爬取wiki中的关系
可复用资源项目配置0.安装基本环境：确保安装好python3和Neo4j（任意版本）安装一系列pip依赖： cd至项目根目录，运行 sudo pip3 install -r requirement.txt1.导入数据：将hudong_pedia.csv导入neo4j：开启neo4j，进入neo4j控制台。将hudong_pedia.csv放入neo4j安装目录下的/import目录。在控制台依次输入：// 将hudong_pedia.csv 导入
LOAD CSV WITH HEADERS  FROM ""file:///hudong_pedia.csv"" AS line  
CREATE (p:HudongItem{title:line.title,image:line.image,detail:line.detail,url:line.url,openTypeList:line.openTypeList,baseInfoKeyList:line.baseInfoKeyList,baseInfoValueList:line.baseInfoValueList})  

// 新增了hudong_pedia2.csv
LOAD CSV WITH HEADERS  FROM ""file:///hudong_pedia2.csv"" AS line  
CREATE (p:HudongItem{title:line.title,image:line.image,detail:line.detail,url:line.url,openTypeList:line.openTypeList,baseInfoKeyList:line.baseInfoKeyList,baseInfoValueList:line.baseInfoValueList})  
// 创建索引
CREATE CONSTRAINT ON (c:HudongItem)
ASSERT c.title IS UNIQUE
以上两步的意思是，将hudong_pedia.csv导入neo4j作为结点，然后对titile属性添加UNIQUE（唯一约束/索引）（如果导入的时候出现neo4j jvm内存溢出，可以在导入前，先把neo4j下的conf/neo4j.conf中的dbms.memory.heap.initial_size 和dbms.memory.heap.max_size调大点。导入完成后再把值改回去）进入/wikidataSpider/wikidataProcessing中，将new_node.csv,wikidata_relation.csv,wikidata_relation2.csv三个文件放入neo4j的import文件夹中（运行relationDataProcessing.py可以得到这3个文件），然后分别运行// 导入新的节点
LOAD CSV WITH HEADERS FROM ""file:///new_node.csv"" AS line
CREATE (:NewNode { title: line.title })

//添加索引
CREATE CONSTRAINT ON (c:NewNode)
ASSERT c.title IS UNIQUE

//导入hudongItem和新加入节点之间的关系
LOAD CSV  WITH HEADERS FROM ""file:///wikidata_relation2.csv"" AS line
MATCH (entity1:HudongItem{title:line.HudongItem}) , (entity2:NewNode{title:line.NewNode})
CREATE (entity1)-[:RELATION { type: line.relation }]->(entity2)

LOAD CSV  WITH HEADERS FROM ""file:///wikidata_relation.csv"" AS line
MATCH (entity1:HudongItem{title:line.HudongItem1}) , (entity2:HudongItem{title:line.HudongItem2})
CREATE (entity1)-[:RELATION { type: line.relation }]->(entity2)
导入实体属性(数据来源: 互动百科)将attributes.csv放到neo4j的import目录下，然后执行LOAD CSV WITH HEADERS FROM ""file:///attributes.csv"" AS line
MATCH (entity1:HudongItem{title:line.Entity}), (entity2:HudongItem{title:line.Attribute})
CREATE (entity1)-[:RELATION { type: line.AttributeName }]->(entity2);
                                                            
LOAD CSV WITH HEADERS FROM ""file:///attributes.csv"" AS line
MATCH (entity1:HudongItem{title:line.Entity}), (entity2:NewNode{title:line.Attribute})
CREATE (entity1)-[:RELATION { type: line.AttributeName }]->(entity2);
                                                            
LOAD CSV WITH HEADERS FROM ""file:///attributes.csv"" AS line
MATCH (entity1:NewNode{title:line.Entity}), (entity2:NewNode{title:line.Attribute})
CREATE (entity1)-[:RELATION { type: line.AttributeName }]->(entity2);
                                                            
LOAD CSV WITH HEADERS FROM ""file:///attributes.csv"" AS line
MATCH (entity1:NewNode{title:line.Entity}), (entity2:HudongItem{title:line.Attribute})
CREATE (entity1)-[:RELATION { type: line.AttributeName }]->(entity2)  

//我们建索引的时候带了label，因此只有使用label时才会使用索引，这里我们的实体有两个label，所以一共做2*2=4次。当然，可以建立全局索引，即对于不同的label使用同一个索引
                                                            
          
                                                                                                                         
导入气候名称:将wikidataSpider/weatherData/static_weather_list.csv放在指定的位置(import文件夹下)//导入节点
LOAD CSV WITH HEADERS FROM ""file:///static_weather_list.csv"" AS line
MERGE (:Weather { title: line.title })

//添加索引
CREATE CONSTRAINT ON (c:Weather)
ASSERT c.title IS UNIQUE
导入气候与植物的关系
将wikidataSpider/weatherData/weather_plant.csv放在指定的位置(import文件夹下)
//导入hudongItem和新加入节点之间的关系
LOAD CSV  WITH HEADERS FROM ""file:///weather_plant.csv"" AS line
MATCH (entity1:Weather{title:line.Weather}) , (entity2:HudongItem{title:line.Plant})
CREATE (entity1)-[:Weather2Plant { type: line.relation }]->(entity2)
导入城市的气候

将city_weather.csv放在指定的位置(import 文件夹下)
(这步大约需要15分钟左右)
//导入城市对应的气候
LOAD CSV WITH HEADERS FROM ""file:///city_weather.csv"" AS line
MATCH (city{title:line.city}) , (weather{title:line.weather})
CREATE (city)-[:CityWeather { type: line.relation }]->(weather)
以上步骤是导入爬取到的关系2.下载词向量模型：（如果只是为了运行项目，步骤2可以不做，预测结果已经离线处理好了）~~http://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.zh.zip将wiki.zh.bin放入 KNN_predict 目录 。~~3.修改Neo4j用户进入demo/Model/neo_models.py,修改第9行的neo4j账号密码，改成你自己的4.启动服务进入demo目录，然后运行脚本：sudo sh django_server_start.sh
这样就成功的启动了django。我们进入8000端口主页面，输入文本，即可看到以下命名实体和分词的结果（确保django和neo4j都处于开启状态）(update 2018.11.11)添加了农业知识问答(update 2018.10.26)农业实体识别+实体分类点击实体的超链接，可以跳转到词条页面（词云采用了词向量技术）：实体查询实体查询部分，我们能够搜索出与某一实体相关的实体，以及它们之间的关系：关系查询关系查询即查询三元组关系entity1-[relation]->entity2 , 分为如下几种情况:下图所示，是指定关系relation和第二个实体entity2的查询结果知识的树形结构农业知识概览部分，我们能够列出某一农业分类下的词条列表，这些概念以树形结构组织在一起：农业分类的树形图：训练集标注我们还制作了训练集的手动标注页面，每次会随机的跳出一个未标注过的词条。链接：http://localhost:8000/tagging-get , 手动标注的结果会追加到/label_data/labels.txt文件末尾：我们将这部分做成了小工具，可复用：https://github.com/qq547276542/LabelMarker(update 2018.04.07)  同样的，我们制作了标注关系提取训练集的工具，如下图所示如果Statement的标签是对的，点击True按钮；否则选择一个关系，或者输入其它关系。若当前句子无法判断，则点击Change One按钮换一条数据。说明:　Statement是/wikidataSpider/TrainDataBaseOnWiki/finalData中train_data.txt中的数据，我们将它转化成json,导入到mongoDB中。标注好的数据同样存在MongoDB中另一个Collection中。关于Mongo的使用方法可以参考官方tutorial，或者利用这篇文章简单了解一下 我们在MongoDB中使用两个Collections，一个是train_data，即未经人工标注的数据；另一个是test_data，即人工标注好的数据。使用方法: 启动neo4j,mongodb之后，进入demo目录，启动django服务，进入127.0.0.1:8000/tagging即可使用思路命名实体识别:使用thulac工具进行分词，词性标注，命名实体识别（仅人名，地名，机构名）为了识别农业领域特定实体，我们需要： 实体分类：特征提取：分类器：KNN算法定义两个页面的相似度sim(p1,p2)：Labels：（命名实体的分类）| Label | NE Tags                                  | Example                                  || ----- | ---------------------------------------- | ---------------------------------------- || 0     | Invalid（不合法）                             | “色调”，“文化”，“景观”，“条件”，“A”，“234年”（不是具体的实体，或一些脏数据） || 1     | Person（人物，职位）                            | “袁隆平”，“副市长”                         || 2     | Location（地点，区域）                          | “福建省”，“三明市”，“大明湖”                        || 3     | Organization（机构，会议）                      | “华东师范大学”，“上海市农业委员会”                      || 4     | Political economy（政治经济名词）                | “惠农补贴”，“基本建设投资”                          || 5     | Animal（动物学名词，包括畜牧类，爬行类，鸟类，鱼类，等）          | “绵羊”，“淡水鱼”，“麻雀”                          || 6     | Plant（植物学名词，包括水果，蔬菜，谷物，草药，菌类，植物器官，其他植物）  | “苹果”，“小麦”，“生菜”                           || 7     | Chemicals（化学名词，包括肥料，农药，杀菌剂，其它化学品，术语等）    | “氮”，“氮肥”，“硝酸盐”，“吸湿剂”                     || 8     | Climate（气候，季节）                           | “夏天”，“干旱”                                || 9     | Food items（动植物产品）                        | “奶酪”，“牛奶”，“羊毛”，“面粉”                      || 10    | Diseases（动植物疾病）                          | “褐腐病”，“晚疫病”                              || 11    | Natural Disaster（自然灾害）                   | “地震”，“洪水”，“饥荒”                           || 12    | Nutrients（营养素，包括脂肪，矿物质，维生素，碳水化合物等）       | “维生素A”，""钙""                               || 13    | Biochemistry（生物学名词，包括基因相关，人体部位，组织器官，细胞，细菌，术语） | “染色体”，“血红蛋白”，“肾脏”，“大肠杆菌”                 || 14    | Agricultural implements（农机具，一般指机械或物理设施）  | “收割机”，“渔网”                               || 15    | Technology(农业相关术语，技术和措施)                 | “延后栽培""，“卫生防疫”，“扦插”                       || 16    | other（除上面类别之外的其它名词实体，可以与农业无关但必须是实体）      | “加速度""，“cpu”，“计算机”，“爱鸟周”，“人民币”，“《本草纲目》”，“花岗岩” |关系抽取使用远程监督方法构建数据集，利用tensorflow训练PCNN模型详情见： "
https://github.com/deepset-ai/haystack,":mag: LLM orchestration framework to build customizable, production-ready LLM applications. Connect components (models, vector DBs, file converters) to pipelines or agents that can interact with your data. With advanced retrieval methods, it's best suited for building RAG, question answering, semantic search or conversational agent chatbots.","|         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     || ------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- || CI/CD   |       || Docs    |                                                                                                                                                                                                                                                                                                                                                                                                                                  || Package |                                                                                                                                                                                                 || Meta    |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | is an end-to-end NLP framework that enables you to build applications powered by LLMs, Transformer models, vector search and more. Whether you want to perform question answering, answer generation, semantic document search, or build tools that are capable of complex decision-making and query resolution, you can use the state-of-the-art NLP models with Haystack to build end-to-end NLP applications solving your use case.QuickstartHaystack is built around the concept of pipelines. A pipeline is a powerful structure that performs an NLP task. It's made up of components connected together. For example, you can connect a  and a  to build a Generative Question Answering pipeline that uses your own data.Try out how Haystack answers questions about Game of Thrones using the Retrieval Augmented Generation (RAG) approach 👇First, run the minimal Haystack installation:pip install farm-haystack
Then, index your data to the DocumentStore, build a RAG pipeline, and ask a question on your data: from haystack.document_stores import InMemoryDocumentStore
from haystack.utils import build_pipeline, add_example_data, print_answers

# We are model agnostic :) Here, you can choose from: ""anthropic"", ""cohere"", ""huggingface"", and ""openai"".
provider = ""openai""
API_KEY = ""sk-..."" # ADD YOUR KEY HERE

# We support many different databases. Here, we load a simple and lightweight in-memory database.
document_store = InMemoryDocumentStore(use_bm25=True)

# Download and add Game of Thrones TXT articles to Haystack DocumentStore.
# You can also provide a folder with your local documents.
add_example_data(document_store, ""data/GoT_getting_started"")

# Build a pipeline with a Retriever to get relevant documents to the query and a PromptNode interacting with LLMs using a custom prompt.
pipeline = build_pipeline(provider, API_KEY, document_store)

# Ask a question on the data you just added.
result = pipeline.run(query=""Who is the father of Arya Stark?"")

# For details, like which documents were used to generate the answer, look into the <result> object
print_answers(result, details=""medium"")
The output of the pipeline will reference the documents used to generate the answer:'Query: Who is the father of Arya Stark?'
'Answers:'
[{'answer': 'The father of Arya Stark is Lord Eddard Stark of '
                'Winterfell. [Document 1, Document 4, Document 5]'}]
Congratulations, you have just built your first Haystack app!Core Concepts🏃‍♀️ [<marko.inline.RawText object at 0x000001592FE1B688>] This is the standard Haystack structure that builds on top of your data to perform various NLP tasks such as retrieval augmented generation, question answering and more. The data in a Pipeline flows from one Node to the next. You define how Nodes interact with each other, and how one Node pushes data to the next.An example pipeline would consist of one  Node and one . When the pipeline runs with a query, the Retriever first retrieves the relevant context to the query from your data, and then the PromptNode uses an LLM to generate the final answer.⚛️ [<marko.inline.RawText object at 0x000001592FD92F08>] Each Node achieves one thing. Such as preprocessing documents, retrieving documents, using language models to answer questions and so on.🕵️ [<marko.inline.RawText object at 0x000001592FD568C8>] (since 1.15) An Agent is a component that is powered by an LLM, such as GPT-3. It can decide on the next best course of action so as to get to the result of a query. It uses the Tools available to it to achieve this. While a pipeline has a clear start and end, an Agent is able to decide whether the query has been resolved or not. It may also make use of a Pipeline as a Tool.🛠️ [<marko.inline.RawText object at 0x000001592FE1BE08>] You can think of a Tool as an expert, that is able to do something really well. Such as a calculator, good at mathematics. Or a , good at retrieving pages from the internet. A Node or pipeline in Haystack can also be used as a Tool. A Tool is a component that is used by an Agent, to resolve complex queries.🗂️ [<marko.inline.RawText object at 0x000001592FD99048>] A DocumentStore is database where you store your text data for Haystack to access. Haystack DocumentStores are available with ElasticSearch, Opensearch, Weaviate, Pinecone, FAISS and more. For a full list of available DocumentStores, check out our .What to Build with HaystackFeaturesResources|                                                                        |                                                                                                                                                                                                                                                   || ---------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- || 📒                              | Components, Pipeline Nodes, Guides, API Reference                                                                                                                                                                                                 || 💾  | How to install Haystack                                                                                                                                                                                                                           || 🎓                    | See what Haystack can do with our Notebooks & Scripts                                                                                                                                                                                             || 🎉      | A repository that lists extra Haystack packages and components that can be installed separately.                                                                                                                                                  || 🔰                 | A repository containing Haystack demo applications with Docker Compose and a REST API                                                                                                                                                             || 🖖        | , , ,  || 💙  | We welcome all contributions!                                                                                                                                                                                                                     || 📊                 | Speed & Accuracy of Retriever, Readers and DocumentStores                                                                                                                                                                                         || 🔭               | Public roadmap of Haystack                                                                                                                                                                                                                        || 📰                              | Learn about the latest with Haystack and NLP                                                                                                                                                                                                      || ☎️                                   | We're hiring! Have a look at our open positions                                                                                                                                                                                                   |💾 InstallationFor a detailed installation guide see . There you’ll find instructions for custom installations handling Windows and Apple Silicon.Basic InstallationUse  to install a basic version of Haystack's latest release:pip install farm-haystack
This command installs everything needed for basic Pipelines that use an in-memory DocumentStore and external LLM provider (e.g. OpenAI).Full InstallationTo use more advanced features, like certain DocumentStores, inference with local transformer models, FileConverters, OCR, or Ray,you need to install further dependencies. The following command installs the  of Haystack and all its dependencies:pip install 'farm-haystack[all]' ## or 'all-gpu' for the GPU-enabled dependencies
If you want to install only the dependencies needed for model inference on your local hardware (not remote API endpoints), such as torch and sentence-transformers, you can use the following command:pip install 'farm-haystack[inference]' ## installs torch, sentence-transformers, sentencepiece, and huggingface-hub
If you want to try out the newest features that are not in an official release yet, you can install the unstable version from the main branch with the following command:pip install git+https://github.com/deepset-ai/haystack.git@main#egg=farm-haystack
To be able to make changes to Haystack code, first of all clone this repo:git clone https://github.com/deepset-ai/haystack.git
Then move into the cloned folder and install the project with , including the development dependencies:cd haystack && pip install -e '.[dev]'
If you want to contribute to the Haystack repo, check our  first.See the list of  to check which ones you want to install (for example, , , or other).Installing the REST APIHaystack comes packaged with a REST API so that you can deploy it as a service. Run the following command from the root directory of the Haystack repo to install REST_API:pip install rest_api/
You can find out more about our PyPi package on our .🔰DemosYou can find some of our hosted demos with instructions to run them locally too on our  repository:dizzy: [<marko.inline.RawText object at 0x000001592FD3A448>]🐥 [<marko.inline.RawText object at 0x000001592FD3A8C8>]🌎 [<marko.inline.RawText object at 0x000001592FFA23C8>]🖖 CommunityIf you have a feature request or a bug report, feel free to open an . We regularly check these and you can expect a quick response. If you'd like to discuss a topic, or get more general advice on how to make Haystack work for your project, you can start a thread in  or our . We also check  and .💙 ContributingWe are very open to the community's contributions - be it a quick fix of a typo, or a completely new feature! You don't need to be a Haystack expert to provide meaningful improvements. To learn how to get started, check out our  first.Who Uses HaystackHere's a list of projects and companies using Haystack. Want to add yours? Open a PR, add it to the list and let theworld know that you use Haystack!"
https://github.com/Jrohy/multi-v2ray,v2ray/xray多用户管理部署程序,"multi-v2rayV2ray/Xray多用户管理脚本，向导式管理[新增|删除|修改]传输协议  特色功能安装命令source <(curl -sL https://multi.netlify.app/v2ray.sh) --zh
升级命令(保留配置文件更新)source <(curl -sL https://multi.netlify.app/v2ray.sh) -k
卸载命令source <(curl -sL https://multi.netlify.app/v2ray.sh) --remove
命令行参数v2ray/xray [-h|help] [options]
    -h, help             查看帮助
    -v, version          查看版本号
    start                启动 V2Ray
    stop                 停止 V2Ray
    restart              重启 V2Ray
    status               查看 V2Ray 运行状态
    new                  重建新的v2ray json配置文件
    update               更新 V2Ray 到最新Release版本
    update [version]     更新 V2Ray 到指定版本
    update.sh            更新 multi-v2ray 到最新版本
    add                  新增端口组
    add [protocol]       新增一种协议的组, 端口随机, 如 v2ray add utp 为新增utp协议
    del                  删除端口组
    info                 查看配置
    port                 修改端口
    tls                  修改tls
    tfo                  修改tcpFastOpen
    stream               修改传输协议
    cdn                  走cdn
    stats                v2ray流量统计
    iptables             iptables流量统计
    clean                清理日志
    log                  查看日志
    rm                   卸载core
Docker运行默认创建mkcp + 随机一种伪装头配置文件(如果使用xray则换成镜像jrohy/xray)：docker run -d --name v2ray --privileged --restart always --network host jrohy/v2ray
自定义v2ray配置文件:docker run -d --name v2ray --privileged -v /path/config.json:/etc/v2ray/config.json --restart always --network host jrohy/v2ray
查看v2ray配置:docker exec v2ray bash -c ""v2ray info""
warning: 如果用centos，需要先关闭防火墙systemctl stop firewalld.service
systemctl disable firewalld.service
建议安装完v2ray后强烈建议开启BBR等加速: 使用Trojan和VLESS协议建议自行安装个nginx, 能让v2ray顺利Fallback到默认的80端口依赖v2ray docker: https://hub.docker.com/r/jrohy/v2rayxray docker: https://hub.docker.com/r/jrohy/xraypip: https://pypi.org/project/v2ray-util/python3: https://github.com/Jrohy/python3-installacme: https://github.com/acmesh-official/acme.sh"
https://github.com/hellerve/programming-talks,Awesome & interesting talks about programming,"Programming TalksI watch a lot of talks that I love to share with my friends, fellows and coworkers.As I consider all GitHubbers my friends (oh yeah!), I decided it's time to share thelist.There are talks on programming language specifics as well as a more general section I call ""theory"".But don't expect to always get theoretical computer science for every talk there;most of them are on the architecture and design of software.I welcome every contribution to the list; for guidelines look .Disclaimer: I did not give any of the talks on the list and am responsible neitherfor their content nor for their presentation. All links below will direct you toexternal sites (mostly YouTube, really), be aware of that. If you are one of the peopleresponsible for the talks or the platform presenting it and want it removed,tell me and I'll sort it out with you.[A] after a talk name denotes a talk that someone thought could be listened to as audio, without needing the video. This may not reflect your experience with the talk, but you can make a pull request to change it.Names to look out forTo make choosing the right speakers a tad easier, let me give you a quick overviewover my favourite speakers in no particular order:ContentsOn Programming LanguagesAlpacaAPLAssemblyBashCClojureC++CrystalCSS(yeah, I know, stylesheets are not traditionally programming)DElixirElmErlangF#FactorFregeGoHackettHaskellIdrisJava & AndroidJavaScript(There is a good list of talks about JS to be found )JuliaLispObjective COCamlPrologPureScriptPython(There is a good list of talks about Python to be found )RacketRubyRustScalaSchemeSmalltalkSwiftUnisonVimLWolfram LanguageZigOn theoryCompilers/InterpretersComputer Graphics and VisionCreative TechnologyDatabasesData ScienceData Structures & AlgorithmsDebuggingDevOpsDistributed SystemsEntrepreneurshipFunctional ProgrammingGame DevelopmentHardwareLogic ProgrammingMachine LearningMathematicsThose are not necessarily programming-related and possibly very advanced mathematics.On LanguagesOn the Industry/CommunityOperating SystemsPerformance EngineeringProgramming Language DesignProgram SynthesisResearchRoboticsSecuritySoftware DevelopmentSystem ArchitectureTestingTheoretical Computer ScienceType TheoryUX/UIWeb DevelopmentMiscellaneousContributingGenerally, a lot of talks are welcome on this list. The topic does not matter too much - itshould be linked to Computer Science - but the format does. Talks are welcome ifI hope that is straightforward, understandable and makes sense.When adding a new section/subsection or entry, please try to ensure it's sorted accordingly:"
https://github.com/StevenBlack/hosts,"🔒 Consolidating and extending hosts files from several well-curated sources. Optionally pick extensions for porn, social media, and other categories.","Take Note!With the exception of issues and PRs regarding changes to, all other issues regarding the content of theproduced hosts files should be made with the appropriate data source thatcontributed the content in question. The contact information for all of the datasources can be found in the  directory.Unified hosts file with base extensionsThis repository consolidates several reputable  files, and merges theminto a unified hosts file with duplicates removed. A variety of tailored hostsfiles are provided.Therefore this repository is a hosts file aggregator.List of all hosts file variantsThis repository offers,in addition to the base variant, with and without the unified hosts included.The Non GitHub mirror is the link to use for some hosts file managers like that don't workwith GitHub download links.| Host file recipe | Readme | Raw hosts | Unique domains | Non GitHub mirror || ---------------- | :----: | :-------: | :------------: | :---------------: |Unified hosts = (adware + malware) |  |  | 148,939 | Unified hosts + fakenews |  |  | 151,133 | fakenews |  |  | 2,194 | Unified hosts + gambling |  |  | 156,608 | gambling |  |  | 7,681 | Unified hosts + porn |  |  | 202,196 | porn |  |  | 53,896 | Unified hosts + social |  |  | 151,763 | social |  |  | 2,841 | Unified hosts + fakenews + gambling |  |  | 158,802 | fakenews + gambling |  |  | 9,875 | Unified hosts + fakenews + porn |  |  | 204,390 | fakenews + porn |  |  | 56,090 | Unified hosts + fakenews + social |  |  | 153,957 | fakenews + social |  |  | 5,035 | Unified hosts + gambling + porn |  |  | 209,865 | gambling + porn |  |  | 61,577 | Unified hosts + gambling + social |  |  | 159,432 | gambling + social |  |  | 10,522 | Unified hosts + porn + social |  |  | 205,019 | porn + social |  |  | 56,736 | Unified hosts + fakenews + gambling + porn |  |  | 212,059 | fakenews + gambling + porn |  |  | 63,771 | Unified hosts + fakenews + gambling + social |  |  | 161,626 | fakenews + gambling + social |  |  | 12,716 | Unified hosts + fakenews + porn + social |  |  | 207,213 | fakenews + porn + social |  |  | 58,930 | Unified hosts + gambling + porn + social |  |  | 212,688 | gambling + porn + social |  |  | 64,417 | Unified hosts + fakenews + gambling + porn + social |  |  | 214,882 | fakenews + gambling + porn + social |  |  | 66,611 | Expectation: These unified hosts files should serve all devices, regardlessof OS.Sources of hosts data unified in this variantUpdated  files from the following locations are always unified andincluded:| Host file source | Home page | Raw hosts | License | Issues | Description || ---------------- | :-------: | :-------: | :-----: | :----: | ----------- |Steven Black's ad-hoc list | |  | MIT | | Additional sketch domains as I come across them.AdAway | |  | CC BY 3.0 | | AdAway is an open source ad blocker for Android using the hosts file.add.2o7Net | |  | MIT | | 2o7Net tracking sites based on  content.add.Dead | |  | MIT | | Dead sites based on  content.add.Risk | |  | MIT | | Risk content sites based on  content.add.Spam | |  | MIT | | Spam sites based on  content.Mitchell Krog's - Badd Boyz Hosts | |  | MIT | | Sketchy domains and Bad Referrers from my Nginx and Apache Bad Bot and Spam Referrer BlockershostsVN | |  | MIT | | Hosts block ads of VietnameseKADhosts | |  | CC BY-SA 4.0 | | Fraud/adware/scam websites.MetaMask eth-phishing-detect | |  | DON'T BE A DICK PUBLIC LICENSE | | Phishing domains targeting Ethereum users.minecraft-hosts | |  | CC0-1.0 | | Minecraft related tracker hostsMVPS hosts file | |  | CC BY-NC-SA 4.0 | | The purpose of this site is to provide the user with a high quality custom HOSTS file.shady-hosts | |  | CC0-1.0 | | Analytics, ad, and activity monitoring hostsDan Pollock –  | |  | non-commercial with attribution | | How to make the internet not suck (as much).Tiuxo hostlist - ads | |  | CC BY 4.0 | | Categorized hosts files for DNS based content blockingUncheckyAds | |  | MIT | | Windows installers ads sources sites based on https://unchecky.com/ content.URLHaus | |  | CC0 | | A project from  with the goal of sharing malicious URLs.yoyo.org | |  |  | | Blocking with ad server and tracking server hostnames.ExtensionsThe unified hosts file is optionally extensible. Extensions are used to includedomains by category. Currently, we offer the following categories: ,, , and .Extensions are optional, and can be combined in various ways with the base hostsfile. The combined products are stored in thefolder.Data for extensions are stored in thefolder. You manage extensions by curating this folder tree, where you will findthe data for , , , and  extension data that wemaintain and provide for you.Generate your own unified hosts fileYou have three options to generate your own hosts file. You can use ourcontainer image, build your own image, or do it in your own environment. Option#1 is easiest if you have Linux with Docker installed.Option 1: Use our container image (Linux only)We assume you have Docker available on your host. Just run the followingcommand. Set extensions to your preference.docker run --pull always --rm -it -v /etc/hosts:/etc/hosts \
ghcr.io/stevenblack/hosts:latest updateHostsFile.py --auto \
--replace --extensions gambling porn
If you want to add custom hosts or a whitelist, create either or both files asper  and add thefollowing arguments before  depending onwhich you wish to use.-v ""path/to/myhosts:/hosts/myhosts"" \
-v ""path/to/whitelist:/hosts/whitelist"" \
You can rerun this exact command later to update based on the latest availablehosts (for example, add it to a weekly cron job).Option 2: Generate your own container imageWe provide the usedby the previous step, which you can use to create a container image witheverything you need. The container will contain Python 3 and all its dependencyrequirements, and a copy of the latest version of this repository.Build the Docker container from the root of this repo like this:docker build --no-cache . -t stevenblack-hosts
Then run your command as such:docker run --rm -it stevenblack-hosts updateHostsFile.py
Option 3: Generate it in your own environmentTo generate your own amalgamated hosts files you will need Python 3.6 or later.First, install the dependencies with:pip3 install --user -r requirements.txt
Note we recommend the  flag which installs the required dependenciesat the user level. More information about it can be found on pip.Option 4: Generate it in Google ColabSpin up a free remote  environment.Common steps regardless of your development environmentTo run unit tests, in the top-level directory, run:python3 testUpdateHostsFile.py
The  script will generate a unified hosts file based on thesources in the local  subfolder. The script will prompt you whether itshould fetch updated versions (from locations defined by the  textfile in each source's folder). Otherwise, it will use the  file that'salready there.python3 updateHostsFile.py [--auto] [--replace] [--ip nnn.nnn.nnn.nnn] [--extensions ext1 ext2 ext3]
Command line options, or : display help., or : run the script without prompting. When  is invoked,, or : Make a backup of existing hosts file(s) as you generateover them., or : the names ofsubfolders below the  folder containing additional category-specifichosts files to include in the amalgamation. Example:  or., or : skip the prompt for flushing the DNS cache. Onlyactive when  is also active., or : the IP address to use as thetarget. Default is ., or :  (default) or , keep the commentsthat appear on the same line as domains. The default is ., or : skip fetching updates from hosts data sources., or : place the generated source file ina subfolder. If the subfolder does not exist, it will be created., or : trigger replacing your active hosts, or :  (default) or , omit the standardsection at the top, containing lines like . This is usefulfor configuring proximate DNS services on the local network., or :  (default) or , skip the generation of thereadmeData.json file used for generating readme.md files. This is useful if youare generating host files with additional whitelists or blacklists and want tokeep your local checkout of this repo unmodified.:  (default) or , do not include the unified hostsfile in the final hosts file. Usually used together with ., or :  (default) or , Compress the hosts fileignoring non-necessary lines (empty lines and comments) and putting multipledomains in each line. Reducing the number of lines of the hosts file improvesthe performances under Windows (with DNS Client service enabled)., or :  (default) or , like , but putseach domain on a separate line. This is necessary because many implementationsof URL blockers that rely on  files do not conform to the standard whichallows multiple hosts on a single line., or : Append the givenblacklist file in hosts format to the generated hosts file., or : Use the given whitelistfile to remove hosts from the generated hosts file.How do I control which sources are unified?Add one or more additional sources, each in a subfolder of the  folder,and specify the  key in its  file.Add one or more optional extensions, which originate from subfolders of the folder. Again the url in  controls where thisextension finds its updates.Create an optional  file. The contents of this file (containing alisting of additional domains in  file format) are appended to theunified hosts file during the update process. A sample  is included,and may be modified as you need.How do I include my own custom domain mappings?If you have custom hosts records, place them in file . The contents ofthis file are prepended to the unified hosts file during the update process.The  file is not tracked by git, so any changes you make won't beoverridden when you  this repo from  in the future.How do I prevent domains from being included?The domains you list in the  file are excluded from the final hostsfile.The  uses partial matching. Therefore if you whitelist, that domain and all its subdomains won't be merged intothe final hosts file.The  is not tracked by git, so any changes you make won't beoverridden when you  this repo from  in the future.How can I contribute hosts records?If you discover sketchy domains you feel should be included here, here are someways to contribute them.Option 1: contact one of our hosts sourcesThe best way to get new domains included is to submit an issue to any of thedata providers whose home pages are.This is best because once you submit new domains, they will be curated andupdated by the dedicated folks who maintain these sources.Option 2: Fork this repository, add your domains to Steven Black's personal data file, and submit a pull requestFork this hosts this repo and add your links to.Then, submit a pull request.WARNING: this is less desirable than Option 1 because the ongoing curationfalls on us. So this creates more work for us.Option 3: create your own hosts list as a repo on GitHubIf you're able to curate your own collection of sketchy domains, then curateyour own hosts list. Then signal the existence of your repo as and we may includeyour new repo into the collection of sources we pull whenever we create newversions.What is a hosts file?A hosts file, named  (with no file extension), is a plain-text file usedby all operating systems to map hostnames to IP addresses.In most operating systems, the  file is preferential to . Thereforeif a domain name is resolved by the  file, the request never leaves yourcomputer.Having a smart  file goes a long way towards blocking malware, adware,and other irritants.For example, to nullify requests to some doubleclick.net servers, adding theselines to your hosts file will do it:# block doubleClick's servers
0.0.0.0 ad.ae.doubleclick.net
0.0.0.0 ad.ar.doubleclick.net
0.0.0.0 ad.at.doubleclick.net
0.0.0.0 ad.au.doubleclick.net
0.0.0.0 ad.be.doubleclick.net
# etc...
We recommend using  instead of Traditionally most host files use , the loopback address, toestablish an IP connection to the local machine.We prefer to use , which is defined as a non-routable meta-address usedto designate an invalid, unknown, or non-applicable target.Using  is empirically faster, possibly because there's no wait for atimeout resolution. It also does not interfere with a web server that may berunning on the local PC.Why not use  instead of ?We tried that. Using  doesn't work universally.Location of your hosts fileTo modify your current  file, look for it in the following places andmodify it with a text editor.GentooGentoo users may findin  Gentoo overlayNixOSTo install hosts file on your machine add the following into your:{
  networking.extraHosts = let
    hostsPath = https://raw.githubusercontent.com/StevenBlack/hosts/master/hosts;
    hostsFile = builtins.fetchurl hostsPath;
  in builtins.readFile ""${hostsFile}"";
}
Nix FlakeNixOS installations which are managed through flakes can use the hosts filelike this:{
  inputs.hosts.url = github:StevenBlack/hosts;
  outputs = { self, nixpkgs, hosts }: {
    nixosConfigurations.my-hostname = {
      system = ""<architecture>"";
      modules = [
        hosts.nixosModule {
          networking.stevenBlackHosts.enable = true;
        }
      ];
    };
  };
}
The hosts extensions are also available with the following options:{
  networking.stevenBlackHosts = {
    blockFakenews = true;
    blockGambling = true;
    blockPorn = true;
    blockSocial = true;
  };
}
Updating hosts file on Windows(NOTE: See also some third-party Hosts managers, listed below.)On Linux and macOS, run the Python script. On Windows more work is required dueto compatibility issues so it's preferable to run the batch file as follows:updateHostsWindows.bat
This file MUST be run in command prompt with administrator privileges in therepository directory. In addition to updating the hosts file, it can alsoreplace the existing hosts file, and reload the DNS cache. It goes withoutsaying that for this to work, you must be connected to the internet.To open a command prompt as administrator in the repository's directory, do thefollowing:You can also refer to the ""Third-Party Hosts Managers"" section for furtherrecommended solutions from third parties.Warning: Using this  file in Windows may require disabling DNS Cache serviceWindows has issues with larger hosts files. Recent changes in security withinWindows 10 denies access to changing services via other tools except registryhacks. Use the  file to make proper changes tothe Windows registry. You will need to reboot your device once that's done. Seethefor more details.Disabling the DNS Cache Service can cause issues with services and applications like WSL and it's possible to compress the hosts file and negate the need to disable the DNS caching service. You can try the C++ Windows command line tool at  (the recommended method) or the PowerShell compression script and check out the guide located at the  repository.Reloading hosts fileYour operating system will cache DNS lookups. You can either reboot or run thefollowing commands to manually flush your DNS cache once the new hosts file isin place.The Google Chrome browser may require manually cleaning up its DNS Cache on page to thereafter see the changes in your hostsfile. See: WindowsOpen a command prompt with administrator privileges and run this command:ipconfig /flushdns
LinuxOpen a Terminal and run with root privileges:macOSAs described in ,open a Terminal and run:sudo dscacheutil -flushcache;sudo killall -HUP mDNSResponder
Release managementThis repository uses , anexcellent CLI release tool for GitHub repos and npm packages, to automatecreating . This is whytheandfiles are bundled.Goals of this unified hosts fileThe goals of this repo are to:A high-quality source is defined here as one that is actively curated. A hostssource should be frequently updated by its maintainers with both additions andremovals. The larger the hosts file, the higher the level of curation isexpected.It is expected that this unified hosts file will serve both desktop and mobiledevices under a variety of operating systems.Third-Party Hosts ManagersInteresting ApplicationsContributePlease read our.Among other things, this explains how we organize files and folders in thisrepository.We are always interested in discovering well-curated sources of hosts. If youfind one, please open an  todraw our attention.Before you create or respond to any issue, please read our.Logo by  Thank you!."
https://github.com/cea-sec/miasm,Reverse engineering framework in Python,"What is Miasm?Miasm is a free and open source (GPLv2) reverse engineering framework.Miasm aims to analyze / modify / generate binary programs. Here isa non exhaustive list of features:See the official  for more examples and demos.Table of ContentsBasic examplesAssembling / DisassemblingImport Miasm x86 architecture:>>> from miasm.arch.x86.arch import mn_x86
>>> from miasm.core.locationdb import LocationDB
Get a location db:>>> loc_db = LocationDB()
Assemble a line:>>> l = mn_x86.fromstring('XOR ECX, ECX', loc_db, 32)
>>> print(l)
XOR        ECX, ECX
>>> mn_x86.asm(l)
['1\xc9', '3\xc9', 'g1\xc9', 'g3\xc9']
Modify an operand:>>> l.args[0] = mn_x86.regs.EAX
>>> print(l)
XOR        EAX, ECX
>>> a = mn_x86.asm(l)
>>> print(a)
['1\xc8', '3\xc1', 'g1\xc8', 'g3\xc1']
Disassemble the result:>>> print(mn_x86.dis(a[0], 32))
XOR        EAX, ECX
Using  abstraction:>>> from miasm.analysis.machine import Machine
>>> mn = Machine('x86_32').mn
>>> print(mn.dis('\x33\x30', 32))
XOR        ESI, DWORD PTR [EAX]
For MIPS:>>> mn = Machine('mips32b').mn
>>> print(mn.dis(b'\x97\xa3\x00 ', ""b""))
LHU        V1, 0x20(SP)
Intermediate representationCreate an instruction:>>> machine = Machine('arml')
>>> instr = machine.mn.dis('\x00 \x88\xe0', 'l')
>>> print(instr)
ADD        R2, R8, R0
Create an intermediate representation object:>>> lifter = machine.lifter_model_call(loc_db)
Create an empty ircfg:>>> ircfg = lifter.new_ircfg()
Add instruction to the pool:>>> lifter.add_instr_to_ircfg(instr, ircfg)
Print current pool:>>> for lbl, irblock in ircfg.blocks.items():
...     print(irblock)
loc_0:
R2 = R8 + R0

IRDst = loc_4

Working with IR, for instance by getting side effects:>>> for lbl, irblock in ircfg.blocks.items():
...     for assignblk in irblock:
...         rw = assignblk.get_rw()
...         for dst, reads in rw.items():
...             print('read:   ', [str(x) for x in reads])
...             print('written:', dst)
...             print()
...
read:    ['R8', 'R0']
written: R2

read:    []
written: IRDst

More information on Miasm IR is in the .EmulationGiving a shellcode:00000000 8d4904      lea    ecx, [ecx+0x4]
00000003 8d5b01      lea    ebx, [ebx+0x1]
00000006 80f901      cmp    cl, 0x1
00000009 7405        jz     0x10
0000000b 8d5bff      lea    ebx, [ebx-1]
0000000e eb03        jmp    0x13
00000010 8d5b01      lea    ebx, [ebx+0x1]
00000013 89d8        mov    eax, ebx
00000015 c3          ret
>>> s = b'\x8dI\x04\x8d[\x01\x80\xf9\x01t\x05\x8d[\xff\xeb\x03\x8d[\x01\x89\xd8\xc3'
Import the shellcode thanks to the  abstraction:>>> from miasm.analysis.binary import Container
>>> c = Container.from_string(s, loc_db)
>>> c
<miasm.analysis.binary.ContainerUnknown object at 0x7f34cefe6090>
Disassembling the shellcode at address :>>> from miasm.analysis.machine import Machine
>>> machine = Machine('x86_32')
>>> mdis = machine.dis_engine(c.bin_stream, loc_db=loc_db)
>>> asmcfg = mdis.dis_multiblock(0)
>>> for block in asmcfg.blocks:
...  print(block)
...
loc_0
LEA        ECX, DWORD PTR [ECX + 0x4]
LEA        EBX, DWORD PTR [EBX + 0x1]
CMP        CL, 0x1
JZ         loc_10
->      c_next:loc_b    c_to:loc_10
loc_10
LEA        EBX, DWORD PTR [EBX + 0x1]
->      c_next:loc_13
loc_b
LEA        EBX, DWORD PTR [EBX + 0xFFFFFFFF]
JMP        loc_13
->      c_to:loc_13
loc_13
MOV        EAX, EBX
RET
Initializing the JIT engine with a stack:>>> jitter = machine.jitter(loc_db, jit_type='python')
>>> jitter.init_stack()
Add the shellcode in an arbitrary memory location:>>> run_addr = 0x40000000
>>> from miasm.jitter.csts import PAGE_READ, PAGE_WRITE
>>> jitter.vm.add_memory_page(run_addr, PAGE_READ | PAGE_WRITE, s)
Create a sentinelle to catch the return of the shellcode:def code_sentinelle(jitter):
    jitter.running = False
    jitter.pc = 0
    return True

>>> jitter.add_breakpoint(0x1337beef, code_sentinelle)
>>> jitter.push_uint32_t(0x1337beef)
Active logs:>>> jitter.set_trace_log()
Run at arbitrary address:>>> jitter.init_run(run_addr)
>>> jitter.continue_run()
RAX 0000000000000000 RBX 0000000000000000 RCX 0000000000000000 RDX 0000000000000000
RSI 0000000000000000 RDI 0000000000000000 RSP 000000000123FFF8 RBP 0000000000000000
zf 0000000000000000 nf 0000000000000000 of 0000000000000000 cf 0000000000000000
RIP 0000000040000000
40000000 LEA        ECX, DWORD PTR [ECX+0x4]
RAX 0000000000000000 RBX 0000000000000000 RCX 0000000000000004 RDX 0000000000000000
RSI 0000000000000000 RDI 0000000000000000 RSP 000000000123FFF8 RBP 0000000000000000
zf 0000000000000000 nf 0000000000000000 of 0000000000000000 cf 0000000000000000
....
4000000e JMP        loc_0000000040000013:0x40000013
RAX 0000000000000000 RBX 0000000000000000 RCX 0000000000000004 RDX 0000000000000000
RSI 0000000000000000 RDI 0000000000000000 RSP 000000000123FFF8 RBP 0000000000000000
zf 0000000000000000 nf 0000000000000000 of 0000000000000000 cf 0000000000000000
RIP 0000000040000013
40000013 MOV        EAX, EBX
RAX 0000000000000000 RBX 0000000000000000 RCX 0000000000000004 RDX 0000000000000000
RSI 0000000000000000 RDI 0000000000000000 RSP 000000000123FFF8 RBP 0000000000000000
zf 0000000000000000 nf 0000000000000000 of 0000000000000000 cf 0000000000000000
RIP 0000000040000013
40000015 RET
>>>

Interacting with the jitter:>>> jitter.vm
ad 1230000 size 10000 RW_ hpad 0x2854b40
ad 40000000 size 16 RW_ hpad 0x25e0ed0

>>> hex(jitter.cpu.EAX)
'0x0L'
>>> jitter.cpu.ESI = 12
Symbolic executionInitializing the IR pool:>>> lifter = machine.lifter_model_call(loc_db)
>>> ircfg = lifter.new_ircfg_from_asmcfg(asmcfg)
Initializing the engine with default symbolic values:>>> from miasm.ir.symbexec import SymbolicExecutionEngine
>>> sb = SymbolicExecutionEngine(lifter)
Launching the execution:>>> symbolic_pc = sb.run_at(ircfg, 0)
>>> print(symbolic_pc)
((ECX + 0x4)[0:8] + 0xFF)?(0xB,0x10)
Same, with step logs (only changes are displayed):>>> sb = SymbolicExecutionEngine(lifter, machine.mn.regs.regs_init)
>>> symbolic_pc = sb.run_at(ircfg, 0, step=True)
Instr LEA        ECX, DWORD PTR [ECX + 0x4]
Assignblk:
ECX = ECX + 0x4
________________________________________________________________________________
ECX                = ECX + 0x4
________________________________________________________________________________
Instr LEA        EBX, DWORD PTR [EBX + 0x1]
Assignblk:
EBX = EBX + 0x1
________________________________________________________________________________
EBX                = EBX + 0x1
ECX                = ECX + 0x4
________________________________________________________________________________
Instr CMP        CL, 0x1
Assignblk:
zf = (ECX[0:8] + -0x1)?(0x0,0x1)
nf = (ECX[0:8] + -0x1)[7:8]
pf = parity((ECX[0:8] + -0x1) & 0xFF)
of = ((ECX[0:8] ^ (ECX[0:8] + -0x1)) & (ECX[0:8] ^ 0x1))[7:8]
cf = (((ECX[0:8] ^ 0x1) ^ (ECX[0:8] + -0x1)) ^ ((ECX[0:8] ^ (ECX[0:8] + -0x1)) & (ECX[0:8] ^ 0x1)))[7:8]
af = ((ECX[0:8] ^ 0x1) ^ (ECX[0:8] + -0x1))[4:5]
________________________________________________________________________________
af                 = (((ECX + 0x4)[0:8] + 0xFF) ^ (ECX + 0x4)[0:8] ^ 0x1)[4:5]
pf                 = parity((ECX + 0x4)[0:8] + 0xFF)
zf                 = ((ECX + 0x4)[0:8] + 0xFF)?(0x0,0x1)
ECX                = ECX + 0x4
of                 = ((((ECX + 0x4)[0:8] + 0xFF) ^ (ECX + 0x4)[0:8]) & ((ECX + 0x4)[0:8] ^ 0x1))[7:8]
nf                 = ((ECX + 0x4)[0:8] + 0xFF)[7:8]
cf                 = (((((ECX + 0x4)[0:8] + 0xFF) ^ (ECX + 0x4)[0:8]) & ((ECX + 0x4)[0:8] ^ 0x1)) ^ ((ECX + 0x4)[0:8] + 0xFF) ^ (ECX + 0x4)[0:8] ^ 0x1)[7:8]
EBX                = EBX + 0x1
________________________________________________________________________________
Instr JZ         loc_key_1
Assignblk:
IRDst = zf?(loc_key_1,loc_key_2)
EIP = zf?(loc_key_1,loc_key_2)
________________________________________________________________________________
af                 = (((ECX + 0x4)[0:8] + 0xFF) ^ (ECX + 0x4)[0:8] ^ 0x1)[4:5]
EIP                = ((ECX + 0x4)[0:8] + 0xFF)?(0xB,0x10)
pf                 = parity((ECX + 0x4)[0:8] + 0xFF)
IRDst              = ((ECX + 0x4)[0:8] + 0xFF)?(0xB,0x10)
zf                 = ((ECX + 0x4)[0:8] + 0xFF)?(0x0,0x1)
ECX                = ECX + 0x4
of                 = ((((ECX + 0x4)[0:8] + 0xFF) ^ (ECX + 0x4)[0:8]) & ((ECX + 0x4)[0:8] ^ 0x1))[7:8]
nf                 = ((ECX + 0x4)[0:8] + 0xFF)[7:8]
cf                 = (((((ECX + 0x4)[0:8] + 0xFF) ^ (ECX + 0x4)[0:8]) & ((ECX + 0x4)[0:8] ^ 0x1)) ^ ((ECX + 0x4)[0:8] + 0xFF) ^ (ECX + 0x4)[0:8] ^ 0x1)[7:8]
EBX                = EBX + 0x1
________________________________________________________________________________
>>>
Retry execution with a concrete ECX. Here, the symbolic / concolic execution reach the shellcode's end:>>> from miasm.expression.expression import ExprInt
>>> sb.symbols[machine.mn.regs.ECX] = ExprInt(-3, 32)
>>> symbolic_pc = sb.run_at(ircfg, 0, step=True)
Instr LEA        ECX, DWORD PTR [ECX + 0x4]
Assignblk:
ECX = ECX + 0x4
________________________________________________________________________________
af                 = (((ECX + 0x4)[0:8] + 0xFF) ^ (ECX + 0x4)[0:8] ^ 0x1)[4:5]
EIP                = ((ECX + 0x4)[0:8] + 0xFF)?(0xB,0x10)
pf                 = parity((ECX + 0x4)[0:8] + 0xFF)
IRDst              = ((ECX + 0x4)[0:8] + 0xFF)?(0xB,0x10)
zf                 = ((ECX + 0x4)[0:8] + 0xFF)?(0x0,0x1)
ECX                = 0x1
of                 = ((((ECX + 0x4)[0:8] + 0xFF) ^ (ECX + 0x4)[0:8]) & ((ECX + 0x4)[0:8] ^ 0x1))[7:8]
nf                 = ((ECX + 0x4)[0:8] + 0xFF)[7:8]
cf                 = (((((ECX + 0x4)[0:8] + 0xFF) ^ (ECX + 0x4)[0:8]) & ((ECX + 0x4)[0:8] ^ 0x1)) ^ ((ECX + 0x4)[0:8] + 0xFF) ^ (ECX + 0x4)[0:8] ^ 0x1)[7:8]
EBX                = EBX + 0x1
________________________________________________________________________________
Instr LEA        EBX, DWORD PTR [EBX + 0x1]
Assignblk:
EBX = EBX + 0x1
________________________________________________________________________________
af                 = (((ECX + 0x4)[0:8] + 0xFF) ^ (ECX + 0x4)[0:8] ^ 0x1)[4:5]
EIP                = ((ECX + 0x4)[0:8] + 0xFF)?(0xB,0x10)
pf                 = parity((ECX + 0x4)[0:8] + 0xFF)
IRDst              = ((ECX + 0x4)[0:8] + 0xFF)?(0xB,0x10)
zf                 = ((ECX + 0x4)[0:8] + 0xFF)?(0x0,0x1)
ECX                = 0x1
of                 = ((((ECX + 0x4)[0:8] + 0xFF) ^ (ECX + 0x4)[0:8]) & ((ECX + 0x4)[0:8] ^ 0x1))[7:8]
nf                 = ((ECX + 0x4)[0:8] + 0xFF)[7:8]
cf                 = (((((ECX + 0x4)[0:8] + 0xFF) ^ (ECX + 0x4)[0:8]) & ((ECX + 0x4)[0:8] ^ 0x1)) ^ ((ECX + 0x4)[0:8] + 0xFF) ^ (ECX + 0x4)[0:8] ^ 0x1)[7:8]
EBX                = EBX + 0x2
________________________________________________________________________________
Instr CMP        CL, 0x1
Assignblk:
zf = (ECX[0:8] + -0x1)?(0x0,0x1)
nf = (ECX[0:8] + -0x1)[7:8]
pf = parity((ECX[0:8] + -0x1) & 0xFF)
of = ((ECX[0:8] ^ (ECX[0:8] + -0x1)) & (ECX[0:8] ^ 0x1))[7:8]
cf = (((ECX[0:8] ^ 0x1) ^ (ECX[0:8] + -0x1)) ^ ((ECX[0:8] ^ (ECX[0:8] + -0x1)) & (ECX[0:8] ^ 0x1)))[7:8]
af = ((ECX[0:8] ^ 0x1) ^ (ECX[0:8] + -0x1))[4:5]
________________________________________________________________________________
af                 = 0x0
EIP                = ((ECX + 0x4)[0:8] + 0xFF)?(0xB,0x10)
pf                 = 0x1
IRDst              = ((ECX + 0x4)[0:8] + 0xFF)?(0xB,0x10)
zf                 = 0x1
ECX                = 0x1
of                 = 0x0
nf                 = 0x0
cf                 = 0x0
EBX                = EBX + 0x2
________________________________________________________________________________
Instr JZ         loc_key_1
Assignblk:
IRDst = zf?(loc_key_1,loc_key_2)
EIP = zf?(loc_key_1,loc_key_2)
________________________________________________________________________________
af                 = 0x0
EIP                = 0x10
pf                 = 0x1
IRDst              = 0x10
zf                 = 0x1
ECX                = 0x1
of                 = 0x0
nf                 = 0x0
cf                 = 0x0
EBX                = EBX + 0x2
________________________________________________________________________________
Instr LEA        EBX, DWORD PTR [EBX + 0x1]
Assignblk:
EBX = EBX + 0x1
________________________________________________________________________________
af                 = 0x0
EIP                = 0x10
pf                 = 0x1
IRDst              = 0x10
zf                 = 0x1
ECX                = 0x1
of                 = 0x0
nf                 = 0x0
cf                 = 0x0
EBX                = EBX + 0x3
________________________________________________________________________________
Instr LEA        EBX, DWORD PTR [EBX + 0x1]
Assignblk:
IRDst = loc_key_3
________________________________________________________________________________
af                 = 0x0
EIP                = 0x10
pf                 = 0x1
IRDst              = 0x13
zf                 = 0x1
ECX                = 0x1
of                 = 0x0
nf                 = 0x0
cf                 = 0x0
EBX                = EBX + 0x3
________________________________________________________________________________
Instr MOV        EAX, EBX
Assignblk:
EAX = EBX
________________________________________________________________________________
af                 = 0x0
EIP                = 0x10
pf                 = 0x1
IRDst              = 0x13
zf                 = 0x1
ECX                = 0x1
of                 = 0x0
nf                 = 0x0
cf                 = 0x0
EBX                = EBX + 0x3
EAX                = EBX + 0x3
________________________________________________________________________________
Instr RET
Assignblk:
IRDst = @32[ESP[0:32]]
ESP = {ESP[0:32] + 0x4 0 32}
EIP = @32[ESP[0:32]]
________________________________________________________________________________
af                 = 0x0
EIP                = @32[ESP]
pf                 = 0x1
IRDst              = @32[ESP]
zf                 = 0x1
ECX                = 0x1
of                 = 0x0
nf                 = 0x0
cf                 = 0x0
EBX                = EBX + 0x3
ESP                = ESP + 0x4
EAX                = EBX + 0x3
________________________________________________________________________________
>>>
How does it work?Miasm embeds its own disassembler, intermediate language andinstruction semantic. It is written in Python.To emulate code, it uses LLVM, GCC, Clang or Python to JIT theintermediate representation. It can emulate shellcodes and all or parts ofbinaries. Python callbacks can be executed to interact with the execution, forinstance to emulate library functions effects.DocumentationTODOAn auto-generated documentation is available:Obtaining MiasmSoftware requirementsMiasm uses:To enable code JIT, one of the following module is mandatory:'optional' Miasm can also use:ConfigurationTo use the jitter, GCC or LLVM is recommended$ cd miasm_directory
$ python setup.py build
$ sudo python setup.py install
If something goes wrong during one of the jitter modules compilation, Miasm willskip the error and disable the corresponding module (see the compilationoutput).Windows & IDAMost of Miasm's IDA plugins use a subset of Miasm functionality.A quick way to have them working is to add:All features excepting JITter related ones will be available. For a more complete installation, please refer to above paragraphs.TestingMiasm comes with a set of regression tests. To run all of them:cd miasm_directory/test

# Run tests using our own test runner
python test_all.py

# Run tests using standard frameworks (slower, require 'parameterized')
python -m unittest test_all.py        # sequential, requires 'unittest'
python -m pytest test_all.py          # sequential, requires 'pytest'
python -m pytest -n auto test_all.py  # parallel, requires 'pytest' and 'pytest-xdist'
Some options can be specified:They already use MiasmToolsBlog posts / papers / conferencesBooks"
https://github.com/etianen/django-reversion,django-reversion is an extension to the Django web framework that provides version control for model instances.,"================django-reversion|PyPI latest| |PyPI Version| |PyPI License| |Github actions| |Docs|django-reversion is an extension to the Django web framework that providesversion control for model instances.RequirementsFeaturesDocumentationCheck out the latest  documentation at _Issue tracking and source code can be found at the_.You can keep up to date with the latest announcements by joining the_.UpgradingPlease check the _ before upgradingyour installation of django-reversion.ContributingBug reports, bug fixes, and new features are always welcome. Please raise issues on the_, and submitpull requests for any new code... code:: bash$ pip install django psycopg2 mysqlclient -e .
.. code:: bash$ tests/manage.py test tests
ContributorsThe django-reversion project was developed by _ and contributedto by _... |Docs| image:: https://readthedocs.org/projects/django-reversion/badge/?version=latest:target: http://django-reversion.readthedocs.org/en/latest/?badge=latest.. |PyPI Version| image:: https://img.shields.io/pypi/pyversions/django-reversion.svg?maxAge=60:target: https://pypi.python.org/pypi/django-reversion.. |PyPI License| image:: https://img.shields.io/pypi/l/django-reversion.svg?maxAge=120:target: https://github.com/rhenter/django-reversion/blob/master/LICENSE.. |PyPI latest| image:: https://img.shields.io/pypi/v/django-reversion.svg?maxAge=120:target: https://pypi.python.org/pypi/django-reversion.. |Github actions| image:: https://github.com/etianen/django-reversion/workflows/Python%20package/badge.svg:target: https://github.com/etianen/django-reversion"
https://github.com/xchaoinfo/fuck-login,模拟登录一些知名的网站，为了方便爬取需要登录的网站,fuck-login本项目不在继续维护了(This project is not maintained)xchaoinfo 2018.06.08模拟登录一些常见的网站主要基于以下的 Python 的第三 library DoneTodolisttips of pull request欢迎大家一起来 pull request 技术交流捐赠如果本项目对你有用，欢迎对本项目进行捐赠，捐赠时候请留下你的github ID，当然您也可以匿名捐赠。something to add
https://github.com/hylang/hy,A dialect of Lisp that's embedded in Python,"HyLisp and Python should love each other. Let's make it happen.Hy is a Lisp dialect that's embedded in Python. Since Hy transforms its Lispcode into Python abstract syntax tree (AST) objects, you have the wholebeautiful world of Python at your fingertips, in Lisp form.To install the latest release of Hy, just use the command . Then you can start an interactive read-eval-print loop (REPL) withthe command , or run a Hy program with .Hy is tested on all released and currently maintained versions of CPython (onLinux and Windows), and on recent versions of PyPy and Pyodide.ProjectHy's current maintainer is . He takes responsibility for answering user questions, which should primarily be asked on Stack Overflow or GitHub Discussions, but feel free to  if he's missed a question or you've found a serious security issue.(fan art from the one and only )"
https://github.com/gpuweb/gpuweb,Where the GPU for the Web work happens!,"W3C GPU for the Web Community GroupThis is the repository for the W3CWebGPU API and WebGPU Shading Language (WGSL) specifications.This specification is formally standardized by the W3C.We use the  and issue tracker asthe main sources of information related to the work.This repository will hold the actual specification, examples, etc.Work-in-progress specification: Work-in-progress WGSL specification: CharterThe  ismaintained in a .Membership is opento anyone. We especially encourage hardware vendors, browser engine developers,3d software engineers and any Web Developers with expertise in graphics toparticipate. You'll need a W3C account to join, and if you're affiliated with aW3C member, your W3C representative will confirm your participation. If you'renot a W3C member, you're still welcome. All participants are required to agreeto the .ContributionsYou are not required to be a member of the Community Group or Working Group in order to, errors, fixes or make suggestions.Anyone with a GitHub account can do so.In order to assure that WebGPU specifications can be implemented on a Royalty-Free(RF) basis, all significant contributions need to be made with RF commitments.Members of the Working Group, and members of the Community Group who have signed thehave already committed to the terms of the.Non-members will be requested to provide an RF commitment under terms similar tothe W3C Patent Policy.All contributions must comply with the group's.See Code of ConductThis group operates under .CommunicationOur primary public chat channel is via Matrix ( at .For asynchronous concerns, we use GitHub for both our  and our .Both the Community Group and the Working Group have W3C email lists as well, though these are largely administrative."
https://github.com/corna/me_cleaner,Tool for partial deblobbing of Intel ME/TXE firmware images,"mecleaner me is a Python script able to modify an Intel ME firmware image withthe final purpose of reducing its ability to interact with the system.Intel MEIntel ME is a co-processor integrated in all post-2006 Intel boards, which isthe base hardware for many Intel features like Intel AMT, Intel Boot Guard,Intel PAVP and many others. To provide such features, it requires full access tothe system, including memory (through DMA) and network access (transparent tothe user).Unlike many other firmware components, the Intel ME firmware can't be neitherdisabled nor reimplemented, as it is tightly integrated in the boot process andit is signed.This poses an issue both to the free firmware implementations like , which are forced to rely on a proprietary, obscureand always-on blob, and to the privacy-aware users, who are reasonably worriedabout such firmware, running on the lowest privilege ring on x86.What can be doneBefore Nehalem (ME version 6, 2008/2009) the ME firmware could be removedcompletely from the flash chip by setting a couple of bits inside the flashdescriptor, effectively disabling it.Starting from Nehalem the Intel ME firmware can't be removed anymore: without avalid firmware the PC shuts off forcefully after 30 minutes, probably as anattempt to enforce the Intel Anti-Theft policies.However, while Intel ME can't be turned off completely, it is still possible tomodify its firmware up to a point where Intel ME is active only during the bootprocess, effectively disabling it during the normal operation, which is whatme tries to accomplish.Platform supportme currently works on ; while this doesn'tmean it works on all the boards (due to the different firmware implementations),it has been proven quite reliable on a great number of them.Usageme should handle all the steps necessary to the modification of anIntel ME firmware with the command:  $ python me_cleaner.py -S -O modified_image.bin original_dump.bin
However, obtaining the original firmware and flashing back the modified one isusually not trivial, as the Intel ME firmware region is often non-writable fromthe OS (and it's not a safe option anyways), requiring the use of an externalSPI programmer.ResultsFor generation 1 (before Nehalem, ME version <= 5) this tool removes the wholeME firmware and disables it completely.For generation 2 (Nehalem-Broadwell, ME version between 6 and 10) this toolremoves almost everything, leaving only the two fundamental modules needed forthe correct boot,  and . The firmware size is reduced from 1.5 MB(non-AMT firmware) or 5 MB (AMT firmware) to ~90 kB.For generation 3 (from Skylake onwards, ME version >= 11) the ME subsystem andthe firmware structure have changed, requiring substantial changesin me. The fundamental modules required for the correct boot are nowfour (,  ,  and ) and the minimum firmware size is~300 kB (from the 2 MB of the non-AMT firmware and the 7 MB of the AMT one).On some boards the OEM firmware fails to boot without a valid Intel ME firmware;in the other cases the system should work with minor inconveniences (like longerboot times or warning messages) or without issues at all.Obviously, the features provided by Intel ME won't be functional anymore afterthe modifications.DocumentationThe detailed documentation about the working of me can be found onthe page .Various guides and tutorials are available on the Internet, however a goodstarting point is the ."
https://github.com/angr/angr,A powerful and user-friendly binary analysis platform!,"angrangr is a platform-agnostic binary analysis framework.It is brought to you by , , their associated CTF team, , the open source community, and [<marko.inline.RawText object at 0x000001592FD7FC88>].Project LinksHomepage: https://angr.ioProject repository: https://github.com/angr/angrDocumentation: https://docs.angr.ioAPI Documentation: https://api.angr.io/en/latest/What is angr?angr is a suite of Python 3 libraries that let you load a binary and do a lot of cool things to it:The most common angr operation is loading a binary:  If you do this in an enhanced REPL like IPython, you can use tab-autocomplete to browse the  and their docstrings.The short version of ""how to install angr"" is .Exampleangr does a lot of binary analysis stuff.To get you started, here's a simple example of using symbolic execution to get a flag in a CTF challenge.import angr

project = angr.Project(""angr-doc/examples/defcamp_r100/r100"", auto_load_libs=False)

@project.hook(0x400844)
def print_flag(state):
    print(""FLAG SHOULD BE:"", state.posix.dumps(0))
    project.terminate_execution()

project.execute()
Quick Start"
https://github.com/hardikvasa/google-images-download,Python Script to download hundreds of images from 'Google Images'. It is a ready-to-run code!,"Google Images Download######################Python Script for 'searching' and 'downloading' hundreds of Google images to the local hard disk!DocumentationDisclaimerThis program lets you download tons of images from Google.Please do not download or use any image that violates its copyright terms.Google Images is a search engine that merely indexes images and allows you to find them.It does NOT produce its own images and, as such, it doesn't own copyright on any of them.The original creators of the images own the copyrights.Images published in the United States are automatically copyrighted by their owners,even if they do not explicitly carry a copyright warning.You may not reproduce copyright images without their owner's permission,except in ""fair use"" cases,or you could risk running into lawyer's warnings, cease-and-desist letters, and copyright suits.Please be very careful before its usage! Use this script/code only for educational purposes."
https://github.com/pallets/werkzeug,The comprehensive WSGI web application library.,"Werkzeugwerkzeug German noun: ""tool"". Etymology: werk (""work""), zeug (""stuff"")Werkzeug is a comprehensive _ web application library. It began asa simple collection of various utilities for WSGI applications and hasbecome one of the most advanced WSGI utility libraries.It includes:Werkzeug doesn't enforce any dependencies. It is up to the developer tochoose a template engine, database adapter, and even how to handlerequests. It can be used to build all sorts of end user applicationssuch as blogs, wikis, or bulletin boards._ wraps Werkzeug, using it to handle the details of WSGI whileproviding more structure and patterns for defining powerfulapplications... _WSGI: https://wsgi.readthedocs.io/en/latest/.. _Flask: https://www.palletsprojects.com/p/flask/InstallingInstall and update using _:.. code-block:: textpip install -U Werkzeug
.. _pip: https://pip.pypa.io/en/stable/getting-started/A Simple Example.. code-block:: pythonfrom werkzeug.wrappers import Request, Response

@Request.application
def application(request):
    return Response('Hello, World!')

if __name__ == '__main__':
    from werkzeug.serving import run_simple
    run_simple('localhost', 4000, application)
DonateThe Pallets organization develops and supports Werkzeug and otherpopular packages. In order to grow the community of contributors andusers, and allow the maintainers to devote more time to the projects,_... _please donate today: https://palletsprojects.com/donateLinks"
https://github.com/evilsocket/opensnitch,OpenSnitch is a GNU/Linux interactive application firewall inspired by Little Snitch.,"Key featuresDownloadDownload deb/rpm packages for your system from https://github.com/evilsocket/opensnitch/releasesInstallationdebrpmThen run:  or launch the GUI from the Applications menu.Please, refer to  for detailed information.OpenSnitch in actionExamples of OpenSnitch intercepting unexpected connections:https://github.com/evilsocket/opensnitch/discussions/categories/show-and-tellHave you seen a connection you didn't expect? In the pressDonationsIf you find OpenSnitch useful and want to donate to the dedicated developers, you can do it from the Sponsor this project section on the right side of this repository.You can see here who are the current maintainers of OpenSnitch:https://github.com/evilsocket/opensnitch/commits/masterContributorsTranslating"
https://github.com/corpnewt/gibMacOS,Py2/py3 script that can download macOS components direct from Apple,"Py2/py3 script that can download macOS components direct from AppleCan also now build Internet Recovery USB installers from Windows using  and .NOTE: As of macOS 11 (Big Sur), Apple has changed the way they distribute macOS, and internet recovery USBs can no longer be built via MakeInstall on Windows.  macOS versions through Catalina will still work though.NOTE 2: As of macOS 11 (Big Sur), Apple distributes the OS via an InstallAssistant.pkg file.   is not needed to create the install application when in macOS in this case - and you can simply run , which will place the install app in your /Applications folder on macOS.Thanks to:"
https://github.com/pyro-ppl/pyro,Deep universal probabilistic programming with Python and PyTorch," | | |Pyro is a flexible, scalable deep probabilistic programming library built on PyTorch.  Notably, it was designed with these principles in mind:Pyro was originally developed at Uber AI and is now actively maintained by community contributors, including a dedicated team at the .In 2019, Pyro  a project of the Linux Foundation, a neutral space for collaboration on open source software, open standards, open data, and open hardware.For more information about the high level motivation for Pyro, check out our .For additional blog posts, check out work on  and in Pyro.InstallingInstalling a stable Pyro releaseInstall using pip:pip install pyro-ppl
Install from source:git clone git@github.com:pyro-ppl/pyro.git
cd pyro
git checkout master  # master is pinned to the latest release
pip install .
Install with extra packages:To install the dependencies required to run the probabilistic models included in the / directories, please use the following command:pip install pyro-ppl[extras] 
Make sure that the models come from the same release version of the  as you have installed.Installing Pyro dev branchFor recent features you can install Pyro from source.Install Pyro using pip:pip install git+https://github.com/pyro-ppl/pyro.git
or, with the  dependency to run the probabilistic models included in the / directories:pip install git+https://github.com/pyro-ppl/pyro.git#egg=project[extras]
Install Pyro from source:git clone https://github.com/pyro-ppl/pyro
cd pyro
pip install .  # pip install .[extras] for running models in examples/tutorials
Running Pyro from a Docker ContainerRefer to the instructions .CitationIf you use Pyro, please consider citing:@article{bingham2019pyro,
  author    = {Eli Bingham and
               Jonathan P. Chen and
               Martin Jankowiak and
               Fritz Obermeyer and
               Neeraj Pradhan and
               Theofanis Karaletsos and
               Rohit Singh and
               Paul A. Szerlip and
               Paul Horsfall and
               Noah D. Goodman},
  title     = {Pyro: Deep Universal Probabilistic Programming},
  journal   = {J. Mach. Learn. Res.},
  volume    = {20},
  pages     = {28:1--28:6},
  year      = {2019},
  url       = {http://jmlr.org/papers/v20/18-403.html}
}
"
https://github.com/apachecn/ailearning,AiLearning：数据分析+机器学习实战+线性代数+PyTorch+NLTK+TF2,"路线图1.机器学习 - 基础| Version | Supported          || ------- | ------------------ || 3.6.x   | :x:                || 2.7.x   | :white_check_mark: |注意事项: 基本介绍学习文档| 模块 | 章节 | 类型 | 负责人(GitHub) | QQ || --- | --- | --- | --- | --- || 机器学习实战 |  | 介绍 |  | 1306014226 || 机器学习实战 |  | 分类 |  | 279393323 || 机器学习实战 |  | 分类 |  | 844300439 || 机器学习实战 |  | 分类 |  | 1003324213244970749 || 机器学习实战 |  | 分类 |  | 529925688 || 机器学习实战 |  | 分类 |  | 934969547 || 网上组合内容 |  | 分类 |  | 529815144 || 机器学习实战 |  | 回归 |  | 529925688 || 机器学习实战 |  | 回归 |  | 529925688 || 机器学习实战 |  | 聚类 |  | 827106588 || 机器学习实战 |  | 频繁项集 |  | 1049498972 || 机器学习实战 |  | 频繁项集 |  | 842725815 || 机器学习实战 |  | 工具 |  | 835670618 || 机器学习实战 |  | 工具 |  | 714974242 || 机器学习实战 |  | 工具 |  | 1003324213 || Ml项目实战 |  | 项目 |   |  || 第一期的总结 |  | 总结 | 总结 | 529815144 |网站视频当然我知道，第一句就会被吐槽，因为科班出身的人，不屑的吐了一口唾沫，说傻X，还评论 Andrew Ng 的视频。。我还知道还有一部分人，看 Andrew Ng 的视频就是看不懂，那神秘的数学推导，那迷之微笑的英文版的教学，我何尝又不是这样走过来的？？ 我的心可能比你们都痛，因为我在网上收藏过上10部《机器学习》相关视频，外加国内本土风格的教程: 7月+小象 等等，我都很难去听懂，直到有一天，被一个百度的高级算法分析师推荐说: 《机器学习实战》还不错，通俗易懂，你去试试？？我试了试，还好我的Python基础和调试能力还不错，基本上代码都调试过一遍，很多高大上的 ""理论+推导""，在我眼中变成了几个 ""加减乘除+循环""，我想这不就是像我这样的程序员想要的入门教程么？很多程序员说机器学习 TM 太难学了，是的，真 TM 难学，我想最难的是: 没有一本像《机器学习实战》那样的作者愿意以程序员 Coding 角度去给大家讲解！！最近几天，GitHub 涨了 300颗 star，加群的200人， 现在还在不断的增加++，我想大家可能都是感同身受吧！很多想入门新手就是被忽悠着收藏收藏再收藏，但是最后还是什么都没有学到，也就是""资源收藏家""，也许新手要的就是 。没错，我可以给你们的一份，因为我们还通过视频记录下来我们的学习过程。水平当然也有限，不过对于新手入门，绝对没问题，如果你还不会，那算我输！！| 概率 | 统计 | 线性代数 || - | - | - ||   | | |||| - | - || AcFun | B站 ||  |  || 优酷 | 网易云课堂 ||  |  || 机器学习 | 深度学习 || - | - ||  |  |2.深度学习| Version | Supported          || ------- | ------------------ || 3.6.x   | :white_check_mark: || 2.7.x   | :x:                |入门基础Pytorch - 教程-- 待更新TensorFlow 2.0 - 教程-- 待更新切分（分词）词性标注命名实体识别句法分析WordNet可以被看作是一个同义词词典词干提取（stemming）与词形还原（lemmatization）TensorFlow 2.0学习网址3.自然语言处理| Version | Supported          || ------- | ------------------ || 3.6.x   | :white_check_mark: || 2.7.x   | :x:                |学习过程中-内心复杂的变化！！！自从学习NLP以后，才发现国内与国外的典型区别:
1. 对资源的态度是完全相反的:
  1) 国内: 就好像为了名气，举办工作装逼的会议，就是没有干货，全部都是象征性的PPT介绍，不是针对在做的各位
  2）国外: 就好像是为了推动nlp进步一样，分享者各种干货资料和具体的实现。（特别是: python自然语言处理）
2. 论文的实现: 
  1) 各种高大上的论文实现，却还是没看到一个像样的GitHub项目！（可能我的搜索能力差了点，一直没找到）
  2）国外就不举例了，我看不懂！
3. 开源的框架
  1）国外的开源框架:  tensorflow/pytorch 文档+教程+视频（官方提供）
  2) 国内的开源框架: 额额，还真举例不出来！但是牛逼吹得不比国外差！（MXNet虽然有众多国人参与开发，但不能算是国内开源框架。基于MXNet的动手学深度学习(http://zh.d2l.ai & https://discuss.gluon.ai/t/topic/753)中文教程,已经由沐神(李沐)以及阿斯顿·张讲授录制，公开发布(文档+第一季教程+视频）。)
每一次深入都要去翻墙，每一次深入都要Google，每一次看着国内的说: 哈工大、讯飞、中科大、百度、阿里多牛逼，但是资料还是得国外去找！
有时候真的挺恨的！真的有点瞧不起自己国内的技术环境！

当然谢谢国内很多博客大佬，特别是一些入门的Demo和基本概念。【深入的水平有限，没看懂】
1.使用场景 （百度公开课）应用领域中文分词:1.文本分类（Text Classification）文本分类是指标记句子或文档，例如电子邮件垃圾邮件分类和情感分析。下面是一些很好的初学者文本分类数据集。有关更多信息，请参阅帖子:。比赛地址: https://www.kaggle.com/c/word2vec-nlp-tutorial通过AUC 来评估模型的效果2.语言模型（Language Modeling）语言建模涉及开发一种统计模型，用于预测句子中的下一个单词或一个单词中的下一个单词。它是语音识别和机器翻译等任务中的前置任务。它是语音识别和机器翻译等任务中的前置任务。下面是一些很好的初学者语言建模数据集。3.图像字幕（Image Captioning）mage字幕是为给定图像生成文本描述的任务。下面是一些很好的初学者图像字幕数据集。4.机器翻译（Machine Translation）机器翻译是将文本从一种语言翻译成另一种语言的任务。下面是一些很好的初学者机器翻译数据集。5.问答系统（Question Answering）问答是一项任务，其中提供了一个句子或文本样本，从中提出问题并且必须回答问题。下面是一些很好的初学者问题回答数据集。6.语音识别（Speech Recognition）语音识别是将口语的音频转换为人类可读文本的任务。下面是一些很好的初学者语音识别数据集。7.自动文摘（Document Summarization）文档摘要是创建较大文档的简短有意义描述的任务。下面是一些很好的初学者文档摘要数据集。。Graph图计算【慢慢更新】知识图谱进一步阅读如果您希望更深入，本节提供了其他数据集列表。参考致谢最近无意收到群友推送的链接，发现得到大佬高度的认可，并在热心的推广。在此感谢:赞助我们"
https://github.com/jackfrued/Python-100-Days,Python - 100天从新手到大师,Python - 100天从新手到大师Python应用领域和职业发展分析简单的说，Python是一个“优雅”、“明确”、“简单”的编程语言。Python在以下领域都有用武之地。作为一名Python开发者，根据个人的喜好和职业规划，可以选择的就业领域也非常多。给初学者的几个建议：Day01~15 - Day01 - Day02 - Day03 - Day04 - Day05 - Day06 - Day07 - Day08 - Day09 - Day10 - Day11 - Day12 - Day13 - Day14 - Day15 - Day16~Day20 - Day21~30 - Day31~35 - Day36~40 - Day41~55 - Day41 - Day42 - Day43 - Day44 - Day45 - Day46 - Day47 - Day48 - Day49 - Day50 - Day51 - Day52 - Day53 - Day54 - Day55 - Day56~60 - Day61~65 - Day61 - Day62 - 数据抓取和解析Day63 - Python中的并发编程Day64 - Day65 - Day66~80 - Day66 - Day67 - Day68 - Day69 - Day70 - Day71 - Day72 - Day73 - Day74 - Day75 - Day76 - Day77 - Day78 - Day79 - Day80 - Day81~90 - Day81 - Day82 - Day83 - Day84 - Day85 - Day86 - Day87 - Day88 - Day89 - Day90 - Day91~100 - 第91天：项目选题和理解业务第92天：第93天：第94天：第95天：[使用Django开发商业项目](./Day91-100/95.使用Django开发商业项	目.md)项目开发中的公共问题REST API设计项目中的重点难点剖析第96天：单元测试Django项目部署性能测试自动化测试第97天：第98天：第99天：第100天：
https://github.com/XPixelGroup/BasicSR,"Open Source Image and Video Restoration Toolbox for Super-resolution, Denoise, Deblurring, etc. Currently, it includes EDSR, RCAN, SRResNet, SRGAN, ESRGAN, EDVR, BasicVSR, SwinIR, ECBSR, etc. Also support StyleGAN2, DFDNet.","English | 简体中文⚡ | 🔧 | 💻 | 🐢 | 🏰📕 | 📊  | 📝 |  | ⏳ | ❓🚀 We add , which provides guidance and templates of using BasicSR as a python package. 🚀 📢 技术交流QQ群：320960100 &emsp; 入群答案：互帮互助共同进步 🧭  (QQ、微信) &emsp;&emsp;  BasicSR (Basic Super Restoration) is an open-source image and video restoration toolbox based on PyTorch, such as super-resolution, denoise, deblurring, JPEG artifacts removal, etc.BasicSR (Basic Super Restoration) 是一个基于 PyTorch 的开源 图像视频复原工具箱, 比如 超分辨率, 去噪, 去模糊, 去 JPEG 压缩噪声等.🚩 New Features/UpdatesIf BasicSR helps your research or work, please help to ⭐ this repo or recommend it to your friends. Thanks😊 Other recommended projects:▶️ : A practical algorithm for general image restoration▶️ : A practical algorithm for real-world face restoration ▶️ : A collection that provides useful face-relation functions.▶️ : A PyQt5-based image viewer that is handy for view and comparison. ▶️ : Open source of paper figures (, , , )(, )⚡ HOWTOsWe provide simple pipelines to train/test/inference models for a quick start.These pipelines/commands cannot cover all the cases and more details are in the following sections.| GAN                  |                                                |                                                        |          |                                                |                                                        || :------------------- | :--------------------------------------------: | :----------------------------------------------------: | :------- | :--------------------------------------------: | :----------------------------------------------------: || StyleGAN2            |  |  |          |                                                |                                                        || Face Restoration |                                                |                                                        |          |                                                |                                                        || DFDNet               |                       -                        |     |          |                                                |                                                        || Super Resolution |                                                |                                                        |          |                                                |                                                        || ESRGAN               |                     TODO                     |                         TODO                         | SRGAN    |                     TODO                     |                         TODO                         || EDSR                 |                     TODO                     |                         TODO                         | SRResNet |                     TODO                     |                         TODO                         || RCAN                 |                     TODO                     |                         TODO                         | SwinIR   |  |  || EDVR                 |                     TODO                     |                         TODO                         | DUF      |                       -                        |                         TODO                         || BasicVSR             |                     TODO                     |                         TODO                         | TOF      |                       -                        |                         TODO                         || Deblurring       |                                                |                                                        |          |                                                |                                                        || DeblurGANv2          |                       -                        |                         TODO                         |          |                                                |                                                        || Denoise          |                                                |                                                        |          |                                                |                                                        || RIDNet               |                       -                        |                         TODO                         | CBDNet   |                       -                        |                         TODO                         |✨ Projects that use BasicSRIf you use  in your open-source projects, welcome to contact me (by  or opening an issue/pull request). I will add your projects to the above list 😊📜 License and AcknowledgementThis project is released under the .More details about license and acknowledgement are in .🌏 CitationsIf BasicSR helps your research or work, please cite BasicSR.The following is a BibTeX reference. The BibTeX entry requires the  LaTeX package.@misc{basicsr,
  author =       {Xintao Wang and Liangbin Xie and Ke Yu and Kelvin C.K. Chan and Chen Change Loy and Chao Dong},
  title =        {{BasicSR}: Open Source Image and Video Restoration Toolbox},
  howpublished = {\url{https://github.com/XPixelGroup/BasicSR}},
  year =         {2022}
}
📧 ContactIf you have any questions, please email , . (start from 2022-11-06)"
https://github.com/OctoPrint/OctoPrint,OctoPrint is the snappy web interface for your 3D printer!,"OctoPrint provides a snappy web interface for controlling consumer 3D printers. It is Free Softwareand released under the .Its website can be found at .The community forum is available at . It also serves as a central knowledge base.An invite to the Discord server can be found at .The FAQ can be accessed by following .The documentation is located at .The official plugin repository can be reached at .OctoPrint's development wouldn't be possible without the You are currently looking at the source code repository of OctoPrint. If you already installed it(e.g. by using the Raspberry Pi targeted distribution ) and onlywant to find out how to use it,  might be of more interest for you. You might also want to subscribe to join where there are other active users who might beable to help you with any questions you might have.ContributingContributions of all kinds are welcome, not only in the form of code but also with regards to the, debugging helpin the , support of other users on orand also .If you think something is bad about OctoPrint or its documentation the way it is, please helpin any way to make it better instead of just complaining about it -- this is an Open Source Projectafter all :)For information about how to go about submitting bug reports or pull requests, please see the project's.InstallationInstallation instructions for installing from source for different operatingsystems can be found .If you want to run OctoPrint on a Raspberry Pi, you really should take a look at which is a custom SD card image that includes OctoPrint plus dependencies.The generic steps that should basically be done regardless of operating systemand runtime environment are the following (as regular, please keep your hands off of the  command here!) - this assumesyou already have Python 3.7+, pip and virtualenv and their dependencies set up on your system:You may then start the OctoPrint server via , see for details.After installation, please make sure you follow the first-run wizard and set upaccess control as necessary.DependenciesOctoPrint depends on a few python modules to do its job. Those are automatically installed when installingOctoPrint via .OctoPrint currently supports Python 3.7, 3.8, 3.9, 3.10 and 3.11.UsageRunning the pip install viapip install OctoPrint
installs the  script in your Python installation's scripts folder(which, depending on whether you installed OctoPrint globally or into a virtual env, will be in your  or not). Thefollowing usage examples assume that the  script is on your .You can start the server viaoctoprint serve
By default it binds to all interfaces on port 5000 (so pointing your browser to will do the trick). If you want to change that, use the additional command line parameters  and ,which accept the host ip to bind to and the numeric port number respectively. If for example you want the serverto only listen on the local interface on port 8080, the command line would beoctoprint serve --host=127.0.0.1 --port=8080
Alternatively, the host and port on which to bind can be defined via the config file.If you want to run OctoPrint as a daemon (only supported on Linux), useoctoprint daemon {start|stop|restart} [--pid PIDFILE]
If you do not supply a custom pidfile location via , it will be created at .You can also specify the config file or the base directory (for basing off the ,  and  folders),e.g.:octoprint serve --config /path/to/another/config.yaml --basedir /path/to/my/basedir
To start OctoPrint in safe mode - which disables all third party plugins that do not come bundled with OctoPrint - usethe  flag:octoprint serve --safe
See  for more information on the available command line parameters.OctoPrint also ships with a  script in its source directory. You can invoke it to start the server. Ittakes the same command line arguments as the  script.ConfigurationIf not specified via the command line, the config file  for OctoPrint is expected in the settings folder,which is located at  on Linux, at  on Windows andat  on MacOS.A comprehensive overview of all available configuration settings can be found.Please note that the most commonly used configuration settings can also easilybe edited from OctoPrint's settings dialog.Special ThanksCross-browser testing services are kindly provided by .Profiling is done with the help of .Error tracking is powered and sponsored by ."
https://github.com/huggingface/pytorch-image-models,"PyTorch image models, scripts, pretrained weights -- ResNet, ResNeXT, EfficientNet, NFNet, Vision Transformer (ViT), MobileNet-V3/V2, RegNet, DPN, CSPNet, Swin Transformer, MaxViT, CoAtNet, ConvNeXt, and more","PyTorch Image ModelsWhat's New❗Updates after Oct 10, 2022 are available in version >= 0.9❗Oct 20, 2023Sep 1, 2023Aug 28, 2023Aug 25, 2023Aug 11, 2023Aug 3, 2023July 27, 2023May 11, 2023May 10, 2023April 27, 2023April 21, 2023April 12, 2023April 5, 2023March 31, 2023| model                                                                                                                |top1  |top5  |img_size|param_count|gmacs |macts ||----------------------------------------------------------------------------------------------------------------------|------|------|--------|-----------|------|------||  |88.612|98.704|256     |846.47     |198.09|124.45|| convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_384                                                               |88.312|98.578|384     |200.13     |101.11|126.74|| convnext_large_mlp.clip_laion2b_soup_ft_in12k_in1k_320                                                               |87.968|98.47 |320     |200.13     |70.21 |88.02 || convnext_base.clip_laion2b_augreg_ft_in12k_in1k_384                                                                  |87.138|98.212|384     |88.59      |45.21 |84.49 || convnext_base.clip_laion2b_augreg_ft_in12k_in1k                                                                      |86.344|97.97 |256     |88.59      |20.09 |37.55 || model                                              |top1  |top5  |param_count|img_size||----------------------------------------------------|------|------|-----------|--------||  |90.054|99.042|305.08     |448     || eva02_large_patch14_448.mim_in22k_ft_in22k_in1k    |89.946|99.01 |305.08     |448     || eva_giant_patch14_560.m30m_ft_in22k_in1k           |89.792|98.992|1014.45    |560     || eva02_large_patch14_448.mim_in22k_ft_in1k          |89.626|98.954|305.08     |448     || eva02_large_patch14_448.mim_m38m_ft_in1k           |89.57 |98.918|305.08     |448     || eva_giant_patch14_336.m30m_ft_in22k_in1k           |89.56 |98.956|1013.01    |336     || eva_giant_patch14_336.clip_ft_in1k                 |89.466|98.82 |1013.01    |336     || eva_large_patch14_336.in22k_ft_in22k_in1k          |89.214|98.854|304.53     |336     || eva_giant_patch14_224.clip_ft_in1k                 |88.882|98.678|1012.56    |224     || eva02_base_patch14_448.mim_in22k_ft_in22k_in1k     |88.692|98.722|87.12      |448     || eva_large_patch14_336.in22k_ft_in1k                |88.652|98.722|304.53     |336     || eva_large_patch14_196.in22k_ft_in22k_in1k          |88.592|98.656|304.14     |196     || eva02_base_patch14_448.mim_in22k_ft_in1k           |88.23 |98.564|87.12      |448     || eva_large_patch14_196.in22k_ft_in1k                |87.934|98.504|304.14     |196     || eva02_small_patch14_336.mim_in22k_ft_in1k          |85.74 |97.614|22.13      |336     || eva02_tiny_patch14_336.mim_in22k_ft_in1k           |80.658|95.524|5.76       |336     |March 22, 2023Feb 26, 2023Feb 20, 2023Feb 16, 2023Feb 7, 2023Jan 20, 2023|model                                                                                                                   |top1 |top5 |samples / sec  |Params (M)     |GMAC  |Act (M)||------------------------------------------------------------------------------------------------------------------------|----:|----:|--------------:|--------------:|-----:|------:||                    |88.53|98.64|          21.76|         475.77|534.14|1413.22||                    |88.32|98.54|          42.53|         475.32|292.78| 668.76||                        |88.20|98.53|          50.87|         119.88|138.02| 703.99||                      |88.04|98.40|          36.42|         212.33|244.75| 942.15||                      |87.98|98.56|          71.75|         212.03|132.55| 445.84||                        |87.92|98.54|         104.71|         119.65| 73.80| 332.90||        |87.81|98.37|         106.55|         116.14| 70.97| 318.95||  |87.47|98.37|         149.49|         116.09| 72.98| 213.74||            |87.39|98.31|         160.80|          73.88| 47.69| 209.43||        |86.89|98.02|         375.86|         116.14| 23.15|  92.64||  |86.64|98.02|         501.03|         116.09| 24.20|  62.77||                                          |86.60|97.92|          50.75|         119.88|138.02| 703.99||                      |86.57|97.89|         631.88|          73.87| 15.09|  49.22||                                        |86.52|97.88|          36.04|         212.33|244.75| 942.15||            |86.49|97.90|         620.58|          73.88| 15.18|  54.78||                                          |86.29|97.80|         101.09|         119.65| 73.80| 332.90||                                        |86.23|97.69|          70.56|         212.03|132.55| 445.84||                                        |86.10|97.76|          88.63|          69.13| 67.26| 383.77||                                          |85.67|97.58|         144.25|          31.05| 33.49| 257.59||                                        |85.54|97.46|         188.35|          69.02| 35.87| 183.65||                                          |85.11|97.38|         293.46|          30.98| 17.53| 123.42||                                        |84.93|96.97|         247.71|         211.79| 43.68| 127.35||          |84.90|96.96|        1025.45|          41.72|  8.11|  40.13||                                          |84.85|96.99|         358.25|         119.47| 24.04|  95.01||                      |84.63|97.06|         575.53|          66.01| 14.67|  58.38||                              |84.61|96.74|         625.81|          73.88| 15.18|  54.78||                        |84.49|96.76|         693.82|          64.90| 10.75|  49.30||                                        |84.43|96.83|         647.96|          68.93| 11.66|  53.17||                          |84.23|96.78|         807.21|          29.15|  6.77|  46.92||                                        |83.62|96.38|         989.59|          41.72|  8.04|  34.60||                                    |83.50|96.50|        1100.53|          29.06|  5.11|  33.11||                                          |83.41|96.59|        1004.94|          30.92|  5.60|  35.78||                              |83.36|96.45|        1093.03|          41.69|  7.85|  35.47||                              |83.11|96.33|        1276.88|          23.70|  6.26|  23.05||                        |83.03|96.34|        1341.24|          16.78|  4.37|  26.05||                          |82.96|96.26|        1283.24|          15.50|  4.47|  31.92||                                    |82.93|96.23|        1218.17|          15.45|  4.46|  30.28||                                  |82.39|96.19|        1600.14|          27.44|  4.67|  22.04||                                        |82.39|95.84|        1831.21|          27.44|  4.43|  18.73||                        |82.05|95.87|        2109.09|          15.15|  2.62|  20.34||                                |81.95|95.92|        2525.52|          14.70|  2.47|  12.80||                                  |81.70|95.64|        2344.52|          15.14|  2.41|  15.41||                          |80.53|95.21|        1594.71|           7.52|  1.85|  24.86|Jan 11, 2023Jan 6, 2023Jan 5, 2023Dec 23, 2022 🎄☃Dec 8, 2022| model                                     | top1 | param_count |  gmac | macts | hub                                     ||:------------------------------------------|-----:|------------:|------:|------:|:----------------------------------------|| eva_large_patch14_336.in22k_ft_in22k_in1k | 89.2 |       304.5 | 191.1 | 270.2 |  || eva_large_patch14_336.in22k_ft_in1k       | 88.7 |       304.5 | 191.1 | 270.2 |  || eva_large_patch14_196.in22k_ft_in22k_in1k | 88.6 |       304.1 |  61.6 |  63.5 |  || eva_large_patch14_196.in22k_ft_in1k       | 87.9 |       304.1 |  61.6 |  63.5 |  |Dec 6, 2022| model                                    |   top1 |   param_count |   gmac |   macts | hub                                     ||:-----------------------------------------|-------:|--------------:|-------:|--------:|:----------------------------------------|| eva_giant_patch14_560.m30m_ft_in22k_in1k |   89.8 |        1014.4 | 1906.8 |  2577.2 |  || eva_giant_patch14_336.m30m_ft_in22k_in1k |   89.6 |        1013   |  620.6 |   550.7 |  || eva_giant_patch14_336.clip_ft_in1k       |   89.4 |        1013   |  620.6 |   550.7 |  || eva_giant_patch14_224.clip_ft_in1k       |   89.1 |        1012.6 |  267.2 |   192.6 |  |Dec 5, 2022| model                                            |   top1 |   param_count |   gmac |   macts | hub                                                                                  ||:-------------------------------------------------|-------:|--------------:|-------:|--------:|:-------------------------------------------------------------------------------------|| vit_huge_patch14_clip_336.laion2b_ft_in12k_in1k  |   88.6 |         632.5 |  391   |   407.5 |   || vit_large_patch14_clip_336.openai_ft_in12k_in1k  |   88.3 |         304.5 |  191.1 |   270.2 |   || vit_huge_patch14_clip_224.laion2b_ft_in12k_in1k  |   88.2 |         632   |  167.4 |   139.4 |   || vit_large_patch14_clip_336.laion2b_ft_in12k_in1k |   88.2 |         304.5 |  191.1 |   270.2 |  || vit_large_patch14_clip_224.openai_ft_in12k_in1k  |   88.2 |         304.2 |   81.1 |    88.8 |   || vit_large_patch14_clip_224.laion2b_ft_in12k_in1k |   87.9 |         304.2 |   81.1 |    88.8 |  || vit_large_patch14_clip_224.openai_ft_in1k        |   87.9 |         304.2 |   81.1 |    88.8 |         || vit_large_patch14_clip_336.laion2b_ft_in1k       |   87.9 |         304.5 |  191.1 |   270.2 |        || vit_huge_patch14_clip_224.laion2b_ft_in1k        |   87.6 |         632   |  167.4 |   139.4 |         || vit_large_patch14_clip_224.laion2b_ft_in1k       |   87.3 |         304.2 |   81.1 |    88.8 |        || vit_base_patch16_clip_384.laion2b_ft_in12k_in1k  |   87.2 |          86.9 |   55.5 |   101.6 |   || vit_base_patch16_clip_384.openai_ft_in12k_in1k   |   87   |          86.9 |   55.5 |   101.6 |    || vit_base_patch16_clip_384.laion2b_ft_in1k        |   86.6 |          86.9 |   55.5 |   101.6 |         || vit_base_patch16_clip_384.openai_ft_in1k         |   86.2 |          86.9 |   55.5 |   101.6 |          || vit_base_patch16_clip_224.laion2b_ft_in12k_in1k  |   86.2 |          86.6 |   17.6 |    23.9 |   || vit_base_patch16_clip_224.openai_ft_in12k_in1k   |   85.9 |          86.6 |   17.6 |    23.9 |    || vit_base_patch32_clip_448.laion2b_ft_in12k_in1k  |   85.8 |          88.3 |   17.9 |    23.9 |   || vit_base_patch16_clip_224.laion2b_ft_in1k        |   85.5 |          86.6 |   17.6 |    23.9 |         || vit_base_patch32_clip_384.laion2b_ft_in12k_in1k  |   85.4 |          88.3 |   13.1 |    16.5 |   || vit_base_patch16_clip_224.openai_ft_in1k         |   85.3 |          86.6 |   17.6 |    23.9 |          || vit_base_patch32_clip_384.openai_ft_in12k_in1k   |   85.2 |          88.3 |   13.1 |    16.5 |    || vit_base_patch32_clip_224.laion2b_ft_in12k_in1k  |   83.3 |          88.2 |    4.4 |     5   |   || vit_base_patch32_clip_224.laion2b_ft_in1k        |   82.6 |          88.2 |    4.4 |     5   |         || vit_base_patch32_clip_224.openai_ft_in1k         |   81.9 |          88.2 |    4.4 |     5   |          || model                              |   top1 |   param_count |   gmac |   macts | hub                                                                    ||:-----------------------------------|-------:|--------------:|-------:|--------:|:-----------------------------------------------------------------------|| maxvit_xlarge_tf_512.in21k_ft_in1k |   88.5 |         475.8 |  534.1 |  1413.2 |  || maxvit_xlarge_tf_384.in21k_ft_in1k |   88.3 |         475.3 |  292.8 |   668.8 |  || maxvit_base_tf_512.in21k_ft_in1k   |   88.2 |         119.9 |  138   |   704   |    || maxvit_large_tf_512.in21k_ft_in1k  |   88   |         212.3 |  244.8 |   942.2 |   || maxvit_large_tf_384.in21k_ft_in1k  |   88   |         212   |  132.6 |   445.8 |   || maxvit_base_tf_384.in21k_ft_in1k   |   87.9 |         119.6 |   73.8 |   332.9 |    || maxvit_base_tf_512.in1k            |   86.6 |         119.9 |  138   |   704   |             || maxvit_large_tf_512.in1k           |   86.5 |         212.3 |  244.8 |   942.2 |            || maxvit_base_tf_384.in1k            |   86.3 |         119.6 |   73.8 |   332.9 |             || maxvit_large_tf_384.in1k           |   86.2 |         212   |  132.6 |   445.8 |            || maxvit_small_tf_512.in1k           |   86.1 |          69.1 |   67.3 |   383.8 |            || maxvit_tiny_tf_512.in1k            |   85.7 |          31   |   33.5 |   257.6 |             || maxvit_small_tf_384.in1k           |   85.5 |          69   |   35.9 |   183.6 |            || maxvit_tiny_tf_384.in1k            |   85.1 |          31   |   17.5 |   123.4 |             || maxvit_large_tf_224.in1k           |   84.9 |         211.8 |   43.7 |   127.4 |            || maxvit_base_tf_224.in1k            |   84.9 |         119.5 |   24   |    95   |             || maxvit_small_tf_224.in1k           |   84.4 |          68.9 |   11.7 |    53.2 |            || maxvit_tiny_tf_224.in1k            |   83.4 |          30.9 |    5.6 |    35.8 |             |Oct 15, 2022Oct 10, 2022Sept 23, 2022Sept 7, 2022IntroductionPyTorch Image Models () is a collection of image models, layers, utilities, optimizers, schedulers, data-loaders / augmentations, and reference training / validation scripts that aim to pull together a wide variety of SOTA models with ability to reproduce ImageNet training results.The work of many others is present here. I've tried to make sure all source material is acknowledged via links to github, arxiv papers, etc in the README, documentation, and code docstrings. Please let me know if I missed anything.ModelsAll model architecture families include variants with pretrained weights. There are specific model variants without any weights, it is NOT a bug. Help training new or better weights is always appreciated.FeaturesSeveral (less common) features that I often utilize in my projects are included. Many of their additions are the reason why I maintain my own set of models, instead of using others' via PIP:ResultsModel validation results can be found in the Getting Started (Documentation)The official documentation can be found at https://huggingface.co/docs/hub/timm. Documentation contributions are welcome. by  is an extensive blog post covering many aspects of  in detail. is an alternate set of documentation for . A big thanks to  for his efforts creating timmdocs. is a good resource for browsing the models within .Train, Validation, Inference ScriptsThe root folder of the repository contains reference train, validation, and inference scripts that work with the included models and other features of this repository. They are adaptable for other datasets and use cases with a little hacking. See .Awesome PyTorch ResourcesOne of the greatest assets of PyTorch is the community and their contributions. A few of my favourite resources that pair well with the models and components here are listed below.Object Detection, Instance and Semantic SegmentationComputer Vision / Image AugmentationKnowledge DistillationMetric LearningTraining / FrameworksLicensesCodeThe code here is licensed Apache 2.0. I've taken care to make sure any third party code included or adapted has compatible (permissive) licenses such as MIT, BSD, etc. I've made an effort to avoid any GPL / LGPL conflicts. That said, it is your responsibility to ensure you comply with licenses here and conditions of any dependent licenses. Where applicable, I've linked the sources/references for various components in docstrings. If you think I've missed anything please create an issue.Pretrained WeightsSo far all of the pretrained weights available here are pretrained on ImageNet with a select few that have some additional pretraining (see extra note below). ImageNet was released for non-commercial research purposes only (https://image-net.org/download). It's not clear what the implications of that are for the use of pretrained weights from that dataset. Any models I have trained with ImageNet are done for research purposes and one should assume that the original dataset license applies to the weights. It's best to seek legal advice if you intend to use the pretrained weights in a commercial product.Pretrained on more than ImageNetSeveral weights included or references here were pretrained with proprietary datasets that I do not have access to. These include the Facebook WSL, SSL, SWSL ResNe(Xt) and the Google Noisy Student EfficientNet models. The Facebook models have an explicit non-commercial license (CC-BY-NC 4.0, https://github.com/facebookresearch/semi-supervised-ImageNet1K-models, https://github.com/facebookresearch/WSL-Images). The Google models do not appear to have any restriction beyond the Apache 2.0 license (and ImageNet concerns). In either case, you should contact Facebook or Google with any questions.CitingBibTeX@misc{rw2019timm,
  author = {Ross Wightman},
  title = {PyTorch Image Models},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  doi = {10.5281/zenodo.4414861},
  howpublished = {\url{https://github.com/rwightman/pytorch-image-models}}
}
Latest DOI"
https://github.com/iGhibli/iOS-DeviceSupport,"This repository holds the device support files for the iOS, and I will update it regularly.","iOS-DeviceSupportThis repository holds the device support files for the iOS, and I will update it regularly.UsageSee docs: Below command will try to unzip all new device support files to .sudo ./deploy.py
You can use  if your Xcode is not in  or has different name.sudo ./deploy.py -t /Applications/Xcode\ 9.app
./deploy.py -h
usage: deploy.py [-h] [-t TARGET]

optional arguments:
  -h, --help  show this help message and exit
  -t TARGET   The path for Xcode
Supported versions"
https://github.com/vipstone/faceai,一款入门级的人脸、视频、文字检测以及识别的项目.,"功能查看功能预览↓↓↓开发环境教程其他教程功能预览绘制脸部轮廓人脸68个关键点标识头像特效合成性别识别表情识别数字化妆视频人脸检测视频人脸识别视频人脸识别图片修复图片自动上色技术方案技术实现方案介绍人脸识别：OpenCV / Dlib

人脸检测：face_recognition

性别识别：keras + tensorflow

文字识别：Tesseract OCR
TODO换脸——待完善眼睛移动方向检测——待完善Dlib性能优化方案Dlib模型训练方法Tesseract模型训练方法贡献者名单（特别感谢）	微信打赏"
https://github.com/wistbean/learn_python3_spider,python爬虫教程系列、从0到1学习python爬虫，包括浏览器抓包，手机APP抓包，如 fiddler、mitmproxy，各种爬虫涉及的模块的使用，如：requests、beautifulSoup、selenium、appium、scrapy等，以及IP代理，验证码识别，Mysql，MongoDB数据库的python使用，多线程多进程爬虫的使用，css 爬虫加密逆向破解，JS爬虫逆向，分布式爬虫，爬虫项目实战实例等,learn_python3_spider接下来就是，学习python的正确姿势！peace.python爬虫教程从0到1爬虫负基础Python 进阶爬虫书籍「打骨折」python爬虫前，抓包python爬虫库的使用python爬虫进阶：python爬虫反爬Python websocket 爬虫：Python 分布式爬虫爬虫实战教程爬虫实例源代码图文教程 | 相关源码---- | --- |  |  |  |  |  | 、 |  |  |  | 爬虫技巧python爬虫段子python相关公众号获取 Python 相关帅书微信搜索id：fxxkpython名称：学习 Python 的正确姿势进去发送「帅书」即可领取。Python 视频号通往Python高手之路小帅b手把手带你：
https://github.com/PyMySQL/PyMySQL,MySQL client library for Python,"PyMySQLThis package contains a pure-Python MySQL client library, based on .RequirementsInstallationPackage is uploaded on .You can install it with pip:$ python3 -m pip install PyMySQL
To use ""sha256_password"" or ""caching_sha2_password"" for authenticate,you need to install additional dependency:$ python3 -m pip install PyMySQL[rsa]
To use MariaDB's ""ed25519"" authentication method, you need to installadditional dependency:$ python3 -m pip install PyMySQL[ed25519]
DocumentationDocumentation is available online: For support, please refer to the.ExampleThe following examples make use of a simple tableCREATE TABLE `users` (
    `id` int(11) NOT NULL AUTO_INCREMENT,
    `email` varchar(255) COLLATE utf8_bin NOT NULL,
    `password` varchar(255) COLLATE utf8_bin NOT NULL,
    PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin
AUTO_INCREMENT=1 ;
import pymysql.cursors

# Connect to the database
connection = pymysql.connect(host='localhost',
                             user='user',
                             password='passwd',
                             database='db',
                             cursorclass=pymysql.cursors.DictCursor)

with connection:
    with connection.cursor() as cursor:
        # Create a new record
        sql = ""INSERT INTO `users` (`email`, `password`) VALUES (%s, %s)""
        cursor.execute(sql, ('webmaster@python.org', 'very-secret'))

    # connection is not autocommit by default. So you must commit to save
    # your changes.
    connection.commit()

    with connection.cursor() as cursor:
        # Read a single record
        sql = ""SELECT `id`, `password` FROM `users` WHERE `email`=%s""
        cursor.execute(sql, ('webmaster@python.org',))
        result = cursor.fetchone()
        print(result)
This example will print:{'password': 'very-secret', 'id': 1}
ResourcesLicensePyMySQL is released under the MIT License. See LICENSE for moreinformation."
https://github.com/xonsh/xonsh,":shell: Python-powered, cross-platform, Unix-gazing shell.","xonsh.. class:: center**xonsh** is a Python-powered, cross-platform, Unix-gazing shell language and command prompt.

The language is a superset of Python 3.6+ with additional shell primitives.
xonsh (pronounced *conch*) is meant for the daily use of experts and novices alike.

.. image:: https://raw.githubusercontent.com/xonsh/xonsh/main/docs/_static/what_is_xonsh.png
        :alt: What is xonsh?
        :align: center
.. class:: centerIf you like xonsh, :star: the repo, `write a tweet`_ and stay tuned by watching releases.

.. image:: https://badges.gitter.im/xonsh/xonsh.svg
        :target: https://gitter.im/xonsh/xonsh?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge
        :alt: Join the chat at https://gitter.im/xonsh/xonsh

.. image:: https://travis-ci.org/xonsh/xonsh.svg?branch=main
        :target: https://travis-ci.org/xonsh/xonsh
        :alt: Travis

.. image:: https://ci.appveyor.com/api/projects/status/github/xonsh/xonsh?svg=true
        :target: https://ci.appveyor.com/project/xonsh/xonsh
        :alt: Appveyor

.. image:: https://img.shields.io/badge/Google%20Cloud%20Shell-xonsh-green
        :target: https://ssh.cloud.google.com/cloudshell/editor?cloudshell_git_repo=https://github.com/xonsh/xonsh.git
        :alt: Open in Google Cloud Shell
.. image:: https://codecov.io/gh/xonsh/xonsh/branch/master/graphs/badge.svg?branch=main
        :target: https://codecov.io/github/xonsh/xonsh?branch=main
        :alt: codecov.io
.. image:: https://repology.org/badge/tiny-repos/xonsh.svg
        :target: https://repology.org/project/xonsh/versions
        :alt: repology.org
First stepsInstall xonsh from pip:.. code-block:: xonshconpython -m pip install 'xonsh[full]'
And visit https://xon.sh for more information:ExtensionsXonsh has the certain term for extensions and additional materials - xontrib - the short version of ""contribution"" word.Projects that use xonsh or compatibleJupyter-based interactive notebooks via _:The xonsh shell communityThe xonsh shell is developed by a community of volunteers. There are few ways to help the xonsh shell:We welcome new contributors!.. _write a tweet: https://twitter.com/intent/tweet?text=xonsh%20is%20a%20Python-powered,%20cross-platform,%20Unix-gazing%20shell%20language%20and%20command%20prompt.&url=https://github.com/xonsh/xonsh"
https://github.com/donnemartin/gitsome,A supercharged Git/GitHub command line interface (CLI).  An official integration for GitHub and GitHub Enterprise: https://github.com/works-with/category/desktop-tools,"gitsome   Why ?The Git Command LineAlthough the standard Git command line is a great tool to manage your Git-powered repos, it can be tough to remember the usage of:The Git command line does not integrate with GitHub, forcing you to toggle between command line and browser. - A Supercharged Git/GitHub CLI With Autocomplete aims to supercharge your standard git/shell interface by focusing on:Deep GitHub IntegrationNot all GitHub workflows work well in a terminal;  attempts to target those that do. includes 29 GitHub integrated commands that work with [<marko.inline.RawText object at 0x000001592FDC75C8>] shells:$ gh <command> [param] [options]
Run  commands along with  and  commands to unlock even more GitHub integrations!Git and GitHub Autocompleter With Interactive HelpYou can run the optional shell: $ gitsome
to enable autocompletion and interactive help for the following:General Autocompleter autocompletes the following:To enable additional autocompletions, check out the  section.Fish-Style Auto-Suggestions supports Fish-style auto-suggestions.  Use the  key to complete a suggestion.Python REPL is powered by , which supports a Python REPL.Run Python commands alongside shell commands:Additional  features can be found in the .Command History keeps track of commands you enter and stores them in .  Use the up and down arrow keys to cycle through the command history.Customizable HighlightingYou can control the ansi colors used for highlighting by updating your  file.Color options include:'black', 'red', 'green', 'yellow',
'blue', 'magenta', 'cyan', 'white'
For no color, set the value(s) to .   can appear as light gray on some terminals.Available Platforms is available for Mac, Linux, Unix, , and .TODO is just getting started.  Feel free to IndexGitHub Integration CommandsInstallation and TestsMiscGitHub Integration Commands SyntaxUsage:$ gh <command> [param] [options]
GitHub Integration Commands Listing  configure            Configure gitsome.
  create-comment       Create a comment on the given issue.
  create-issue         Create an issue.
  create-repo          Create a repo.
  emails               List all the user's registered emails.
  emojis               List all GitHub supported emojis.
  feed                 List all activity for the given user or repo.
  followers            List all followers and the total follower count.
  following            List all followed users and the total followed count.
  gitignore-template   Output the gitignore template for the given language.
  gitignore-templates  Output all supported gitignore templates.
  issue                Output detailed information about the given issue.
  issues               List all issues matching the filter.
  license              Output the license template for the given license.
  licenses             Output all supported license templates.
  me                   List information about the logged in user.
  notifications        List all notifications.
  octo                 Output an Easter egg or the given message from Octocat.
  pull-request         Output detailed information about the given pull request.
  pull-requests        List all pull requests.
  rate-limit           Output the rate limit.  Not available for Enterprise.
  repo                 Output detailed information about the given filter.
  repos                List all repos matching the given filter.
  search-issues        Search for all issues matching the given query.
  search-repos         Search for all repos matching the given query.
  starred              Output starred repos.
  trending             List trending repos for the given language.
  user                 List information about the given user.
  view                 View the given index in the terminal or a browser.
GitHub Integration Commands Reference: COMMANDS.mdSee the  for a detailed discussion of all GitHub integration commands, parameters, options, and examples.Check out the next section for a quick reference.GitHub Integration Commands Quick ReferenceConfiguring To properly integrate with GitHub, you must first configure :$ gh configure
For GitHub Enterprise users, run with the  flag:$ gh configure -e
Listing FeedsListing Your News Feed$ gh feed
Listing A User's Activity FeedView your activity feed or another user's activity feed, optionally through a pager with .  The  is available for many commands.$ gh feed donnemartin -p
Listing A Repo's Activity Feed$ gh feed donnemartin/gitsome -p
Listing Notifications$ gh notifications
Listing Pull RequestsView all pull requests for your repos:$ gh pull-requests
Filtering IssuesView all open issues where you have been mentioned:$ gh issues --issue_state open --issue_filter mentioned
View all issues, filtering for only those assigned to you, regardless of state (open, closed):$ gh issues --issue_state all --issue_filter assigned
For more information about the filter and state qualifiers, visit the  reference in .Filtering Starred Repos$ gh starred ""repo filter""
Searching Issues and ReposSearching IssuesSearch issues that have the most +1s:$ gh search-issues ""is:open is:issue sort:reactions-+1-desc"" -p
Search issues that have the most comments:$ gh search-issues ""is:open is:issue sort:comments-desc"" -p
Search issues with the ""help wanted"" tag:$ gh search-issues ""is:open is:issue label:\""help wanted\"""" -p
Search issues that have your user name tagged @donnemartin:$ gh search-issues ""is:issue donnemartin is:open"" -p
Search all your open private issues:$ gh search-issues ""is:open is:issue is:private"" -p
For more information about the query qualifiers, visit the .Searching ReposSearch all Python repos created on or after 2015, with >= 1000 stars:$ gh search-repos ""created:>=2015-01-01 stars:>=1000 language:python"" --sort stars -p
For more information about the query qualifiers, visit the .Listing Trending Repos and DevsView trending repos:$ gh trending [language] [-w/--weekly] [-m/--monthly] [-d/--devs] [-b/--browser]
View trending devs (devs are currently only supported in browser):$ gh trending [language] --devs --browser
Viewing ContentThe  commandView the previously listed notifications, pull requests, issues, repos, users etc, with HTML nicely formatted for your terminal, or optionally in your browser:$ gh view [#] [-b/--browser]
The  commandView an issue:$ gh issue donnemartin/saws/1
The  commandView a pull request:$ gh pull-request donnemartin/awesome-aws/2
Setting Up List all available  templates:$ gh gitignore-templates
Set up your :$ gh gitignore-template Python > .gitignore
Setting Up List all available  templates:$ gh licenses
Set up your  or :$ gh license MIT > LICENSE
Summoning OctocatCall on Octocat to say the given message or an Easter egg:$ gh octo [say]
Viewing ProfilesViewing A User's Profile$ gh user octocat
Viewing Your ProfileView your profile with the  command or with the following shortcut:$ gh me
Creating Comments, Issues, and ReposCreate a comment:$ gh create-comment donnemartin/gitsome/1 -t ""hello world""
Create an issue:$ gh create-issue donnemartin/gitsome -t ""title"" -b ""body""
Create a repo:$ gh create-repo gitsome
Option: View in a PagerMany  commands support a  option that displays results in a pager, where available.Usage:$ gh <command> [param] [options] -p
$ gh <command> [param] [options] --pager
Option: View in a BrowserMany  commands support a  option that displays results in your default browser instead of your terminal.Usage:$ gh <command> [param] [options] -b
$ gh <command> [param] [options] --browser
See the  for a detailed listing of all GitHub integration commands, parameters, options, and examples.Having trouble remembering these commands?  Check out the handy  to guide you through each command.Note, you can combine InstallationPip Installation  is hosted on .  The following command will install :$ pip3 install gitsome
You can also install the latest  from GitHub source which can contain changes not yet pushed to PyPI:$ pip3 install git+https://github.com/donnemartin/gitsome.git
If you are not installing in a , you might need to run with :$ sudo pip3 install gitsome
Depending on your setup, you might also want to run  with the :$ sudo -H pip3 install gitsome
For most linux users,  can be installed on your system using the  package.For example, Ubuntu users can run:$ sudo apt-get install python3-pip
See this  for more details.Virtual Environment InstallationYou can install Python packages in a  to avoid potential issues with dependencies or permissions.If you are a Windows user or if you would like more details on , check out this .Install  and :$ pip3 install virtualenv
$ pip3 install virtualenvwrapper
$ export WORKON_HOME=~/.virtualenvs
$ source /usr/local/bin/virtualenvwrapper.sh
Create a   and install :$ mkvirtualenv gitsome
$ pip3 install gitsome
If the  install does not work, you might be running Python 2 by default.  Check what version of Python you are running:$ python --version
If the call above results in Python 2, find the path for Python 3:$ which python3  # Python 3 path for mkvirtualenv's --python option
Install Python 3 if needed.  Set the Python version when calling :$ mkvirtualenv --python [Python 3 path from above] gitsome
$ pip3 install gitsome
If you want to activate the   again later, run:$ workon gitsome
To deactivate the  , run:$ deactivate
Running as a Docker ContainerYou can run gitsome in a Docker container to avoid installing Python and  locally. To install Docker check out the .Once you have docker installed you can run gitsome:$ docker run -ti --rm mariolet/gitsome
You can use Docker volumes to let gitsome access your working directory, your local .gitsomeconfig and .gitconfig:$ docker run -ti --rm -v $(pwd):/src/              \
   -v ${HOME}/.gitsomeconfig:/root/.gitsomeconfig  \
   -v ${HOME}/.gitconfig:/root/.gitconfig          \
   mariolet/gitsome
If you are running this command often you will probably want to define an alias:$ alias gitsome=""docker run -ti --rm -v $(pwd):/src/              \
                  -v ${HOME}/.gitsomeconfig:/root/.gitsomeconfig  \
                  -v ${HOME}/.gitconfig:/root/.gitconfig          \
                  mariolet/gitsome""
To build the Docker image from sources:$ git clone https://github.com/donnemartin/gitsome.git
$ cd gitsome
$ docker build -t gitsome .
Starting the  ShellOnce installed, run the optional  autocompleter with interactive help:$ gitsome
Running the optional  shell will provide you with autocompletion, interactive help, fish-style suggestions, a Python REPL, etc.Running  CommandsRun GitHub-integrated commands:$ gh <command> [param] [options]
Note: Running the  shell is not required to execute  commands.  After   you can run  commands from any shell.Running the  CommandTo properly integrate with GitHub,  must be properly configured:$ gh configure
For GitHub Enterprise UsersRun with the  flag:$ gh configure -e
View more details in the  section.Enabling Bash CompletionsBy default,  looks at the following .To add additional bash completions, update the  file with the location of your bash completions.If  does not exist, create it:$ touch ~/.xonshrc
For example, if additional completions are found in , add the following line in :$BASH_COMPLETIONS.append('/usr/local/etc/my_bash_completion.d/completion.bash')
You will need to restart  for the changes to take effect.Enabling  Tab Completions Outside of You can run  commands outside of the  shell completer.  To enable  tab completions for this workflow, copy the  file locally.Let bash know completion is available for the  command within your current session:$ source /path/to/gh_complete.sh
To enable tab completion for all terminal sessions, add the following to your  file:source /path/to/gh_complete.sh
Reload your :$ source ~/.bashrc
Tip:  is the short form of , so you can run this instead:$ . ~/.bashrc
For Zsh Users includes a module which is compatible with bash completions.Download the  file as above and append the following to your :autoload bashcompinit
bashcompinit
source /path/to/gh_complete.sh
Reload your : $ source ~/.zshrc
Optional: Installing  or Displaying the avatar for the  and  commands will require installing the optional  or  dependency.Windows* and Mac:$ pip3 install Pillow
*See the  section for limitations on the avatar.Ubuntu users, check out these Supported Python Versions is powered by  which does not currently support Python 2.x, as discussed in this .Supported PlatformsWindows Support has been tested on Windows 10 with  and .Although you can use the standard Windows command prompt, you'll probably have a better experience with either  or .Text Only AvatarThe commands  and  will always have the  flag enabled, since  does not support the ansi avatar on Windows.Config FileOn Windows, the  file can be found in .  For example:C:\Users\dmartin\.gitsomeconfig
Developer InstallationIf you're interested in contributing to , run the following commands:$ git clone https://github.com/donnemartin/gitsome.git
$ cd gitsome
$ pip3 install -e .
$ pip3 install -r requirements-dev.txt
$ gitsome
$ gh <command> [param] [options]
If you get an error while installing saying that you need Python 3.4+, it could be because your  command is configured for an older version of Python. To fix this issue, it is recommended to install :$ sudo apt-get install python3-pip
See this  for more details.Continuous IntegrationContinuous integration details are available on .Unit Tests and Code CoverageRun unit tests in your active Python environment:$ python tests/run_tests.py
Run unit tests with  on multiple Python environments:$ tox
DocumentationSource code documentation will soon be available on .  Check out the .Run the following to build the docs:$ scripts/update_docs.sh
ContributingContributions are welcome!Review the  for details on how to:CreditsContact InfoFeel free to contact me to discuss any issues, questions, or comments.My contact info can be found on my .LicenseI am providing code and resources in this repository to you under an open source license.  Because this is my personal repository, the license you receive to my code and resources is from me and not my employer (Facebook).Copyright 2016 Donne Martin

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
"
https://github.com/huashengdun/webssh,:seedling: Web based ssh client,"WebSSHIntroductionA simple web application to be used as an ssh client to connect to your ssh servers. It is written in Python, base on tornado, paramiko and xterm.js.FeaturesPreviewHow it works+---------+     http     +--------+    ssh    +-----------+
| browser | <==========> | webssh | <=======> | ssh server|
+---------+   websocket  +--------+    ssh    +-----------+
RequirementsQuickstartServer options# start a http server with specified listen address and listen port
wssh --address='2.2.2.2' --port=8000

# start a https server, certfile and keyfile must be passed
wssh --certfile='/path/to/cert.crt' --keyfile='/path/to/cert.key'

# missing host key policy
wssh --policy=reject

# logging level
wssh --logging=debug

# log to file
wssh --log-file-prefix=main.log

# more options
wssh --help
Browser console// connect to your ssh server
wssh.connect(hostname, port, username, password, privatekey, passphrase, totp);

// pass an object to wssh.connect
var opts = {
  hostname: 'hostname',
  port: 'port',
  username: 'username',
  password: 'password',
  privatekey: 'the private key text',
  passphrase: 'passphrase',
  totp: 'totp'
};
wssh.connect(opts);

// without an argument, wssh will use the form data to connect
wssh.connect();

// set a new encoding for client to use
wssh.set_encoding(encoding);

// reset encoding to use the default one
wssh.reset_encoding();

// send a command to the server
wssh.send('ls -l');
Custom FontTo use custom font, put your font file in the directory  and restart the server.URL ArgumentsSupport passing arguments by url (query or fragment) like following examples:Passing form data (password must be encoded in base64, privatekey not supported)http://localhost:8888/?hostname=xx&username=yy&password=str_base64_encoded
Passing a terminal background colorhttp://localhost:8888/#bgcolor=green
Passing a terminal font colorhttp://localhost:8888/#fontcolor=red
Passing a user defined titlehttp://localhost:8888/?title=my-ssh-server
Passing an encodinghttp://localhost:8888/#encoding=gbk
Passing a font sizehttp://localhost:8888/#fontsize=24
Passing a command executed right after loginhttp://localhost:8888/?command=pwd
Passing a terminal typehttp://localhost:8888/?term=xterm-256color
Use DockerStart up the appdocker-compose up
Tear down the appdocker-compose down
TestsRequirementspip install pytest pytest-cov codecov flake8 mock
Use unittest to run all testspython -m unittest discover tests
Use pytest to run all testspython -m pytest tests
DeploymentRunning behind an Nginx serverwssh --address='127.0.0.1' --port=8888 --policy=reject
# Nginx config example
location / {
    proxy_pass http://127.0.0.1:8888;
    proxy_http_version 1.1;
    proxy_read_timeout 300;
    proxy_set_header Upgrade $http_upgrade;
    proxy_set_header Connection ""upgrade"";
    proxy_set_header Host $http_host;
    proxy_set_header X-Real-IP $remote_addr;
    proxy_set_header X-Real-PORT $remote_port;
}
Running as a standalone serverwssh --port=8080 --sslport=4433 --certfile='cert.crt' --keyfile='cert.key' --xheaders=False --policy=reject
Tips"
https://github.com/a1studmuffin/SpaceshipGenerator,A Blender script to procedurally generate 3D spaceships,"Spaceship GeneratorA Blender script to procedurally generate 3D spaceships from a random seed.UsageHow it worksWatch on YouTube: https://www.youtube.com/watch?v=xJZyXqJ6nogExtreme examplesThe following screenshots were created using extreme values for the number of hull segments and asymmetry segments to show how the algorithm works.Tips and TricksCreditsWritten for fun as part of the  June 2016 .Released under the .Authored and maintained by Michael Davies.Special thanks to  for bugfixes, a proper GUI and build script. Also to  for bugfixing, and  for the 2.80 port."
https://github.com/formspree/formspree,The successor to this repository is actively maintained at https://formspree.io. Its source code is not available,"FORMSPREE.IOFunctional HTML forms. Hosted at .Just send your form to our URL and we'll forward it to your email. No PHP, Javascript or sign up required — perfect for static sites!Example:<form action=""https://formspree.io/you@email.com"">
    <input type=""text"" name=""name"">
    <input type=""email"" name=""_replyto"">
    <input type=""submit"" value=""Send"">
</form>
Setting it up is easy and free. Here's how:You don't even have to register.1. Setup the HTML formChange your form's action-attribute to this and replace your@email.com with your own email.2. Submit the form and confirm your email addressGo to your website and submit the form once. This will send you an email asking to confirm your email address, so that no one can start sending you spam from random websites.3. All set, receive emailsFrom now on, when someone submits that form, we'll forward you the data as email.Advanced features:Form inputs can have specially named name-attributes, which alter functionality. They are all prefixed with an underscore._replytoThis value is used for the email's Reply-To field. This way you can directly ""Reply"" to the email to respond to the person who originally submitted the form._nextBy default, after submitting a form the user is shown the Formspree ""Thank You"" page. You can provide an alternative URL for that page._subjectThis value is used for the email's subject, so that you can quickly reply to submissions without having to edit the subject line each time._ccThis value is used for the email's CC Field. This lets you send a copy of each submission to another email address. If you want to cc multiple emails, simply make the cc field a list of emails each separated by a comma._gotchaAdd this ""honeypot"" field to avoid spam by fooling scrapers. If a value is provided, the submission will be silently ignored. The input should be hidden with CSS.Using AJAXFormspree Gold users can submit forms via AJAX. This even works cross-origin. The trick is to set the Accept header to application/json. If you're using jQuery this can be done like so:$.ajax({
    url: ""https://formspree.io/FORM_ID"",
    method: ""POST"",
    data: {message: ""hello!""},
    dataType: ""json""
});
If you are experiencing issues, please take a look at the  in the wikiRunning your own copy of FormspreeRunning on localhostYou'll need a  account, PostgreSQL, Redis and Python 2.7 and should install , and create a  for the server.Once your environment is setup, create a postgresql database, clone the source and cd into the root of the Formspree repository. Then run:pip install -r requirements.txt
then create a  file with your configuration like the following:API_ROOT='http://127.0.0.1:5000'
CONTACT_EMAIL='support@example.com'
DATABASE_URL='postgresql://<username>@127.0.0.1:5432/formspree'
DEBUG='True'
DEFAULT_SENDER='no-reply@localhost.com'
LOG_LEVEL='debug'
MONTHLY_SUBMISSIONS_LIMIT='100'
NONCE_SECRET='y0ur_n0nc3_s3cr3t'
HASHIDS_SALT='a salt'
REDISTOGO_URL='127.0.0.1:6379'
SECRET_KEY='y0ur_s3cr3t_k3y'
SENDGRID_PASSWORD='<password>'
SENDGRID_USERNAME='<username>'
SERVICE_NAME='LocalFormspree'
SERVICE_URL='http://127.0.0.1:5000'
TEST_DATABASE_URL='postgresql://<username>@127.0.0.1:5432/formspree-test'
Tell the Flask CLI about the application by typing  or if using a virtualenv you can Make sure you have a postgresql database called  and create the necessary tables by running:flask db upgrade
And you are ready to run the server:flask run
Running testsREDISTOGO_URL='0.0.0.0:6379' \
TEST_DATABASE_URL=postgresql://<username>@127.0.0.1:5432/formspree-test \
NONCE_SECRET='y0ur_n0nc3_s3cr3t' \
HASHIDS_SALT='a salt' \
SECRET_KEY='y0ur_s3cr3t_k3y' \
STRIPE_TEST_PUBLISHABLE_KEY='<STRIPE PUBLISHABLE>' \
STRIPE_TEST_SECRET_KEY='<STRIPE SECRET>' \
python -m unittest discover
Make sure that you do NOT use your actual  database when running tests. Doing so will cause you to lose all data located in your  database. Instead create a new database called .You can also use  to automate running tests. After installing, run  to run the entire test suite. To run a single test file, run . In this case, it will run only .Running on HerokuYou will need to install the .Once your environment is setup, clone the source and cd into the root of the Formspree repository. Then run:heroku apps:create [your project name]
thengit push heroku
Your new project will be running at [your project name].herokuapp.com.DependenciesFormspree requires a PostgreSQL database and uses SendGrid to send emails. If you're deploying to Heroku you can get a free Heroku Postgres database and a SendGrid account by runningheroku addons:create heroku-postgresql:hobby-dev
heroku addons:create sendgrid
Configuring FormspreeTake a look at the  file for a list of environment variables that should be set in order for Formspree to work correctly.ContributingFormspree is an open source project managed on GitHub. We welcome all contributions from the community, but please be sure to take a look at the  before opening an issue or pull request."
https://github.com/mailpile/Mailpile,"A free & open modern, fast email client with user-friendly encryption and privacy features","Welcome to Mailpile!IMPORTANT NOTEDevelopment on this codebase has halted, until thehas completed.Apologies to those who have unanswered, out-standing pull requests andissues. 😢 Your efforts are appreciated!If you rely on this code and have your own branch which you activelymaintain, let us know: we would be happy to link to it.If you need to run Mailpile v1 to access legacy data, consider usingour .Introduction (Obsolete)Mailpile () is a modern, fast web-mail clientwith user-friendly encryption and privacy features. The development ofMailpile is funded byand all code related to the project is and will be released under an OSIapproved Free Software license.Mailpile places great emphasis on providing a clean, elegant userinterface and pleasant user experience. In particular, Mailpile aims tomake it easy and convenient to receive and send PGP encrypted or signede-mail.Mailpile's primary user interface is web-based, but it also has a basiccommand-line interface and an API for developers. Using web technologyfor the interface allows Mailpile to function both as a local desktopapplication (accessed by visiting  in the browser) or aremote web-mail on a personal server or VPS.The core of Mailpile is a fast search engine, custom written to dealwith large volumes of e-mail on consumer hardware. The search engineallows e-mail to be organized using tags (similar to GMail's labels) andthe application can be configured to automatically tag incoming maileither based on static rules or bayesian classifiers.Trying MailpileIf you need to run Mailpile v1 to access legacy data, consider usingour .Credits and LicenseBjarni R. Einarsson () created this!  If youthink it's neat, you should also check out PageKite:.  and joined the team in 2013 and madethis a real project (not just a toy search engine).The original GMail team deserve a mention for their inspiring work:wishing the Free Software world had something like GMail is whatmotivated Bjarni to start working on Mailpile. We would also like tothank Edward Snowden for inspiring us to try and make PGP usable forjournalists and everday folks!Contributors:And of course, we couldn't do this without .This program is free software: you can redistribute it and/or modify itunder the terms of the GNU Affero General Public License as published bythe Free Software Foundation. See the file  for details."
https://github.com/pandas-dev/pandas,"Flexible and powerful data analysis / manipulation library for Python, providing labeled data structures similar to R data.frame objects, statistical functions, and much more","pandas: powerful Python data analysis toolkit| | || --- | --- || Testing |   || Package |     || Meta |     |What is it?pandas is a Python package that provides fast, flexible, and expressive datastructures designed to make working with ""relational"" or ""labeled"" data botheasy and intuitive. It aims to be the fundamental high-level building block fordoing practical, real world data analysis in Python. Additionally, it hasthe broader goal of becoming the most powerful and flexible open source data. It is already well onits way towards this goal.Table of ContentsMain FeaturesHere are just a few of the things that pandas does well:Where to get itThe source code is currently hosted on GitHub at:https://github.com/pandas-dev/pandasBinary installers for the latest released version are available at the  and on .# conda
conda install -c conda-forge pandas
# or PyPI
pip install pandas
The list of changes to pandas between each release can be found. For fulldetails, see the commit logs at https://github.com/pandas-dev/pandas.DependenciesSee the  for minimum supported versions of required, recommended and optional dependencies.Installation from sourcesTo install pandas from source you need  in addition to the normaldependencies above. Cython can be installed from PyPI:pip install cython
In the  directory (same one where you found this file aftercloning the git repo), execute:pip install .
or for installing in :python -m pip install -ve . --no-build-isolation --config-settings=editable-verbose=true
See the full instructions for .LicenseDocumentationThe official documentation is hosted on .BackgroundWork on  started at  (a quantitative hedge fund) in 2008 andhas been under active development since then.Getting HelpFor usage questions, the best place to go to is .Further, general questions and discussions can also take place on the .Discussion and DevelopmentMost development discussions take place on GitHub in this repo, via the .Further, the  can also be used for specialized discussions or design issues, and a  is available for quick development related questions.There are also frequent  for project maintainers open to the community as well as monthly  to help support new contributors.Additional information on the communication channels can be found on the  page.Contributing to pandasAll contributions, bug reports, bug fixes, documentation improvements, enhancements, and ideas are welcome.A detailed overview on how to contribute can be found in the [<marko.inline.RawText object at 0x000001592FDE3848>].If you are simply looking to start working with the pandas codebase, navigate to the  and start looking through interesting issues. There are a number of issues listed under  and  where you could start out.You can also triage issues which may include reproducing bug reports, or asking for vital information such as version numbers or reproduction instructions. If you would like to start triaging issues, one easy way to get started is to .Or maybe through using pandas you have an idea of your own or are looking for something in the documentation and thinking ‘this can be improved’...you can do something about it!Feel free to ask questions on the  or on .As contributors and maintainers to this project, you are expected to abide by pandas' code of conduct. More information can be found at: "
https://github.com/Gerapy/Gerapy,"Distributed Crawler Management Framework Based on Scrapy, Scrapyd, Django and Vue.js","GerapyDistributed Crawler Management Framework Based on Scrapy, Scrapyd, Scrapyd-Client, Scrapyd-API, Django and Vue.js.DocumentationDocumentation is available online at  and .SupportGerapy is developed based on Python 3.x. Python 2.x may be supported later.UsageInstall Gerapy by pip:pip3 install gerapy
After the installation, you need to do these things below to run Gerapy server:If you have installed Gerapy successfully, you can use command . If not, check the installation.First use this command to initialize the workspace:gerapy init
Now you will get a folder named . Also you can specify the name of your workspace by this command:gerapy init <workspace>
Then  to this folder, and run this command to initialize the Database:cd gerapy
gerapy migrate
Next you need to create a superuser by this command:gerapy createsuperuser
Then you can runserver by this command:gerapy runserver
Then you can visit  to enjoy it. Also you can vist  to get the admin management backend.If you want to run Gerapy in public, just run like this:gerapy runserver 0.0.0.0:8000
Then it will run with public host and port 8000.In Gerapy, You can create a configurable project and then configure and generate code of Scrapy automatically. But this module is unstable, we're trying to refine it.Also you can drag your Scrapy Project to  folder. Then refresh web, it will appear in the Project Index Page and comes to un-configurable, but you can edit this project through the web page.As for deployment, you can move to Deploy Page. Firstly you need to build your project and add client in the Client Index Page, then you can deploy the project just by clicking button.After the deployment, you can manage the job in Monitor Page.DockerJust run this command:docker-compose up
Then it will run at port 8000. You can use the temp admin account (username: admin, password: admin) to login. And please change the password later for safety.Command Usage:docker run -d -v <workspace>:/home/gerapy -p <public_port>:<container_port> germey/gerapy
Please specify your workspace to mount Gerapy workspace by  and specify server port by .If you run Gerapy by Docker, you can visit Gerapy website such as  and enjoy it, no need to do other initialzation things.TodoListCommunicationIf you have any questions or ideas, you can send  or , your suggestions are really import for us, thanks for your contirbution."
https://github.com/donnemartin/interactive-coding-challenges,120+ interactive Python coding interview challenges (algorithms and data structures).  Includes Anki flashcards.,"interactive-coding-challenges120+ continually updated, interactive, and test-driven coding challenges, with .Challenges focus on algorithms and data structures found in coding interviews.Each challenge has one or more reference solutions that are:Challenges will soon provide on-demand  to help you arrive at the optimal solution.Notebooks also detail:Also included are unit tested reference implementations of various  and .Challenge SolutionsAnki Flashcards: Coding and DesignThe provided  uses spaced repetition to help you retain key concepts.Great for use while on-the-go.Design Resource: The System Design PrimerLooking for resources to help you prep for the System Design and Object-Oriented Design interviews?Check out the sister repo , which contains additional Anki decks:Notebook StructureEach challenge has two notebooks, a challenge notebook with unit tests for you to solve and a solution notebook for reference.Problem StatementConstraintsTest CasesAlgorithmHintsCode (Challenge: Implement Me!)Unit TestIndexChallenges CategoriesFormat: Challenge Category - Number of ChallengesTotal number of challenges: 120Reference Implementations: Data StructuresUnit tested, fully functional implementations of the following data structures:Reference Implementations: AlgorithmsUnit tested, fully functional implementations of the following algorithms:Reference Implementations: TODOInstalling and Running ChallengesMiscChallengesArrays and Strings| Challenge | Static Notebook ||--------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------|| Determine if a string contains unique characters |  │  || Determine if a string is a permutation of another |  │  || Determine if a string is a rotation of another |  │  || Compress a string |  │  || Reverse characters in a string |  │  || Given two strings, find the single different char |  │  || Find two indices that sum to a specific value |  │  || Implement a hash table |  │  || Implement fizz buzz |  │  || Find the first non-repeated character in a string |  │  || Remove specified characters in a string |  │  || Reverse words in a string |  │  || Convert a string to an integer |  │  || Convert an integer to a string |  │  || Add a challenge |  │  |Linked Lists| Challenge | Static Notebook ||--------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------|| Remove duplicates from a linked list |  │  || Find the kth to last element of a linked list |  │  || Delete a node in the middle of a linked list |  │  || Partition a linked list around a given value |  │  || Add two numbers whose digits are stored in a linked list |  │  || Find the start of a linked list loop |  │  || Determine if a linked list is a palindrome |  │  || Implement a linked list |  │  || Determine if a list is cyclic or acyclic |  │  || Add a challenge |  │  |Stacks and Queues| Challenge | Static Notebook ||--------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------|| Implement n stacks using a single array |  │  || Implement a stack that keeps track of its minimum element |  │  || Implement a set of stacks class that wraps a list of capacity-bounded stacks |  │  || Implement a queue using two stacks |  │  || Sort a stack using another stack as a buffer |  │  || Implement a stack |  │  || Implement a queue |  │  || Implement a priority queue backed by an array |  │  || Add a challenge |  │  |Graphs and Trees| Challenge | Static Notebooks ||--------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------|| Implement depth-first search (pre-, in-, post-order) on a tree |   │  || Implement breadth-first search on a tree |  │  || Determine the height of a tree |  │  || Create a binary search tree with minimal height from a sorted array |  │  || Create a linked list for each level of a binary tree |  │  || Check if a binary tree is balanced |  │  || Determine if a tree is a valid binary search tree |  │  || Find the in-order successor of a given node in a binary search tree |  │  || Find the second largest node in a binary search tree |  │  || Find the lowest common ancestor |  │  || Invert a binary tree |  │  || Implement a binary search tree |  │  || Implement a min heap |  │  || Implement a trie |  │  || Implement depth-first search on a graph |   │  || Implement breadth-first search on a graph |  │  || Determine if there is a path between two nodes in a graph |  │  || Implement a graph |  │  || Find a build order given a list of projects and dependencies. |   │  || Find the shortest path in a weighted graph. |   │  || Find the shortest path in an unweighted graph. |   │  || Add a challenge |  │  |Sorting| Challenge | Static Notebooks ||--------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------|| Implement selection sort |  │  || Implement insertion sort |  │  || Implement quick sort |  │  || Implement merge sort |  │  || Implement radix sort |  │  || Sort an array of strings so all anagrams are next to each other |  │  || Find an item in a sorted, rotated array |  │  || Search a sorted matrix for an item |  │  || Find an int not in an input of n integers |  │  ||  Given sorted arrays A, B, merge B into A in sorted order |  │  || Implement a stable selection sort |  │  || Make an unstable sort stable |  │  || Implement an efficient, in-place version of quicksort |  │  || Given two sorted arrays, merge one into the other in sorted order |  │  || Find an element in a rotated and sorted array of integers |  │  || Add a challenge |  │  |Recursion and Dynamic Programming| Challenge | Static Notebooks ||--------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------|| Implement fibonacci recursively, dynamically, and iteratively |  │  || Maximize items placed in a knapsack |  │  || Maximize unbounded items placed in a knapsack |  │  || Find the longest common subsequence |  │  || Find the longest increasing subsequence |  │  || Minimize the cost of matrix multiplication |  │  || Maximize stock prices given k transactions |  │  || Find the minimum number of ways to represent n cents given an array of coins |  │  || Find the unique number of ways to represent n cents given an array of coins |  │  || Print all valid combinations of n-pairs of parentheses |  │  || Navigate a maze |  │  || Print all subsets of a set |  │  || Print all permutations of a string |  │  || Find the magic index in an array |  │  || Find the number of ways to run up n steps |  │  || Implement the Towers of Hanoi with 3 towers and N disks |  │  || Implement factorial recursively, dynamically, and iteratively |  │  || Perform a binary search on a sorted array of integers |  │  || Print all combinations of a string |  │  || Implement a paint fill function |  │  || Find all permutations to represent n cents, given 1, 5, 10, 25 cent coins |  │  || Add a challenge |  │  |Mathematics and Probability| Challenge | Static Notebooks ||--------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------|| Generate a list of primes |  │  || Find the digital root |  │  || Create a class supporting insert, max, min, mean, mode in O(1) |  │  || Determine if a number is a power of two |  │  || Add two numbers without the + or - sign |  │  || Subtract two numbers without the + or - sign |  │  || Check if a number is prime |  │  || Determine if two lines on a Cartesian plane intersect |  │  || Using only add, implement multiply, subtract, and divide for ints |  │  || Find the kth number such that the only prime factors are 3, 5, and 7 |  │  || Add a challenge |  │  |Bit Manipulation| Challenge | Static Notebooks ||--------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------|| Implement common bit manipulation operations |  │  || Determine number of bits to flip to convert a into b |  │  || Draw a line on a screen |  │  || Flip a bit to maximize the longest sequence of 1s |  │  || Get the next largest and next smallest numbers |  │  || Merge two binary numbers |  │  || Swap odd and even bits in an integer |  │  || Print the binary representation of a number between 0 and 1 |  │  || Determine the number of 1s in the binary representation of a given integer |  │  || Add a challenge |  │  |Online Judges| Challenge | Static Notebooks ||--------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------|| Find the longest substring with at most k distinct chars |  │  || Find the highest product of three numbers |  │  || Maximize stocks profit from 1 buy and 1 sell |  │  || Move all zeroes in a list to the end |  │  || Find the products of every other int |  │  || Given a list of entries and exits, find the busiest period |  │  || Determine an island's perimeter |  │  || Format license keys |  │  || Find the longest absolute file path |  │  || Merge tuple ranges |  │  || Assign cookies |  │  || Determine if you can win in Nim |  │  || Check if a magazine could have been used to create a ransom note |  │  || Find the number of times a sentence can fit on a screen |  │  || Utopian tree |  │  || Maximizing xor |  │  || Add a challenge |  │  |Repo Structureinteractive-coding-challenges        # Repo
├─ arrays_strings                    # Category of challenges
│  ├─ rotation                       # Challenge folder
│  │  ├─ rotation_challenge.ipynb    # Challenge notebook
│  │  ├─ rotation_solution.ipynb     # Solution notebook
│  │  ├─ test_rotation.py            # Unit test*
│  ├─ compress
│  │  ├─ compress_challenge.ipynb
│  │  ├─ compress_solution.ipynb
│  │  ├─ test_compress.py
│  ├─ ...
├─ linked_lists
│  ├─ palindrome
│  │  └─ ...
│  ├─ ...
├─ ...
The notebooks (.ipynb) read/write the associated unit test (.py) file.Notebook InstallationZero InstallThis README contains links to  , which hosts dynamic notebooks of the repo's contents online with no installation needed.Jupyter NotebookRun:pip install jupyter
For detailed instructions, scripts, and tools to more optimally set up your development environment, check out the  repo.For more details on notebook installation, follow the directions .More information on IPython/Jupyter Notebooks can be found .Running ChallengesNotebooksChallenges are provided in the form of IPython/Jupyter Notebooks and have been tested with Python 2.7 and Python 3.x.If you need to install IPython/Jupyter Notebook, see the Run the notebook of challenges:$ git clone https://github.com/donnemartin/interactive-coding-challenges.git
$ cd interactive-coding-challenges
$ jupyter notebook
This will launch your web browser with the list of challenge categories:To debug your solution with pdb, refer to the following .Note: If your solution is different from those listed in the Solution Notebook, consider submitting a pull request so others can benefit from your work.  Review the  for details.Future DevelopmentChallenges, solutions, and unit tests are presented in the form of IPython/Jupyter Notebooks.ContributingContributions are welcome!Review the  for details on how to:CreditsResourcesImagesContact InfoFeel free to contact me to discuss any issues, questions, or comments.My contact info can be found on my .LicenseI am providing code and resources in this repository to you under an open source license.  Because this is my personal repository, the license you receive to my code and resources is from me and not my employer (Facebook).Copyright 2015 Donne Martin

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
"
https://github.com/ShangtongZhang/DeepRL,Modularized Implementation of Deep RL Algorithms in PyTorch,"DeepRL@@ I am looking for self-motivated students interested in RL at different levels! @@
@@ Visit https://shangtongzhang.github.io/people/ for more details. @@
Modularized implementation of popular deep RL algorithms in PyTorch.Easy switch between toy tasks and challenging games.Implemented algorithms:The DQN agent, as well as C51 and QR-DQN, has an asynchronous actor for data generation and an asynchronous replay buffer for transferring data to GPU.Using 1 RTX 2080 Ti and 3 threads, the DQN agent runs for 10M steps (40M frames, 2.5M gradient updates) for Breakout within 6 hours.DependencyUsage contains examples for all the implemented algorithms. contains the environment for generating the curves below.Please use this bibtex if you want to cite this repo@misc{deeprl,
  author = {Zhang, Shangtong},
  title = {Modularized Implementation of Deep RL Algorithms in PyTorch},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub Repository},
  howpublished = {\url{https://github.com/ShangtongZhang/DeepRL}},
}
Curves (commit )BreakoutNoFrameskip-v4 (1 run)MujocoReferencesCode of My Papers"
https://github.com/NVlabs/SPADE,Semantic Image Synthesis with SPADE,"Semantic Image Synthesis with SPADENew implementation available at imaginaire repositoryWe have a reimplementation of the SPADE method that is more performant. It is avaiable at  |    |  |  | Semantic Image Synthesis with Spatially-Adaptive Normalization.,  , ,  and .In CVPR 2019 (Oral).Copyright (C) 2019 NVIDIA Corporation.All rights reserved.Licensed under the  (Attribution-NonCommercial-ShareAlike 4.0 International)The code is released for academic research use only. For commercial use or business inquiries, please contact .For press and other inquiries, please contact InstallationClone this repo.git clone https://github.com/NVlabs/SPADE.git
cd SPADE/
This code requires PyTorch 1.0 and python 3+. Please install dependencies bypip install -r requirements.txt
This code also requires the Synchronized-BatchNorm-PyTorch rep.cd models/networks/
git clone https://github.com/vacancy/Synchronized-BatchNorm-PyTorch
cp -rf Synchronized-BatchNorm-PyTorch/sync_batchnorm .
cd ../../
To reproduce the results reported in the paper, you would need an NVIDIA DGX1 machine with 8 V100 GPUs.Dataset PreparationFor COCO-Stuff, Cityscapes or ADE20K, the datasets must be downloaded beforehand. Please download them on the respective webpages. In the case of COCO-stuff, we put a few sample images in this code repo.Preparing COCO-Stuff Dataset. The dataset can be downloaded . In particular, you will need to download train2017.zip, val2017.zip, stuffthingmaps_trainval2017.zip, and annotations_trainval2017.zip. The images, labels, and instance maps should be arranged in the same directory structure as in . In particular, we used an instance map that combines both the boundaries of ""things instance map"" and ""stuff label map"". To do this, we used a simple script . Please install  using  and refer to the script to generate instance maps.Preparing ADE20K Dataset. The dataset can be downloaded , which is from . After unzipping the datgaset, put the jpg image files  and png label files  in the same directory. There are different modes to load images by specifying  along with . . There are options such as , which resizes the images into square images of side length  and randomly crops to .  scales the image to have a short side of length  and crops to  x  square. To see all modes, please use  and take a look at . By default at the training phase, the images are randomly flipped horizontally. To prevent this use .Generating Images Using Pretrained ModelOnce the dataset is ready, the result images can be generated using pretrained models.Generating Landscape Image using GauGANIn the paper and the demo video, we showed GauGAN, our interactive app that generates realistic landscape images from the layout users draw. The model was trained on landscape images scraped from Flickr.com. We released an online demo that has the same features. Please visit . The model weights are not released. Training New ModelsNew models can be trained with the following commands.# To train on the Facades or COCO dataset, for example.
python train.py --name [experiment_name] --dataset_mode facades --dataroot [path_to_facades_dataset]
python train.py --name [experiment_name] --dataset_mode coco --dataroot [path_to_coco_dataset]

# To train on your own custom dataset
python train.py --name [experiment_name] --dataset_mode custom --label_dir [path_to_labels] -- image_dir [path_to_images] --label_nc [num_labels]
There are many options you can specify. Please use . The specified options are printed to the console. To specify the number of GPUs to utilize, use . If you want to use the second and third GPUs for example, use .To log training, use  for Tensorboard. The logs are stored at .TestingTesting is similar to testing pretrained models.python test.py --name [name_of_experiment] --dataset_mode [dataset_mode] --dataroot [path_to_dataset]
Use  to specify the output directory.  will specify the maximum number of images to generate. By default, it loads the latest checkpoint. It can be changed using .Code StructureOptionsThis code repo contains many options. Some options belong to only one specific model, and some options have different default values depending on other options. To address this, the  class dynamically loads and sets options depending on what model, network, and datasets are used. This is done by calling the static method  of various classes. It takes in the of  package and modifies the list of options. For example, since COCO-stuff dataset contains a special label ""unknown"", when COCO-stuff dataset is used, it sets  automatically at . You can take a look at  of , or  to get a sense of how this works.VAE-Style Training with an Encoder For Style Control and Multi-Modal OutputsTo train our model along with an image encoder to enable multi-modal outputs as in Figure 15 of the , please use . The model will create  in addition to  and  and train with KL-Divergence loss.CitationIf you use this code for your research, please cite our papers.@inproceedings{park2019SPADE,
  title={Semantic Image Synthesis with Spatially-Adaptive Normalization},
  author={Park, Taesung and Liu, Ming-Yu and Wang, Ting-Chun and Zhu, Jun-Yan},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year={2019}
}
AcknowledgmentsThis code borrows heavily from pix2pixHD. We thank Jiayuan Mao for his Synchronized Batch Normalization code."
https://github.com/google-deepmind/learning-to-learn,Learning to Learn in TensorFlow," in TensorFlowDependenciesTrainingpython train.py --problem=mnist --save_path=./mnist
Command-line flags:Evaluationpython evaluate.py --problem=mnist --optimizer=L2L --path=./mnist
Command-line flags:ProblemsThe training and evaluation scripts support the following problems (see for more details):New problems can be implemented very easily. You can see in  thatthe  method from the  class is given a functionthat returns the TensorFlow operation that generates the loss function we wantto minimize (see  for an example).It's important that all operations with Python side effects (e.g. queuecreation) must be done outside of the function passed to . The function in  is a good example of a loss function thatuses TensorFlow queues.Disclaimer: This is not an official Google product."
https://github.com/jackzhenguo/python-small-examples,告别枯燥，致力于打造 Python 实用小例子，更多Python良心教程见 Python中文网 http://www.zglg.work,"介绍告别枯燥，告别枯燥，致力于打造 Python 经典小例子、小案例。 License允许按照要求转载，但禁止用于任何商用目的。如果转载本库小例子、小案例，请备注下方链接：Python小例子 https://github.com/jackzhenguo/python-small-examplesPython 小例子基本操作| 小例子 | 链接                    | 标签              | 版本          | 难度 || ---- | ---------------------------------- | ---- | ---- | ---- ||1   | |	运算  |	v1|	⭐⭐ || 2    |  | max   | V4.0        | ⭐️⭐️ || 3    |                 | bin,oct,hex | V4.0            |  ⭐️⭐️|| 4    |          | chr,ord | V1.0     | ⭐️⭐️ || 5    |           | all   | V2.0      | ⭐️⭐️⭐️ || 6    |         | any | V2.0    | ⭐️⭐️⭐️ || 7    |             | bool        | V2.0        | ⭐️⭐️⭐️ || 8    |                 | complex         | V1.0            | ⭐️⭐️⭐️ || 9    |               | divmod        | V1.0          | ⭐️⭐️ || 10    |             | float       | V1.0        | ⭐️⭐️ || 11   |                | int            | V1.0           | ⭐️ || 12   |                    | pow                | V1.0               | ⭐️ || 13   |                | round          | V1.0           | ⭐️ || 14   |                | compare       | V1.0           | ⭐️⭐️ || 15   |            | bytes,utf-8 | V1.0       | ⭐️⭐️ || 16   |      | str  | V1.0 | ⭐️⭐️ || 17   |    | compile | V1.0 | ⭐️⭐️⭐️ || 18   |              | eval         | V1.0         | ⭐️⭐️⭐️⭐️ || 19   |            | format     | V1.0       | ⭐️⭐️⭐️⭐️ || 20   |              | pack,unpack | V1.0         | ⭐️⭐️ || 21   |                | dict           | V1.0           | ⭐️⭐️ || 22   |                | frozenset | V1.0           | ⭐️⭐️ || 23   |            | set        | V1.0       | ⭐️⭐️ || 24   |                  | tuple            | V1.0             | ⭐️⭐️ || 25   |   | getsizeof | V1.0 | ⭐️⭐️⭐️ || 26 |  | tuple | V1.0 | ⭐️⭐ || 27 |  | list | V1.0 | ⭐️⭐ || 28 |  | list  | V1.0 | ⭐️⭐⭐ || 29 |  | list heapq | v1.0 | ⭐️⭐⭐⭐ || 30 |  | str  | V1.0 | ⭐️⭐⭐⭐⭐ || 31 |  | set  | V1.0 | ⭐️⭐ || 32 |  | for  | V1.0 | ⭐️⭐⭐ || 33 |  | generator  | V1.0 | ⭐️⭐⭐⭐⭐ |函数和模块常见用法| 小例子 | 链接                    | 标签              | 版本          | 难度 || ---- | ---------------------------------- | ---- | ---- | ---- || 1   |            | operator   | V2.0       | ⭐️⭐️⭐️⭐️ || 2   |  | range | V1.0 | ⭐️⭐️ || 3   |            | range | V1.0       | ⭐️⭐️ || 4   |      | sorted | V1.0 | ⭐️⭐️⭐️ || 5   |                | sum            | V1.0           | ⭐️⭐️ || 6   |  | variable parameter | V2.0 | ⭐️⭐️⭐️⭐️ || 7   |           | slice     | V2.0      | ⭐️⭐️⭐️⭐️⭐️ || 8   |   | lambda | V3.0 | ⭐️⭐️⭐️⭐️ || 9   |        | enumerate | V1.0   | ⭐️⭐️⭐️ || 10   |   | filter | V1.5 | ⭐️⭐️⭐️ || 11   |     | hash | V1.0 | ⭐️⭐️ || 12   |  | namedtuple | V1.0 | ⭐️⭐️⭐️ || 13   |  | reverse | V1.0 | ⭐️⭐️ || 14   |  | reversed | V1.0 | ⭐️⭐️ || 15   |  | join | V1.0 | ⭐️⭐️ || 16   |  | encode | V1.0 | ⭐️⭐️ || 17 |  | itertools, groupby,lambda | V1.0 | ⭐️⭐️⭐️ || 18 |  | itemgetter,itertools,groupby | V1.0 | ⭐️⭐️⭐️⭐️ || 19 |  | operator,itemgetter,itertools | V1.0 | ⭐️⭐️⭐️⭐️⭐️ || 20 |  | sum,generator | V1.0 | ⭐️⭐️⭐️⭐️⭐️ || 21 |  | function | V1.0 | ⭐️⭐⭐ || 22 |  | function paremeter | V1.0 | ⭐️⭐⭐ || 23 |  | lambda | V1.0 | ⭐️⭐⭐ || 24 |  | sort heapq | v1.0 | ⭐️⭐⭐⭐ |面向对象| 小例子 | 链接                    | 标签              | 版本          | 难度 || ---- | ---------------------------------- | ---- | ---- | ---- || 1   |            | object     | V1.0       | ⭐️ || 2   |          | callable | V2.5   | ⭐️⭐️⭐️⭐️ || 3   |          |  | V2.5     | ⭐️⭐️⭐️ || 4   |                  | classmethod      | V1.5             | ⭐️⭐️⭐️ || 5   |            | delattr,hasattr | V1.5       | ⭐️⭐️ || 6   |    | dir | V1.5 | ⭐️⭐️ || 7   |        | getattr | V1.5   | ⭐️⭐️ || 8   |      | hasattr | V1.5 | ⭐️⭐️⭐️ || 9   |              | id           | V1.0         | ⭐️ || 10   |     | isinstance   | V1.5         | ⭐️⭐️⭐️ || 11   |  | issubclass | V1.5 | ⭐️⭐️⭐️ || 12   |      | property | V2.5 | ⭐️⭐️⭐️⭐️⭐️ || 13   |            | type      | V1.0       | ⭐️ || 14   |      | type, | V2.0 | ⭐️⭐️⭐️⭐️⭐️ || 15 |  | mutable  | V1.0 | ⭐️⭐⭐ || 16 |  | OOP del   | V1.0 | ⭐️⭐⭐⭐ || 17 |  | staticmethod | V1.0 | ⭐️⭐⭐ |正则| 小例子 | 链接                    | 标签              | 版本          | 难度 || ---- | ---------------------------------- | ---- | ---- | ---- || 1   |  | re,r | V3.0 | ⭐️⭐️⭐️ || 2   |  | re | V3.0 | ⭐️⭐️⭐️ || 3   |  | re,\ | V3.0 | ⭐️⭐️⭐️ || 4   |  | re,findall | V3.0 | ⭐️⭐️⭐️ || 5   |  | re,\s,\w,\d | V3.0 | ⭐️⭐️⭐️ || 6   |  | re,+,* | V3.0 | ⭐️⭐️⭐️ || 7   |  | () | V3.0 | ⭐️⭐️⭐️⭐️ || 8   |  | re | V1.0 | ⭐️⭐️⭐️⭐️ || 9   |  | re | V1.0 | ⭐️⭐️⭐️⭐️⭐️ || 10   |  | re | V1.0 | ⭐️⭐️⭐️⭐️ || 11   |  | re | V1.0 | ⭐️⭐️⭐️⭐️⭐️ || 12   |  | str,re,float | V1.0 | ⭐️⭐️⭐️⭐️⭐️ || 13 |  | re findall | v2 | ⭐️⭐⭐⭐ |装饰器迭代器生成器| 小例子 | 链接                    | 标签              | 版本          | 难度 || ---- | ---------------------------------- | ---- | ---- | ---- || 1 |  | decorator | V1.0 | ⭐️⭐️⭐️ || 2 |  | decorator | V1.0 | ⭐️⭐️⭐️⭐️ || 3 |  | decorator,nonlocal | V1.5 | ⭐️⭐️⭐️⭐️ || 4 |  | Iterator | V3.0 | ⭐️⭐️⭐️⭐️ || 5   |       | iter, | V1.5  | ⭐️⭐️⭐️ || 6   |  | reversed | V1.0 | ⭐️⭐️ || 7   |      | zip  | V1.5 | ⭐️⭐️⭐️ || 8 |  | yield,generator | V1.0 | ⭐️⭐️⭐️ || 9 |  | list,yield,generator | V1.0 | ⭐️⭐️⭐️ || 10   |  | itertools,chain | V1.0 | ⭐️⭐️⭐️⭐️⭐️ || 11   |  | product | V1.0 | ⭐️⭐️⭐️⭐️⭐️ || 12 |  | yield,range | V1.0 | ⭐️⭐️⭐️ |绘图| 小例子 | 链接                    | 标签              | 版本          | 难度 || ---- | ---------------------------------- | ---- | ---- | ---- || 1 |  | turtle | V1.0 | ⭐️⭐️⭐️ || 2 |  | turtle | V1.0 | ⭐️⭐️⭐️ || 3 |  | WordCloud | V1.0 | ⭐️⭐️⭐ || 4 |  | plotly | V1.0 | ⭐️⭐ || 5 |  | seaborn | V1.0 | ⭐️⭐ || 6 |  | pyecharts | V1.0 | ⭐️⭐ || 7 |  | pyecharts | V1.0 | ⭐️⭐ || 8 |  | pyecharts | V1.0 | ⭐️⭐ || 9 |  | pyecharts | V1.0 | ⭐️⭐ || 10 |  | pyecharts | V1.0 | ⭐️⭐ || 11 |  | pyecharts | V1.0 | ⭐️⭐ || 12 |  | pyecharts | V1.0 | ⭐️⭐ || 13 |  | matplotlib | V1.0 | ⭐️⭐ || 14 |  | seaborn | V1.0 | ⭐️⭐⭐⭐ || 15 |  | numpy pyecharts  | V1.0 | ⭐️⭐⭐ || 16 |  | pillow  | V1.0 | ⭐️⭐⭐ |数据分析| 小例子 | 链接                    | 标签              | 版本          | 难度 || ---- | ---------------------------------- | ---- | ---- | ---- || 1 |  | deepnote | v1.0 | ⭐️⭐⭐ || 2 |  | NumPy pad | V1.0 | ⭐️⭐⭐⭐ || 3 |  | NumPy diag | V1.0 | ⭐️⭐⭐ || 4 |  | Pandas cut | v1.0 | ⭐️⭐⭐ || 5 |  | Pandas dropna fillna | v1.0 | ⭐️⭐⭐ || 6 |  | pandas apply | v1.0 | ⭐️⭐⭐ || 7 |  | pandas map | v1.0 | ⭐️⭐⭐ || 8 |  | pandas category | v1.0 | ⭐️⭐⭐ || 9 |  | pandas rank | v1.0 | ⭐️⭐⭐|| 10 |  | pandas resample | v1.0 | ⭐️⭐⭐ || 11 |  | pandas util | v1.0 | ⭐️⭐⭐ || 12 |  | pandas isnull sum | v1.0 | ⭐️⭐⭐ || 13 |  | pandas dataframe | v1.0 | ⭐️⭐⭐ || 14 |  | pandas count | v1.0 | ⭐️⭐⭐ || 15 |  | pandas split | v1.0 | ⭐️⭐⭐ || 16 |  | pandas melt | v1.0 | ⭐️⭐⭐ || 17 |  | pandas melt | v1.0 | ⭐️⭐⭐ || 18 |  | pandas sample | v1.0 | ⭐️⭐⭐ || 19 |  | pandas apply | v1.0 | ⭐️⭐⭐⭐ |其他常用| 小例子 | 链接                    | 标签              | 版本          | 难度 || ---- | ---------------------------------- | ---- | ---- | ---- || 1   |   | help | V1.0 | ⭐️ || 2   |      | input | V1.0 | ⭐️ || 3   |  | open,read,write,with,mode | V2.0 | ⭐️⭐️⭐️ || 4   |  | operator | V1.0 | ⭐️⭐️⭐️⭐️ || 5   |   | json | V2.0 | ⭐️⭐️⭐️⭐️⭐️ || 6   |  | os,splitext | V1.0 | ⭐️⭐️ || 7   |  | os,split | V1.0 | ⭐️⭐️ || 8   |  | argparse,listdir | V1.0 | ⭐️⭐️⭐️⭐️ || 9   |  | os,listdir,splitext | V1.0 | ⭐️⭐️⭐️⭐️ || 10   |  | os,listdir,splitext | V1.0 | ⭐️⭐️⭐️⭐️ || 11   |  | zipfile | V1.0 | ⭐️⭐️⭐️⭐️ || 12   |  | hashlib | V1.0 | ⭐️⭐️⭐️⭐️ || 13   |  | calendar | V1.0 | ⭐️⭐️ || 14   |  | calendar | V1.0 | ⭐️⭐️⭐️ || 15   |  | calendar,datetime | V1.0 | ⭐️⭐️⭐️ || 16   |  | datetime | V1.0 | ⭐️⭐️ || 17 |  | calendar,datetime | V1.0 | ⭐️⭐️ || 18 |  | time,datetime | V1.0 | ⭐️⭐️ || 19 |  | time,datetime | V1.0 | ⭐️⭐️ || 20 |  | time,datetime | V1.0 | ⭐️⭐️ || 21 |  | Calendar,monthrange | V4.0 | ⭐️⭐️⭐️ || 22 |  | threading | V1.0 | ⭐️⭐️ || 23 |  | threading | V1.0 | ⭐️⭐️ || 24 |  | threading | V1.0 | ⭐️⭐️⭐️ || 25 |  | threading | V1.0 | ⭐️⭐️⭐️ || 26 |  | threading | V1.0 | ⭐️⭐️⭐️ || 27 |  | threading,lock | V1.0 | ⭐️⭐️⭐️ || 28 |  | time,datetime,format | V1.0 | ⭐️⭐️⭐️ || 29   |  | nonlocal | V2.0 | ⭐️⭐️⭐️⭐️⭐️ || 30   |     | global | V2.0 | ⭐️⭐️⭐️⭐️⭐️ || 31 |  | global | V1.0 | ⭐️⭐⭐ || 32 |  | debugger  | V1.0 | ⭐️⭐⭐ || 33 |  | chardet  | V1.0 | ⭐️⭐⭐ || 34 |  | SQLite | v1.0 | ⭐️⭐⭐⭐ || 35 |  | python json | v1.0 | ⭐️⭐⭐⭐ || 36 |  | python json | v1.0 | ⭐️⭐⭐⭐ || 37 |  | pip install | v1.0 | ⭐️⭐⭐ |工作常用案例| 小例子 | 链接                    | 标签              | 版本          | 难度 || ---- | ---------------------------------- | ---- | ---- | ---- || 1   |  | operator | V1.0 | ⭐️⭐️⭐️ || 2   |       | list,sort,round | V1.0  | ⭐️⭐️⭐️⭐️ || 3   |     | for,range,format | V1.0 | ⭐️⭐️⭐️ || 4   |  | recursion,list,isinstance | V1.0 | ⭐️⭐️⭐️⭐️ || 5   |   | list,ceil | V1.0 | ⭐️⭐️⭐️ || 6   |        | list,filter | V1.0   | ⭐️⭐️⭐️⭐️ || 7   |      | max,lambda | V1.0 | ⭐️⭐️⭐️⭐️⭐️ || 8   |      | max,lambda,count | V1.0 | ⭐️⭐️⭐️⭐️ || 9   |  | max,lambda | V1.0 | ⭐️⭐️⭐️⭐️ || 10   |      | set  | V1.0 | ⭐️⭐️⭐️ || 11   |     | range,float | V1.0 | ⭐️⭐️⭐️⭐️ || 12   |       | lambda | V1.0  | ⭐️⭐️⭐️⭐️ || 13   |   | map,lambda | V1.0 | ⭐️⭐️⭐️ || 14   |      | max,lambda | V1.0 | ⭐️⭐️⭐️⭐️ || 15   |      | **   | V1.0 | ⭐️⭐️⭐️ || 16   |     | heapq,nlargest | V1.0 | ⭐️⭐️⭐️ || 17   |  | collections,Counter | V1.0 | ⭐️⭐️⭐️ || 18   |  | ChainMap | V1.0 | ⭐️⭐️⭐️⭐️⭐️ || 19   |  | random,sample | V1.0 | ⭐️⭐️⭐️ || 20   |  | shuffle | V1.0 | ⭐️⭐️⭐️ || 21   |  | random,uniform | V1.0 | ⭐️⭐️⭐️ || 22   |  | random,gauss | V1.0 | ⭐️⭐️⭐️⭐️ || 23   |  | collections,defaultdict | V1.0 | ⭐️⭐️⭐️⭐️ || 24   |  | str | V1.0 | ⭐️⭐️⭐️ || 25 |  | enumerator | V1.0 | ⭐️⭐️⭐️ || 26 |  | calendar,datetime | V1.0 | ⭐️⭐️⭐️⭐️ || 27 |  | Counter | V1.0 | ⭐️⭐️⭐️⭐️⭐️ || 28 |  | math asin | V1.0 | ⭐️⭐️⭐️⭐️⭐️ || 29 |  | chardet | V1.0 | ⭐️⭐️⭐️⭐️⭐️ || 30 |  | json | V1.0 | ⭐️⭐️⭐️⭐️⭐️ |"
https://github.com/uber/causalml,Uplift modeling and causal inference with machine learning algorithms,"DisclaimerThis project is stable and being incubated for long-term support. It may contain new experimental code, for which APIs are subject to change.Causal ML: A Python Package for Uplift Modeling and Causal Inference with MLCausal ML is a Python package that provides a suite of uplift modeling and causal inference methods using machine learning algorithms based on recentresearch . It provides a standard interface that allows user to estimate the Conditional Average Treatment Effect (CATE) or Individual TreatmentEffect (ITE) from experimental or observational data. Essentially, it estimates the causal impact of intervention  on outcome  for userswith observed features , without strong assumptions on the model form. Typical use cases includeThe package currently supports the following methodsInstallationInstallation with  is recommended. environment files for Python 3.7, 3.8 and 3.9 are available in the repository. To use models under the  module (e.g. ), additional dependency of  is required. For detailed instructions, see below.Install using :Install  with:wget https://repo.anaconda.com/miniconda/Miniconda3-py38_23.5.0-3-Linux-x86_64.sh
bash Miniconda3-py38_23.5.0-3-Linux-x86_64.sh -b
source miniconda3/bin/activate 
conda init
source ~/.bashrc 
Install from Directly install from the conda-forge channel using conda.conda install -c conda-forge causalml
Install with the  virtual environmentThis will create a new  virtual environment named , where  is in . e.g.  or . If you want to change the name of the environment, update the relevant YAML file in git clone https://github.com/uber/causalml.git
cd causalml/envs/
conda env create -f environment-py38.yml	# for the virtual environment with Python 3.8 and CausalML
conda activate causalml-py38
(causalml-py38)
Install  with git clone https://github.com/uber/causalml.git
cd causalml/envs/
conda env create -f environment-tf-py38.yml	# for the virtual environment with Python 3.8 and CausalML
conda activate causalml-tf-py38
(causalml-tf-py38) pip install -U numpy			# this step is necessary to fix [#338](https://github.com/uber/causalml/issues/338)
Install from :pip install causalml
Install  with pip install causalml[tf]
pip install -U numpy							# this step is necessary to fix [#338](https://github.com/uber/causalml/issues/338)
Install from source:Create a clean conda environmentconda create -n causalml-py38 python=3.8
conda activate causalml-py38
conda install -c conda-forge cxx-compiler
conda install python-graphviz
conda install -c conda-forge xorg-libxrender
Then:git clone https://github.com/uber/causalml.git
cd causalml
pip install .
python setup.py build_ext --inplace
with :pip install .[tf]
Quick StartAverage Treatment Effect Estimation with S, T, X, and R Learnersfrom causalml.inference.meta import LRSRegressor
from causalml.inference.meta import XGBTRegressor, MLPTRegressor
from causalml.inference.meta import BaseXRegressor
from causalml.inference.meta import BaseRRegressor
from xgboost import XGBRegressor
from causalml.dataset import synthetic_data

y, X, treatment, _, _, e = synthetic_data(mode=1, n=1000, p=5, sigma=1.0)

lr = LRSRegressor()
te, lb, ub = lr.estimate_ate(X, treatment, y)
print('Average Treatment Effect (Linear Regression): {:.2f} ({:.2f}, {:.2f})'.format(te[0], lb[0], ub[0]))

xg = XGBTRegressor(random_state=42)
te, lb, ub = xg.estimate_ate(X, treatment, y)
print('Average Treatment Effect (XGBoost): {:.2f} ({:.2f}, {:.2f})'.format(te[0], lb[0], ub[0]))

nn = MLPTRegressor(hidden_layer_sizes=(10, 10),
                 learning_rate_init=.1,
                 early_stopping=True,
                 random_state=42)
te, lb, ub = nn.estimate_ate(X, treatment, y)
print('Average Treatment Effect (Neural Network (MLP)): {:.2f} ({:.2f}, {:.2f})'.format(te[0], lb[0], ub[0]))

xl = BaseXRegressor(learner=XGBRegressor(random_state=42))
te, lb, ub = xl.estimate_ate(X, treatment, y, e)
print('Average Treatment Effect (BaseXRegressor using XGBoost): {:.2f} ({:.2f}, {:.2f})'.format(te[0], lb[0], ub[0]))

rl = BaseRRegressor(learner=XGBRegressor(random_state=42))
te, lb, ub =  rl.estimate_ate(X=X, p=e, treatment=treatment, y=y)
print('Average Treatment Effect (BaseRRegressor using XGBoost): {:.2f} ({:.2f}, {:.2f})'.format(te[0], lb[0], ub[0]))
See the  for details.Interpretable Causal MLCausal ML provides methods to interpret the treatment effect models trained as follows:Meta Learner Feature Importancesfrom causalml.inference.meta import BaseSRegressor, BaseTRegressor, BaseXRegressor, BaseRRegressor
from causalml.dataset.regression import synthetic_data

# Load synthetic data
y, X, treatment, tau, b, e = synthetic_data(mode=1, n=10000, p=25, sigma=0.5)
w_multi = np.array(['treatment_A' if x==1 else 'control' for x in treatment]) # customize treatment/control names

slearner = BaseSRegressor(LGBMRegressor(), control_name='control')
slearner.estimate_ate(X, w_multi, y)
slearner_tau = slearner.fit_predict(X, w_multi, y)

model_tau_feature = RandomForestRegressor()  # specify model for model_tau_feature

slearner.get_importance(X=X, tau=slearner_tau, model_tau_feature=model_tau_feature,
                        normalize=True, method='auto', features=feature_names)

# Using the feature_importances_ method in the base learner (LGBMRegressor() in this example)
slearner.plot_importance(X=X, tau=slearner_tau, normalize=True, method='auto')

# Using eli5's PermutationImportance
slearner.plot_importance(X=X, tau=slearner_tau, normalize=True, method='permutation')

# Using SHAP
shap_slearner = slearner.get_shap_values(X=X, tau=slearner_tau)

# Plot shap values without specifying shap_dict
slearner.plot_shap_values(X=X, tau=slearner_tau)

# Plot shap values WITH specifying shap_dict
slearner.plot_shap_values(X=X, shap_dict=shap_slearner)

# interaction_idx set to 'auto' (searches for feature with greatest approximate interaction)
slearner.plot_shap_dependence(treatment_group='treatment_A',
                              feature_idx=1,
                              X=X,
                              tau=slearner_tau,
                              interaction_idx='auto')
See the  for details.Uplift Tree Visualizationfrom IPython.display import Image
from causalml.inference.tree import UpliftTreeClassifier, UpliftRandomForestClassifier
from causalml.inference.tree import uplift_tree_string, uplift_tree_plot

uplift_model = UpliftTreeClassifier(max_depth=5, min_samples_leaf=200, min_samples_treatment=50,
                                    n_reg=100, evaluationFunction='KL', control_name='control')

uplift_model.fit(df[features].values,
                 treatment=df['treatment_group_key'].values,
                 y=df['conversion'].values)

graph = uplift_tree_plot(uplift_model.fitted_uplift_tree, features)
Image(graph.create_png())
See the  for details.ContributingWe welcome community contributors to the project. Before you start, please read our  and check out  first.VersioningWe document versions and changes in our .LicenseThis project is licensed under the Apache 2.0 License - see the  file for details.ReferencesDocumentationConference Talks and Publications by CausalML TeamCitationTo cite CausalML in publications, you can refer to the following sources:Whitepaper:Bibtex:LiteratureRelated projects"
https://github.com/xinntao/ESRGAN,ECCV18 Workshops - Enhanced SRGAN. Champion PIRM Challenge on Perceptual Super-Resolution. The training codes are in BasicSR.,"ESRGAN (Enhanced SRGAN) [:rocket: ] []:sparkles: New Updates.We have extended ESRGAN to , which is a more practical algorithm for real-world image restoration. For example, it can also remove annoying JPEG compression artifacts.  You are recommended to have a try :smiley:In the  repo,Welcome to open issues or open discussions in the  repo.Here are some examples for Real-ESRGAN:As there may be some repos have dependency on this ESRGAN repo, we will not modify this ESRGAN repo (especially the codes).The following is the original README:The training codes are in :rocket: . This repo only provides simple testing codes, pretrained models and the network interpolation demo. is an open source image and video super-resolution toolbox based on PyTorch (will extend to more restoration tasks in the future). It includes methods such as EDSR, RCAN, SRResNet, SRGAN, ESRGAN, EDVR, etc. It now also supports StyleGAN2.Enhanced Super-Resolution Generative Adversarial NetworksBy Xintao Wang, , Shixiang Wu, , Yihao Liu, , , We won the first place in  (region 3) and got the best perceptual index.The paper is accepted to .:triangular_flag_on_post: Add .BibTeX@InProceedings{wang2018esrgan,
    author = {Wang, Xintao and Yu, Ke and Wu, Shixiang and Gu, Jinjin and Liu, Yihao and Dong, Chao and Qiao, Yu and Loy, Chen Change},
    title = {ESRGAN: Enhanced super-resolution generative adversarial networks},
    booktitle = {The European Conference on Computer Vision Workshops (ECCVW)},
    month = {September},
    year = {2018}
}
The RRDB_PSNR PSNR_oriented model trained with DF2K dataset (a merged dataset with  and  (proposed in )) is also able to achive high PSNR performance.| Method | Training dataset | Set5 | Set14 | BSD100 | Urban100 | Manga109 ||:---:|:---:|:---:|:---:|:---:|:---:|:---:|| | 291| 30.48/0.8628 |27.50/0.7513|26.90/0.7101|24.52/0.7221|27.58/0.8555||  | DIV2K | 32.46/0.8968 | 28.80/0.7876 | 27.71/0.7420 | 26.64/0.8033 | 31.02/0.9148 ||  |  DIV2K | 32.63/0.9002 | 28.87/0.7889 | 27.77/0.7436 | 26.82/ 0.8087| 31.22/ 0.9173||RRDB(ours)| DF2K| 32.73/0.9011 |28.99/0.7917 |27.85/0.7455 |27.03/0.8153 |31.66/0.9196|Quick TestDependenciesTest modelsgit clone https://github.com/xinntao/ESRGAN
cd ESRGAN
python test.py
Network interpolation demoYou can interpolate the RRDB_ESRGAN and RRDB_PSNR models with alpha in [0, 1].Perceptual-driven SR ResultsYou can download all the resutls from . (:heavy_check_mark: included;  :heavy_minus_sign: not included; :o: TODO)HR images can be downloaed from .| Datasets |LR |  |  |  |  ||:---:|:---:|:---:|:---:|:---:|:---:|| Set5 |:heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark:| :o: || Set14 | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark:| :o: || BSDS100 | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark:| :o: ||  (val, test) | :heavy_check_mark: | :heavy_check_mark: | :heavy_minus_sign: | :heavy_check_mark:| :heavy_check_mark: ||  |:heavy_check_mark: | :heavy_check_mark: | :heavy_minus_sign: | :heavy_check_mark:| :o: || urban100 | :heavy_check_mark: | :heavy_check_mark: | :heavy_minus_sign: | :heavy_check_mark:| :o: ||  (val, test) | :heavy_check_mark: | :heavy_check_mark: | :heavy_minus_sign: | :heavy_check_mark:| :o: |ESRGANWe improve the  from three aspects:In contrast to SRGAN, which claimed that deeper models are increasingly difficult to train, our deeper ESRGAN model shows its superior performance with easy training.Network InterpolationWe propose the network interpolation strategy to balance the visual quality and PSNR.We show the smooth animation with the interpolation parameters changing from 0 to 1.Interestingly, it is observed that the network interpolation strategy provides a smooth control of the RRDB_PSNR model and the fine-tuned ESRGAN model.Qualitative ResultsPSNR (evaluated on the Y channel) and the perceptual index used in the PIRM-SR challenge are also provided for reference.Ablation StudyOverall visual comparisons for showing the effects of each component inESRGAN. Each column represents a model with its configurations in the top.The red sign indicates the main improvement compared with the previous model.BN artifactsWe empirically observe that BN layers tend to bring artifacts. These artifacts,namely BN artifacts, occasionally appear among iterations and different settings,violating the needs for a stable performance over training. We find thatthe network depth, BN position, training dataset and training losshave impact on the occurrence of BN artifacts.Useful techniques to train a very deep networkWe find that residual scaling and smaller initialization can help to train a very deep network. More details are in the Supplementary File attached in our .The influence of training patch sizeWe observe that training a deeper network benefits from a larger patch size. Moreover, the deeper model achieves more improvement (∼0.12dB) than the shallower one (∼0.04dB) since larger model capacity is capable of taking full advantage oflarger training patch size. (Evaluated on Set5 dataset with RGB channels.)"
https://github.com/rougier/matplotlib-cheatsheet,Matplotlib 3.1 cheat sheet. ,Brand new cheatsheets and handoutsSources now at https://github.com/matplotlib/cheatsheets Book at https://github.com/rougier/scientific-visualization-book
https://github.com/ThilinaRajapakse/simpletransformers,"Transformers for Classification, NER, QA, Language Modelling, Language Generation, T5, Multi-Modal, and Conversational AI"," Simple TransformersThis library is based on the  library by HuggingFace.  lets you quickly train and evaluate Transformer models. Only 3 lines of code are needed to initialize, train, and evaluate a model.Supported Tasks:Table of contentsSetupWith Conda$ conda create -n st python pandas tqdm
$ conda activate st
Using Cuda:$ conda install pytorch>=1.6 cudatoolkit=11.0 -c pytorch
Without using Cuda$ conda install pytorch cpuonly -c pytorch
$ pip install simpletransformers
Optional$ pip install wandb
UsageAll documentation is now live at  models are built with a particular Natural Language Processing (NLP) task in mind. Each such model comes equipped with features and functionality designed to best fit the task that they are intended to perform. The high-level process of using Simple Transformers models follows the same pattern.However, there are necessary differences between the different models to ensure that they are well suited for their intended task. The key differences will typically be the differences in input/output data formats and any task specific features/configuration options. These can all be found in the documentation section for each task.The currently implemented task-specific  models, along with their task, are given below.| Task                                                      | Model                           || --------------------------------------------------------- | ------------------------------- || Binary and multi-class text classification                |            || Conversational AI (chatbot training)                      |                    || Language generation                                       |        || Language model training/fine-tuning                       |          || Multi-label text classification                           |  || Multi-modal classification (text and image data combined) |  || Named entity recognition                                  |                       || Question answering                                        |         || Regression                                                |            || Sentence-pair classification                              |            || Text Representation Generation                            |            || Document Retrieval                                        |                 |A quick examplefrom simpletransformers.classification import ClassificationModel, ClassificationArgs
import pandas as pd
import logging


logging.basicConfig(level=logging.INFO)
transformers_logger = logging.getLogger(""transformers"")
transformers_logger.setLevel(logging.WARNING)

# Preparing train data
train_data = [
    [""Aragorn was the heir of Isildur"", 1],
    [""Frodo was the heir of Isildur"", 0],
]
train_df = pd.DataFrame(train_data)
train_df.columns = [""text"", ""labels""]

# Preparing eval data
eval_data = [
    [""Theoden was the king of Rohan"", 1],
    [""Merry was the king of Rohan"", 0],
]
eval_df = pd.DataFrame(eval_data)
eval_df.columns = [""text"", ""labels""]

# Optional model configuration
model_args = ClassificationArgs(num_train_epochs=1)

# Create a ClassificationModel
model = ClassificationModel(
    ""roberta"", ""roberta-base"", args=model_args
)

# Train the model
model.train_model(train_df)

# Evaluate the model
result, model_outputs, wrong_predictions = model.eval_model(eval_df)

# Make predictions with the model
predictions, raw_outputs = model.predict([""Sam was a Wizard""])

Experiment Tracking with Weights and BiasesCurrent Pretrained ModelsFor a list of pretrained models, see .The  available for each task can be found under their respective section. Any pretrained model of that typefound in the Hugging Face docs should work. To use any of them set the correct  and  in the dictionary.Contributors ✨Thanks goes to these wonderful people ():This project follows the  specification. Contributions of any kind welcome!If you should be on this list but you aren't, or you are on the list but don't want to be, please don't hesitate to contact me!How to ContributeHow to Update DocsThe latest version of the docs is hosted on , if you want to help document Simple Transformersbelow are the steps to edit the docs.Docs are built using  library, refer to their webpage for a detailed explanation of how it works.AcknowledgementsNone of this would have been possible without the hard work by the HuggingFace team in developing the  library.<div>"
https://github.com/laramies/theHarvester,"E-mails, subdomains and names Harvester - OSINT "," What is this?theHarvester is a simple to use, yet powerful tool designed to be used during the reconnaissance stage of a redteam assessment or penetration test. It performs open source intelligence (OSINT) gathering to help determinea domain's external threat landscape. The tool gathers names, emails, IPs, subdomains, and URLs by usingmultiple public resources that include:Passive modules:Active modules:Modules that require an API key:Documentation to setup API keys can be found at - https://github.com/laramies/theHarvester/wiki/Installation#api-keysInstall and dependencies:Comments, bugs, and requests:Main contributors:Thanks:"
https://github.com/tensorflow/nmt,TensorFlow Neural Machine Translation Tutorial,"Neural Machine Translation (seq2seq) TutorialAuthors: Thang Luong, Eugene Brevdo, Rui Zhao (This version of the tutorial requires If make use of this codebase for your research, please citeIntroductionSequence-to-sequence (seq2seq) models(,) haveenjoyed great success in a variety of tasks such as machine translation, speechrecognition, and text summarization. This tutorial gives readers a fullunderstanding of seq2seq models and shows how to build a competitive seq2seqmodel from scratch. We focus on the task of Neural Machine Translation (NMT)which was the very first testbed for seq2seq models withwild. Theincluded code is lightweight, high-quality, production-ready, and incorporatedwith the latest research ideas. We achieve this goal by:We believe that it is important to provide benchmarks that people can easilyreplicate. As a result, we have provided full experimental results andpretrained our models on the following publicly available datasets:We first build up some basic knowledge about seq2seq models for NMT, explaininghow to build and train a vanilla NMT model. The second part will go into detailsof building a competitive NMT model with attention mechanism. We then discusstips and tricks to build the best possible NMT models (both in speed andtranslation quality) such as TensorFlow best practices (batching, bucketing),bidirectional RNNs, beam search, as well as scaling up to multiple GPUs using GNMT attention.BasicBackground on Neural Machine TranslationBack in the old days, traditional phrase-based translation systems performedtheir task by breaking up source sentences into multiple chunks and thentranslated them phrase-by-phrase. This led to disfluency in the translationoutputs and was not quite like how we, humans, translate. We read the entiresource sentence, understand its meaning, and then produce a translation. NeuralMachine Translation (NMT) mimics that!Specifically, an NMT system first reads the source sentence using an encoderto builda,a sequence of numbers that represents the sentence meaning; a decoder, then,processes the sentence vector to emit a translation, as illustrated inFigure 1. This is often referred to as the encoder-decoder architecture. Inthis manner, NMT addresses the local translation problem in the traditionalphrase-based approach: it can capture long-range dependencies in languages,e.g., gender agreements; syntax structures; etc., and produce much more fluenttranslations as demonstratedby.NMT models vary in terms of their exact architectures. A natural choice forsequential data is the recurrent neural network (RNN), used by most NMT models.Usually an RNN is used for both the encoder and decoder. The RNN models,however, differ in terms of: (a) directionality – unidirectional orbidirectional; (b) depth – single- or multi-layer; and (c) type – ofteneither a vanilla RNN, a Long Short-term Memory (LSTM), or a gated recurrent unit(GRU). Interested readers can find more information about RNNs and LSTM onthis .In this tutorial, we consider as examples a deep multi-layer RNN which isunidirectional and uses LSTM as a recurrent unit. We show an example of such amodel in Figure 2. In this example, we build a model to translate a sourcesentence ""I am a student"" into a target sentence ""Je suis étudiant"". At a highlevel, the NMT model consists of two recurrent neural networks: the encoderRNN simply consumes the input source words without making any prediction; thedecoder, on the other hand, processes the target sentence while predicting thenext words.For more information, we refer readersto  which this tutorial isbased on.Installing the TutorialTo install this tutorial, you need to have TensorFlow installed on your system.This tutorial requires TensorFlow Nightly. To install TensorFlow, followthe .Once TensorFlow is installed, you can download the source code of this tutorialby running:git clone https://github.com/tensorflow/nmt/
Training – How to build our first NMT systemLet's first dive into the heart of building an NMT model with concrete codesnippets through which we will explain Figure 2 in more detail. We defer datapreparation and the full code to later. This part refers tofile.At the bottom layer, the encoder and decoder RNNs receive as input thefollowing: first, the source sentence, then a boundary marker ""s"" whichindicates the transition from the encoding to the decoding mode, and the targetsentence.  For training, we will feed the system with the following tensors,which are in time-major format and contain word indices:Here for efficiency, we train with multiple sentences (batch_size) atonce. Testing is slightly different, so we will discuss it later.EmbeddingGiven the categorical nature of words, the model must first look up the sourceand target embeddings to retrieve the corresponding word representations. Forthis embedding layer to work, a vocabulary is first chosen for each language.Usually, a vocabulary size V is selected, and only the most frequent V words aretreated as unique.  All other words are converted to an ""unknown"" token and allget the same embedding.  The embedding weights, one set per language, areusually learned during training.# Embedding
embedding_encoder = variable_scope.get_variable(
    ""embedding_encoder"", [src_vocab_size, embedding_size], ...)
# Look up embedding:
#   encoder_inputs: [max_time, batch_size]
#   encoder_emb_inp: [max_time, batch_size, embedding_size]
encoder_emb_inp = embedding_ops.embedding_lookup(
    embedding_encoder, encoder_inputs)
Similarly, we can build embedding_decoder and decoder_emb_inp. Note that onecan choose to initialize embedding weights with pretrained word representationssuch as word2vec or Glove vectors. In general, given a large amount of trainingdata we can learn these embeddings from scratch.EncoderOnce retrieved, the word embeddings are then fed as input into the main network,which consists of two multi-layer RNNs – an encoder for the source language anda decoder for the target language. These two RNNs, in principle, can share thesame weights; however, in practice, we often use two different RNN parameters(such models do a better job when fitting large training datasets). Theencoder RNN uses zero vectors as its starting states and is built as follows:# Build RNN cell
encoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)

# Run Dynamic RNN
#   encoder_outputs: [max_time, batch_size, num_units]
#   encoder_state: [batch_size, num_units]
encoder_outputs, encoder_state = tf.nn.dynamic_rnn(
    encoder_cell, encoder_emb_inp,
    sequence_length=source_sequence_length, time_major=True)
Note that sentences have different lengths to avoid wasting computation, we telldynamic_rnn the exact source sentence lengths throughsource_sequence_length. Since our input is time major, we settime_major=True. Here, we build only a single layer LSTM, encoder_cell. Wewill describe how to build multi-layer LSTMs, add dropout, and use attention ina later section.DecoderThe decoder also needs to have access to the source information, and onesimple way to achieve that is to initialize it with the last hidden state of theencoder, encoder_state. In Figure 2, we pass the hidden state at the sourceword ""student"" to the decoder side.# Build RNN cell
decoder_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)
# Helper
helper = tf.contrib.seq2seq.TrainingHelper(
    decoder_emb_inp, decoder_lengths, time_major=True)
# Decoder
decoder = tf.contrib.seq2seq.BasicDecoder(
    decoder_cell, helper, encoder_state,
    output_layer=projection_layer)
# Dynamic decoding
outputs, _ = tf.contrib.seq2seq.dynamic_decode(decoder, ...)
logits = outputs.rnn_output
Here, the core part of this code is the BasicDecoder object, decoder, whichreceives decoder_cell (similar to encoder_cell), a helper, and the previousencoder_state as inputs. By separating out decoders and helpers, we can reusedifferent codebases, e.g., TrainingHelper can be substituted withGreedyEmbeddingHelper to do greedy decoding. See morein.Lastly, we haven't mentioned projection_layer which is a dense matrix to turnthe top hidden states to logit vectors of dimension V. We illustrate thisprocess at the top of Figure 2.projection_layer = layers_core.Dense(
    tgt_vocab_size, use_bias=False)
LossGiven the logits above, we are now ready to compute our training loss:crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(
    labels=decoder_outputs, logits=logits)
train_loss = (tf.reduce_sum(crossent * target_weights) /
    batch_size)
Here, target_weights is a zero-one matrix of the same size asdecoder_outputs. It masks padding positions outside of the target sequencelengths with values 0.[<marko.inline.RawText object at 0x000001592FF3F308>]: It's worth pointing out that we divide the loss bybatch_size, so our hyperparameters are ""invariant"" to batch_size. Some peopledivide the loss by (batch_size * num_time_steps), which plays down theerrors made on short sentences. More subtly, our hyperparameters (applied to theformer way) can't be used for the latter way. For example, if both approachesuse SGD with a learning of 1.0, the latter approach effectively uses a muchsmaller learning rate of 1 / num_time_steps.Gradient computation & optimizationWe have now defined the forward pass of our NMT model. Computing thebackpropagation pass is just a matter of a few lines of code:# Calculate and clip gradients
params = tf.trainable_variables()
gradients = tf.gradients(train_loss, params)
clipped_gradients, _ = tf.clip_by_global_norm(
    gradients, max_gradient_norm)
One of the important steps in training RNNs is gradient clipping. Here, we clipby the global norm.  The max value, max_gradient_norm, is often set to a valuelike 5 or 1. The last step is selecting the optimizer.  The Adam optimizer is acommon choice.  We also select a learning rate.  The value of learning_ratecan is usually in the range 0.0001 to 0.001; and can be set to decrease astraining progresses.# Optimization
optimizer = tf.train.AdamOptimizer(learning_rate)
update_step = optimizer.apply_gradients(
    zip(clipped_gradients, params))
In our own experiments, we use standard SGD (tf.train.GradientDescentOptimizer)with a decreasing learning rate schedule, which yields better performance. Seethe .Hands-on – Let's train an NMT modelLet's train our very first NMT model, translating from Vietnamese to English!The entry point of our codeis.We will use a small-scale parallel corpus of TED talks (133K trainingexamples) for this exercise. All data we used here can be foundat:. Wewill use tst2012 as our dev dataset, and tst2013 as our test dataset.Run the following command to download the data for training NMT model:Run the following command to start the training:mkdir /tmp/nmt_model
python -m nmt.nmt \
    --src=vi --tgt=en \
    --vocab_prefix=/tmp/nmt_data/vocab  \
    --train_prefix=/tmp/nmt_data/train \
    --dev_prefix=/tmp/nmt_data/tst2012  \
    --test_prefix=/tmp/nmt_data/tst2013 \
    --out_dir=/tmp/nmt_model \
    --num_train_steps=12000 \
    --steps_per_stats=100 \
    --num_layers=2 \
    --num_units=128 \
    --dropout=0.2 \
    --metrics=bleu
The above command trains a 2-layer LSTM seq2seq model with 128-dim hidden unitsand embeddings for 12 epochs. We use a dropout value of 0.2 (keep probability0.8). If no error, we should see logs similar to the below with decreasingperplexity values as we train.# First evaluation, global step 0
  eval dev: perplexity 17193.66
  eval test: perplexity 17193.27
# Start epoch 0, step 0, lr 1, Tue Apr 25 23:17:41 2017
  sample train data:
    src_reverse: </s> </s> Điều đó , dĩ nhiên , là câu chuyện trích ra từ học thuyết của Karl Marx .
    ref: That , of course , was the <unk> distilled from the theories of Karl Marx . </s> </s> </s>
  epoch 0 step 100 lr 1 step-time 0.89s wps 5.78K ppl 1568.62 bleu 0.00
  epoch 0 step 200 lr 1 step-time 0.94s wps 5.91K ppl 524.11 bleu 0.00
  epoch 0 step 300 lr 1 step-time 0.96s wps 5.80K ppl 340.05 bleu 0.00
  epoch 0 step 400 lr 1 step-time 1.02s wps 6.06K ppl 277.61 bleu 0.00
  epoch 0 step 500 lr 1 step-time 0.95s wps 5.89K ppl 205.85 bleu 0.00
See  for more details.We can start Tensorboard to view the summary of the model during training:tensorboard --port 22222 --logdir /tmp/nmt_model/
Training the reverse direction from English and Vietnamese can be done simply by changing:Inference – How to generate translationsWhile you're training your NMT models (and once you have trained models), youcan obtain translations given previously unseen source sentences. This processis called inference. There is a clear distinction between training and inference(testing): at inference time, we only have access to the source sentence,i.e., encoder_inputs. There are many ways to perform decoding.  Decodingmethods include greedy, sampling, and beam-search decoding. Here, we willdiscuss the greedy decoding strategy.The idea is simple and we illustrate it in Figure 3:Step 3 is what makes inference different from training. Instead of alwaysfeeding the correct target words as an input, inference uses words predicted bythe model. Here's the code to achieve greedy decoding.  It is very similar tothe training decoder.# Helper
helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(
    embedding_decoder,
    tf.fill([batch_size], tgt_sos_id), tgt_eos_id)

# Decoder
decoder = tf.contrib.seq2seq.BasicDecoder(
    decoder_cell, helper, encoder_state,
    output_layer=projection_layer)
# Dynamic decoding
outputs, _ = tf.contrib.seq2seq.dynamic_decode(
    decoder, maximum_iterations=maximum_iterations)
translations = outputs.sample_id
Here, we use GreedyEmbeddingHelper instead of TrainingHelper. Since we donot know the target sequence lengths in advance, we use maximum_iterations tolimit the translation lengths. One heuristic is to decode up to two times thesource sentence lengths.maximum_iterations = tf.round(tf.reduce_max(source_sequence_length) * 2)
Having trained a model, we can now create an inference file and translate somesentences:cat > /tmp/my_infer_file.vi
# (copy and paste some sentences from /tmp/nmt_data/tst2013.vi)

python -m nmt.nmt \
    --out_dir=/tmp/nmt_model \
    --inference_input_file=/tmp/my_infer_file.vi \
    --inference_output_file=/tmp/nmt_model/output_infer

cat /tmp/nmt_model/output_infer # To view the inference as output
Note the above commands can also be run while the model is still being trainedas long as there exists a trainingcheckpoint. See  for more details.IntermediateHaving gone through the most basic seq2seq model, let's get more advanced! Tobuild state-of-the-art neural machine translation systems, we will need more""secret sauce"": the attention mechanism, which was first introducedby , then later refinedby  and others. The keyidea of the attention mechanism is to establish direct short-cut connectionsbetween the target and the source by paying ""attention"" to relevant sourcecontent as we translate. A nice byproduct of the attention mechanism is aneasy-to-visualize alignment matrix between the source and target sentences (asshown in Figure 4).Remember that in the vanilla seq2seq model, we pass the last source state fromthe encoder to the decoder when starting the decoding process. This works wellfor short and medium-length sentences; however, for long sentences, the singlefixed-size hidden state becomes an information bottleneck. Instead of discardingall of the hidden states computed in the source RNN, the attention mechanismprovides an approach that allows the decoder to peek at them (treating them as adynamic memory of the source information). By doing so, the attention mechanismimproves the translation of longer sentences. Nowadays, attention mechanisms arethe defacto standard and have been successfully applied to many other tasks(including image caption generation, speech recognition, and textsummarization).Background on the Attention MechanismWe now describe an instance of the attention mechanism proposed in (Luong etal., 2015), which has been used in several state-of-the-art systems includingopen-source toolkits such as  and in the TFseq2seq API in this tutorial. We will also provide connections to other variantsof the attention mechanism.As illustrated in Figure 5, the attention computation happens at every decodertime step.  It consists of the following stages:Here, the function  is used to compared the target hidden state $$h_t$$with each of the source hidden states $$\overline{h}_s$$, and the result is normalized toproduced attention weights (a distribution over source positions). There arevarious choices of the scoring function; popular scoring functions include themultiplicative and additive forms given in Eq. (4). Once computed, the attentionvector $$a_t$$ is used to derive the softmax logit and loss.  This is similar to thetarget hidden state at the top layer of a vanilla seq2seq model. The function can also take other forms.Various implementations of attention mechanisms can be foundin.[<marko.inline.RawText object at 0x000001592FDEB0C8>]As hinted in the above equations, there are many different attention variants.These variants depend on the form of the scoring function and the attentionfunction, and on whether the previous state $$h_{t-1}$$ is used instead of$$h_t$$ in the scoring function as originally suggested in (Bahdanau et al.,2015). Empirically, we found that only certain choices matter. First, the basicform of attention, i.e., direct connections between target and source, needs tobe present. Second, it's important to feed the attention vector to the nexttimestep to inform the network about past attention decisions as demonstrated in(Luong et al., 2015). Lastly, choices of the scoring function can often resultin different performance. See more in the section.Attention Wrapper APIIn our implementation ofthe,we borrow some terminologyfrom  in their work onmemory networks. Instead of having readable & writable memory, the attentionmechanism presented in this tutorial is a read-only memory. Specifically, theset of source hidden states (or their transformed versions, e.g.,$$W\overline{h}_s$$ in Luong's scoring style or $$W_2\overline{h}_s$$ inBahdanau's scoring style) is referred to as the ""memory"". At each time step,we use the current target hidden state as a ""query"" to decide on which partsof the memory to read.  Usually, the query needs to be compared with keyscorresponding to individual memory slots. In the above presentation of theattention mechanism, we happen to use the set of source hidden states (or theirtransformed versions, e.g., $$W_1h_t$$ in Bahdanau's scoring style) as""keys"". One can be inspired by this memory-network terminology to derive otherforms of attention!Thanks to the attention wrapper, extending our vanilla seq2seq code withattention is trivial. This part refers tofile First, we need to define an attention mechanism, e.g., from (Luong et al.,2015):# attention_states: [batch_size, max_time, num_units]
attention_states = tf.transpose(encoder_outputs, [1, 0, 2])

# Create an attention mechanism
attention_mechanism = tf.contrib.seq2seq.LuongAttention(
    num_units, attention_states,
    memory_sequence_length=source_sequence_length)
In the previous  section, encoder_outputs is the set of allsource hidden states at the top layer and has the shape of [max_time, (since we use dynamic_rnn with time_major set toTrue for efficiency). For the attention mechanism, we need to make sure the""memory"" passed in is batch major, so we need to transposeattention_states. We pass source_sequence_length to the attention mechanismto ensure that the attention weights are properly normalized (over non-paddingpositions only).Having defined an attention mechanism, we use AttentionWrapper to wrap thedecoding cell:decoder_cell = tf.contrib.seq2seq.AttentionWrapper(
    decoder_cell, attention_mechanism,
    attention_layer_size=num_units)
The rest of the code is almost the same as in the Section !Hands-on – building an attention-based NMT modelTo enable attention, we need to use one of , , or  as the value of the  flag during training. Theflag specifies which attention mechanism we are going to use. In addition, weneed to create a new directory for the attention model, so we don't reuse thepreviously trained basic NMT model.Run the following command to start the training:mkdir /tmp/nmt_attention_model

python -m nmt.nmt \
    --attention=scaled_luong \
    --src=vi --tgt=en \
    --vocab_prefix=/tmp/nmt_data/vocab  \
    --train_prefix=/tmp/nmt_data/train \
    --dev_prefix=/tmp/nmt_data/tst2012  \
    --test_prefix=/tmp/nmt_data/tst2013 \
    --out_dir=/tmp/nmt_attention_model \
    --num_train_steps=12000 \
    --steps_per_stats=100 \
    --num_layers=2 \
    --num_units=128 \
    --dropout=0.2 \
    --metrics=bleu
After training, we can use the same inference command with the new out_dir forinference:python -m nmt.nmt \
    --out_dir=/tmp/nmt_attention_model \
    --inference_input_file=/tmp/my_infer_file.vi \
    --inference_output_file=/tmp/nmt_attention_model/output_infer
Tips & TricksBuilding Training, Eval, and Inference GraphsWhen building a machine learning model in TensorFlow, it's often best to buildthree separate graphs:Building separate graphs has several benefits:The primary source of complexity becomes how to share Variables across the threegraphs in a single machine setting. This is solved by using a separate sessionfor each graph. The training session periodically saves checkpoints, and theeval session and the infer session restore parameters from checkpoints. Theexample below shows the main differences between the two approaches.Before: Three models in a single graph and sharing a single Sessionwith tf.variable_scope('root'):
  train_inputs = tf.placeholder()
  train_op, loss = BuildTrainModel(train_inputs)
  initializer = tf.global_variables_initializer()

with tf.variable_scope('root', reuse=True):
  eval_inputs = tf.placeholder()
  eval_loss = BuildEvalModel(eval_inputs)

with tf.variable_scope('root', reuse=True):
  infer_inputs = tf.placeholder()
  inference_output = BuildInferenceModel(infer_inputs)

sess = tf.Session()

sess.run(initializer)

for i in itertools.count():
  train_input_data = ...
  sess.run([loss, train_op], feed_dict={train_inputs: train_input_data})

  if i % EVAL_STEPS == 0:
    while data_to_eval:
      eval_input_data = ...
      sess.run([eval_loss], feed_dict={eval_inputs: eval_input_data})

  if i % INFER_STEPS == 0:
    sess.run(inference_output, feed_dict={infer_inputs: infer_input_data})
After: Three models in three graphs, with three Sessions sharing the same Variablestrain_graph = tf.Graph()
eval_graph = tf.Graph()
infer_graph = tf.Graph()

with train_graph.as_default():
  train_iterator = ...
  train_model = BuildTrainModel(train_iterator)
  initializer = tf.global_variables_initializer()

with eval_graph.as_default():
  eval_iterator = ...
  eval_model = BuildEvalModel(eval_iterator)

with infer_graph.as_default():
  infer_iterator, infer_inputs = ...
  infer_model = BuildInferenceModel(infer_iterator)

checkpoints_path = ""/tmp/model/checkpoints""

train_sess = tf.Session(graph=train_graph)
eval_sess = tf.Session(graph=eval_graph)
infer_sess = tf.Session(graph=infer_graph)

train_sess.run(initializer)
train_sess.run(train_iterator.initializer)

for i in itertools.count():

  train_model.train(train_sess)

  if i % EVAL_STEPS == 0:
    checkpoint_path = train_model.saver.save(train_sess, checkpoints_path, global_step=i)
    eval_model.saver.restore(eval_sess, checkpoint_path)
    eval_sess.run(eval_iterator.initializer)
    while data_to_eval:
      eval_model.eval(eval_sess)

  if i % INFER_STEPS == 0:
    checkpoint_path = train_model.saver.save(train_sess, checkpoints_path, global_step=i)
    infer_model.saver.restore(infer_sess, checkpoint_path)
    infer_sess.run(infer_iterator.initializer, feed_dict={infer_inputs: infer_input_data})
    while data_to_infer:
      infer_model.infer(infer_sess)
Notice how the latter approach is ""ready"" to be converted to a distributedversion.One other difference in the new approach is that instead of using feed_dictsto feed data at each session.run call (and thereby performing our ownbatching, bucketing, and manipulating of data), we use stateful iteratorobjects.  These iterators make the input pipeline much easier in both thesingle-machine and distributed setting. We will cover the new input datapipeline (as introduced in TensorFlow 1.2) in the next section.Data Input PipelinePrior to TensorFlow 1.2, users had two options for feeding data to theTensorFlow training and eval pipelines:The first approach is easier for users who aren't familiar with TensorFlow orneed to do exotic input modification (i.e., their own minibatch queueing) thatcan only be done in Python.  The second and third approaches are more standardbut a little less flexible; they also require starting multiple python threads(queue runners).  Furthermore, if used incorrectly queues can lead to deadlocksor opaque error messages.  Nevertheless, queues are significantly more efficientthan using feed_dict and are the standard for both single-machine anddistributed training.Starting in TensorFlow 1.2, there is a new system available for reading datainto TensorFlow models: dataset iterators, as found in the tf.datamodule. Data iterators are flexible, easy to reason about and to manipulate, andprovide efficiency and multithreading by leveraging the TensorFlow C++ runtime.A dataset can be created from a batch data Tensor, a filename, or a Tensorcontaining multiple filenames.  Some examples:# Training dataset consists of multiple files.
train_dataset = tf.data.TextLineDataset(train_files)

# Evaluation dataset uses a single file, but we may
# point to a different file for each evaluation round.
eval_file = tf.placeholder(tf.string, shape=())
eval_dataset = tf.data.TextLineDataset(eval_file)

# For inference, feed input data to the dataset directly via feed_dict.
infer_batch = tf.placeholder(tf.string, shape=(num_infer_examples,))
infer_dataset = tf.data.Dataset.from_tensor_slices(infer_batch)
All datasets can be treated similarly via input processing.  This includesreading and cleaning the data, bucketing (in the case of training and eval),filtering, and batching.To convert each sentence into vectors of word strings, for example, we use thedataset map transformation:dataset = dataset.map(lambda string: tf.string_split([string]).values)
We can then switch each sentence vector into a tuple containing both the vectorand its dynamic length:dataset = dataset.map(lambda words: (words, tf.size(words))
Finally, we can perform a vocabulary lookup on each sentence.  Given a lookuptable object table, this map converts the first tuple elements from a vector ofstrings to a vector of integers.dataset = dataset.map(lambda words, size: (table.lookup(words), size))
Joining two datasets is also easy.  If two files contain line-by-linetranslations of each other and each one is read into its own dataset, then a newdataset containing the tuples of the zipped lines can be created via:source_target_dataset = tf.data.Dataset.zip((source_dataset, target_dataset))
Batching of variable-length sentences is straightforward. The followingtransformation batches batch_size elements from source_target_dataset, andrespectively pads the source and target vectors to the length of the longestsource and target vector in each batch.batched_dataset = source_target_dataset.padded_batch(
        batch_size,
        padded_shapes=((tf.TensorShape([None]),  # source vectors of unknown size
                        tf.TensorShape([])),     # size(source)
                       (tf.TensorShape([None]),  # target vectors of unknown size
                        tf.TensorShape([]))),    # size(target)
        padding_values=((src_eos_id,  # source vectors padded on the right with src_eos_id
                         0),          # size(source) -- unused
                        (tgt_eos_id,  # target vectors padded on the right with tgt_eos_id
                         0)))         # size(target) -- unused
Values emitted from this dataset will be nested tuples whose tensors have aleftmost dimension of size batch_size.  The structure will be:Finally, bucketing that batches similarly-sized source sentences together isalso possible.  Please see thefile formore details and the full implementation.Reading data from a Dataset requires three lines of code: create the iterator,get its values, and initialize it.batched_iterator = batched_dataset.make_initializable_iterator()

((source, source_lengths), (target, target_lengths)) = batched_iterator.get_next()

# At initialization time.
session.run(batched_iterator.initializer, feed_dict={...})
Once the iterator is initialized, every session.run call that accesses sourceor target tensors will request the next minibatch from the underlying dataset.Other details for better NMT modelsBidirectional RNNsBidirectionality on the encoder side generally gives better performance (withsome degradation in speed as more layers are used). Here, we give a simplifiedexample of how to build an encoder with a single bidirectional layer:# Construct forward and backward cells
forward_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)
backward_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units)

bi_outputs, encoder_state = tf.nn.bidirectional_dynamic_rnn(
    forward_cell, backward_cell, encoder_emb_inp,
    sequence_length=source_sequence_length, time_major=True)
encoder_outputs = tf.concat(bi_outputs, -1)
The variables encoder_outputs and encoder_state can be used in the same wayas in Section Encoder. Note that, for multiple bidirectional layers, we need tomanipulate the encoder_state a bit, see , method_build_bidirectional_rnn() for more details.Beam searchWhile greedy decoding can give us quite reasonable translation quality, a beamsearch decoder can further boost performance. The idea of beam search is tobetter explore the search space of all possible translations by keeping around asmall set of top candidates as we translate. The size of the beam is calledbeam width; a minimal beam width of, say size 10, is generally sufficient. Formore information, we refer readers to Section 7.2.3of . Here's an example of howbeam search can be done:# Replicate encoder infos beam_width times
decoder_initial_state = tf.contrib.seq2seq.tile_batch(
    encoder_state, multiplier=hparams.beam_width)

# Define a beam-search decoder
decoder = tf.contrib.seq2seq.BeamSearchDecoder(
        cell=decoder_cell,
        embedding=embedding_decoder,
        start_tokens=start_tokens,
        end_token=end_token,
        initial_state=decoder_initial_state,
        beam_width=beam_width,
        output_layer=projection_layer,
        length_penalty_weight=0.0,
        coverage_penalty_weight=0.0)

# Dynamic decoding
outputs, _ = tf.contrib.seq2seq.dynamic_decode(decoder, ...)
Note that the same dynamic_decode() API call is used, similar to theSection . Once decoded, we can access the translations asfollows:translations = outputs.predicted_ids
# Make sure translations shape is [batch_size, beam_width, time]
if self.time_major:
   translations = tf.transpose(translations, perm=[1, 2, 0])
See , method _build_decoder() for more details.HyperparametersThere are several hyperparameters that can lead to additionalperformances. Here, we list some based on our own experience [ Disclaimers:others might not agree on things we wrote! ].[<marko.inline.RawText object at 0x000001592FEA6588>]: while Adam can lead to reasonable results for ""unfamiliar""architectures, SGD with scheduling will generally lead to better performance ifyou can train with SGD.[<marko.inline.RawText object at 0x000001593006B7C8>]: Bahdanau-style attention often requires bidirectionality on theencoder side to work well; whereas Luong-style attention tends to work well fordifferent settings. For this tutorial code, we recommend using the two improvedvariants of Luong & Bahdanau-style attentions: scaled_luong & normed.Multi-GPU trainingTraining a NMT model may take several days. Placing different RNN layers ondifferent GPUs can improve the training speed. Here’s an example to createRNN layers on multiple GPUs.cells = []
for i in range(num_layers):
  cells.append(tf.contrib.rnn.DeviceWrapper(
      tf.contrib.rnn.LSTMCell(num_units),
      ""/gpu:%d"" % (num_layers % num_gpus)))
cell = tf.contrib.rnn.MultiRNNCell(cells)
In addition, we need to enable the  option in to parallelize the gradients computation.You may notice the speed improvement of the attention based NMT model is verysmall as the number of GPUs increases. One major drawback of the standardattention architecture is using the top (final) layer’s output to queryattention at each time step. That means each decoding step must wait itsprevious step completely finished; hence, we can’t parallelize the decodingprocess by simply placing RNN layers on multiple GPUs.The parallelizes the decoder's computation by using the bottom (first) layer’soutput to query attention. Therefore, each decoding step can start as soon asits previous step's first layer and attention computation finished. Weimplemented the architecture in,a subclass of tf.contrib.rnn.MultiRNNCell. Here’s an example of how to createa decoder cell with the GNMTAttentionMultiCell.cells = []
for i in range(num_layers):
  cells.append(tf.contrib.rnn.DeviceWrapper(
      tf.contrib.rnn.LSTMCell(num_units),
      ""/gpu:%d"" % (num_layers % num_gpus)))
attention_cell = cells.pop(0)
attention_cell = tf.contrib.seq2seq.AttentionWrapper(
    attention_cell,
    attention_mechanism,
    attention_layer_size=None,  # don't add an additional dense layer.
    output_attention=False,)
cell = GNMTAttentionMultiCell(attention_cell, cells)
BenchmarksIWSLT English-VietnameseTrain: 133K examples, vocab=vocab.(vi|en), train=train.(vi|en)dev=tst2012.(vi|en),test=tst2013.(vi|en), .[<marko.inline.RawText object at 0x000001592FE432C8>]. We train 2-layer LSTMs of 512 units with bidirectionalencoder (i.e., 1 bidirectional layers for the encoder), embedding dimis 512. LuongAttention (scale=True) is used together with dropout keep_prob of0.8. All parameters are uniformly. We use SGD with learning rate 1.0 as follows:train for 12K steps (~ 12 epochs); after 8K steps, we start halving learningrate every 1K step.[<marko.inline.RawText object at 0x000001592FF26F88>].Below are the averaged results of 2 models(,).We measure the translation quality in terms of BLEU scores .Systems | tst2012 (dev) | test2013 (test)--- | :---: | :---:NMT (greedy) | 23.2 | 25.5NMT (beam=10) | 23.8 | 26.1 | - | 23.3Training Speed: (0.37s step-time, 15.3K wps) on K40m & (0.17s step-time, 32.2K wps) on TitanX.Here, step-time means the time taken to run one mini-batch (of size 128). For wps, we count words on both the source and target.WMT German-EnglishTrain: 4.5M examples, vocab=vocab.bpe.32000.(de|en),train=train.tok.clean.bpe.32000.(de|en), dev=newstest2013.tok.bpe.32000.(de|en),test=newstest2015.tok.bpe.32000.(de|en),[<marko.inline.RawText object at 0x000001592FF25B48>]. Our training hyperparameters are similar to theEnglish-Vietnamese experiments except for the following details. The data issplit into subword units using (32K operations). We train 4-layer LSTMs of 1024 units with bidirectionalencoder (i.e., 2 bidirectional layers for the encoder), embedding dimis 1024. We train for 350K steps (~ 10 epochs); after 170K steps, we starthalving learning rate every 17K step.[<marko.inline.RawText object at 0x000001592FF25608>].The first 2 rows are the averaged results of 2 models(,).Results in the third row is with GNMT attention(); trained with 4 GPUs.Systems | newstest2013 (dev) | newstest2015--- | :---: | :---:NMT (greedy) | 27.1 | 27.6NMT (beam=10) | 28.0 | 28.9NMT + GNMT attention (beam=10) | 29.0 | 29.9 | - | 29.3These results show that our code builds strong baseline systems for NMT.(Note that WMT systems generally utilize a huge amount monolingual data which we currently do not.)Training Speed: (2.1s step-time, 3.4K wps) on Nvidia K40m & (0.7s step-time, 8.7K wps) on Nvidia TitanX for standard models.To see the speed-ups with GNMT attention, we benchmark on K40m only:Systems | 1 gpu | 4 gpus | 8 gpus--- | :---: | :---: | :---:NMT (4 layers) | 2.2s, 3.4K | 1.9s, 3.9K | -NMT (8 layers) | 3.5s, 2.0K | - | 2.9s, 2.4KNMT + GNMT attention (4 layers) | 2.6s, 2.8K | 1.7s, 4.3K | -NMT + GNMT attention (8 layers) | 4.2s, 1.7K | - | 1.9s, 3.8KThese results show that without GNMT attention, the gains from using multiple gpus are minimal.With GNMT attention, we obtain from 50%-100% speed-ups with multiple gpus.WMT English-German &mdash; Full ComparisonThe first 2 rows are our models with GNMTattention:,.Systems | newstest2014 | newstest2015--- | :---: | :---:Ours &mdash; NMT + GNMT attention (4 layers) | 23.7 | 26.5Ours &mdash; NMT + GNMT attention (8 layers) | 24.4 | 27.6 | 20.6 | 24.9OpenNMT  | 19.3 | -tf-seq2seq  | 22.2 | 25.2GNMT  | 24.6 | -The above results show our models are very competitive among models of similar architectures.[Note that OpenNMT uses smaller models and the current best result (as of this writing) is 28.4 obtained by the Transformer network  which has a significantly different architecture.]Standard HParamsWe have providedfor using pre-trained checkpoint for inference or training NMT architecturesused in the Benchmark.We will use the WMT16 German-English data, you can download the data by thefollowing command.nmt/scripts/wmt16_en_de.sh /tmp/wmt16
Here is an example command for loading the pre-trained GNMT WMT German-Englishcheckpoint for inference.python -m nmt.nmt \
    --src=de --tgt=en \
    --ckpt=/path/to/checkpoint/translate.ckpt \
    --hparams_path=nmt/standard_hparams/wmt16_gnmt_4_layer.json \
    --out_dir=/tmp/deen_gnmt \
    --vocab_prefix=/tmp/wmt16/vocab.bpe.32000 \
    --inference_input_file=/tmp/wmt16/newstest2014.tok.bpe.32000.de \
    --inference_output_file=/tmp/deen_gnmt/output_infer \
    --inference_ref_file=/tmp/wmt16/newstest2014.tok.bpe.32000.en
Here is an example command for training the GNMT WMT German-English model.python -m nmt.nmt \
    --src=de --tgt=en \
    --hparams_path=nmt/standard_hparams/wmt16_gnmt_4_layer.json \
    --out_dir=/tmp/deen_gnmt \
    --vocab_prefix=/tmp/wmt16/vocab.bpe.32000 \
    --train_prefix=/tmp/wmt16/train.tok.clean.bpe.32000 \
    --dev_prefix=/tmp/wmt16/newstest2013.tok.bpe.32000 \
    --test_prefix=/tmp/wmt16/newstest2015.tok.bpe.32000
Other resourcesFor deeper reading on Neural Machine Translation and sequence-to-sequencemodels, we highly recommend the following materialsby;;and .There's a wide variety of tools for building seq2seq models, so we pick one perlanguage:Stanford NMT[Matlab] tf-seq2seq[TensorFlow] Nemantus[Theano] OpenNMT  [Torch]OpenNMT-py  [PyTorch]AcknowledgmentWe would like to thank Denny Britz, Anna Goldie, Derek Murray, and Cinjon Resnick for their work bringing new features to TensorFlow and the seq2seq library. Additional thanks go to Lukasz Kaiser for the initial help on the seq2seq codebase; Quoc Le for the suggestion to replicate GNMT; Yonghui Wu and Zhifeng Chen for details on the GNMT systems; as well as the Google Brain team for their support and feedback!ReferencesBibTex@article{luong17,
  author  = {Minh{-}Thang Luong and Eugene Brevdo and Rui Zhao},
  title   = {Neural Machine Translation (seq2seq) Tutorial},
  journal = {https://github.com/tensorflow/nmt},
  year    = {2017},
}
"
https://github.com/scrapy-plugins/scrapy-splash,Scrapy+Splash for JavaScript integration,"==============================================Scrapy & JavaScript integration through Splash.. image:: https://img.shields.io/pypi/v/scrapy-splash.svg:target: https://pypi.python.org/pypi/scrapy-splash:alt: PyPI Version.. image:: https://github.com/scrapy-plugins/scrapy-splash/workflows/Tests/badge.svg:target: https://github.com/scrapy-plugins/scrapy-splash/actions/workflows/tests.yml:alt: Test Status.. image:: http://codecov.io/github/scrapy-plugins/scrapy-splash/coverage.svg?branch=master:target: http://codecov.io/github/scrapy-plugins/scrapy-splash?branch=master:alt: Code CoverageThis library provides Scrapy_ and JavaScript integration using Splash_.The license is BSD 3-clause... _Scrapy: https://github.com/scrapy/scrapy.. _Splash: https://github.com/scrapinghub/splashInstallationInstall scrapy-splash using pip::$ pip install scrapy-splash
Scrapy-Splash uses Splash_ HTTP API, so you also need a Splash instance.Usually to install & run Splash, something like this is enough::$ docker run -p 8050:8050 scrapinghub/splash
Check Splash _ for more info... _install docs: http://splash.readthedocs.org/en/latest/install.htmlConfiguration.. note::Steps (4) and (5) are necessary because Scrapy doesn't provide a way
to override request fingerprints calculation algorithm globally; this
could change in future.
There are also some additional options available.Put them into your  if you want to change the defaults:UsageRequestsThe easiest way to render requests with Splash is touse ::yield SplashRequest(url, self.parse_result,
    args={
        # optional; parameters passed to Splash HTTP API
        'wait': 0.5,

        # 'url' is prefilled from request url
        # 'http_method' is set to 'POST' for POST requests
        # 'body' is set to request body for POST requests
    },
    endpoint='render.json', # optional; default is render.html
    splash_url='<url>',     # optional; overrides SPLASH_URL
    slot_policy=scrapy_splash.SlotPolicy.PER_DOMAIN,  # optional
)
Alternatively, you can use regular scrapy.Request and Request  key::yield scrapy.Request(url, self.parse_result, meta={
    'splash': {
        'args': {
            # set rendering arguments here
            'html': 1,
            'png': 1,

            # 'url' is prefilled from request url
            # 'http_method' is set to 'POST' for POST requests
            # 'body' is set to request body for POST requests
        },

        # optional parameters
        'endpoint': 'render.json',  # optional; default is render.json
        'splash_url': '<url>',      # optional; overrides SPLASH_URL
        'slot_policy': scrapy_splash.SlotPolicy.PER_DOMAIN,
        'splash_headers': {},       # optional; a dict with headers sent to Splash
        'dont_process_response': True, # optional, default is False
        'dont_send_headers': True,  # optional, default is False
        'magic_response': False,    # optional, default is True
    }
})
Use  API in middlewares or when scrapy.Requestsubclasses are used (there is also  described below).For example,  allows to create a middleware which enablesSplash for all outgoing requests by default. is a convenient utility to fill ;it should be easier to use in most cases. For each key there is a corresponding  keyword argument: for example,to set  use ... _HTTP API docs: http://splash.readthedocs.org/en/latest/api.htmlUse  if you want to make a via splash. It accepts the same arguments as ,and also , like  from scrapy::>>> SplashFormRequest('http://example.com', formdata={'foo': 'bar'})
<POST http://example.com>
 is also supported, and works as describedin _.Responsesscrapy-splash returns Response subclasses for Splash requests:To use standard Response classes set or pass  argument to SplashRequest.All these responses set  to the URL of the original request(i.e. to the URL of a website you want to render), not to the URL of therequested Splash endpoint. ""True"" URL is still available as.SplashJsonResponse provide extra features:When  is updated in SplashJsonResponse(either from 'html' or from 'body' keys) familiar and  methods are available.To turn off special handling of JSON result keys either set or pass argument to SplashRequest.Session HandlingSplash itself is stateless - each request starts from a clean state.In order to support sessions the following is required:For (2) and (3) Splash provides  and methods which can be used in Splash Lua scripts.scrapy-splash provides helpers for (1) and (4): to send current cookiesin 'cookies' field and merge cookies back from 'cookies' response fieldset  to the sessionidentifier. If you only want a single session use the same  forall request; any value like '1' or 'foo' is fine.For scrapy-splash session handling to work you must use  endpointand a Lua script which accepts 'cookies' argument and returns 'cookies'field in the result::function main(splash)splash:init_cookies(splash.args.cookies)   -- ... your script

   return {
       cookies = splash:get_cookies(),
       -- ... other results, e.g. html
   }
endSplashRequest sets  automatically for  endpoint,i.e. cookie handling is enabled by default if you use SplashRequest, endpoint and a compatible Lua rendering script.If you want to start from the same set of cookies, but then 'fork' sessionsset  in addition to. Request cookies will be fetched from cookiejar ,but response cookies will be merged back to the  cookiejar.Standard Scrapy  argument can be used with to add cookies to the current Splash cookiejar.ExamplesGet HTML contents::import scrapy
from scrapy_splash import SplashRequest

class MySpider(scrapy.Spider):
    start_urls = [""http://example.com"", ""http://example.com/foo""]

    def start_requests(self):
        for url in self.start_urls:
            yield SplashRequest(url, self.parse, args={'wait': 0.5})

    def parse(self, response):
        # response.body is a result of render.html call; it
        # contains HTML processed by a browser.
        # ...
Get HTML contents and a screenshot::import json
import base64
import scrapy
from scrapy_splash import SplashRequest

class MySpider(scrapy.Spider):

    # ...
        splash_args = {
            'html': 1,
            'png': 1,
            'width': 600,
            'render_all': 1,
        }
        yield SplashRequest(url, self.parse_result, endpoint='render.json',
                            args=splash_args)

    # ...
    def parse_result(self, response):
        # magic responses are turned ON by default,
        # so the result under 'html' key is available as response.body
        html = response.body

        # you can also query the html result as usual
        title = response.css('title').extract_first()

        # full decoded JSON data is available as response.data:
        png_bytes = base64.b64decode(response.data['png'])

        # ...
Run a simple _::import json
import base64
from scrapy_splash import SplashRequest


class MySpider(scrapy.Spider):

    # ...
        script = """"""
        function main(splash)
            assert(splash:go(splash.args.url))
            return splash:evaljs(""document.title"")
        end
        """"""
        yield SplashRequest(url, self.parse_result, endpoint='execute',
                            args={'lua_source': script})

    # ...
    def parse_result(self, response):
        doc_title = response.text
        # ...
More complex _ example - get a screenshot of an HTMLelement by its CSS selector (it requires Splash 2.1+).Note how are arguments passed to the script::import json
import base64
from scrapy_splash import SplashRequest

script = """"""
-- Arguments:
-- * url - URL to render;
-- * css - CSS selector to render;
-- * pad - screenshot padding size.

-- this function adds padding around region
function pad(r, pad)
  return {r[1]-pad, r[2]-pad, r[3]+pad, r[4]+pad}
end

-- main script
function main(splash)

  -- this function returns element bounding box
  local get_bbox = splash:jsfunc([[
    function(css) {
      var el = document.querySelector(css);
      var r = el.getBoundingClientRect();
      return [r.left, r.top, r.right, r.bottom];
    }
  ]])

  assert(splash:go(splash.args.url))
  assert(splash:wait(0.5))

  -- don't crop image by a viewport
  splash:set_viewport_full()

  local region = pad(get_bbox(splash.args.css), splash.args.pad)
  return splash:png{region=region}
end
""""""

class MySpider(scrapy.Spider):


    # ...
        yield SplashRequest(url, self.parse_element_screenshot,
            endpoint='execute',
            args={
                'lua_source': script,
                'pad': 32,
                'css': 'a.title'
            }
         )

    # ...
    def parse_element_screenshot(self, response):
        image_data = response.body  # binary image data in PNG format
        # ...
Use a Lua script to get an HTML response with cookies, headers, bodyand method set to correct values;  argument value is cachedon Splash server and is not sent with each request (it requires Splash 2.1+)::import scrapy
from scrapy_splash import SplashRequest

script = """"""
function main(splash)
  splash:init_cookies(splash.args.cookies)
  assert(splash:go{
    splash.args.url,
    headers=splash.args.headers,
    http_method=splash.args.http_method,
    body=splash.args.body,
    })
  assert(splash:wait(0.5))

  local entries = splash:history()
  local last_response = entries[#entries].response
  return {
    url = splash:url(),
    headers = last_response.headers,
    http_status = last_response.status,
    cookies = splash:get_cookies(),
    html = splash:html(),
  }
end
""""""

class MySpider(scrapy.Spider):


    # ...
        yield SplashRequest(url, self.parse_result,
            endpoint='execute',
            cache_args=['lua_source'],
            args={'lua_source': script},
            headers={'X-My-Header': 'value'},
        )

    def parse_result(self, response):
        # here response.body contains result HTML;
        # response.headers are filled with headers from last
        # web page loaded to Splash;
        # cookies from all responses and from JavaScript are collected
        # and put into Set-Cookie response header, so that Scrapy
        # can remember them.
.. _Splash Lua Script: http://splash.readthedocs.org/en/latest/scripting-tutorial.htmlHTTP Basic AuthIf you need to use HTTP Basic Authentication to access Splash, use the and  optional settings::SPLASH_USER = 'user'
SPLASH_PASS = 'userpass'
Another option is : it allows to setcustom headers which are sent to Splash server; add Authorization headerto  if you want to change credentials per-request::import scrapy
from w3lib.http import basic_auth_header

class MySpider(scrapy.Spider):
    # ...
    def start_requests(self):
        auth = basic_auth_header('user', 'userpass')
        yield SplashRequest(url, self.parse,
                            splash_headers={'Authorization': auth})
WARNING: Don't use _(i.e.  /  spider attributes) for Splashauthentication: if you occasionally send a non-Splash request from your spider,you may expose Splash credentials to a remote website, as HttpAuthMiddlewaresets credentials for all requests unconditionally... _HttpAuthMiddleware: http://doc.scrapy.org/en/latest/topics/downloader-middleware.html#module-scrapy.downloadermiddlewares.httpauthWhy not use the Splash HTTP API directly?The obvious alternative to scrapy-splash would be to send requests directlyto the Splash _. Take a look at the example below and makesure to read the observations after it::import json

import scrapy
from scrapy.http.headers import Headers

RENDER_HTML_URL = ""http://127.0.0.1:8050/render.html""

class MySpider(scrapy.Spider):
    start_urls = [""http://example.com"", ""http://example.com/foo""]

    def start_requests(self):
        for url in self.start_urls:
            body = json.dumps({""url"": url, ""wait"": 0.5}, sort_keys=True)
            headers = Headers({'Content-Type': 'application/json'})
            yield scrapy.Request(RENDER_HTML_URL, self.parse, method=""POST"",
                                 body=body, headers=headers)

    def parse(self, response):
        # response.body is a result of render.html call; it
        # contains HTML processed by a browser.
        # ...
It works and is easy enough, but there are some issues that you should beaware of:scrapy-splash utlities allow to handle such edge cases and reducethe boilerplate... _HTTP API: http://splash.readthedocs.org/en/latest/api.html.. _timeout: http://splash.readthedocs.org/en/latest/api.html#arg-timeoutGetting helpBest approach to get any other help is to ask a question on _.. _reporting Scrapy bugs: https://doc.scrapy.org/en/master/contributing.html#reporting-bugs.. _Splash FAQ: http://splash.readthedocs.io/en/stable/faq.html#website-is-not-rendered-correctly.. _Stack Overflow: https://stackoverflow.com/questions/tagged/scrapy-splash?sort=frequent&pageSize=15&mixed=1ContributingSource code and bug tracker are on github:https://github.com/scrapy-plugins/scrapy-splashTo run tests, install ""tox"" Python package and then run  commandfrom the source checkout.To run integration tests, start Splash and set SPLASH_URL env variableto Splash address before running  command::docker run -d --rm -p8050:8050 scrapinghub/splash:3.0SPLASH_URL=http://127.0.0.1:8050 tox -e py36"
https://github.com/awesto/django-shop,A Django based shop system,"django-SHOPDjango-SHOP aims to be a the easy, fun and fast e-commerce counterpart to.Here you can find the .Build the database model out of the product's properties – not vice versaMost e-commerce systems are shipped with a predefined database model for products. But products canvary a lot, and it simply is impossible to create a model which fits for all of them. This isesspecially true for products with a hierarchy of variants. In many popular e-commerce platforms,you either have far too many attributes per product, and/or the really required attributes aremissing.In django-SHOP implementations, the product models reflect their pysical properties making itpossible to create complete and deep hierarchies of variations, but without having to fiddle withunneeded properties. It furthermore avoids the need for an, whichis considered a database anti-pattern, because it produces far too many table joins, when filteringby property.Don't build pages using hard-coded templates – compose themWith the advent of frameworks, such as Angular, React, Vue and Aurelia, building web-applicationsshifted from a page-centric to a component-based approach.In django-SHOP, you are in full control over the page's layout, since all components areencapsulated and independent from each other. This means that instead of adopting the Catalog, Cart,Checkout and Order pages, use the django-CMS plugin system to compose everything required forthose pages.All Views are either HTML or RESTful servicesBrowser based navigation is important, but nowadays it's only one of many channels clients use tocommunicate with a web-server. Consider Single Page Applications or other native clients, where weuse RESTful APIs instead of pure HTTP.This substantially reduces the payload having to be transferred. It furthermore gives the client asmoother user experience, since only the content has to be updated, rather than having to do fullpage reloads.Programmable cart modifiersDuring checkout, taxes have to be applied or attributed. Depending on the shipping destination, theproduct group and other factors, this computation can either be simple or quite demanding.Django-SHOP offers a pluggable interface to create modifiers which calculate the cart's totals,taxes and other costs.This same interface can be extended to compute the weight and shipping costs. It also can be usedfor subtracting discounts or to add additional charges. Programmable workflow for fulfilment and deliveryFulfilling and shipping orders probably requires the most individual adaption for an e-commerce business.Django-SHOP offers a programmable interface for order by using a finitestate machine to adopt the workflow. Each order may have several states, but the only actionsallowed are limited to explicitly defined state transitions.It's modularWhenever possible, extra features should be added by third party libraries. This implies thatdjango-SHOP aims to provide an API, which allows merchants to add every feature they desire.Currently there are third party libraries for several Payment Service Providers, such as, , and .An open interface allows you to add any other provider.Shipping Service Providers may be added as third party library as well. With, ship your orders using one or more parcel servicesavailable for your region.Start by building your own demoInstead of providing an accessible online demo, django-SHOP can be set up in less than threeminutes and preconfigured to your needs. Having access to the product models, you can immediatlystart to play arround with, rename, and modify them to reflect the properties of your products.This is the easiest way to get a shop up and running out of the box with the flexibility of awebsite that you could have built from scratch.If you want to start with a fresh demo, please use the preparedand follow the instructions. Audience of django-SHOP usersSpecifically, we aim at providing a clean, modular and Pythonic/Djangonic implementation of ane-commerce framework, that a moderately experienced Django developer should be able to pick upand run easily. Pure Django models are used to describe each product type, and so the Django admincan be used to build a minimalistic editor for each of them.ConsultancyWe provide full consultancy support and are available for building complete e-commerce systems basedon django-SHOP. Please contact office@awesto.com for further questions.DocumentationRead the full documentation on Read-the-docs:"
https://github.com/cool-RR/PySnooper,Never use print for debugging again,"PySnooper - Never use print for debugging againPySnooper is a poor man's debugger. If you've used Bash, it's like  for Python, except it's fancier.Your story: You're trying to figure out why your Python code isn't doing what you think it should be doing. You'd love to use a full-fledged debugger with breakpoints and watches, but you can't be bothered to set one up right now.You want to know which lines are running and which aren't, and what the values of the local variables are.Most people would use  lines, in strategic locations, some of them showing the values of variables.PySnooper lets you do the same, except instead of carefully crafting the right  lines, you just add one decorator line to the function you're interested in. You'll get a play-by-play log of your function, including which lines ran and   when, and exactly when local variables were changed.What makes PySnooper stand out from all other code intelligence tools? You can use it in your shitty, sprawling enterprise codebase without having to do any setup. Just slap the decorator on, as shown below, and redirect the output to a dedicated log file by specifying its path as the first argument.ExampleWe're writing a function that converts a number to binary, by returning a list of bits. Let's snoop on it by adding the  decorator:import pysnooper

@pysnooper.snoop()
def number_to_bits(number):
    if number:
        bits = []
        while number:
            number, remainder = divmod(number, 2)
            bits.insert(0, remainder)
        return bits
    else:
        return [0]

number_to_bits(6)
The output to stderr is:Or if you don't want to trace an entire function, you can wrap the relevant part in a  block:import pysnooper
import random

def foo():
    lst = []
    for i in range(10):
        lst.append(random.randrange(1, 1000))

    with pysnooper.snoop():
        lower = min(lst)
        upper = max(lst)
        mid = (lower + upper) / 2
        print(lower, mid, upper)

foo()
which outputs something like:New var:....... i = 9
New var:....... lst = [681, 267, 74, 832, 284, 678, ...]
09:37:35.881721 line        10         lower = min(lst)
New var:....... lower = 74
09:37:35.882137 line        11         upper = max(lst)
New var:....... upper = 832
09:37:35.882304 line        12         mid = (lower + upper) / 2
74 453.0 832
New var:....... mid = 453.0
09:37:35.882486 line        13         print(lower, mid, upper)
Elapsed time: 00:00:00.000344
FeaturesIf stderr is not easily accessible for you, you can redirect the output to a file:@pysnooper.snoop('/my/log/file.log')
You can also pass a stream or a callable instead, and they'll be used.See values of some expressions that aren't local variables:@pysnooper.snoop(watch=('foo.bar', 'self.x[""whatever""]'))
Show snoop lines for functions that your function calls:@pysnooper.snoop(depth=2)
See  <------Installation with PipThe best way to install PySnooper is with Pip:$ pip install pysnooper
Other installation optionsConda with conda-forge channel:$ conda install -c conda-forge pysnooper
Arch Linux:$ yay -S python-pysnooper
Fedora Linux:$ dnf install python3-pysnooper
LicenseCopyright (c) 2019 Ram Rachum and collaborators, released under the MIT license.Media Coverageand  (22 April 2019)"
https://github.com/tkipf/pygcn,Graph Convolutional Networks in PyTorch,"Graph Convolutional Networks in PyTorchPyTorch implementation of Graph Convolutional Networks (GCNs) for semi-supervised classification [1].For a high-level introduction to GCNs, see:Thomas Kipf,  (2016)Note: There are subtle differences between the TensorFlow implementation in https://github.com/tkipf/gcn and this PyTorch re-implementation. This re-implementation serves as a proof of concept and is not intended for reproduction of the results reported in [1].This implementation makes use of the Cora dataset from [2].InstallationRequirementsUsageReferences[1] [2] CitePlease cite our paper if you use this code in your own work:@article{kipf2016semi,
  title={Semi-Supervised Classification with Graph Convolutional Networks},
  author={Kipf, Thomas N and Welling, Max},
  journal={arXiv preprint arXiv:1609.02907},
  year={2016}
}
"
https://github.com/PyQt5/PyQt,PyQt Examples（PyQt各种测试和例子） PyQt4 PyQt5,各种各样的PyQt测试和例子 论坛是专门针对PyQt5学习和提升开设的网站，分享大家平时学习中记录的笔记和例子，以及对遇到的问题进行收集整理。如果您觉得这里的东西对您有帮助，别忘了帮忙点一颗:star:小星星:star: | QQ群 or 状态目录其它项目感谢所有捐助者的鼓励， 列出了捐助者名单（由于一些收款渠道无法知道对方是谁，如有遗漏请联系我修改）or
https://github.com/Dod-o/Statistical-Learning-Method_Code,手写实现李航《统计学习方法》书中全部算法,前言力求每行代码都有注释，重要部分注明公式来源。具体会追求下方这样的代码，学习者可以照着公式看程序，让代码有据可查。如果时间充沛的话，可能会试着给每一章写一篇博客。先放个博客链接吧：。    注：其中Mnist数据集已转换为csv格式，由于体积为107M超过限制，改为压缩包形式。下载后务必先将Mnist文件内压缩包直接解压。【Updates】书籍出版：目前已与人民邮电出版社签订合同，未来将结合该repo整理出版机器学习实践相关书籍。同时会在book分支中对代码进行重构，欢迎在issue中提建议！同时issue中现有的问题也会考虑进去。（Feb 12 2022）线下培训：女朋友计划近期开办ML/MLP/CV线下培训班，地点北上广深杭，目标各方向快速入门，正在筹备。这里帮她打个广告，可以添加微信15324951814（备注线下培训）。本人也会被拉过去义务评估课程质量。。。（Feb 12 2022）无监督部分更新：部分无监督算法已更新！！！ 该部分由提供，在此感谢！ 有其他算法补充的同学也欢迎添加我微信并pr！（Jan 27 2021）实现监督部分第二章 感知机：博客：实现：第三章 K近邻：博客：实现：第四章 朴素贝叶斯：博客：实现：    第五章 决策树：博客：实现：    第六章 逻辑斯蒂回归与最大熵模型：博客：逻辑斯蒂回归：博客：最大熵：        实现：逻辑斯蒂回归：实现：最大熵：       第七章 支持向量机：博客：实现：    第八章 提升方法：实现：    第九章 EM算法及其推广：实现：    第十章 隐马尔可夫模型：实现：    无监督部分第十四章 聚类方法实现：实现：第十六章 主成分分析实现：第十七章 潜在语意分析实现：第十八章 概率潜在语意分析实现：第二十章 潜在狄利克雷分配实现：第二十一章 PageRank算法实现：许可 / License本项目内容许可遵循。The content of this project itself is licensed under the 联系欢迎pr，有疑问也可通过issue、微信或邮件联系。此外如果有需要MSRA实习内推的同学，欢迎骚扰。Wechat: lvtengchao（备注“blog-学校/单位-姓名”）Email: lvtengchao@pku.edu.cn      
https://github.com/chainer/chainer,A flexible framework of neural networks for deep learning,"[<marko.inline.RawText object at 0x000001592FDB9908>, <marko.inline.Link object at 0x000001592FD3A248>, <marko.inline.RawText object at 0x000001592FDB93C8>]Chainer: A deep learning framework| | | Tutorials ()| Examples (, )| | Forum (, )| Slack invitation (, )| Twitter (, )Chainer is a Python-based deep learning framework aiming at flexibility.It provides automatic differentiation APIs based on the define-by-run approach (a.k.a. dynamic computational graphs) as well as object-oriented high-level APIs to build and train neural networks.It also supports CUDA/cuDNN using  for high performance training and inference.For more details about Chainer, see the documents and resources listed above and join the community in Forum, Slack, and Twitter.InstallationFor more details, see the To install Chainer, use .$ pip install chainer
To enable CUDA support,  is required.Refer to the .Docker imageWe are providing the official Docker image.This image supports .Login to the environment with the following command, and run the Python interpreter to use Chainer with CUDA and cuDNN support.$ nvidia-docker run -it chainer/chainer /bin/bash
ContributionSee the .ChainerXSee the .LicenseMIT License (see  file).More informationReferencesTokui, Seiya, et al. ""Chainer: A Deep Learning Framework for Accelerating the Research Cycle."" Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM, 2019. Tokui, S., Oono, K., Hido, S. and Clayton, J.,Chainer: a Next-Generation Open Source Framework for Deep Learning,Proceedings of Workshop on Machine Learning Systems(LearningSys) in, (2015), Akiba, T., Fukuda, K. and Suzuki, S.,ChainerMN: Scalable Distributed Deep Learning Framework,Proceedings of Workshop on ML Systems in, (2017), "
https://github.com/voila-dashboards/voila,Voilà turns Jupyter notebooks into standalone web applications,"Rendering of live Jupyter notebooks with interactive widgets.IntroductionVoilà turns Jupyter notebooks into standalone web applications.Unlike the usual HTML-converted notebooks, each user connecting to the Voilàtornado application gets a dedicated Jupyter kernel which can execute thecallbacks to changes in Jupyter interactive widgets.InstallationVoilà can be installed with the mamba (or conda) package manager from conda-forgemamba install -c conda-forge voila
or from PyPIpip install voila
JupyterLab preview extensionVoilà provides a JupyterLab extension that displays a Voilà preview of your Notebook in a side-pane.Starting with JupyterLab 3.0, the extension is automatically installed after installing with .If you would like to install the extension from source, run the following command.jupyter labextension install @voila-dashboards/jupyterlab-preview
UsageAs a standalone tornado applicationTo render the  example notebook as a standalone app, run.To serve a directory of jupyter notebooks, run  with no argument.For example, to render the example notebook  from this repository with Voilà, you can first update your current environment with the requirements of this notebook (in this case in a  and render the notebook withmamba env update -f .binder/environment.yml
cd notebooks/
voila bqplot.ipynb
For more command line options (e.g., to specify an alternate port number),run .As a server extension to  or Voilà can also be used as a Jupyter server extension, both with the server or with.To install the Jupyter server extension, runjupyter serverextension enable voila
jupyter server extension enable voila
When running the Jupyter server, the Voilà app is accessible from the base urlsuffixed with .DocumentationTo get started with using Voilà, check out the full documentation:https://voila.readthedocs.io/ExamplesThe following two examples show how a standalone Jupyter notebook can be turned into a separate app, from the command-line integration.Rendering a notebook including interactive widgets and rich mime-type renderingRendering a notebook making use of a custom widget library ()Showing the source code for a Voilà notebookThe sources of the Jupyter notebook can be displayed in a Voilà app if option  is set to .Voilà dashboards with other language kernelsVoilà is built upon Jupyter standard formats and protocols, and is agnostic to the programming language of the notebook. In this example, we present an example of a Voilà application powered by the C++ Jupyter kernel , and the  project.The Voilà GalleryThe  is a collection of live dashboards and applications built with Voilà and Jupyter widgets.Most of the examples rely on widget libraries such as ipywidgets, ipyleaflet, ipyvolume, bqplot and ipympl, and showcase how to build complex web applications entirely based on notebooks.New examples can be added to the gallery by following the steps listed in the  repository.DevelopmentSee  to know how to contribute and set up a development environment.Related projectsVoilà depends on  and.LicenseWe use a shared copyright model that enables all contributors to maintain thecopyright on their contributions.This software is licensed under the BSD-3-Clause license. See the file for details."
https://github.com/facebookresearch/detr,End-to-End Object Detection with Transformers,"DE⫶TR: End-to-End Object Detection with TransformersPyTorch training code and pretrained models for DETR (DEtection TRansformer).We replace the full complex hand-crafted object detection pipeline with a Transformer, and match Faster R-CNN with a ResNet-50, obtaining 42 AP on COCO using half the computation power (FLOPs) and the same number of parameters. Inference in 50 lines of PyTorch.What it is. Unlike traditional computer vision techniques, DETR approaches object detection as a direct set prediction problem. It consists of a set-based global loss, which forces unique predictions via bipartite matching, and a Transformer encoder-decoder architecture.Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. Due to this parallel nature, DETR is very fast and efficient.About the code. We believe that object detection should not be more difficult than classification,and should not require complex libraries for training and inference.DETR is very simple to implement and experiment with, and we provide ashowing how to do inference with DETR in only a few lines of PyTorch code.Training code follows this idea - it is not a library,but simply a  importing model and criteriondefinitions with standard training loops.Additionnally, we provide a Detectron2 wrapper in the d2/ folder. See the readme there for more information.For details see  by Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko.See our  to learn more about end to end object detection with transformers.Model ZooWe provide baseline DETR and DETR-DC5 models, and plan to include more in future.AP is computed on COCO 2017 val5k, and inference time is over the first 100 val5k COCO images,with torchscript transformer.COCO val5k evaluation results can be found in this .The models are also available via torch hub,to load DETR R50 with pretrained weights simply do:model = torch.hub.load('facebookresearch/detr:main', 'detr_resnet50', pretrained=True)
COCO panoptic val5k models:Checkout our to see how to use and visualize DETR's panoptic segmentation prediction.NotebooksWe provide a few notebooks in colab to help you get a grasp on DETR:Usage - Object detectionThere are no extra compiled components in DETR and package dependencies are minimal,so the code is very simple to use. We provide instructions how to install dependencies via conda.First, clone the repository locally:git clone https://github.com/facebookresearch/detr.git
Then, install PyTorch 1.5+ and torchvision 0.6+:conda install -c pytorch pytorch torchvision
Install pycocotools (for evaluation on COCO) and scipy (for training):conda install cython scipy
pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'
That's it, should be good to train and evaluate detection models.(optional) to work with panoptic install panopticapi:pip install git+https://github.com/cocodataset/panopticapi.git
Data preparationDownload and extract COCO 2017 train and val images with annotations from.We expect the directory structure to be the following:path/to/coco/
  annotations/  # annotation json files
  train2017/    # train images
  val2017/      # val images
TrainingTo train baseline DETR on a single node with 8 gpus for 300 epochs run:python -m torch.distributed.launch --nproc_per_node=8 --use_env main.py --coco_path /path/to/coco 
A single epoch takes 28 minutes, so 300 epoch trainingtakes around 6 days on a single machine with 8 V100 cards.To ease reproduction of our results we providefor 150 epoch schedule (3 days on a single machine), achieving 39.5/60.3 AP/AP50.We train DETR with AdamW setting learning rate in the transformer to 1e-4 and 1e-5 in the backbone.Horizontal flips, scales and crops are used for augmentation.Images are rescaled to have min size 800 and max size 1333.The transformer is trained with dropout of 0.1, and the whole model is trained with grad clip of 0.1.EvaluationTo evaluate DETR R50 on COCO val5k with a single GPU run:python main.py --batch_size 2 --no_aux_loss --eval --resume https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth --coco_path /path/to/coco
We provide results for all DETR detection models in this.Note that numbers vary depending on batch size (number of images) per GPU.Non-DC5 models were trained with batch size 2, and DC5 with 1,so DC5 models show a significant drop in AP if evaluated with morethan 1 image per GPU.Multinode trainingDistributed training is available via Slurm and :pip install submitit
Train baseline DETR-6-6 model on 4 nodes for 300 epochs:python run_with_submitit.py --timeout 3000 --coco_path /path/to/coco
Usage - SegmentationWe show that it is relatively straightforward to extend DETR to predict segmentation masks. We mainly demonstrate strong panoptic segmentation results.Data preparationFor panoptic segmentation, you need the panoptic annotations additionally to the coco dataset (see above for the coco dataset). You need to download and extract the .We expect the directory structure to be the following:path/to/coco_panoptic/
  annotations/  # annotation json files
  panoptic_train2017/    # train panoptic annotations
  panoptic_val2017/      # val panoptic annotations
TrainingWe recommend training segmentation in two stages: first train DETR to detect all the boxes, and then train the segmentation head.For panoptic segmentation, DETR must learn to detect boxes for both stuff and things classes. You can train it on a single node with 8 gpus for 300 epochs with:python -m torch.distributed.launch --nproc_per_node=8 --use_env main.py --coco_path /path/to/coco  --coco_panoptic_path /path/to/coco_panoptic --dataset_file coco_panoptic --output_dir /output/path/box_model
For instance segmentation, you can simply train a normal box model (or used a pre-trained one we provide).Once you have a box model checkpoint, you need to freeze it, and train the segmentation head in isolation.For panoptic segmentation you can train on a single node with 8 gpus for 25 epochs:python -m torch.distributed.launch --nproc_per_node=8 --use_env main.py --masks --epochs 25 --lr_drop 15 --coco_path /path/to/coco  --coco_panoptic_path /path/to/coco_panoptic  --dataset_file coco_panoptic --frozen_weights /output/path/box_model/checkpoint.pth --output_dir /output/path/segm_model
For instance segmentation only, simply remove the  and  arguments from the above command line.LicenseDETR is released under the Apache 2.0 license. Please see the  file for more information.ContributingWe actively welcome your pull requests! Please see  and  for more info."
https://github.com/dabeaz/curio,Good Curio!,"CurioCurio is a coroutine-based library for concurrent Python systemsprogramming using async/await.  It provides standard programmingabstractions such as tasks, sockets, files, locks, and queues aswell as some advanced features such as support for structuredconcurrency. It works on Unix and Windows and has zero dependencies.You'll find it to be familiar, small, fast, and fun.Important Notice: October 25, 2022The Curio project is no longer making package releases.  I'm more thanhappy to accept bug reports and may continue to work on it from timeto time as the mood strikes.  If you want the absolute latest version, youshould vendor the source code from here. Curio has no dependenciesother than the Python standard library.  --DaveCurio is DifferentOne of the most important ideas from software architecture is the""separation of concerns.""  This can take many forms such as utilizingabstraction layers, object oriented programming, aspects, higher-orderfunctions, and so forth.  However, another effective form of it existsin the idea of separating execution environments.  For example, ""usermode"" versus ""kernel mode"" in operating systems.  This is theunderlying idea in Curio, but applied to ""asynchronous"" versus""synchronous"" execution.A fundamental problem with asynchronous code is that it involves acompletely different evaluation model that doesn't compose well withordinary applications or with other approaches to concurrency such asthread programing.  Although the addition of ""async/await"" to Pythonhelps clarify such code, ""async"" libraries still tend to be a confusedmess of functionality that mix asynchronous and synchronousfunctionality together in the same environment--often bolting it alltogether with an assortment of hacks that try to sort out all ofassociated API confusion.Curio strictly separates asynchronous code from synchronous code.Specifically, all functionality related to the asynchronousenvironment utilizes ""async/await"" features and syntax--withoutexception.  Moreover, interactions between async and sync code iscarefully managed through a small set of simple mechanisms such asevents and queues.  As a result, Curio is small, fast, andsignificantly easier to reason about.A Simple ExampleHere is a concurrent TCP echo server directly implemented using sockets:.. code:: python# echoserv.py

from curio import run, spawn
from curio.socket import *

async def echo_server(address):
    sock = socket(AF_INET, SOCK_STREAM)
    sock.setsockopt(SOL_SOCKET, SO_REUSEADDR, 1)
    sock.bind(address)
    sock.listen(5)
    print('Server listening at', address)
    async with sock:
        while True:
            client, addr = await sock.accept()
            await spawn(echo_client, client, addr, daemon=True)

async def echo_client(client, addr):
    print('Connection from', addr)
    async with client:
         while True:
             data = await client.recv(100000)
             if not data:
                 break
             await client.sendall(data)
    print('Connection closed')

if __name__ == '__main__':
    run(echo_server, ('',25000))
If you've done network programming with threads, it looks almostidentical. Moreover, it can handle thousands of clients even though nothreads are being used inside.Core FeaturesCurio supports standard synchronization primitives (events, locks,recursive locks, semaphores, and condition variables), queues,subprocesses, as well as running tasks in threads and processes.  Thetask model fully supports cancellation, task groups, timeouts,monitoring, and other features critical to writing reliable code.Read the _ formore in-depth coverage.  The _ is a goodstarting point.  The _ describes how tocarry out common programming tasks.Talks Related to CurioConcepts related to Curio's design and general issues related to asyncprogramming have been described by Curio's creator, _, invarious conference talks and tutorials:Questions and AnswersQ: What is the point of the Curio project?A: Curio is async programming, reimagined as something smaller, faster, and easierto reason about. It is meant to be both educational and practical.Q: Is Curio implemented using asyncio?A: No. Curio is a standalone library directly created from low-level I/O primitives.Q: Is Curio meant to be a clone of asyncio?A: No. Although Curio provides a significant amount of overlappingfunctionality, the API is different.  Compatibility with otherlibaries is not a goal.Q: Is Curio meant to be compatible with other async libraries?A: No. Curio is a stand-alone project that emphasizes a certainsoftware architecture based on separation of environments.  Otherlibraries have largely ignored this concept, preferring to simplyprovide variations on the existing approach found in asyncio.Q: Can Curio interoperate with other event loops?A: It depends on what you mean by the word ""interoperate.""  Curio'spreferred mechanism of communication with the external world is aqueue.  It is possible to communicate between Curio, threads, andother event loops using queues.  Q: How fast is Curio?A: Curio's primary goal is to be an async library that is minimal andunderstandable. Performance is not the primary concern.  That said, inrough benchmarking of a simple echo server, Curio is more than twiceas fast as comparable code using coroutines in  or.  This was last measured on OS-X using Python 3.9.  Keep inmind there is a lot more to overall application performance than theperformance of a simple echo server so your mileage mightvary. However, as a runtime environment, Curio doesn't introduce a lot ofextra overhead. See the  directory for varioustesting programs.Q: What is the future of Curio?A: Curio should be viewed as a library of basic programmingprimitives.  At this time, it is considered to befeature-complete--meaning that it is not expected to sprout many newcapabilities.  It may be updated from time to time to fix bugs orsupport new versions of Python.Q: Can I contribute?A: Curio is not a community-based project seeking developersor maintainers.  However, having it work reliably is important. If you'vefound a bug or have an idea for making it better, pleasefile an _. ContributorsThe following people contributed ideas to early stages of the Curio project:Brett Cannon, Nathaniel Smith, Alexander Zhukov, Laura Dickinson, and Sandeep Gupta.WhoCurio is the creation of David Beazley (@dabeaz) who is alsoresponsible for its maintenance.  http://www.dabeaz.comP.S.If you want to learn more about concurrent programming more generally, you shouldcome take a _!.. |--| unicode:: U+2013   .. en dash.. |---| unicode:: U+2014  .. em dash, trimming surrounding whitespace:trim:"
https://github.com/Rapptz/discord.py,An API wrapper for Discord written in Python.,"discord.py.. image:: https://discord.com/api/guilds/336642139381301249/embed.png:target: https://discord.gg/r3sSKJJ:alt: Discord server invite.. image:: https://img.shields.io/pypi/v/discord.py.svg:target: https://pypi.python.org/pypi/discord.py:alt: PyPI version info.. image:: https://img.shields.io/pypi/pyversions/discord.py.svg:target: https://pypi.python.org/pypi/discord.py:alt: PyPI supported Python versionsA modern, easy to use, feature-rich, and async ready API wrapper for Discord written in Python.Key FeaturesInstallingPython 3.8 or higher is requiredTo install the library without full voice support, you can just run the following command:.. code:: sh# Linux/macOS
python3 -m pip install -U discord.py

# Windows
py -3 -m pip install -U discord.py
Otherwise to get voice support you should run the following command:.. code:: sh# Linux/macOS
python3 -m pip install -U ""discord.py[voice]""

# Windows
py -3 -m pip install -U discord.py[voice]
To install the development version, do the following:.. code:: sh$ git clone https://github.com/Rapptz/discord.py
$ cd discord.py
$ python3 -m pip install -U .[voice]
Optional Packages
* `PyNaCl <https://pypi.org/project/PyNaCl/>`__ (for voice support)

Please note that when installing voice support on Linux, you must install the following packages via your favourite package manager (e.g. ``apt``, ``dnf``, etc) before running the above commands:

* libffi-dev (or ``libffi-devel`` on some systems)
* python-dev (e.g. ``python3.8-dev`` for Python 3.8)

Quick Example
--------------

.. code:: py

    import discord

    class MyClient(discord.Client):
        async def on_ready(self):
            print('Logged on as', self.user)

        async def on_message(self, message):
            # don't respond to ourselves
            if message.author == self.user:
                return

            if message.content == 'ping':
                await message.channel.send('pong')

    intents = discord.Intents.default()
    intents.message_content = True
    client = MyClient(intents=intents)
    client.run('token')

Bot Example
~~~~~~~~~~~~~

.. code:: py

    import discord
    from discord.ext import commands

    intents = discord.Intents.default()
    intents.message_content = True
    bot = commands.Bot(command_prefix='>', intents=intents)

    @bot.command()
    async def ping(ctx):
        await ctx.send('pong')

    bot.run('token')

You can find more examples in the examples directory.

Links
------

- `Documentation <https://discordpy.readthedocs.io/en/latest/index.html>`_
- `Official Discord Server <https://discord.gg/r3sSKJJ>`_
- `Discord API <https://discord.gg/discord-api>`_
"
https://github.com/Urinx/WeixinBot,网页版微信API，包含终端版微信及微信机器人,"WeixinBot   网页版微信API，包含终端版微信及微信机器人ContentsDemo为了确保能正常运行示例脚本，请安装所需的第三方包。pip install -r requirements.txt
注：下面演示的图片与功能可能不是最新的，具体请看源码。按照操作指示在手机微信上扫描二维码然后登录，你可以选择是否开启自动回复模式。开启自动回复模式后，如果接收到的是文字消息就会自动回复，包括群消息。名片，链接，动画表情和地址位置消息。网页版上有的功能目前基本上都能支持。Web Weixin Pipeline       +--------------+     +---------------+   +---------------+
       |              |     |               |   |               |
       |   Get UUID   |     |  Get Contact  |   | Status Notify |
       |              |     |               |   |               |
       +-------+------+     +-------^-------+   +-------^-------+
               |                    |                   |
               |                    +-------+  +--------+
               |                            |  |
       +-------v------+               +-----+--+------+      +--------------+
       |              |               |               |      |              |
       |  Get QRCode  |               |  Weixin Init  +------>  Sync Check  <----+
       |              |               |               |      |              |    |
       +-------+------+               +-------^-------+      +-------+------+    |
               |                              |                      |           |
               |                              |                      +-----------+
               |                              |                      |
       +-------v------+               +-------+--------+     +-------v-------+
       |              | Confirm Login |                |     |               |
+------>    Login     +---------------> New Login Page |     |  Weixin Sync  |
|      |              |               |                |     |               |
|      +------+-------+               +----------------+     +---------------+
|             |
|QRCode Scaned|
+-------------+
Web Weixin API登录| API | 获取 UUID || --- | --------- || url | https://login.weixin.qq.com/jslogin || method | POST || data | URL Encode || params | appid:   fun: new   lang: zhCN   _:  |返回数据(String):window.QRLogin.code = 200; window.QRLogin.uuid = ""xxx""
| API | 绑定登陆（webwxpushloginurl） || --- | --------- || url | https://wx.qq.com/cgi-bin/mmwebwx-bin/webwxpushloginurl || method | GET || params | uin: xxx |返回数据(String):{'msg': 'all ok', 'uuid': 'xxx', 'ret': '0'}

通过这种方式可以省掉扫二维码这步操作，更加方便
| API | 生成二维码 || --- | --------- || url | https://login.weixin.qq.com/l/  || method | GET || API | 二维码扫描登录 || --- | --------- || url | https://login.weixin.qq.com/cgi-bin/mmwebwx-bin/login || method | GET || params | tip: 1  0   uuid: xxx  _:  |返回数据(String):window.code=xxx;

xxx:
	408 登陆超时
	201 扫描成功
	200 确认登录

当返回200时，还会有
window.redirect_uri=""https://wx.qq.com/cgi-bin/mmwebwx-bin/webwxnewloginpage?ticket=xxx&uuid=xxx&lang=xxx&scan=xxx"";
| API | webwxnewloginpage || --- | --------- || url | https://wx.qq.com/cgi-bin/mmwebwx-bin/webwxnewloginpage || method | GET || params | ticket: xxx  uuid: xxx  lang: zh_CN   scan: xxx  fun: new |返回数据(XML):<error>
	<ret>0</ret>
	<message>OK</message>
	<skey>xxx</skey>
	<wxsid>xxx</wxsid>
	<wxuin>xxx</wxuin>
	<pass_ticket>xxx</pass_ticket>
	<isgrayscale>1</isgrayscale>
</error>
微信初始化| API | webwxinit || --- | --------- || url | https://wx.qq.com/cgi-bin/mmwebwx-bin/webwxinit?pass_ticket=xxx&skey=xxx&r=xxx || method | POST || data | JSON || header | ContentType: application/json; charset=UTF-8 || params | {  &nbsp;&nbsp;&nbsp;&nbsp; BaseRequest: {  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;	Uin: xxx, 	&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Sid: xxx,  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;	Skey: xxx,  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; DeviceID: xxx,  &nbsp;&nbsp;&nbsp;&nbsp; }  } |返回数据(JSON):{
	""BaseResponse"": {
		""Ret"": 0,
		""ErrMsg"": """"
	},
	""Count"": 11,
	""ContactList"": [...],
	""SyncKey"": {
		""Count"": 4,
		""List"": [
			{
				""Key"": 1,
				""Val"": 635705559
			},
			...
		]
	},
	""User"": {
		""Uin"": xxx,
		""UserName"": xxx,
		""NickName"": xxx,
		""HeadImgUrl"": xxx,
		""RemarkName"": """",
		""PYInitial"": """",
		""PYQuanPin"": """",
		""RemarkPYInitial"": """",
		""RemarkPYQuanPin"": """",
		""HideInputBarFlag"": 0,
		""StarFriend"": 0,
		""Sex"": 1,
		""Signature"": ""Apt-get install B"",
		""AppAccountFlag"": 0,
		""VerifyFlag"": 0,
		""ContactFlag"": 0,
		""WebWxPluginSwitch"": 0,
		""HeadImgFlag"": 1,
		""SnsFlag"": 17
	},
	""ChatSet"": xxx,
	""SKey"": xxx,
	""ClientVersion"": 369297683,
	""SystemTime"": 1453124908,
	""GrayScale"": 1,
	""InviteStartCount"": 40,
	""MPSubscribeMsgCount"": 2,
	""MPSubscribeMsgList"": [...],
	""ClickReportInterval"": 600000
}
| API | webwxstatusnotify || --- | --------- || url | https://wx.qq.com/cgi-bin/mmwebwx-bin/webwxstatusnotify?lang=zh_CN&pass_ticket=xxx || method | POST || data | JSON || header | ContentType: application/json; charset=UTF-8 || params | {  &nbsp;&nbsp;&nbsp;&nbsp; BaseRequest: { Uin: xxx, Sid: xxx, Skey: xxx, DeviceID: xxx },  &nbsp;&nbsp;&nbsp;&nbsp; Code: 3,  &nbsp;&nbsp;&nbsp;&nbsp; FromUserName: ,  &nbsp;&nbsp;&nbsp;&nbsp; ToUserName: ,  &nbsp;&nbsp;&nbsp;&nbsp; ClientMsgId:   } |返回数据(JSON):{
	""BaseResponse"": {
		""Ret"": 0,
		""ErrMsg"": """"
	},
	...
}
获取联系人信息| API | webwxgetcontact || --- | --------- || url | https://wx.qq.com/cgi-bin/mmwebwx-bin//webwxgetcontact?pass_ticket=xxx&skey=xxx&r=xxx || method | POST || data | JSON || header | ContentType: application/json; charset=UTF-8 |返回数据(JSON):{
	""BaseResponse"": {
		""Ret"": 0,
		""ErrMsg"": """"
	},
	""MemberCount"": 334,
	""MemberList"": [
		{
			""Uin"": 0,
			""UserName"": xxx,
			""NickName"": ""Urinx"",
			""HeadImgUrl"": xxx,
			""ContactFlag"": 3,
			""MemberCount"": 0,
			""MemberList"": [],
			""RemarkName"": """",
			""HideInputBarFlag"": 0,
			""Sex"": 0,
			""Signature"": ""你好，我们是地球三体组织。在这里，你将感受到不一样的思维模式，以及颠覆常规的世界观。而我们的目标，就是以三体人的智慧，引领人类未来科学技术500年。"",
			""VerifyFlag"": 8,
			""OwnerUin"": 0,
			""PYInitial"": ""URINX"",
			""PYQuanPin"": ""Urinx"",
			""RemarkPYInitial"": """",
			""RemarkPYQuanPin"": """",
			""StarFriend"": 0,
			""AppAccountFlag"": 0,
			""Statues"": 0,
			""AttrStatus"": 0,
			""Province"": """",
			""City"": """",
			""Alias"": ""Urinxs"",
			""SnsFlag"": 0,
			""UniFriend"": 0,
			""DisplayName"": """",
			""ChatRoomId"": 0,
			""KeyWord"": ""gh_"",
			""EncryChatRoomId"": """"
		},
		...
	],
	""Seq"": 0
}
| API | webwxbatchgetcontact || --- | --------- || url | https://wx.qq.com/cgi-bin/mmwebwx-bin/webwxbatchgetcontact?type=ex&r=xxx&pass_ticket=xxx || method | POST || data | JSON || header | ContentType: application/json; charset=UTF-8 || params | {  &nbsp;&nbsp;&nbsp;&nbsp; BaseRequest: { Uin: xxx, Sid: xxx, Skey: xxx, DeviceID: xxx },  &nbsp;&nbsp;&nbsp;&nbsp; Count: ,  &nbsp;&nbsp;&nbsp;&nbsp; List: [  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; { UserName: , EncryChatRoomId: """" },  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ...  &nbsp;&nbsp;&nbsp;&nbsp; ],  } |返回数据(JSON)同上同步刷新| API | synccheck || --- | --------- || protocol | https || host | webpush.weixin.qq.com  webpush.wx2.qq.com  webpush.wx8.qq.com  webpush.wx.qq.com  webpush.web2.wechat.com  webpush.web.wechat.com || path | /cgi-bin/mmwebwx-bin/synccheck || method | GET || data | URL Encode || params | r:   sid: xxx  uin: xxx  skey: xxx  deviceid: xxx  synckey: xxx  _:  |返回数据(String):window.synccheck={retcode:""xxx"",selector:""xxx""}

retcode:
	0 正常
	1100 失败/登出微信
selector:
	0 正常
	2 新的消息
	7 进入/离开聊天界面
| API | webwxsync || --- | --------- || url | https://wx.qq.com/cgi-bin/mmwebwx-bin/webwxsync?sid=xxx&skey=xxx&pass_ticket=xxx || method | POST || data | JSON || header | ContentType: application/json; charset=UTF-8 || params | {  &nbsp;&nbsp;&nbsp;&nbsp; BaseRequest: { Uin: xxx, Sid: xxx, Skey: xxx, DeviceID: xxx },  &nbsp;&nbsp;&nbsp;&nbsp; SyncKey: xxx,  &nbsp;&nbsp;&nbsp;&nbsp; rr:   } |返回数据(JSON):{
	'BaseResponse': {'ErrMsg': '', 'Ret': 0},
	'SyncKey': {
		'Count': 7,
		'List': [
			{'Val': 636214192, 'Key': 1},
			...
		]
	},
	'ContinueFlag': 0,
	'AddMsgCount': 1,
	'AddMsgList': [
		{
			'FromUserName': '',
			'PlayLength': 0,
			'RecommendInfo': {...},
			'Content': """", 
			'StatusNotifyUserName': '',
			'StatusNotifyCode': 5,
			'Status': 3,
			'VoiceLength': 0,
			'ToUserName': '',
			'ForwardFlag': 0,
			'AppMsgType': 0,
			'AppInfo': {'Type': 0, 'AppID': ''},
			'Url': '',
			'ImgStatus': 1,
			'MsgType': 51,
			'ImgHeight': 0,
			'MediaId': '', 
			'FileName': '',
			'FileSize': '',
			...
		},
		...
	],
	'ModChatRoomMemberCount': 0,
	'ModContactList': [],
	'DelContactList': [],
	'ModChatRoomMemberList': [],
	'DelContactCount': 0,
	...
}
消息接口| API | webwxsendmsg || --- | ------------ || url | https://wx.qq.com/cgi-bin/mmwebwx-bin/webwxsendmsg?pass_ticket=xxx || method | POST || data | JSON || header | ContentType: application/json; charset=UTF-8 || params | {  &nbsp;&nbsp;&nbsp;&nbsp; BaseRequest: { Uin: xxx, Sid: xxx, Skey: xxx, DeviceID: xxx },  &nbsp;&nbsp;&nbsp;&nbsp; Msg: {  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Type: 1 ,  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Content: ,  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; FromUserName: ,  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ToUserName: ,  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; LocalID: ,  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ClientMsgId:   &nbsp;&nbsp;&nbsp;&nbsp; }  } |返回数据(JSON):{
	""BaseResponse"": {
		""Ret"": 0,
		""ErrMsg"": """"
	},
	...
}
| API | webwxrevokemsg || --- | ------------ || url | https://wx.qq.com/cgi-bin/mmwebwx-bin/webwxrevokemsg || method | POST || data | JSON || header | ContentType: application/json; charset=UTF-8 || params | {  &nbsp;&nbsp;&nbsp;&nbsp; BaseRequest: { Uin: xxx, Sid: xxx, Skey: xxx, DeviceID: xxx },  &nbsp;&nbsp;&nbsp;&nbsp; SvrMsgId: msg_id,  &nbsp;&nbsp;&nbsp;&nbsp; ToUserName: user_id,  &nbsp;&nbsp;&nbsp;&nbsp; ClientMsgId: local_msg_id   } |返回数据(JSON):{
	""BaseResponse"": {
		""Ret"": 0,
		""ErrMsg"": """"
	}
}
发送表情| API | webwxsendmsgemotion || --- | ------------ || url | https://wx2.qq.com/cgi-bin/mmwebwx-bin/webwxsendemoticon?fun=sys&f=json&pass_ticket=xxx || method | POST || data | JSON || header | ContentType: application/json; charset=UTF-8 || params | {  &nbsp;&nbsp;&nbsp;&nbsp; BaseRequest: { Uin: xxx, Sid: xxx, Skey: xxx, DeviceID: xxx },  &nbsp;&nbsp;&nbsp;&nbsp; Msg: {  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Type: 47 ,  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; EmojiFlag: 2,  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; MediaId: ,  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; FromUserName: ,  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ToUserName: ,  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; LocalID: ,  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ClientMsgId:   &nbsp;&nbsp;&nbsp;&nbsp; }  } |图片接口| API | webwxgeticon || --- | ------------ || url | https://wx.qq.com/cgi-bin/mmwebwx-bin/webwxgeticon || method | GET || params | seq:   username:   skey: xxx || API | webwxgetheadimg || --- | --------------- || url | https://wx.qq.com/cgi-bin/mmwebwx-bin/webwxgetheadimg || method | GET || params | seq:   username:   skey: xxx || API | webwxgetmsgimg || --- | --------------- || url | https://wx.qq.com/cgi-bin/mmwebwx-bin/webwxgetmsgimg || method | GET || params | MsgID:   type: slave  or   skey: xxx |多媒体接口| API | webwxgetvideo || --- | --------------- || url | https://wx.qq.com/cgi-bin/mmwebwx-bin/webwxgetvideo || method | GET || params | msgid:   skey: xxx || API | webwxgetvoice || --- | --------------- || url | https://wx.qq.com/cgi-bin/mmwebwx-bin/webwxgetvoice || method | GET || params | msgid:   skey: xxx |账号类型| 类型 | 说明 || :--: | --- || 个人账号 | 以开头，例如： || 群聊 | 以开头，例如： || 公众号/服务号 | 以开头，但其 & 8 != 0  :  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 一般个人公众号/服务号：8  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 一般企业的服务号：24  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 微信官方账号：56 || 特殊账号 | 像文件传输助手之类的账号，有特殊的ID，目前已知的有： , , , , , , , , , , , , , , , , , , , , , , , , , , , ,  |消息类型消息一般格式：{
	""FromUserName"": """",
	""ToUserName"": """",
	""Content"": """",
	""StatusNotifyUserName"": """",
	""ImgWidth"": 0,
	""PlayLength"": 0,
	""RecommendInfo"": {...},
	""StatusNotifyCode"": 4,
	""NewMsgId"": """",
	""Status"": 3,
	""VoiceLength"": 0,
	""ForwardFlag"": 0,
	""AppMsgType"": 0,
	""Ticket"": """",
	""AppInfo"": {...},
	""Url"": """",
	""ImgStatus"": 1,
	""MsgType"": 1,
	""ImgHeight"": 0,
	""MediaId"": """",
	""MsgId"": """",
	""FileName"": """",
	""HasProductId"": 0,
	""FileSize"": """",
	""CreateTime"": 1454602196,
	""SubMsgType"": 0
}
| MsgType | 说明 || ------- | --- || 1  | 文本消息 || 3  | 图片消息 || 34 | 语音消息 || 37 | 好友确认消息 || 40 | POSSIBLEFRIEND_MSG || 42 | 共享名片 || 43 | 视频消息 || 47 | 动画表情 || 48 | 位置消息 || 49 | 分享链接 || 50 | VOIPMSG || 51 | 微信初始化消息 || 52 | VOIPNOTIFY || 53 | VOIPINVITE || 62 | 小视频 || 9999 | SYSNOTICE || 10000 | 系统消息 || 10002 | 撤回消息 |微信初始化消息MsgType: 51
FromUserName: 自己ID
ToUserName: 自己ID
StatusNotifyUserName: 最近联系的联系人ID
Content:
	<msg>
	    <op id='4'>
	        <username>
	        	// 最近联系的联系人
	            filehelper,xxx@chatroom,wxid_xxx,xxx,...
	        </username>
	        <unreadchatlist>
	            <chat>
	                <username>
	                	// 朋友圈
	                    MomentsUnreadMsgStatus
	                </username>
	                <lastreadtime>
	                    1454502365
	                </lastreadtime>
	            </chat>
	        </unreadchatlist>
	        <unreadfunctionlist>
	        	// 未读的功能账号消息，群发助手，漂流瓶等
	        </unreadfunctionlist>
	    </op>
	</msg>
文本消息MsgType: 1
FromUserName: 发送方ID
ToUserName: 接收方ID
Content: 消息内容
图片消息MsgType: 3
FromUserName: 发送方ID
ToUserName: 接收方ID
MsgId: 用于获取图片
Content:
	<msg>
		<img length=""6503"" hdlength=""0"" />
		<commenturl></commenturl>
	</msg>
小视频消息MsgType: 62
FromUserName: 发送方ID
ToUserName: 接收方ID
MsgId: 用于获取小视频
Content:
	<msg>
		<img length=""6503"" hdlength=""0"" />
		<commenturl></commenturl>
	</msg>
地理位置消息MsgType: 1
FromUserName: 发送方ID
ToUserName: 接收方ID
Content: http://weixin.qq.com/cgi-bin/redirectforward?args=xxx
// 属于文本消息，只不过内容是一个跳转到地图的链接
名片消息MsgType: 42
FromUserName: 发送方ID
ToUserName: 接收方ID
Content:
	<?xml version=""1.0""?>
	<msg bigheadimgurl="""" smallheadimgurl="""" username="""" nickname=""""  shortpy="""" alias="""" imagestatus=""3"" scene=""17"" province="""" city="""" sign="""" sex=""1"" certflag=""0"" certinfo="""" brandIconUrl="""" brandHomeUrl="""" brandSubscriptConfigUrl="""" brandFlags=""0"" regionCode="""" />

RecommendInfo:
	{
		""UserName"": ""xxx"", // ID
		""Province"": ""xxx"", 
		""City"": ""xxx"", 
		""Scene"": 17, 
		""QQNum"": 0, 
		""Content"": """", 
		""Alias"": ""xxx"", // 微信号
		""OpCode"": 0, 
		""Signature"": """", 
		""Ticket"": """", 
		""Sex"": 0, // 1:男, 2:女
		""NickName"": ""xxx"", // 昵称
		""AttrStatus"": 4293221, 
		""VerifyFlag"": 0
	}
语音消息MsgType: 34
FromUserName: 发送方ID
ToUserName: 接收方ID
MsgId: 用于获取语音
Content:
	<msg>
		<voicemsg endflag=""1"" cancelflag=""0"" forwardflag=""0"" voiceformat=""4"" voicelength=""1580"" length=""2026"" bufid=""216825389722501519"" clientmsgid=""49efec63a9774a65a932a4e5fcd4e923filehelper174_1454602489"" fromusername="""" />
	</msg>
动画表情MsgType: 47
FromUserName: 发送方ID
ToUserName: 接收方ID
Content:
	<msg>
		<emoji fromusername = """" tousername = """" type=""2"" idbuffer=""media:0_0"" md5=""e68363487d8f0519c4e1047de403b2e7"" len = ""86235"" productid=""com.tencent.xin.emoticon.bilibili"" androidmd5=""e68363487d8f0519c4e1047de403b2e7"" androidlen=""86235"" s60v3md5 = ""e68363487d8f0519c4e1047de403b2e7"" s60v3len=""86235"" s60v5md5 = ""e68363487d8f0519c4e1047de403b2e7"" s60v5len=""86235"" cdnurl = ""http://emoji.qpic.cn/wx_emoji/eFygWtxcoMF8M0oCCsksMA0gplXAFQNpiaqsmOicbXl1OC4Tyx18SGsQ/"" designerid = """" thumburl = ""http://mmbiz.qpic.cn/mmemoticon/dx4Y70y9XctRJf6tKsy7FwWosxd4DAtItSfhKS0Czr56A70p8U5O8g/0"" encrypturl = ""http://emoji.qpic.cn/wx_emoji/UyYVK8GMlq5VnJ56a4GkKHAiaC266Y0me0KtW6JN2FAZcXiaFKccRevA/"" aeskey= ""a911cc2ec96ddb781b5ca85d24143642"" ></emoji> 
		<gameext type=""0"" content=""0"" ></gameext>
	</msg>
普通链接或应用分享消息MsgType: 49
AppMsgType: 5
FromUserName: 发送方ID
ToUserName: 接收方ID
Url: 链接地址
FileName: 链接标题
Content:
	<msg>
		<appmsg appid=""""  sdkver=""0"">
			<title></title>
			<des></des>
			<type>5</type>
			<content></content>
			<url></url>
			<thumburl></thumburl>
			...
		</appmsg>
		<appinfo>
			<version></version>
			<appname></appname>
		</appinfo>
	</msg>
音乐链接消息MsgType: 49
AppMsgType: 3
FromUserName: 发送方ID
ToUserName: 接收方ID
Url: 链接地址
FileName: 音乐名

AppInfo: // 分享链接的应用
	{
		Type: 0, 
		AppID: wx485a97c844086dc9
	}

Content:
	<msg>
		<appmsg appid=""wx485a97c844086dc9""  sdkver=""0"">
			<title></title>
			<des></des>
			<action></action>
			<type>3</type>
			<showtype>0</showtype>
			<mediatagname></mediatagname>
			<messageext></messageext>
			<messageaction></messageaction>
			<content></content>
			<contentattr>0</contentattr>
			<url></url>
			<lowurl></lowurl>
			<dataurl>
				http://ws.stream.qqmusic.qq.com/C100003i9hMt1bgui0.m4a?vkey=6867EF99F3684&amp;guid=ffffffffc104ea2964a111cf3ff3edaf&amp;fromtag=46
			</dataurl>
			<lowdataurl>
				http://ws.stream.qqmusic.qq.com/C100003i9hMt1bgui0.m4a?vkey=6867EF99F3684&amp;guid=ffffffffc104ea2964a111cf3ff3edaf&amp;fromtag=46
			</lowdataurl>
			<appattach>
				<totallen>0</totallen>
				<attachid></attachid>
				<emoticonmd5></emoticonmd5>
				<fileext></fileext>
			</appattach>
			<extinfo></extinfo>
			<sourceusername></sourceusername>
			<sourcedisplayname></sourcedisplayname>
			<commenturl></commenturl>
			<thumburl>
				http://imgcache.qq.com/music/photo/album/63/180_albumpic_143163_0.jpg
			</thumburl>
			<md5></md5>
		</appmsg>
		<fromusername></fromusername>
		<scene>0</scene>
		<appinfo>
			<version>29</version>
			<appname>摇一摇搜歌</appname>
		</appinfo>
		<commenturl></commenturl>
	</msg>
群消息MsgType: 1
FromUserName: @@xxx
ToUserName: @xxx
Content:
	@xxx:<br/>xxx
红包消息MsgType: 49
AppMsgType: 2001
FromUserName: 发送方ID
ToUserName: 接收方ID
Content: 未知
注：根据网页版的代码可以看到未来可能支持查看红包消息，但目前走的是系统消息，见下。系统消息MsgType: 10000
FromUserName: 发送方ID
ToUserName: 自己ID
Content:
	""你已添加了 xxx ，现在可以开始聊天了。""
	""如果陌生人主动添加你为朋友，请谨慎核实对方身份。""
	""收到红包，请在手机上查看""
Discussion Group如果你希望和 WeixinBot 的其他开发者交流，或者有什么问题和建议，欢迎大家加入微信群【Youth fed the dog】一起讨论。扫描下面的二维码添加机器人为好友，并回复【Aidog】获取入群链接。注：这个不是群的二维码，是机器人拉你入群，记得回复机器人【Aidog】哦~ （secret code: Aidog）Recent Update"
https://github.com/jaungiers/LSTM-Neural-Network-for-Time-Series-Prediction,LSTM built using Keras Python package to predict time series steps and sequences. Includes sin wave and stock market data,LSTM Neural Network for Time Series PredictionLSTM built using the Keras Python package to predict time series steps and sequences. Includes sine wave and stock market data.RequirementsInstall requirements.txt file to make sure correct versions of libraries are being used.Output for sine wave sequential prediction:Output for stock market multi-dimensional multi-sequential predictions:
https://github.com/openatx/uiautomator2,Android Uiautomator2 Python Wrapper,"uiautomator2    各种库的版本号该项目处于低活跃度的开发中 是Google提供的用来做安卓自动化测试的一个Java库，基于Accessibility服务。功能很强，可以对第三方App进行测试，获取屏幕上任意一个APP的任意一个控件属性，并对其进行任意操作，但有两个缺点：1. 测试脚本只能使用Java语言 2. 测试脚本要打包成jar或者apk包上传到设备上才能运行。我们希望测试逻辑能够用Python编写，能够在电脑上运行的时候就控制手机。这里要非常感谢 Xiaocong He ()，他将这个想法实现了出来（见），原理是在手机上运行了一个http rpc服务，将uiautomator中的功能开放出来，然后再将这些http接口封装成Python库。因为这个库，已经很久不见更新。所以我们直接fork了一个版本，为了方便做区分我们就在后面加了个2 除了对原有的库的bug进行了修复，还增加了很多新的Feature。主要有以下部分：这里有一份快速参考，适合已经入门的人 ，欢迎多提意见。RequirementsQUICK START先准备一台（不要两台）开启了的安卓手机，连接上电脑，确保执行可以看到连接上的设备。运行 安装uiautomator2运行安装包含httprpc服务的apk到手机+ （在过去的版本中，这一步是必须执行的，但是从1.3.0之后的版本，当运行python代码时就会自动推送这些文件了）命令行运行打开python交互窗口。然后将下面的命令输入到窗口中。import uiautomator2 as u2

d = u2.connect() # connect to device
print(d.info)
这时看到类似下面的输出，就可以正式开始用我们这个库了。因为这个库功能太多，后面还有很多的内容，需要慢慢去看 ....{'currentPackageName': 'net.oneplus.launcher', 'displayHeight': 1920, 'displayRotation': 0, 'displaySizeDpX': 411, 'displaySizeDpY': 731, 'displayWidth': 1080, 'productName': 'OnePlus5', '
screenOn': True, 'sdkInt': 27, 'naturalOrientation': True}
一般情况下都会成功，不过也可能会有意外。可以加QQ群反馈问题，群里有很多大佬可以帮你解决问题。SponsorsThank you to all our sponsors! ✨🍰✨金牌赞助商（Gold Sponsor）霍格沃兹测试开发学社：中国软件测试开发高端教育品牌，产品由国内顶尖软件测试开发技术专家携手打造。为企业与个人提供专业的技能培训与咨询、测试工具与测试平台、测试外包与测试众包服务。领域涵盖App/Web自动化测试、接口自动化测试、性能测试、安全测试、持续交付/DevOps、测试左移、测试右移、精准测试、测试平台开发、测试管理等方向，相关项目[<marko.inline.RawText object at 0x000001592FD89288>][<marko.inline.RawText object at 0x000001592FD89B48>][<marko.inline.RawText object at 0x000001592FD89208>][<marko.inline.RawText object at 0x000001592FD898C8>][<marko.inline.RawText object at 0x000001592FDEB888>][<marko.inline.RawText object at 0x000001592FDA5048>][<marko.inline.RawText object at 0x000001592FDBD508>]常见问题[<marko.inline.RawText object at 0x0000015930046408>][<marko.inline.RawText object at 0x0000015930046E48>][<marko.inline.RawText object at 0x000001592FF09548>][<marko.inline.RawText object at 0x000001592FF09F48>]InstallationConnect to a deviceThere are two ways to connect to the device. Suppose device IP is  and your PC is in the same network.import uiautomator2 as u2

d = u2.connect('10.0.0.1') # alias for u2.connect_wifi('10.0.0.1')
print(d.info)
Suppose the device serial is  (seen from )import uiautomator2 as u2

d = u2.connect('123456f') # alias for u2.connect_usb('123456f')
print(d.info)
import uiautomator2 as u2

d = u2.connect_adb_wifi(""10.0.0.1:5555"")

# Equals to 
# + Shell: adb connect 10.0.0.1:5555
# + Python: u2.connect_usb(""10.0.0.1:5555"")
Calling  with no argument,  will obtain device IP from the environment variable  or .If this environment variable is empty, uiautomator will fall back to  and you need to make sure that there is only one device connected to the computer.Command line其中的代表设备的ip地址如需指定设备需要传入 如 , SubCommand为子命令（init,或者screenshot等）API DocumentsNew command timeoutHow long (in seconds) will wait for a new command from the client before assuming the client quit and ending the uiautomator service （Default 3 minutes）配置accessibility服务的最大空闲时间，超时将自动释放。默认3分钟。d.set_new_command_timeout(300) # change to 5 minutes, unit seconds
Debug HTTP requestsTrace HTTP requests and response to find out how it works.>>> d.debug = True
>>> d.info
12:32:47.182 $ curl -X POST -d '{""jsonrpc"": ""2.0"", ""id"": ""b80d3a488580be1f3e9cb3e926175310"", ""method"": ""deviceInfo"", ""params"": {}}' 'http://127.0.0.1:54179/jsonrpc/0'
12:32:47.225 Response >>>
{""jsonrpc"":""2.0"",""id"":""b80d3a488580be1f3e9cb3e926175310"",""result"":{""currentPackageName"":""com.android.mms"",""displayHeight"":1920,""displayRotation"":0,""displaySizeDpX"":360,""displaySizeDpY"":640,""displayWidth"":1080,""productName""
:""odin"",""screenOn"":true,""sdkInt"":25,""naturalOrientation"":true}}
<<< END
Implicit waitSet default element wait time, unit seconds设置元素查找等待时间（默认20s）d.implicitly_wait(10.0) # 也可以通过d.settings['wait_timeout'] = 10.0 修改
d(text=""Settings"").click() # if Settings button not show in 10s, UiObjectNotFoundError will raised

print(""wait timeout"", d.implicitly_wait()) # get default implicit wait
This function will have influence on , , , , , , etc.App managementThis part showcases how to perform app managementInstall an appWe only support installing an APK from a URLd.app_install('http://some-domain.com/some.apk')
Launch an app# 默认的这种方法是先通过atx-agent解析apk包的mainActivity，然后调用am start -n $package/$activity启动
d.app_start(""com.example.hello_world"")

# 使用 monkey -p com.example.hello_world -c android.intent.category.LAUNCHER 1 启动
# 这种方法有个副作用，它自动会将手机的旋转锁定给关掉
d.app_start(""com.example.hello_world"", use_monkey=True) # start with package name

# 通过指定main activity的方式启动应用，等价于调用am start -n com.example.hello_world/.MainActivity
d.app_start(""com.example.hello_world"", "".MainActivity"")
Stop an app# equivalent to `am force-stop`, thus you could lose data
d.app_stop(""com.example.hello_world"") 
# equivalent to `pm clear`
d.app_clear('com.example.hello_world')
Stop all running apps# stop all
d.app_stop_all()
# stop all app except for com.examples.demo
d.app_stop_all(excludes=['com.examples.demo'])
Get app infod.app_info(""com.examples.demo"")
# expect output
#{
#    ""mainActivity"": ""com.github.uiautomator.MainActivity"",
#    ""label"": ""ATX"",
#    ""versionName"": ""1.1.7"",
#    ""versionCode"": 1001007,
#    ""size"":1760809
#}

# save app icon
img = d.app_icon(""com.examples.demo"")
img.save(""icon.png"")
List all running appsd.app_list_running()
# expect output
# [""com.xxxx.xxxx"", ""com.github.uiautomator"", ""xxxx""]
Wait until app runningpid = d.app_wait(""com.example.android"") # 等待应用运行, return pid(int)
if not pid:
    print(""com.example.android is not running"")
else:
    print(""com.example.android pid is %d"" % pid)

d.app_wait(""com.example.android"", front=True) # 等待应用前台运行
d.app_wait(""com.example.android"", timeout=20.0) # 最长等待时间20s（默认）
Push and pull files检查并维持设备端守护进程处于运行状态d.healthcheck()
~~Auto click permission dialogs~~注意注意 函数，检测发现很不稳定，暂时不要使用，等候通知。Import in version 0.1.1d.disable_popups() # automatic skip popups
d.disable_popups(False) # disable automatic skip popups
If this method is not working on your device, You can make a pull request or create an issue to enhance this function. I'll show you how to do it.Now you know the button text and current package name. Make a pull request by update function  or create an  if you are not familar with git and python.Open SchemeYou can do it wire adb: Also you can do it with python coded.open_url(""https://www.baidu.com"")
d.open_url(""taobao://taobao.com"") # open Taobao app
d.open_url(""appname://appnamehost"")
Basic API UsagesThis part showcases how to perform common device operations:Shell commandsSessionSession represent an app lifecycle. Can be used to start app, detect app crash.Retrieve the device infoGet basic informationd.info
Below is a possible output:{ 
    u'displayRotation': 0,
    u'displaySizeDpY': 640,
    u'displaySizeDpX': 360,
    u'currentPackageName': u'com.android.launcher',
    u'productName': u'takju',
    u'displayWidth': 720,
    u'sdkInt': 18,
    u'displayHeight': 1184,
    u'naturalOrientation': True
}
Get window sizeprint(d.window_size())
# device upright output example: (1080, 1920)
# device horizontal output example: (1920, 1080)
Get current app info. For some android devices, the output could be empty (see Output example 3)print(d.app_current())
# Output example 1: {'activity': '.Client', 'package': 'com.netease.example', 'pid': 23710}
# Output example 2: {'activity': '.Client', 'package': 'com.netease.example'}
# Output example 3: {'activity': None, 'package': None}
Wait activityd.wait_activity("".ApiDemos"", timeout=10) # default timeout 10.0 seconds
# Output: true of false
Get device serial numberprint(d.serial)
# output example: 74aAEDR428Z9
Get WLAN ipprint(d.wlan_ip)
# output example: 10.0.0.1
Get detailed device infoprint(d.device_info)
Below is a possible output:{'udid': '3578298f-b4:0b:44:e6:1f:90-OD103',
 'version': '7.1.1',
 'serial': '3578298f',
 'brand': 'SMARTISAN',
 'model': 'OD103',
 'hwaddr': 'b4:0b:44:e6:1f:90',
 'port': 7912,
 'sdk': 25,
 'agentVersion': 'dev',
 'display': {'width': 1080, 'height': 1920},
 'battery': {'acPowered': False,
  'usbPowered': False,
  'wirelessPowered': False,
  'status': 3,
  'health': 0,
  'present': True,
  'level': 99,
  'scale': 100,
  'voltage': 4316,
  'temperature': 272,
  'technology': 'Li-ion'},
 'memory': {'total': 3690280, 'around': '4 GB'},
 'cpu': {'cores': 8, 'hardware': 'Qualcomm Technologies, Inc MSM8953Pro'},
 'presenceChangedAt': '0001-01-01T00:00:00Z',
 'usingBeganAt': '0001-01-01T00:00:00Z'}
ClipboardGet of set clipboard content设置粘贴板内容或获取内容 (目前已知问题是9.0之后的后台程序无法获取剪贴板的内容)Key EventsYou can find all key code definitions at Gesture interaction with the deviceNote: click, swipe, drag operations support percentage position values. Example: means long click center of screenScreen-relatedSelectorSelector is a handy mechanism to identify a specific UI object in the current window.# Select the object with text 'Clock' and its className is 'android.widget.TextView'
d(text='Clock', className='android.widget.TextView')
Selector supports below parameters. Refer to  for detailed information.Children and siblingsGet the selected ui object status and its informationPerform the click action on the selected UI objectGesture actions for the specific UI objectWatchContext目前的这个watch_context是用threading启动的，每2s检查一次目前还只有click这一种触发操作with d.watch_context() as ctx:
    ctx.when(""^立即(下载|更新)"").when(""取消"").click() # 当同时出现 （立即安装 或 立即取消）和 取消 按钮的时候，点击取消
    ctx.when(""同意"").click()
    ctx.when(""确定"").click()
    # 上面三行代码是立即执行完的，不会有什么等待
    
    ctx.wait_stable() # 开启弹窗监控，并等待界面稳定（两个弹窗检查周期内没有弹窗代表稳定）

    # 使用call函数来触发函数回调
    # call 支持两个参数，d和el，不区分参数位置，可以不传参，如果传参变量名不能写错
    # eg: 当有元素匹配仲夏之夜，点击返回按钮
    ctx.when(""仲夏之夜"").call(lambda d: d.press(""back""))
    ctx.when(""确定"").call(lambda el: el.click())

    # 其他操作

# 为了方便也可以使用代码中默认的弹窗监控逻辑
# 下面是目前内置的默认逻辑，可以加群at群主，增加新的逻辑，或者直接提pr
    # when(""继续使用"").click()
    # when(""移入管控"").when(""取消"").click()
    # when(""^立即(下载|更新)"").when(""取消"").click()
    # when(""同意"").click()
    # when(""^(好的|确定)"").click()
with d.watch_context(builtin=True) as ctx:
    # 在已有的基础上增加
    ctx.when(""@tb:id/jview_view"").when('//*[@content-desc=""图片""]').click()

    # 其他脚本逻辑
另外一种写法ctx = d.watch_context()
ctx.when(""设置"").click()
ctx.wait_stable() # 等待界面不在有弹窗了

ctx.close()
Watcher更推荐用WatchContext 写法更简洁一些~~You can register  to perform some actions when a selector does not find a match.~~2.0.0之前使用的是 uiautomator-jar库中提供的[Watcher]((http://developer.android.com/tools/help/uiautomator/UiWatcher.html)方法，但在实践中发现一旦uiautomator所有的watcher配置都是丢失，这肯定是无法接受的。所以目前采用了后台运行了一个线程的方法(依赖threading库），然后每隔一段时间dump一次hierarchy，匹配到元素之后执行相应的操作。用法举例注册监控# 常用写法，注册匿名监控
d.watcher.when(""安装"").click()

# 注册名为ANR的监控，当出现ANR和Force Close时，点击Force Close
d.watcher(""ANR"").when(xpath=""ANR"").when(""Force Close"").click()

# 其他回调例子
d.watcher.when(""抢红包"").press(""back"")
d.watcher.when(""//*[@text = 'Out of memory']"").call(lambda d: d.shell('am force-stop com.im.qq'))

# 回调说明
def click_callback(d: u2.Device):
    d.xpath(""确定"").click() # 在回调中调用不会再次触发watcher

d.xpath(""继续"").click() # 使用d.xpath检查元素的时候，会触发watcher（目前最多触发5次）
监控操作# 移除ANR的监控
d.watcher.remove(""ANR"")

# 移除所有的监控
d.watcher.remove()

# 开始后台监控
d.watcher.start()
d.watcher.start(2.0) # 默认监控间隔2.0s

# 强制运行所有监控
d.watcher.run()

# 停止监控
d.watcher.stop()

# 停止并移除所有的监控，常用于初始化
d.watcher.reset()
另外文档还是有很多没有写，推荐直接去看源码Global settingsd.HTTP_TIMEOUT = 60 # 默认值60s, http默认请求超时时间

# 当设备掉线时，等待设备在线时长，仅当TMQ=true时有效，支持通过环境变量 WAIT_FOR_DEVICE_TIMEOUT 设置
d.WAIT_FOR_DEVICE_TIMEOUT = 70 
其他的配置，目前已大部分集中到  中，根据后期的需求配置可能会有增减。print(d.settings)
{'operation_delay': (0, 0),
 'operation_delay_methods': ['click', 'swipe'],
 'wait_timeout': 20.0,
 'xpath_debug': False}

# 配置点击前延时0.5s，点击后延时1s
d.settings['operation_delay'] = (.5, 1)

# 修改延迟生效的方法
# 其中 double_click, long_click 都对应click
d.settings['operation_delay_methods'] = ['click', 'swipe', 'drag', 'press']

d.settings['xpath_debug'] = True # 开启xpath插件的调试日志
d.settings['wait_timeout'] = 20.0 # 默认控件等待时间（原生操作，xpath插件的等待时间）
对于随着版本升级，设置过期的配置时，会提示Deprecated，但是不会抛异常。>>> d.settings['click_before_delay'] = 1  
[W 200514 14:55:59 settings:72] d.settings[click_before_delay] deprecated: Use operation_delay instead
uiautomator恢复方式设置细心的你可能发现，实际上手机安装了两个APK，一个在前台可见（小黄车）。一个包名为在后台不可见。这两个apk使用同一个证书签名的。不可见的应用实际上是一个测试包，包含有所有的测试代码，核心的测试服务也是通过其启动的。但是运行的时候，系统却需要那个小黄车一直在运行（在后台运行也可以）。一旦小黄车应用被杀，后台运行的测试服务也很快的会被杀掉。就算什么也不做，应用应用在后台，也会很快被系统回收掉。（这里希望高手指点一下，如何才能不依赖小黄车应用，感觉理论上是可以的，但是目前我还不会）。~~让小黄车在后台运行有两种方式，一种启动应用后，放到后台（默认）。另外通过启动一个后台服务也行。~~~~通过  可以调整该行为。True代表启动应用，False代表启动服务。~~UiAutomator中的超时设置(隐藏方法)>> d.jsonrpc.getConfigurator() 
{'actionAcknowledgmentTimeout': 500,
 'keyInjectionDelay': 0,
 'scrollAcknowledgmentTimeout': 200,
 'waitForIdleTimeout': 0,
 'waitForSelectorTimeout': 0}

>> d.jsonrpc.setConfigurator({""waitForIdleTimeout"": 100})
{'actionAcknowledgmentTimeout': 500,
 'keyInjectionDelay': 0,
 'scrollAcknowledgmentTimeout': 200,
 'waitForIdleTimeout': 100,
 'waitForSelectorTimeout': 0}
为了防止客户端程序响应超时，和目前已改为Refs: Input method这种方法通常用于不知道控件的情况下的输入。第一步需要切换输入法，然后发送adb广播命令，具体使用方法如下d.set_fastinput_ime(True) # 切换成FastInputIME输入法
d.send_keys(""你好123abcEFG"") # adb广播输入
d.clear_text() # 清除输入框所有内容(Require android-uiautomator.apk version >= 1.0.7)
d.set_fastinput_ime(False) # 切换成正常的输入法
d.send_action(""search"") # 模拟输入法的搜索
send_action 说明该函数可以使用的参数有 什么时候该使用这个函数呢？有些时候在EditText中输入完内容之后，调用 or 发现并没有什么反应。这个时候就需要函数了，这里用到了只有输入法才能用的。先broadcast命令发送给输入法操作，由输入法完成后续跟EditText的通信。（原理我不太清楚，有了解的，提issue告诉我)Toast (2.2版本之后有添加回来)Show Toastd.toast.show(""Hello world"")
d.toast.show(""Hello world"", 1.0) # show for 1.0s, default 1.0s
Get Toast# [Args]
# 5.0: max wait timeout. Default 10.0
# 10.0: cache time. return cache toast if already toast already show up in recent 10 seconds. Default 10.0 (Maybe change in the furture)
# ""default message"": return if no toast finally get. Default None
d.toast.get_message(5.0, 10.0, ""default message"")

# common usage
assert ""Short message"" in d.toast.get_message(5.0, default="""")

# clear cached toast
d.toast.reset()
# Now d.toast.get_message(0) is None
XPathJava uiautoamtor中默认是不支持xpath的，所以这里属于扩展的一个功能。速度不是这么的快。For example: 其中一个节点的内容<android.widget.TextView
  index=""2""
  text=""05:19""
  resource-id=""com.netease.cloudmusic:id/qf""
  package=""com.netease.cloudmusic""
  content-desc=""""
  checkable=""false"" checked=""false"" clickable=""false"" enabled=""true"" focusable=""false"" focused=""false""
  scrollable=""false"" long-clickable=""false"" password=""false"" selected=""false"" visible-to-user=""true""
  bounds=""[957,1602][1020,1636]"" />
xpath定位和使用方法有些属性的名字有修改需要注意description -> content-desc
resourceId -> resource-id
常见用法# wait exists 10s
d.xpath(""//android.widget.TextView"").wait(10.0)
# find and click
d.xpath(""//*[@content-desc='分享']"").click()
# check exists
if d.xpath(""//android.widget.TextView[contains(@text, 'Se')]"").exists:
    print(""exists"")
# get all text-view text, attrib and center point
for elem in d.xpath(""//android.widget.TextView"").all():
    print(""Text:"", elem.text)
    # Dictionary eg: 
    # {'index': '1', 'text': '999+', 'resource-id': 'com.netease.cloudmusic:id/qb', 'package': 'com.netease.cloudmusic', 'content-desc': '', 'checkable': 'false', 'checked': 'false', 'clickable': 'false', 'enabled': 'true', 'focusable': 'false', 'focused': 'false','scrollable': 'false', 'long-clickable': 'false', 'password': 'false', 'selected': 'false', 'visible-to-user': 'true', 'bounds': '[661,1444][718,1478]'}
    print(""Attrib:"", elem.attrib)
    # Coordinate eg: (100, 200)
    print(""Position:"", elem.center())
点击查看Screenrecord视频录制这里没有使用手机中自带的screenrecord命令，是通过获取手机图片合成视频的方法，所以需要安装一些其他的依赖，如imageio, imageio-ffmpeg, numpy等因为有些依赖比较大，推荐使用镜像安装。直接运行下面的命令即可。pip3 install -U ""uiautomator2[image]"" -i https://pypi.doubanio.com/simple
使用方法d.screenrecord('output.mp4')

time.sleep(10)
# or do something else

d.screenrecord.stop() # 停止录制后，output.mp4文件才能打开
录制的时候也可以指定fps（当前是20），这个值是率低于minicap输出图片的速度，感觉已经很好了，不建议你修改。Image match图像匹配，在使用这个功能之前你需要先把依赖安装上pip3 install -U ""uiautomator2[image]"" -i https://pypi.doubanio.com/simple
目前开放两个接口imdata = ""target.png"" # 也可以是URL, PIL.Image或OpenCV打开的图像

d.image.match(imdata) 
# 匹配待查找的图片，立刻返回一个结果
# 返回一个dict, eg: {""similarity"": 0.9, ""point"": [200, 300]}

d.image.click(imdata, timeout=20.0)
# 在20s的时间内调用match轮询查找图片，当similarity>0.9时，执行点击操作
该功能还在完善中，图片需要手机的原图裁剪后的图才可以。常见问题很多没写在这个地方的，都放到了这里 Stop UiAutomator停止UiAutomator守护服务https://github.com/openatx/uiautomator2/wiki/Common-issues因为有的存在，Uiautomator会被一直守护着，如果退出了就会被重新启动起来。但是Uiautomator又是霸道的，一旦它在运行，手机上的辅助功能、电脑上的uiautomatorviewer 就都不能用了，除非关掉该框架本身的uiautomator。下面就说下两种关闭方法方法1：直接打开uiautomator app（init成功后，就会安装上的），点击方法2:d.uiautomator.stop()

# d.uiautomator.start() # 启动
# d.uiautomator.running() # 是否在运行
Article Recommended优秀文章推荐 (欢迎QQ群里at我反馈）项目历史Google UiAutomator 2.0和1.x的区别https://www.cnblogs.com/insist8089/p/6898181.html重大更新依赖项目ContributorsOther 其他优秀的项目LICENSE"
https://github.com/crazyguitar/pysheeet,Python Cheat Sheet,".. raw:: html<h1 align=""center"">
<br>
  <a href=""https://www.pythonsheets.com""><img src=""docs/_static/logo.svg"" alt=""pysheeet"" width=200""></a>
</h1>
<p align=""center"">
  <a href=""https://github.com/crazyguitar/pysheeet/actions"">
    <img src=""https://github.com/crazyguitar/pysheeet/actions/workflows/pythonpackage.yml/badge.svg"" alt=""Build Status"">
  </a>
  <a href=""https://coveralls.io/github/crazyguitar/pysheeet?branch=master"">
    <img src=""https://coveralls.io/repos/github/crazyguitar/pysheeet/badge.svg?branch=master"" alt=""Coverage"">
  </a>
  <a href=""https://raw.githubusercontent.com/crazyguitar/pysheeet/master/LICENSE"">
    <img src=""https://img.shields.io/badge/License-MIT-blue.svg"" alt=""License MIT"">
  </a>
</p>
IntroductionPysheeet was created with intention of collecting python code snippets forreducing coding hours and making life easier and faster. Any contributions are welcome.Please feel free to fork and send a pull request to this project.What’s New In Python 3This part only provides a quick glance at some important features in Python 3.If you're interested in all of the most important features, please read theofficial document, _.Cheat SheetAdvanced Cheat SheetAppendixPDF Version_.. _pdf: https://media.readthedocs.org/pdf/pysheeet/latest/pysheeet.pdfHow to run the server.. code-block:: bash$ virtualenv venv
$ . venv/bin/activate
$ pip install -r requirements.txt
$ make
$ python app.py

# URL: localhost:5000
"
https://github.com/lukemelas/EfficientNet-PyTorch,A PyTorch implementation of EfficientNet and EfficientNetV2 (coming soon!),"EfficientNet PyTorchQuickstartInstall with  and load a pretrained EfficientNet with:from efficientnet_pytorch import EfficientNet
model = EfficientNet.from_pretrained('efficientnet-b0')
UpdatesUpdate (April 2, 2021)The  has been released! I am working on implementing it as you read this :) About EfficientNetV2:Here is a comparison: Update (Aug 25, 2020)This update adds: Update (May 14, 2020)This update adds comprehensive comments and documentation (thanks to @workingcoder).Update (January 23, 2020)This update adds a new category of pre-trained model based on adversarial training, called advprop. It is important to note that the preprocessing required for the advprop pretrained models is slightly different from normal ImageNet preprocessing. As a result, by default, advprop models are not used. To load a model with advprop, use:model = EfficientNet.from_pretrained(""efficientnet-b0"", advprop=True)
There is also a new, large  pretrained model that is only available in advprop form. When using these models, replace ImageNet preprocessing code as follows:if advprop:  # for models using advprop pretrained weights
    normalize = transforms.Lambda(lambda img: img * 2.0 - 1.0)
else:
    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                     std=[0.229, 0.224, 0.225])
This update also addresses multiple other issues (, ).Update (October 15, 2019)This update allows you to choose whether to use a memory-efficient Swish activation. The memory-efficient version is chosen by default, but it cannot be used when exporting using PyTorch JIT. For this purpose, we have also included a standard (export-friendly) swish activation function. To switch to the export-friendly version, simply call  after loading your desired model. This update addresses issues  and .Update (October 12, 2019)This update makes the Swish activation function more memory-efficient. It also addresses pull requests , , , and . Thanks to the authors of all the pull requests!Update (July 31, 2019)Upgrade the pip package with The B6 and B7 models are now available. Additionally, all pretrained models have been updated to use AutoAugment preprocessing, which translates to better performance across the board. Usage is the same as before:from efficientnet_pytorch import EfficientNet
model = EfficientNet.from_pretrained('efficientnet-b7')
Update (June 29, 2019)This update adds easy model exporting () and feature extraction ().It is also now incredibly simple to load a pretrained model with a new number of classes for transfer learning:model = EfficientNet.from_pretrained('efficientnet-b1', num_classes=23)
Update (June 23, 2019)The B4 and B5 models are now available. Their usage is identical to the other models:from efficientnet_pytorch import EfficientNet
model = EfficientNet.from_pretrained('efficientnet-b4')
OverviewThis repository contains an op-for-op PyTorch reimplementation of , along with pre-trained models and examples.The goal of this implementation is to be simple, highly extensible, and easy to integrate into your own projects. This implementation is a work in progress -- new features are currently being implemented.At the moment, you can easily:Upcoming features: In the next few days, you will be able to:Table of contentsAbout EfficientNetIf you're new to EfficientNets, here is an explanation straight from the official TensorFlow implementation:EfficientNets are a family of image classification models, which achieve state-of-the-art accuracy, yet being an order-of-magnitude smaller and faster than previous models. We develop EfficientNets based on AutoML and Compound Scaling. In particular, we first use  to develop a mobile-size baseline network, named as EfficientNet-B0; Then, we use the compound scaling method to scale up this baseline to obtain EfficientNet-B1 to B7.EfficientNets achieve state-of-the-art accuracy on ImageNet with an order of magnitude better efficiency:About EfficientNet PyTorchEfficientNet PyTorch is a PyTorch re-implementation of EfficientNet. It is consistent with the , such that it is easy to load weights from a TensorFlow checkpoint. At the same time, we aim to make our PyTorch implementation as simple, flexible, and extensible as possible.If you have any feature requests or questions, feel free to leave them as GitHub issues!InstallationInstall via pip:pip install efficientnet_pytorch
Or install from source:git clone https://github.com/lukemelas/EfficientNet-PyTorch
cd EfficientNet-Pytorch
pip install -e .
UsageLoading pretrained modelsLoad an EfficientNet:from efficientnet_pytorch import EfficientNet
model = EfficientNet.from_name('efficientnet-b0')
Load a pretrained EfficientNet:from efficientnet_pytorch import EfficientNet
model = EfficientNet.from_pretrained('efficientnet-b0')
Details about the models are below:|    Name         |# Params|Top-1 Acc.|Pretrained?||:-----------------:|:--------:|:----------:|:-----------:||  |   5.3M   |    76.3    |      ✓      ||  |   7.8M   |    78.8    |      ✓      ||  |   9.2M   |    79.8    |      ✓      ||  |    12M   |    81.1    |      ✓      ||  |    19M   |    82.6    |      ✓      ||  |    30M   |    83.3    |      ✓      ||  |    43M   |    84.0    |      ✓      ||  |    66M   |    84.4    |      ✓      |Example: ClassificationBelow is a simple, complete example. It may also be found as a jupyter notebook in  or as a .We assume that in your current directory, there is a  file and a  file (ImageNet class names). These are both included in .import json
from PIL import Image
import torch
from torchvision import transforms

from efficientnet_pytorch import EfficientNet
model = EfficientNet.from_pretrained('efficientnet-b0')

# Preprocess image
tfms = transforms.Compose([transforms.Resize(224), transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),])
img = tfms(Image.open('img.jpg')).unsqueeze(0)
print(img.shape) # torch.Size([1, 3, 224, 224])

# Load ImageNet class names
labels_map = json.load(open('labels_map.txt'))
labels_map = [labels_map[str(i)] for i in range(1000)]

# Classify
model.eval()
with torch.no_grad():
    outputs = model(img)

# Print predictions
print('-----')
for idx in torch.topk(outputs, k=5).indices.squeeze(0).tolist():
    prob = torch.softmax(outputs, dim=1)[0, idx].item()
    print('{label:<75} ({p:.2f}%)'.format(label=labels_map[idx], p=prob*100))
Example: Feature ExtractionYou can easily extract features with :from efficientnet_pytorch import EfficientNet
model = EfficientNet.from_pretrained('efficientnet-b0')

# ... image preprocessing as in the classification example ...
print(img.shape) # torch.Size([1, 3, 224, 224])

features = model.extract_features(img)
print(features.shape) # torch.Size([1, 1280, 7, 7])
Example: Export to ONNXExporting to ONNX for deploying to production is now simple:import torch
from efficientnet_pytorch import EfficientNet

model = EfficientNet.from_pretrained('efficientnet-b1')
dummy_input = torch.randn(10, 3, 240, 240)

model.set_swish(memory_efficient=False)
torch.onnx.export(model, dummy_input, ""test-b1.onnx"", verbose=True)
 is a Colab example.ImageNetSee  for details about evaluating on ImageNet.ContributingIf you find a bug, create a GitHub issue, or even better, submit a pull request. Similarly, if you have questions, simply post them as GitHub issues.I look forward to seeing what the community does with these models!"
https://github.com/OlafenwaMoses/ImageAI,A python library built to empower developers to build applications and systems  with self-contained Computer Vision capabilities,"ImageAI (v3.0.3)       An open-source python library built to empower developers to build applications and systems with self-contained Deep Learning and Computer Vision capabilities using simple and few lines of code.If you will like to sponsor this project, kindly visit the .---------------------------------------------------Introducing TheiaEngine.We the creators of ImageAI are glad to announce , the next-generation computer Vision AI API capable of all computer vision tasks in a single API call and available via REST API to all programming languages. Features includeVisit  to try the demo and join in the beta testing today.---------------------------------------------------Developed and maintained by Built with simplicity in mind, ImageAIsupports a list of state-of-the-art Machine Learning algorithms for image prediction, custom image prediction, object detection, video detection, video object trackingand image predictions trainings. ImageAI currently supports image prediction and training using 4 different Machine Learning algorithmstrained on the ImageNet-1000 dataset. ImageAI also supports object detection, video detection and object tracking  using RetinaNet, YOLOv3 and TinyYOLOv3 trained on COCO dataset. Finally, ImageAI allows you to train custom models for performing detection and recognition of new objects. Eventually, ImageAI will provide support for a wider and more specialized aspects of Computer VisionNew Release : ImageAI 3.0.2What's new:TABLE OF CONTENTSInstallationTo install ImageAI, run the python installation instruction below in the command line:FeaturesDocumentationWe have provided full documentation for all ImageAI classes and functions. Visit the link below:SponsorsReal-Time and High Performance ImplementationImageAI provides abstracted and convenient implementations of state-of-the-art Computer Vision technologies. All of ImageAI implementations and code can work on any computer system with moderate CPU capacity. However, the speed of processing for operations like image prediction, object detection and others on CPU is slow and not suitable for real-time applications. To perform real-time Computer Vision operations with high performance, you need to use GPU enabled technologies.ImageAI uses the PyTorch backbone for it's Computer Vision operations. PyTorch supports both CPUs and GPUs ( Specifically NVIDIA GPUs.  You can get one for your PC or get a PC that has one) for machine learning and artificial intelligence algorithms' implementations.Projects Built on ImageAIAI Practice RecommendationsFor anyone interested in building AI systems and using them for business, economic,  social and research purposes, it is critical that the person knows the likely positive, negative and unprecedented impacts the use of such technologies will have.They must also be aware of approaches and practices recommended by experienced industry experts to ensure every use of AI brings overall benefit to mankind.We therefore recommend to everyone that wishes to use ImageAI and other AI tools and resources to read Microsoft's January 2018 publication on AI titled ""The Future Computed : Artificial Intelligence and its role in society"".Kindly follow the link below to download the publication.Contact DeveloperCitationYou can cite ImageAI in your projects and research papers via the BibTeX entry below.  @misc {ImageAI,
    author = ""Moses"",
    title  = ""ImageAI, an open source python library built to empower developers to build applications and systems  with self-contained Computer Vision capabilities"",
    url    = ""https://github.com/OlafenwaMoses/ImageAI"",
    month  = ""mar"",
    year   = ""2018--""
}
References"
https://github.com/NVIDIA/pix2pixHD,Synthesizing and manipulating 2048x1024 images with conditional GANs,"pix2pixHD |  |  Pytorch implementation of our method for high-resolution (e.g. 2048x1024) photorealistic image-to-image translation. It can be used for turning semantic label maps into photo-realistic images or synthesizing portraits from face label maps. 1, 1, 2, Andrew Tao1, 1, 11NVIDIA Corporation, 2UC BerkeleyIn CVPR 2018.  Image-to-image translation at 2k/1k resolutionPrerequisitesGetting StartedInstallationpip install dominate
git clone https://github.com/NVIDIA/pix2pixHD
cd pix2pixHD
Testing#!./scripts/test_1024p.sh
python test.py --name label2city_1024p --netG local --ngf 32 --resize_or_crop none
The test results will be saved to a html file here: .More example scripts can be found in the  directory.DatasetTraining#!./scripts/train_512p.sh
python train.py --name label2city_512p
Multi-GPU training#!./scripts/train_512p_multigpu.sh
python train.py --name label2city_512p --batchSize 8 --gpu_ids 0,1,2,3,4,5,6,7
Note: this is not tested and we trained our model using single GPU only. Please use at your own discretion.Training with Automatic Mixed Precision (AMP) for faster speed#!./scripts/train_512p_fp16.sh
python -m torch.distributed.launch train.py --name label2city_512p --fp16
In our test case, it trains about 80% faster with AMP on a Volta machine.Training at full resolutionTraining with your own datasetMore Training/Test DetailsCitationIf you find this useful for your research, please use the following.@inproceedings{wang2018pix2pixHD,
  title={High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs},
  author={Ting-Chun Wang and Ming-Yu Liu and Jun-Yan Zhu and Andrew Tao and Jan Kautz and Bryan Catanzaro},  
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year={2018}
}
AcknowledgmentsThis code borrows heavily from ."
https://github.com/NVIDIA/vid2vid,Pytorch implementation of our method for high-resolution (e.g. 2048x1024) photorealistic video-to-video translation.,"vid2vid |  |  |  | Pytorch implementation for high-resolution (e.g., 2048x1024) photorealistic video-to-video translation. It can be used for turning semantic label maps into photo-realistic videos, synthesizing people talking from edge maps, or generating human motions from poses. The core of video-to-video translation is image-to-image translation. Some of our work in that space can be found in  and . 1, 1, 2, 1, Andrew Tao1, 1, 11NVIDIA Corporation, 2MIT CSAILIn Neural Information Processing Systems (NeurIPS) 2018  Video-to-Video TranslationPrerequisitesGetting StartedInstallationpip install dominate requests
pip install dlib
git clone https://github.com/NVIDIA/vid2vid
cd vid2vid
TestingDatasetTraining with Cityscapes datasetIf you have TensorFlow installed, you can see TensorBoard logs in  by adding  to the training scripts.Training with face datasetsTraining with pose datasetsTraining with your own datasetMore Training/Test DetailsCitationIf you find this useful for your research, please cite the following paper.@inproceedings{wang2018vid2vid,
   author    = {Ting-Chun Wang and Ming-Yu Liu and Jun-Yan Zhu and Guilin Liu
                and Andrew Tao and Jan Kautz and Bryan Catanzaro},
   title     = {Video-to-Video Synthesis},
   booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},   
   year      = {2018},
}
AcknowledgmentsWe thank Karan Sapra, Fitsum Reda, and Matthieu Le for generating the segmentation maps for us. We also thank Lisa Rhee for allowing us to use her dance videos for training. We thank William S. Peebles for proofreading the paper.This code borrows heavily from  and ."
https://github.com/weskerfoot/DeleteFB,Automate Scrubbing your Facebook Presence,"WARNING:This currently only works for English language Facebook accounts, due to the lack of a usable API.Also, year by year deletion is currently broken. Feel free to fork or make pull requests.Why?I needed a simple and reliable way to delete Facebook posts. There arethird-party apps that claim to do this, but they all require handing over yourcredentials, or are unreliable in other ways. Since this uses Selenium, it ismore reliable, as it uses your real web browser, and it is less likely Facebookwill block or throttle you.As for why you would want to do this in the first place. That is up to you.Personally I wanted a way to delete most of my content on Facebook withoutdeleting my account.Will this really delete posts?I can make no guarantees that Facebook doesn't store the data somewhere foreverin cold storage. However this tool is intended more as a way to clean up youronline presence and not have to worry about what you wrote from years ago.Personally, I did this so I would feel less attached to my Facebook profile(and hence feel the need to use it less).DependenciesInstallationYou have several options to run it.ChromedriverThe tool will attempt to detect the version of Chrome that you have installed and download the appropriate chromedriver. It is possible that it might fail to find your chrome version if you are running on Windows. If that is the case, please try running the docker version.How To Use Itusage: deletefb [-h] [-M {wall,unlike_pages,conversations}] -E EMAIL [-P PASSWORD] -U PROFILE_URL [-F TWO_FACTOR_TOKEN] [-H] [--no-archive] [-Y YEAR]
                [-B CHROMEBIN]

optional arguments:
  -h, --help            show this help message and exit
  -M {wall,unlike_pages,conversations}, --mode {wall,unlike_pages,conversations}
                        The mode you want to run in. Default is `wall' which deletes wall posts
  -E EMAIL, --email EMAIL
                        Your email address associated with the account
  -P PASSWORD, --password PASSWORD
                        Your Facebook password
  -U PROFILE_URL, --profile-url PROFILE_URL
                        The link to your Facebook profile, e.g. https://www.facebook.com/your.name
  -F TWO_FACTOR_TOKEN, --two-factor TWO_FACTOR_TOKEN
                        The code generated by your 2FA device for Facebook
  -H, --headless        Run browser in headless mode (no gui)
  --no-archive          Turn off archiving (on by default)
  -Y YEAR, --year YEAR  The year(s) you want posts deleted.
  -B CHROMEBIN, --chromebin CHROMEBIN
                        Optional path to the Google Chrome (or Chromium) binary
Login2FADelete By YearArchivalHeadless modeBugsIf it stops working or otherwise crashes, delete the latest post manually andstart it again after waiting a minute. I make no guarantees that it will workperfectly for every profile. Please file an issue if you run into any problems."
https://github.com/microsoft/DeepSpeed,"DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.","Latest News  DeepSpeed empowers ChatGPT-like model training with a single click, offering 15x speedup over SOTA RLHF systems with unprecedented cost reduction at all scales; .Extreme Speed and Scale for DL Training and Inference[<marko.inline.Link object at 0x00000159300510C8>, <marko.inline.RawText object at 0x0000015930051688>, <marko.inline.Link object at 0x0000015930051188>, <marko.inline.RawText object at 0x00000159300512C8>, <marko.inline.Link object at 0x000001592FF09108>]. It is an easy-to-use deep learning optimization software suite that powers unprecedented scale and speed for both training and inference. With DeepSpeed you can:DeepSpeed's four innovation pillarsDeepSpeed-TrainingDeepSpeed offers a confluence of system innovations, that has made large scale DL training effective, and efficient, greatly improved ease of use, and redefined the DL training landscape in terms of scale that is possible. These innovations such as ZeRO, 3D-Parallelism, DeepSpeed-MoE, ZeRO-Infinity, etc. fall under the training pillar. Learn more: DeepSpeed-InferenceDeepSpeed brings together innovations in parallelism technology such as tensor, pipeline, expert and ZeRO-parallelism, and combines them with high performance custom inference kernels, communication optimizations and heterogeneous memory technologies to enable inference at an unprecedented scale, while achieving unparalleled latency, throughput and cost reduction. This systematic composition of system technologies for inference falls under the inference pillar. Learn more: DeepSpeed-CompressionTo further increase the inference efficiency, DeepSpeed offers easy-to-use and flexible-to-compose compression techniques for researchers and practitioners to compress their models while delivering faster speed, smaller model size, and significantly reduced compression cost. Moreover, SoTA innovations on compression like ZeroQuant and XTC are included under the compression pillar. Learn more: DeepSpeed4ScienceIn line with Microsoft's mission to solve humanity's most pressing challenges, the DeepSpeed team at Microsoft is responding to this opportunity by launching a new initiative called DeepSpeed4Science, aiming to build unique capabilities through AI system technology innovations to help domain experts to unlock today's biggest science mysteries. Learn more:  and DeepSpeed Software SuiteDeepSpeed LibraryThe  library (this repository) implements and packages the innovations and technologies in DeepSpeed Training, Inference and Compression Pillars into a single easy-to-use, open-sourced repository. It allows for easy composition of multitude of features within a single training, inference or compression pipeline. The DeepSpeed Library is heavily adopted by the DL community, and has been used to enable some of the most powerful models (see ).Model Implementations for Inference (MII) is an open-sourced repository for making low-latency and high-throughput inference accessible to all data scientists by alleviating the need to apply complex system optimization techniques themselves. Out-of-box, MII offers support for thousands of widely used DL models, optimized using DeepSpeed-Inference, that can be deployed with a few lines of code, while achieving significant latency reduction compared to their vanilla open-sourced versions.DeepSpeed on AzureDeepSpeed users are diverse and have access to different environments. We recommend to try DeepSpeed on Azure as it is the simplest and easiest method. The recommended method to try DeepSpeed on Azure is through AzureML . The job submission and data preparation scripts have been made available . For more details on how to use DeepSpeed on Azure, please follow the .DeepSpeed AdoptionDeepSpeed is an important part of Microsoft’s newinitiative to enable next-generation AI capabilities at scale, where you can find moreinformation .DeepSpeed has been used to train many different large-scale models, below is a list of several examples that we are aware of (if you'd like to include your model please submit a PR):DeepSpeed has been integrated with several different popular open-source DL frameworks such as:|                                                                                                | Documentation                                || ---------------------------------------------------------------------------------------------- | -------------------------------------------- | |  ||  |  ||  |  ||  |  ||  |  ||  |  |Build Pipeline Status| Description | Status || ----------- | ------ || NVIDIA |       || AMD |   || CPU |  || PyTorch Nightly |  || Integrations |       || Misc |    |InstallationThe quickest way to get started with DeepSpeed is via pip, this will installthe latest release of DeepSpeed which is not tied to specific PyTorch or CUDAversions. DeepSpeed includes several C++/CUDA extensions that we commonly referto as our 'ops'.  By default, all of these extensions/ops will be builtjust-in-time (JIT) using  to build anddynamically link them at runtime.RequirementsPyPIWe regularly push releases to  and encourage users to install from there in most cases.pip install deepspeed
After installation, you can validate your install and see which extensions/opsyour machine is compatible with via the DeepSpeed environment report.ds_report
If you would like to pre-install any of the DeepSpeed extensions/ops (insteadof JIT compiling) or install pre-compiled ops via PyPI please see our .WindowsWindows support is partially supported with DeepSpeed. On Windows you can build wheel with following steps, currently only inference mode is supported.FeaturesPlease checkout ,  and  pages for full set of features offered along each of these three pillars.Further ReadingAll DeepSpeed documentation, tutorials, and blogs can be found on our website: |                                                                                                | Description                                  || ---------------------------------------------------------------------------------------------- | -------------------------------------------- ||                                    |  First steps with DeepSpeed                  ||                      |  Configuring DeepSpeed                       ||                                |  Generated DeepSpeed API documentation       ||                                                |  Tutorials                                   ||                                                        |  Blogs                                   |ContributingDeepSpeed welcomes your contributions! Please see our guide for more details on formatting, testing,etc.Thanks so much to all of our amazing contributors!Contributor License AgreementThis project welcomes contributions and suggestions. Most contributions require you toagree to a Contributor License Agreement (CLA) declaring that you have the right to, andactually do, grant us the rights to use your contribution. For details, visithttps://cla.opensource.microsoft.com.When you submit a pull request, a CLA bot will automatically determine whether you needto provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simplyfollow the instructions provided by the bot. You will only need to do this once acrossall repos using our CLA.Code of ConductThis project has adopted the . For more information see the or contact with any additional questions or comments.PublicationsVideos"
https://github.com/carykh/jumpcutter,Automatically edits vidx. Explanation here: https://www.youtube.com/watch?v=DQ8orIurGxw,"jumpcutterAutomatically edits videos. Explanation here: https://www.youtube.com/watch?v=DQ8orIurGxwGo here for a more polished version of this software that my friends and I have been working on fr the last year or so: https://jumpcutter.com/Since my GitHub is more like a dumping ground or personal journal, I'm not going to be actively updating this GitHub repo. But if you do want a version of jumpcutter that is actively being worked on, please do check on the version at https://jumpcutter.com/! There's way more developers fixing bugs and adding new features to that tool, and there's a developer's Discord server to discuss anything JC-related, so go check it out!Some heads-up:It uses Python 3.It works on Ubuntu 16.04 and Windows 10. (It might work on other OSs too, we just haven't tested it yet.)This program relies heavily on ffmpeg. It will start subprocesses that call ffmpeg, so be aware of that!As the program runs, it saves every frame of the video as an image file in atemporary folder. If your video is long, this could take a LOT of space.I have processed 17-minute videos completely fine, but be wary if you're gonna go longer.I want to use pyinstaller to turn this into an executable, so non-techy peoplecan use it EVEN IF they don't have Python and all those libraries. Jabrilsrecommended this to me. However, my pyinstaller build did not work. :( HELPBuilding with nix to get a script with all the libraries and ffmpeg,  to get a single binary."
https://github.com/python-openxml/python-docx,Create and modify Word documents with Python,"python-docxpython-docx is a Python library for reading, creating, and updating Microsoft Word 2007+ (.docx) files.Installationpip install python-docx
Example>>> from docx import Document

>>> document = Document()
>>> document.add_paragraph(""It was a dark and stormy night."")
<docx.text.paragraph.Paragraph object at 0x10f19e760>
>>> document.save(""dark-and-stormy.docx"")

>>> document = Document(""dark-and-stormy.docx"")
>>> document.paragraphs[0].text
'It was a dark and stormy night.'
More information is available in the "
https://github.com/praw-dev/praw,"PRAW, an acronym for ""Python Reddit API Wrapper"", is a python package that allows for simple access to Reddit's API.","PRAW: The Python Reddit API Wrapper.. image:: https://img.shields.io/pypi/v/praw.svg:alt: Latest PRAW Version:target: https://pypi.python.org/pypi/praw.. image:: https://img.shields.io/pypi/pyversions/praw:alt: Supported Python Versions:target: https://pypi.python.org/pypi/praw.. image:: https://img.shields.io/pypi/dm/praw:alt: PyPI - Downloads - Monthly:target: https://pypi.python.org/pypi/praw.. image:: https://github.com/praw-dev/praw/actions/workflows/ci.yml/badge.svg?event=push:alt: GitHub Actions Status:target: https://github.com/praw-dev/praw/actions/workflows/ci.yml.. image:: https://coveralls.io/repos/github/praw-dev/praw/badge.svg:alt: Coveralls Coverage:target: https://coveralls.io/github/praw-dev/praw?branch=master.. image:: https://api.securityscorecards.dev/projects/github.com/praw-dev/praw/badge:alt: OpenSSF Scorecard:target: https://api.securityscorecards.dev/projects/github.com/praw-dev/praw.. image:: https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg:alt: Contributor Covenant:target: https://github.com/praw-dev/.github/blob/main/CODE_OF_CONDUCT.md.. image:: https://img.shields.io/badge/pre--commit-enabled-brightgreen?logo=pre-commit&logoColor=white:alt: pre-commit:target: https://github.com/pre-commit/pre-commit.. image:: https://img.shields.io/badge/code%20style-black-000000.svg:alt: Black code style:target: https://github.com/psf/blackPRAW, an acronym for ""Python Reddit API Wrapper"", is a Python package that allows forsimple access to Reddit's API. PRAW aims to be easy to use and internally follows all of_. With PRAW there's noneed to introduce  calls in your code. Give your client an appropriate useragent and you're set... _installation:InstallationPRAW is supported on Python 3.8+. The recommended way to install PRAW is via _... code-block:: bashpip install praw
To install the latest development version of PRAW run the following instead:.. code-block:: bashpip install --upgrade https://github.com/praw-dev/praw/archive/master.zip
For instructions on installing Python and pip see ""The Hitchhiker's Guide to Python""_.QuickstartAssuming you already have a credentials for a script-type OAuth application you caninstantiate an instance of PRAW like so:.. code-block:: pythonimport praw

reddit = praw.Reddit(
    client_id=""CLIENT_ID"",
    client_secret=""CLIENT_SECRET"",
    password=""PASSWORD"",
    user_agent=""USERAGENT"",
    username=""USERNAME"",
)
With the  instance you can then interact with Reddit:.. code-block:: python# Create a submission to r/test
reddit.subreddit(""test"").submit(""Test Submission"", url=""https://reddit.com"")

# Comment on a known submission
submission = reddit.submission(url=""https://www.reddit.com/comments/5e1az9"")
submission.reply(""Super rad!"")

# Reply to the first comment of a weekly top thread of a moderated community
submission = next(reddit.subreddit(""mod"").top(time_filter=""week""))
submission.comments[0].reply(""An automated reply"")

# Output score for the first 256 items on the frontpage
for submission in reddit.front.hot(limit=256):
    print(submission.score)

# Obtain the moderator listing for r/test
for moderator in reddit.subreddit(""test"").moderator():
    print(moderator)
Please see PRAW's _ for more examples ofwhat you can do with PRAW.Discord Bots and Asynchronous EnvironmentsIf you plan on using PRAW in an asynchronous environment, (e.g., discord.py, asyncio) itis strongly recommended to use _. It isthe official asynchronous version of PRAW and its usage is similar and has the samefeatures as PRAW.PRAW Discussion and SupportFor those new to Python, or would otherwise consider themselves a Python beginner,please consider asking questions on the _ subreddit. There are wonderful people there whocan help with general Python and simple PRAW related questions.Otherwise, there are a few official places to ask questions about PRAW:_ is the best place on Reddit to askPRAW related questions. This subreddit is for all Reddit API related discussion soplease tag submissions with [PRAW]. Please perform a search on the subreddit first tosee if anyone has similar questions.Real-time chat can be conducted via the _(please create an issue if that invite link has expired).Please do not directly message any of the contributors via Reddit, email, or Slackunless they have indicated otherwise. We strongly encourage everyone to help others withtheir questions.Please file bugs and feature requests as issues on _ after first searching to ensure a similarissue was not already filed. If such an issue already exists please give it a thumbs upreaction. Comments to issues containing additional information are certainly welcome... note::This project is released with a `Contributor Code of Conduct
<https://github.com/praw-dev/.github/blob/main/CODE_OF_CONDUCT.md>`_. By
participating in this project you agree to abide by its terms.
DocumentationPRAW's documentation is located at https://praw.readthedocs.io/.History_:Timothy Mellor created a github project called ._:The Python package  was registered and uploaded to pypi._:Bryce Boe took over as maintainer of the  package._:Bryce renamed the project  and the repository was relocated to the newly createdpraw-dev organization on GitHub._:Bryce began work on PRAW4, a complete rewrite of PRAW.LicensePRAW's source (v4.0.0+) is provided under the _.Earlier versions of PRAW were released under _.Sponsors.. image:: https://github.com/praw-dev/praw/blob/36fa3060b8938815feb45d07541877c8ce994cbb/docs/package_info/NucleiLogo.png:alt: Nuclei:target: https://nuclei.ai"
https://github.com/clips/pattern,"Web mining module for Python, with tools for scraping, natural language processing, machine learning, network analysis and visualization.","PatternPattern is a web mining module for Python. It has tools for:It is well documented, thoroughly tested with 350+ unit tests and comes bundled with 50+ examples. The source code is licensed under BSD.ExampleThis example trains a classifier on adjectives mined from Twitter using Python 3. First, tweets that contain hashtag #win or #fail are collected. For example: ""$20 tip off a sweet little old lady today #win"". The word part-of-speech tags are then parsed, keeping only adjectives. Each tweet is transformed to a vector, a dictionary of adjective → count items, labeled  or . The classifier uses the vectors to learn which other tweets look more like  or more like .from pattern.web import Twitter
from pattern.en import tag
from pattern.vector import KNN, count

twitter, knn = Twitter(), KNN()

for i in range(1, 3):
    for tweet in twitter.search('#win OR #fail', start=i, count=100):
        s = tweet.text.lower()
        p = '#win' in s and 'WIN' or 'FAIL'
        v = tag(s)
        v = [word for word, pos in v if pos == 'JJ'] # JJ = adjective
        v = count(v) # {'sweet': 1}
        if v:
            knn.train(v, type=p)

print(knn.classify('sweet potato burger'))
print(knn.classify('stupid autocorrect'))
InstallationPattern supports Python 2.7 and Python 3.6. To install Pattern so that it is available in all your scripts, unzip the download and from the command line do:cd pattern-3.6
python setup.py install
If you have pip, you can automatically download and install from the :pip install pattern
If none of the above works, you can make Python aware of the module in three ways:MODULE = '/users/tom/desktop/pattern'
import sys; if MODULE not in sys.path: sys.path.append(MODULE)
from pattern.en import parsetree
DocumentationFor documentation and examples see the .Version3.6LicenseBSD, see  for further details.ReferenceDe Smedt, T., Daelemans, W. (2012). Pattern for Python. Journal of Machine Learning Research, 13, 2031–2035.ContributeThe source code is hosted on GitHub and contributions or donations are welcomed.Bundled dependenciesPattern is bundled with the following data sets, algorithms and Python packages:AcknowledgementsAuthors:Contributors (chronological):"
https://github.com/zeromq/pyzmq,PyZMQ:  Python bindings for zeromq,"PyZMQ: Python bindings for ØMQThis package contains Python bindings for .ØMQ is a lightweight and fast messaging implementation.PyZMQ should work with any reasonable version of Python (≥ 3.7), as well as PyPy.The Cython backend used by CPython supports libzmq ≥ 2.1.4 (including 3.2.x and 4.x),but the CFFI backend used by PyPy only supports libzmq ≥ 3.2.2 (including 4.x).For a summary of changes to pyzmq, see our.ØMQ 3.x, 4.xPyZMQ fully supports the 3.x and 4.x APIs of libzmq,developed at .No code to change, no flags to pass,just build pyzmq against the latest and it should work.PyZMQ does not support the old libzmq 2 API on PyPy.DocumentationSee PyZMQ's Sphinx-generateddocumentation  for APIdetails, and some notes on Python and Cython development. If you want tolearn about using ØMQ in general, the excellent  is the place to start, which has aPython version of every example. We also have some information on our.DownloadingUnless you specifically want to develop PyZMQ, we recommend downloadingthe PyZMQ source code or wheels from,or install with conda.You can also get the latest source code from our GitHub repository, butbuilding from the repository will require that you install recent Cython.Building and installationFor more detail on building pyzmq, see .We build wheels for macOS, Windows, and Linux, so you can get a binary on those platforms with:pip install pyzmq
but compiling from source with  should work in most environments.Especially on macOS, make sure you are using the latest pip (≥ 8), or it may not find the right wheels.If the wheel doesn't work for some reason, or you want to force pyzmq to be compiled(this is often preferable if you already have libzmq installed and configured the way you want it),you can force installation with:pip install --no-binary=:all: pyzmq
When compiling pyzmq (e.g. installing with pip on Linux),it is generally recommended that zeromq be installed separately,via homebrew, apt, yum, etc:# Debian-based
sudo apt-get install libzmq3-dev

# RHEL-based
sudo yum install libzmq3-devel
If this is not available, pyzmq will try to build libzmq as a Python Extension,though this is not guaranteed to work.Building pyzmq from the git repo (including release tags on GitHub) requires Cython.Old versionspyzmq 16 drops support Python 2.6 and 3.2.If you need to use one of those Python versions, you can pin your pyzmq version to before 16:pip install 'pyzmq<16'
For libzmq 2.0.x, use 'pyzmq2.1'pyzmq-2.1.11 was the last version of pyzmq to support Python 2.5,and pyzmq ≥ 2.2.0 requires Python ≥ 2.6.pyzmq-13.0.0 introduces PyPy support via CFFI, which only supports libzmq-3.2.2 and newer.PyZMQ releases ≤ 2.2.0 matched libzmq versioning, but this is no longer the case,starting with PyZMQ 13.0.0 (it was the thirteenth release, so why not?).PyZMQ ≥ 13.0 follows semantic versioning conventions accounting only for PyZMQ itself."
https://github.com/jisaacks/GitGutter,A Sublime Text 2/3 plugin to see git diff in gutter,"GitGutterA  plug-in to show information about files in a git repository:and provides some commands like:Gutter Icons & Status Bar TextThe icons of the default theme have the following meaning:Icon          | Description:-------------:|-------------------------  | inserted line   | modified line   | deleted region borders   | ignored file | untracked fileDiff PopupThe diff popup shows the original content from the commit or the differences between it and the working content.The toolbar provides some commands to interact with or modify the changes.symbol | meaning of the symbol:-----:| ---------------------------------------×      | close the popup⤒      | goto to first change↑      | goto to previous change↓      | goto to next change≈, ≉   | enable/disable difference highlighting⎘      | copy the original content from the commit⟲      | revert a modified hunk to the original state in a commitDocumentationPlease read https://jisaacks.github.io/GitGutter/ for detailed information about"
https://github.com/django-tastypie/django-tastypie,Creating delicious APIs for Django apps since 2010.,"===============django-tastypie.. image:: https://readthedocs.org/projects/django-tastypie/badge/:target: https://django-tastypie.readthedocs.io/:alt: Docs.. image:: https://github.com/django-tastypie/django-tastypie/actions/workflows/python-package.yml/badge.svg:target: https://github.com/django-tastypie/django-tastypie/actions:alt: CI.. image:: https://coveralls.io/repos/django-tastypie/django-tastypie/badge.svg?service=github:target: https://coveralls.io/github/django-tastypie/django-tastypie:alt: Code Coverage.. image:: https://img.shields.io/pypi/v/django-tastypie.svg:target: https://pypi.python.org/pypi/django-tastypie:alt: Version.. image:: https://pypi-badges.global.ssl.fastly.net/svg?package=django-tastypie&timeframe=monthly:target: https://pypi.python.org/pypi/django-tastypie:alt: DownloadsCreating delicious APIs for Django apps since 2010.Currently in beta but being used actively in production on severalsites.RequirementsCoreFormat SupportOptionalWhat's It Look Like?A basic example looks like:.. code:: python# myapp/api.py
# ============
from tastypie.resources import ModelResource
from myapp.models import Entry


class EntryResource(ModelResource):
    class Meta:
        queryset = Entry.objects.all()


# urls.py
# =======
from django.urls.conf import re_path, include
from tastypie.api import Api
from myapp.api import EntryResource

v1_api = Api(api_name='v1')
v1_api.register(EntryResource())

urlpatterns = [
    # The normal jazz here then...
    re_path(r'^api/', include(v1_api.urls)),
]
That gets you a fully working, read-write API for the  model thatsupports all CRUD operations in a RESTful way. JSON/XML/YAML support is alreadythere, and it's easy to add related data/authentication/caching.You can find more in the documentation athttps://django-tastypie.readthedocs.io/.Why Tastypie?There are other API frameworks out there for Django. You need toassess the options available and decide for yourself. That said, here are somecommon reasons for tastypie.Reference MaterialGetting HelpThere are two primary ways of getting help... _: https://stackoverflow.com/questions/tagged/tastypie.. _#tastypie on irc.freenode.net: irc://irc.freenode.net/tastypieSecurityTastypie is committed to providing a flexible and secure API, and was designedwith many security features and options in mind. Due to the complex nature ofAPIs and the constant discovery of new attack vectors and vulnerabilities,no software is immune to security holes. We rely on our community to reportand help us investigate security issues.If you come across a security hole please do not open a Github issue.Instead, drop us an email at We'll then work together to investigate and resolve the problem so we canannounce a solution along with the vulnerability."
https://github.com/newsapps/beeswithmachineguns,A utility for arming (creating) many bees (micro EC2 instances) to attack (load test) targets (web applications).,"h4. Bees with Machine Guns!A utility for arming (creating) many bees (micro EC2 instances) to attack (load test) targets (web applications).Also, retribution for ""this shameful act"":http://kottke.org/10/10/tiny-catapult-for-throwing-pies-at-bees against a proud hive.h2. Dependenciesh2. Installation for usersh2. Installation for developers (w/ virtualenv + virtualenvwrapper)h2. Configuring AWS credentialsBees uses boto to communicate with EC2 and thus supports all the same methods of storing credentials that it does.  These include declaring environment variables, machine-global configuration files, and per-user configuration files. You can read more about these options on ""boto's configuration page"":http://code.google.com/p/boto/wiki/BotoConfig.At minimum, create a .boto file in your home directory with the following contents:The credentials used must have sufficient access to EC2.Make sure the .boto file is only accessible by the current account:h2. UsageA typical bees session looks something like this:A bees session where this is being called from a python file, while specifying content type and a payload file.This is a working example, all of these objects exist in the us-east-1 region.In this case the data.json is a simple json file, mind the path.This spins up 4 servers in security group 'public' using the EC2 keypair 'frakkingtoasters', whose private key is expected to reside at ~/.ssh/frakkingtoasters.pem.Note: the default EC2 security group is called 'default' and by default it locks out SSH access. I recommend creating a 'public' security group for use with the bees and explicitly opening port 22 on that group.It then uses those 4 servers to send 10,000 requests, 250 at a time, to attack OurNewWebbyHotness.com.Lastly, it spins down the 4 servers.  Please remember to do this--we aren't responsible for your EC2 bills.If you wanted 3 agents requesting url A and one requesting url B, your attack would look as follows (empty url -> use previous):For complete options type:h2. Introduction to additions:h4. Additions contributed Hurl integration and multi regional testing.hurl is an http server load tester similar to ab/siege/weighttp/wrk with support for multithreading, parallelism, ssl, url ranges, and an api-server for querying the running performance statistics.  hurl is primarily useful for benchmarking http server applications.For more information about hurl please visit https://github.com/VerizonDigital/hlxMulti regional testing was added so user can call up multiple bees from different regions simultaneously. Users have the ability to “up”, “attack”, and  “down” instances from single command. “regions.json” file is supplied which contains public ami images with hurl pre installed for all regions.What kind of changes were made that's different from the old?Instead of writing bees information into a single ~/.bees file, each zone recognized in arguments creates a new unique bees file. Bees.py was modified to read these files. Up, attack, and down functions are run with threads.example .bees files in user home directoryh4. MotivationHaving the ability to generate a lot of HTTPS requests from many different regions around the world allows us to better test our platforms and services. This is also real helpful when there are tools that need to be tested for such things as location of requests.h4. Hurl Usageh4. bees upCommand line arguments are still the same however to add multiple zones with multiple amis, the values must be comma delimited. The ami and zones must also be in same order for it to work. So for example “-i ami-zone1,ami-zone2,ami-zone3 -z zone1,zone2,zone3”.h4. bees attackIn order to use the hurl platform, --hurl or -j must be supplied. Attacks will run concurrently and return a summarized output. The output is summarized per region. More information can be seen if user supplies the -o, --long_output options.h4. bees downBringing down bees is the same and will bring down all bees for all regionsregions used: eu-west-1b,ap-southeast-1b,us-west-2bSome options were added to work with hurlh4. ExamplesA bringing up bees exampleA bees attack exampleA bees attack example with --long_outputAn example bees downh2. The caveat! (PLEASE READ)(The following was cribbed from our ""original blog post about the bees"":http://blog.apps.chicagotribune.com/2010/07/08/bees-with-machine-guns/.)If you decide to use the Bees, please keep in mind the following important caveat: they are, more-or-less a distributed denial-of-service attack in a fancy package and, therefore, if you point them at any server you don’t own you will behaving unethically, have your Amazon Web Services account locked-out, and be liable in a court of law for any downtime you cause.You have been warned.h2. Troubleshootingh3. EC2 Instances Out Of SyncIf you find yourself in a situation where 'bees report' seems to be out of sync with EC2 instances you know are (or are not) running:This is helpful in cases where BWMG crashes, EC2 instances are terminated outside of the control of BWMG, or other situations where BWMG is out of sync with reality.h2. BugsPlease log your bugs on the ""Github issues tracker"":http://github.com/newsapps/beeswithmachineguns/issues.h2. CreditsThe bees are a creation of the News Applications team at the Chicago Tribune--visit ""our blog"":http://apps.chicagotribune.com/ and read ""our original post about the project"":http://blog.apps.chicagotribune.com/2010/07/%2008/bees-with-machine-guns/.Initial refactoring code and inspiration from ""Jeff Larson"":http://github.com/thejefflarson.Multiple url support from ""timsu"":https://github.com/timsu/beeswithmachineguns.Thanks to everyone who reported bugs against the alpha release.h2. LicenseMIT."
https://github.com/py-pdf/pypdf,"A pure-python PDF library capable of splitting, merging, cropping, and transforming the pages of PDF files","pypdfpypdf is a free and open-source pure-python PDF library capable of splitting,,the pages of PDF files. It can also addcustom data, viewing options, andto PDF files. pypdf canandfrom PDFs as well.See  for a CLI application that uses pypdf to interact with PDFs.InstallationInstall pypdf using pip:pip install pypdf
For using pypdf with AES encryption or decryption, install extra dependencies:pip install pypdf[crypto]
Usagefrom pypdf import PdfReader

reader = PdfReader(""example.pdf"")
number_of_pages = len(reader.pages)
page = reader.pages[0]
text = page.extract_text()
pypdf can do a lot more, e.g. splitting, merging, reading and creatingannotations, decrypting and encrypting, and more. Check out  for additional usageexamples!For questions and answers, visit(tagged with ).ContributionsMaintaining pypdf is a collaborative effort. You can support the project bywriting documentation, helping to narrow down issues, and submitting code.See the  file for more information.Q&AThe experience pypdf users have covers the whole range from beginners whowant to make their live easier to experts who developed software before PDFexisted. You can contribute to the pypdf community by answering questionson ,helping in ,and asking users who report issues for 's (Code + example PDF!).IssuesA good bug ticket includes a MCVE - a minimal complete verifiable example.For pypdf, this means that you must upload a PDF that causes the bug to occuras well as the code you're executing with all of the output. Use to tell us which version you're using.CodeAll code contributions are welcome, but smaller ones have a better chance toget included in a timely manner. Adding unit tests for new features or testcases for bugs you've fixed help us to ensure that the Pull Request (PR) is fine.pypdf includes a test suite which can be executed with :$ pytest
===================== test session starts =====================
platform linux -- Python 3.6.15, pytest-7.0.1, pluggy-1.0.0
rootdir: /home/moose/GitHub/Martin/pypdf
plugins: cov-3.0.0
collected 233 items

tests/test_basic_features.py ..                         [  0%]
tests/test_constants.py .                               [  1%]
tests/test_filters.py .................x.....           [ 11%]
tests/test_generic.py ................................. [ 25%]
.............                                           [ 30%]
tests/test_javascript.py ..                             [ 31%]
tests/test_merger.py .                                  [ 32%]
tests/test_page.py .........................            [ 42%]
tests/test_pagerange.py ................                [ 49%]
tests/test_papersizes.py ..................             [ 57%]
tests/test_reader.py .................................. [ 72%]
...............                                         [ 78%]
tests/test_utils.py ....................                [ 87%]
tests/test_workflows.py ..........                      [ 91%]
tests/test_writer.py .................                  [ 98%]
tests/test_xmp.py ...                                   [100%]

========== 232 passed, 1 xfailed, 1 warning in 4.52s ==========
"
https://github.com/mozillazg/python-pinyin,汉字转拼音(pypinyin),"汉字拼音转换工具（Python 版）|Build| |GitHubAction| |Coverage| |Pypi version| |PyPI downloads| |DOI|将汉字转为拼音。可以用于汉字注音、排序、检索(_) 。最初版本的代码参考了 __ 的实现。.. contents::特性安装.. code-block:: bashpip install pypinyin
使用示例Python 3(Python 2 下把  替换为  即可):.. code-block:: python>>> from pypinyin import pinyin, lazy_pinyin, Style
>>> pinyin('中心')  # or pinyin(['中心'])，参数值为列表时表示输入的是已分词后的数据
[['zhōng'], ['xīn']]
>>> pinyin('中心', heteronym=True)  # 启用多音字模式
[['zhōng', 'zhòng'], ['xīn']]
>>> pinyin('中心', style=Style.FIRST_LETTER)  # 设置拼音风格
[['z'], ['x']]
>>> pinyin('中心', style=Style.TONE2, heteronym=True)
[['zho1ng', 'zho4ng'], ['xi1n']]
>>> pinyin('中心', style=Style.TONE3, heteronym=True)
[['zhong1', 'zhong4'], ['xin1']]
>>> pinyin('中心', style=Style.BOPOMOFO)  # 注音风格
[['ㄓㄨㄥ'], ['ㄒㄧㄣ']]
>>> lazy_pinyin('威妥玛拼音', style=Style.WADEGILES)
['wei', ""t'o"", 'ma', ""p'in"", 'yin']
>>> lazy_pinyin('中心')  # 不考虑多音字的情况
['zhong', 'xin']
>>> lazy_pinyin('战略', v_to_u=True)  # 不使用 v 表示 ü
['zhan', 'lüe']
# 使用 5 标识轻声
>>> lazy_pinyin('衣裳', style=Style.TONE3, neutral_tone_with_five=True)
['yi1', 'shang5']
# 变调  nǐ hǎo -> ní hǎo
>>> lazy_pinyin('你好', style=Style.TONE2, tone_sandhi=True)
['ni2', 'ha3o']
注意事项 ：命令行工具：.. code-block:: console$ pypinyin 音乐
yīn yuè
$ pypinyin -h
文档详细文档请访问：https://pypinyin.readthedocs.io/。项目代码开发方面的问题可以看看 _ 。FAQ拼音有误？+++++++++++++++++++++++++++++可以通过下面的方法提高拼音准确性：.. code-block:: python>> from pypinyin import load_phrases_dict, load_single_dict

>> load_phrases_dict({'桔子': [['jú'], ['zǐ']]})  # 增加 ""桔子"" 词组

>> load_single_dict({ord('还'): 'hái,huán'})  # 调整 ""还"" 字的拼音顺序或覆盖默认拼音
.. code-block:: python# 使用 phrase-pinyin-data 项目中 cc_cedict.txt 文件中的拼音数据优化结果
>>> from pypinyin_dict.phrase_pinyin_data import cc_cedict
>>> cc_cedict.load()

# 使用 pinyin-data 项目中 kXHC1983.txt 文件中的拼音数据优化结果
>>> from pypinyin_dict.pinyin_data import kxhc1983
>>> kxhc1983.load()
.. code-block:: python>>> # 使用其他分词模块分词，比如 jieba 之类，
>>> #或者基于 phrases_dict.py 里的词语数据使用其他分词算法分词
>>> words = list(jieba.cut('每股24.67美元的确定性协议'))
>>> pinyin(words)
为什么没有 y, w, yu 几个声母？++++++++++++++++++++++++++++++++++++++++++++.. code-block:: python>>> from pypinyin import Style, pinyin
>>> pinyin('下雨天', style=Style.INITIALS)
[['x'], [''], ['t']]
因为根据 __ ，y，w，ü (yu) 都不是声母。声母风格（INITIALS）下，“雨”、“我”、“圆”等汉字返回空字符串，因为根据
`《汉语拼音方案》 <http://www.moe.gov.cn/jyb_sjzl/ziliao/A19/195802/t19580201_186000.html>`__ ，
y，w，ü (yu) 都不是声母，在某些特定韵母无声母时，才加上 y 或 w，而 ü 也有其特定规则。    —— @hotoo

**如果你觉得这个给你带来了麻烦，那么也请小心一些无声母的汉字（如“啊”、“饿”、“按”、“昂”等）。
这时候你也许需要的是首字母风格（FIRST_LETTER）**。    —— @hotoo

参考: `hotoo/pinyin#57 <https://github.com/hotoo/pinyin/issues/57>`__,
`#22 <https://github.com/mozillazg/python-pinyin/pull/22>`__,
`#27 <https://github.com/mozillazg/python-pinyin/issues/27>`__,
`#44 <https://github.com/mozillazg/python-pinyin/issues/44>`__
如果觉得这个行为不是你想要的，就是想把 y 当成声母的话，可以指定  ，这个可能会符合你的预期：.. code-block:: python>>> from pypinyin import Style, pinyin
>>> pinyin('下雨天', style=Style.INITIALS)
[['x'], [''], ['t']]
>>> pinyin('下雨天', style=Style.INITIALS, strict=False)
[['x'], ['y'], ['t']]
详见 _ 。存在既没有声母也没有韵母的拼音？+++++++++++++++++++++++++++++++++是的， 模式下存在极少数既没有声母也没有韵母的拼音。比如下面这些拼音（来自汉字 、、、）::ń ńg ňg ǹg ň ǹ m̄ ḿ m̀
尤其需要注意的是  的所有拼音都既没有声母也没有韵母， 的默认拼音既没有声母也没有韵母。详见 _ _ _ 。如何将某一风格的拼音转换为其他风格的拼音？++++++++++++++++++++++++++++++++++++++++++++可以通过  模块提供的辅助函数对标准拼音进行转换，得到不同风格的拼音。比如将  转换为 ，或者获取拼音中的声母或韵母数据：.. code-block:: python>>> from pypinyin.contrib.tone_convert import to_normal, to_tone, to_initials, to_finals
>>> to_normal('zhōng')
'zhong'
>>> to_tone('zhong1')
'zhōng'
>>> to_initials('zhōng')
'zh'
>>> to_finals('zhōng')
'ong'
更多拼音转换的辅助函数，详见  模块的__ 。如何减少内存占用？++++++++++++++++++++如果对拼音的准确性不是特别在意的话，可以通过设置环境变量 和  来节省内存。详见 __更多 FAQ 详见文档中的__ 部分。.. _#13 : https://github.com/mozillazg/python-pinyin/issues/113.. _strict 参数的影响: https://pypinyin.readthedocs.io/zh_CN/master/usage.html#strict拼音数据Related Projects__ https://github.com/hotoo/pinyin__ https://github.com/mozillazg/go-pinyin__ https://github.com/mozillazg/rust-pinyin.. |Build| image:: https://img.shields.io/circleci/project/github/mozillazg/python-pinyin/master.svg:target: https://circleci.com/gh/mozillazg/python-pinyin.. |GitHubAction| image:: https://github.com/mozillazg/python-pinyin/workflows/CI/badge.svg:target: https://github.com/mozillazg/python-pinyin/actions.. |Coverage| image:: https://img.shields.io/coveralls/github/mozillazg/python-pinyin/master.svg:target: https://coveralls.io/github/mozillazg/python-pinyin.. |PyPI version| image:: https://img.shields.io/pypi/v/pypinyin.svg:target: https://pypi.org/project/pypinyin/.. |DOI| image:: https://zenodo.org/badge/12830126.svg:target: https://zenodo.org/badge/latestdoi/12830126.. |PyPI downloads| image:: https://img.shields.io/pypi/dm/pypinyin.svg:target: https://pypi.org/project/pypinyin/.. _Russian translation: https://github.com/mozillazg/python-pinyin/blob/master/README_ru.rst.. _pinyin-data: https://github.com/mozillazg/pinyin-data.. _phrase-pinyin-data: https://github.com/mozillazg/phrase-pinyin-data.. _开发文档: https://pypinyin.readthedocs.io/zh_CN/develop/develop.html.. _#109: https://github.com/mozillazg/python-pinyin/issues/109.. _#259: https://github.com/mozillazg/python-pinyin/issues/259.. _#284: https://github.com/mozillazg/python-pinyin/issues/284"
https://github.com/thumbor/thumbor,thumbor is an open-source photo thumbnail service by globo.com,"thumbor is a smart imaging service that enables on-demand  images.Cropping photos automatically can be a frustrating experience with severed heads involved. thumboruses .thumbor is an HTTP server and you can create as many different images as you want just by varying path parameters:http://<thumbor-server>/300x200/smart/thumbor.readthedocs.io/en/latest/_images/logo-thumbor.png
You should see an image of the thumbor logo in 300x200.Learn more about all you can do in .⚙️ InstallationDecide which installation option you want to use.Option 1: pip# thumbor with main dependencies only
pip install thumbor

# thumbor with OpenCV dependency
pip install thumbor[opencv]

# thumbor with all dependencies
pip install thumbor[all]
Option 2: Binarysudo add-apt-repository ppa:thumbor/ppa
sudo aptitude update
sudo aptitude install thumbor
For more ways, please check out .RunRunning it is as easy as hit:thumbor
After this, you can reach it on https://localhost:8888/unsafe/https://raw.githubusercontent.com/thumbor/thumbor/master/example.jpgTroubles?If you experience any troubles, try running:thumbor-doctor
If you have a  file, you can use that to help thumbor-doctor:thumbor-doctor -c thumbor.conf
If you still need help, please . Remember to send your  output in the issue:thumbor-doctor --nocolor -c thumbor.conf
🎯 Features🌟 Awesome Goodies is a curated list of all things thumbor. There you can find filters, storages, engines, loaders, docker images, extensions in your favorite language and framework, and much more.All of it with a clear indication of each project's quality. Have fun!👍 Contributethumbor is an open-source project with many contributors. Join them or.If you use thumbor, please take 1 minute and answer ? Only 2 questions!Join the chat at https://gitter.im/thumbor/thumbor👀 DemoYou can see thumbor in action at http://thumborize.me/"
https://github.com/lijiejie/subDomainsBrute,A fast sub domain brute tool for pentesters,"subDomainsBrute 1.5A fast sub domain brute tool for pentesters, works with Python3.5+ or Python2.7.高并发的DNS暴力枚举工具，支持Python3.6+和Python2.7，建议使用Python3.8+。InstallPython3.5+ users:   Python2.7 users:    New FeaturesScreenShot使用大字典，扫描qq.comUsageUsage: subDomainsBrute.py [options] target.com

Options:
  --version             show program's version number and exit
  -h, --help            show this help message and exit
  -f FILE               File contains new line delimited subs, default is
                        subnames.txt.
  --full                Full scan, NAMES FILE subnames_full.txt will be used
                        to brute
  -i, --ignore-intranet
                        Ignore domains pointed to private IPs
  -w, --wildcard        Force scan after wildcard test failed
  -t THREADS, --threads=THREADS
                        Num of scan threads, 500 by default
  -p PROCESS, --process=PROCESS
                        Num of scan process, 6 by default
  --no-https            Disable get domain names from HTTPS cert, this can
                        save some time
  -o OUTPUT, --output=OUTPUT
                        Output file name. default is {target}.txt
Change Log"
https://github.com/tgalal/yowsup,The WhatsApp lib,"yowsup For private consultancy feel free to directly schedule it over yowsup is a python library that enables building applications that can communicate with WhatsApp users.The project started as the protocol engine behind  and. Now as a standalonelibrary it can be used to power any custom WhatsApp client.updated: 2021-12-14
yowsup version: 3.3.0
yowsup-cli version: 3.2.1
requires:
- python>=2.7,<=3.7
- consonance==0.1.5
- python-axolotl==0.2.2
- protobuf>=3.6.0
- six==1.10
uses:
 - argparse [yowsup-cli]
 - readline [yowsup-cli]
 - pillow [send images]
 - tqdm [mediasink demo]
 - requests [mediasink demo]
See alsoDuring maintenance of yowsup, several projects have been spawned in order to support different features that getintroduced by WhatsApp. Some of those features are not necessarily exclusive to WhatsApp and therefore it only madesense to maintain some parts as standalone projects:QuickstartInstallationInstall using setup.py to pull all Python dependencies, or pip:pip install yowsup
LinuxYou need to have installed Python headers (probably from python-dev package) and ncurses-dev, then runpython setup.py install
FreeBSD (*BSD)You need to have installed: py27-pip-7.1.2(+), py27-sqlite3-2.7.11_7(+), then runpip install yowsup
Mac OSpython setup.py install
Administrators privileges might be required, if so then run with 'sudo'Windows[build]
compiler=mingw32
License:As of January 1, 2015 yowsup is licensed under the GPLv3+: http://www.gnu.org/licenses/gpl-3.0.html."
https://github.com/521xueweihan/HelloGitHub,":octocat: 分享 GitHub 上有趣、入门级的开源项目。Share interesting, entry-level open source projects on GitHub.",简介HelloGitHub 分享 GitHub 上有趣、入门级的开源项目。每月 28 号以月刊的形式，内容包括：有趣、入门级的开源项目、开源书籍、实战项目、企业级项目等，让你用很短时间感受到开源的魅力，爱上开源！内容获得更好的阅读体验  或 | :card_index: | :jack_o_lantern: | :beer: | :fish_cake: | :octocat: || ------- | ----- | ------------ | ------ | --------- ||  ||  |  |  |  |  ||  |  |  |  |  ||  |  |  |  |  ||  |  |  |  |  ||  |  |  |  |  ||  |  |  |  |  ||  |  |  |  |  ||  |  |  |  |  |欢迎项目成为 HelloGitHub 的赞助声明本作品采用 署名-非商业性使用-禁止演绎 4.0 国际 进行许可。联系我
https://github.com/p-e-w/maybe, :open_file_folder: :rabbit2: :tophat: See what a program does before deciding whether you really want it to happen (NO LONGER MAINTAINED),"rm -rf pic*
Are you sure? Are you one hundred percent sure?...... allows you to run a command and see what it does to your files without actually doing it! After reviewing the operations listed, you can then decide whether you really want these things to happen or not.What is this sorcery?!? runs processes under the control of  (with the help of the excellent  library). When it intercepts a system call that is about to make changes to the file system, it logs that call, and then modifies CPU registers to both redirect the call to an invalid syscall ID (effectively turning it into a no-op) and set the return value of that no-op call to one indicating success of the original call.As a result, the process believes that everything it is trying to do is actually happening, when in reality nothing is.That being said,  should :warning: NEVER :warning: be used to run untrusted code on a system you care about! A process running under  can still do serious damage to your system because only a handful of syscalls are blocked. It can also check whether an operation such as deleting a file succeeded using read-only syscalls, and alter its behavior accordingly. Therefore, a rerun without restrictions is not guaranteed to always produce the displayed operations.Currently,  is best thought of as an (alpha-quality) ""what exactly will this command I typed myself do?"" tool.Installation runs on Linux :penguin: and requires  2.7+/3.3+ :snake:. If you have the  package manager, all you need to do is runpip install maybe
either as a superuser or from a  environment. To develop , clone the repository and runpip install -e .
in its main directory to install the package in editable mode.Usagemaybe [options] command [argument ...]
Positional arguments| Argument | Description || --- | --- ||  | the command to run under 's control ||  | argument(s) to pass to  |Optional arguments| Argument | Description || --- | --- || ,&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; | allow the command to perform the specified operation(s). all other operations will be denied. possible values for  are: , , , , , , ; as well as any filter scopes defined by loaded plugins || , | deny the command the specified operation(s). all other operations will be allowed. see  for a list of possible values for .  and  cannot be combined || , | load the specified  script(s) || ,  | list operations without header, indentation and rerun prompt ||  | colorize output using ANSI escape sequences (/) or automatically decide based on whether stdout is a terminal (, default) || ,  | if specified once, print every filtered syscall. if specified twice, print every syscall, highlighting filtered syscalls ||  | show program's version number and exit || ,  | show a help message and exit |Plugin APIBy default,  intercepts and blocks all syscalls that can make permanent modifications to the system. For more specialized syscall filtering needs,  provides a simple yet powerful plugin API. Filter plugins are written in pure Python and use the same interfaces as .The public API is composed of the following two members:maybe.A   object that can be used to format console output (such as  as documented below). Output formatted with this object automatically complies with the  command line argument.maybe.Add the filter  to the filter registry. If the filter is enabled (which is the default, but can be altered with the  and  command line arguments), it intercepts all calls to  made by the controlled process.  determines the key to be used in conjunction with  and  to enable/disable the filter (multiple filters can share the same key). If  is omitted or , the last part of the plugin's module name is used. itself must conform to the signature .  is a  control object that can be used to inspect and manipulate the process, while  is the list of arguments passed to the syscall in the order in which they appear in the syscall's signature. If an argument represents a (pointer to a) filename, the argument will be of type  and contain the filename, otherwise it will be of type  and contain the numerical value of the argument.When called,  must return a tuple .  can either be a string description of the operation that was prevented by the filter, to be printed after the process terminates, or , in which case nothing will be printed.  can either be a numerical value, in which case the syscall invocation will be prevented and the return value received by the caller will be set to that value, or , in which case the invocation will be allowed to proceed as normal.ExampleHere, 's plugin API is used to implement an exotic type of access control: Restricting read access based on the content of the file in question. If a file being opened for reading contains the word SECRET, the plugin blocks the / syscall and returns an error.from os import O_WRONLY
from os.path import isfile
from maybe import T, register_filter

def filter_open(path, flags):
    if path.startswith(""/home/"") and isfile(path) and not (flags & O_WRONLY):
        with open(path, ""r"") as f:
            if ""SECRET"" in f.read():
                return ""%s %s"" % (T.red(""read secret file""), T.underline(path)), -1
            else:
                return None, None
    else:
        return None, None

register_filter(""open"", lambda process, args:
                filter_open(process.full_path(args[0]), args[1]))
register_filter(""openat"", lambda process, args:
                filter_open(process.full_path(args[1], args[0]), args[2]))
Indeed, the plugin works as expected:[user@localhost]$ maybe --plugin read_secret_file.py --deny read_secret_file -- bash
$ echo ""This is a normal file."" > file_1
$ echo ""This is a SECRET file."" > file_2
$ cat file_1
This is a normal file.
$ cat file_2
cat: file_2: Operation not permitted
LicenseCopyright &copy; 2016-2017 Philipp Emanuel Weidmann ()Released under the terms of the "
https://github.com/numenta/nupic-legacy,"Numenta Platform for Intelligent Computing is an implementation of Hierarchical Temporal Memory (HTM), a theory of intelligence based strictly on the neuroscience of the neocortex."," NuPICAs of September 2023 this repository contains code from legacy Hierarchical Temporal Memory (HTM) Numenta projects that have been in maintenance mode for several years.Numenta Platform for Intelligent ComputingThe Numenta Platform for Intelligent Computing (NuPIC) is a machine intelligence platform that implements the . HTM is a detailed computational theory of the neocortex. At the core of HTM are time-based continuous learning algorithms that store and recall spatial and temporal patterns. NuPIC is suited to a variety of problems, particularly anomaly detection and prediction of streaming data sources. For more information, see  or the .For usage guides, quick starts, and API documentation, see .This project is in Maintenance ModeWe plan to do minor releases only, and limit changes in NuPIC and NuPIC Core to:Installing NuPICNuPIC binaries are available for:DependenciesThe following dependencies are required to install NuPIC on all operating systems.Additional OS X requirements:InstallRun the following to install NuPIC:pip install nupic
Test# From the root of the repo:
py.test tests/unit
Having problems?For any other installation issues, please see our  (post questions there). You can report bugs at https://github.com/numenta/nupic/issues.Live Community Chat: Installing NuPIC From SourceTo install from local source code, run from the repository root:pip install .
Use the optional  argument for a developer install.If you want to build the dependent  from source, you should build and install from  prior to installing nupic (since a PyPI release will be installed if  isn't yet installed)."
https://github.com/thearn/webcam-pulse-detector,A python application that detects and highlights the heart-rate of an individual (using only their own webcam) in real-time.,"webcam-pulse-detector- UPDATE: Now with Python 3.5+ and OpenCV 3.0+ supportStand-alone (no dependancy) precompiled application:The application can be run by simply executing the binary contained in the zip file for your platform.This code can also be run from source by following the instructions below.A python code that detects the heart-rate of an individual using a common webcam or network IP camera.Tested on OSX, Ubuntu, and Windows.How it works:This application uses  to find the location of the user's face, then isolate the forehead region. Data is collectedfrom this location over time to estimate the user's heart rate. This is done by measuring average opticalintensity in the forehead location, in the subimage's green channel alone (a better color mixing ratio may exist, but theblue channel tends to be very noisy). Physiological data can be estimated this way thanks to the optical absorptioncharacteristics of (oxy-) haemoglobin (see http://www.opticsinfobase.org/oe/abstract.cfm?uri=oe-16-26-21434). With good lighting and minimal noise due to motion, a stable heartbeat should beisolated in about 15 seconds. Other physiological waveforms (such as) should also be visible in the raw data stream.Once the user's heart rate has been estimated, real-time phase variation associated with thisfrequency is also computed. This allows for the heartbeat to be exaggerated in the post-process frame rendering,causing the highlighted forehead location to pulse in sync with the user's own heartbeat.Support for detection on multiple simultaneous individuals in a single camera'simage stream is definitely possible, but at the moment only the information from one faceis extracted for analysis.The overall dataflow/execution order for the real-time signal processing looks like:Requirements:Quickstart:python get_pulse.py
python get_pulse_ipcam.py
This was tested on a Wowwee Rovio.Usage notes:"
https://github.com/insanum/gcalcli,Google Calendar Command Line Interface,"gcalcliGoogle Calendar Command Line Interfacegcalcli is a Python application that allows you to access your GoogleCalendar(s) from a command line. It's easy to get your agenda, search forevents, add new events, delete events, edit events, see recently updatedevents, and even import those annoying ICS/vCal invites from MicrosoftExchange and/or other sources. Additionally, gcalcli can be used as a reminderservice and execute any application you want when an event is coming up.gcalcli uses the .RequirementsOptional packagesInstallationCheck your OS distribution for packages.Debian/Ubuntuapt-get install gcalcli
Void Linuxxbps-install gcalcli
Install using nix-env -i gcalcli
Install using  (MacOS)brew install gcalcli
Install from PyPIpip install gcalcli
Install from sourcegit clone https://github.com/insanum/gcalcli.git
cd gcalcli
python setup.py install
Install optional packagepip install vobject
FeaturesScreenshotsHowToUsage provides a series of subcommands with the following functionality:list                list available calendars
edit                edit calendar events
agenda              get an agenda for a time period
updates             get updates since a datetime for a time period
calw                get a week-based agenda in calendar format
calm                get a month agenda in calendar format
quick               quick-add an event to a calendar
add                 add a detailed event to the calendar
import              import an ics/vcal file to a calendar
remind              execute command if event occurs within <mins> time
See the manual (), or run with / for detailed usage.Login InformationOAuth2 is used for authenticating with your Google account. The resulting tokenis placed in the  file. When you first start gcalcli theauthentication process will proceed. Simply follow the instructions.You currently have to use your own Calendar API token. Our Calendar API token is restricted to few users only and waits for Google's approval to be unlocked.HTTP Proxy Supportgcalcli will automatically work with an HTTP Proxy simply by setting up someenvironment variables used by the gdata Python module:http_proxy
https_proxy
proxy-username or proxy_username
proxy-password or proxy_password
Note that these environment variables must be lowercase.Flag File is able to read default configuration information from a flag file.This file is located, by default, at '~/.gcalclirc'.  The flag file takes onecommand line parameter per line.In the current version, the flag file only supports the global options (optionsagainst the  program itself).  The plan, longer term, is to support aa configuration formation (probably toml or ini), which will allow forconfiguration of subcommands (such as , , , etc.)Example:--nocache
--nocolor
--default-calendar=CALENDAR_NAME
--client-secret=API_KEY
Note that long options require an equal sign if specifying a parameter.  Withshort options the equal sign is optional.Configuration Foldersgcalcli is able to store all its necessary information in a specific folder (usethe --configFolder option.) Each folder will contain 2 files: oauth and cache.An optional 3rd file, gcalclirc, can be present for specific flags that you onlywant to apply when using this configuration folder.Importing VCS/VCAL/ICS Files from Exchange (or other)Importing events from files is easy with gcalcli. The 'import' command acceptsa filename on the command line or can read from standard input. Here is a scriptthat can be used as an attachment handler for Thunderbird or in a mailcap entrywith Mutt (or in Mutt you could just use the attachment viewer and pipe command):#!/bin/bash

TERMINAL=evilvte
CONFIG=~/.gcalclirc

$TERMINAL -e bash -c ""echo 'Importing invite...' ; \
                      gcalcli --detail-url=short \
                              --calendar='Eric Davis' \
                              import -v \""$1\"" ; \
                      read -p 'press enter to exit: '""
Note that with Thunderbird you'll have to have the 'Show All Body Parts'extension installed for seeing the calendar attachments when not using'Lightning'. See thisfor more details.Event Popup RemindersThe 'remind' command for gcalcli is used to execute any command as an eventnotification. This can be a notify-send or an xmessage-like popup or whateverelse you can think of. gcalcli does not contain a daemon so you'll have to usesome other tool to ensure gcalcli is run in a timely manner for notifications.Two options are using cron or a loop inside a shell script.Cron:% crontab -l
*/10 * * * * /usr/bin/gcalcli remind
Shell script like your .xinitrc so notifications only occur when you're loggedin via X:#!/bin/bash

[[ -x /usr/bin/dunst ]] && /usr/bin/dunst -config ~/.dunstrc &

if [ -x /usr/bin/gcalcli ]; then
  while true; do
    /usr/bin/gcalcli --calendar=""davis"" remind
    sleep 300
  done &
fi

exec herbstluftwm # :-)
By default gcalcli executes the notify-send command for notifications. Mostcommon Linux desktop enviroments already contain a DBUS notification daemonthat supports libnotify so it should automagically just work. If you're likeme and use nothing that is common I highly recommend the dmenu'ish notification daemon.Note that each time you run this you will get a reminder if you're still insidethe event duration.  Also note that due to time slip between machines, gcalcliwill give you a ~5 minute margin of error.  Plan your cron jobs accordingly.Agenda On Your Root DesktopPut your agenda on your desktop using. The '--conky' option causesgcalcli to output Conky color sequences. Note that you need to use the Conky'execpi' command for the gcalcli output to be parsed for color sequences. Addthe following to your .conkyrc:${execpi 300 gcalcli --conky agenda}
To also get a graphical calendar that shows the next three weeks add:${execpi 300 gcalcli --conky calw 3}
You may need to increase the  in your conkyrc file.  Usershave reported that the default of 256 bytes is too small for busy calendars.Additionaly you need to set  to output unicode-charactersfor box drawing. To avoid misaligned borders use a monospace font like 'DejaVuSans Mono'. On Python2 it might be necessary to set the environment variable if you are using characters beyond ascii. Forexample:${font DejaVu Sans Mono:size=9}${execpi 300 export PYTHONIOENCODING=utf8 && gcalcli --conky --lineart=unicode calw 3}
Agenda Integration With tmuxPut your next event in the left of your 'tmux' status line.  Add the followingto your tmux.conf file:set-option -g status-interval 60
set-option -g status-left ""#[fg=blue,bright]#(gcalcli agenda | head -2 | tail -1)#[default]""
Agenda Integration With screenPut your next event in your 'screen' hardstatus line.  First add a cron jobthat will dump you agenda to a text file:% crontab -e
Then add the following line:*/5 * * * * gcalcli --nocolor --nostarted agenda ""`date`"" > /tmp/gcalcli_agenda.txt
Next create a simple shell script that will extract the first agenda line.Let's call this script 'screen_agenda':#!/bin/bash
head -2 /tmp/gcalcli_agenda.txt | tail -1
Next configure screen's hardstatus line to gather data from a backtick command.Of course your hardstatus line is most likely very different than this (Mineis!):backtick 1 60 60 screen_agenda
hardstatus ""[ %1` ]""
"
https://github.com/byt3bl33d3r/CrackMapExec,A swiss army knife for pentesting networks,"  CrackMapExecThis project was initially created in 2015 by @byt3bl33d3r, in 2019 I started to invest myself in the project. Five years laters this awesome project is still maintained and up to date ! Lot of new additions have been made to create a tool still relevant to the new Active Directory attacks paths and countermeasures setup by Microsoft ! ⚔️You are on the latest up-to-date repository of the project CrackMapExec ! 🎉Official Discord ChannelIf you don't have a Github account, you can ask your question on DiscordAcknowledgments(These are the people who did the hard stuff)This project was originally inspired by:Unintentional contributors:Documentation, Tutorials, ExamplesSee the project's  for documentation and usage examplesInstallationPlease see the installation instructions on the Code ContributorsAwesome code contributors of CME:To do"
https://github.com/python-pillow/Pillow,Python Imaging Library (Fork),"PillowPython Imaging Library (Fork)Pillow is the friendly PIL fork by .PIL is the Python Imaging Library by Fredrik Lundh and Contributors.As of 2019, Pillow development is.OverviewThe Python Imaging Library adds image processing capabilities to your Python interpreter.This library provides extensive file format support, an efficient internal representation, and fairly powerful image processing capabilities.The core image library is designed for fast access to data stored in a few basic pixel formats. It should provide a solid foundation for a general image processing tool.More InformationReport a VulnerabilityTo report a security vulnerability, please follow the procedure described in the ."
https://github.com/Suor/funcy,A fancy and practical functional tools,"Funcy |Build Status|A collection of fancy functional tools focused on practicality.Inspired by clojure, underscore and my own abstractions. Keep reading to get an overviewor ..Works with Python 3.4+ and pypy3.Installation::pip install funcy
OverviewImport stuff from funcy to make things happen:.. code:: pythonfrom funcy import whatever, you, need
Merge collections of same type(works for dicts, sets, lists, tuples, iterators and even strings):.. code:: pythonmerge(coll1, coll2, coll3, ...)
join(colls)
merge_with(sum, dict1, dict2, ...)
Walk through collection, creating its transform (like map but preserves type):.. code:: pythonwalk(str.upper, {'a', 'b'})            # {'A', 'B'}
walk(reversed, {'a': 1, 'b': 2})       # {1: 'a', 2: 'b'}
walk_keys(double, {'a': 1, 'b': 2})    # {'aa': 1, 'bb': 2}
walk_values(inc, {'a': 1, 'b': 2})     # {'a': 2, 'b': 3}
Select a part of collection:.. code:: pythonselect(even, {1,2,3,10,20})                  # {2,10,20}
select(r'^a', ('a','b','ab','ba'))           # ('a','ab')
select_keys(callable, {str: '', None: None}) # {str: ''}
compact({2, None, 1, 0})                     # {1,2}
Manipulate sequences:.. code:: pythontake(4, iterate(double, 1)) # [1, 2, 4, 8]
first(drop(3, count(10)))   # 13

lremove(even, [1, 2, 3])    # [1, 3]
lconcat([1, 2], [5, 6])     # [1, 2, 5, 6]
lcat(map(range, range(4)))  # [0, 0, 1, 0, 1, 2]
lmapcat(range, range(4))    # same
flatten(nested_structure)   # flat iter
distinct('abacbdd')         # iter('abcd')

lsplit(odd, range(5))       # ([1, 3], [0, 2, 4])
lsplit_at(2, range(5))      # ([0, 1], [2, 3, 4])
group_by(mod3, range(5))    # {0: [0, 3], 1: [1, 4], 2: [2]}

lpartition(2, range(5))     # [[0, 1], [2, 3]]
chunks(2, range(5))         # iter: [0, 1], [2, 3], [4]
pairwise(range(5))          # iter: [0, 1], [1, 2], ...
And functions:.. code:: pythonpartial(add, 1)             # inc
curry(add)(1)(2)            # 3
compose(inc, double)(10)    # 21
complement(even)            # odd
all_fn(isa(int), even)      # is_even_int

one_third = rpartial(operator.div, 3.0)
has_suffix = rcurry(str.endswith, 2)
Create decorators easily:.. code:: python@decorator
def log(call):
    print call._func.__name__, call._args
    return call()
Abstract control flow:.. code:: pythonwalk_values(silent(int), {'a': '1', 'b': 'no'})
# => {'a': 1, 'b': None}

@once
def initialize():
    ""...""

with suppress(OSError):
    os.remove('some.file')

@ignore(ErrorRateExceeded)
@limit_error_rate(fails=5, timeout=60)
@retry(tries=2, errors=(HttpError, ServiceDown))
def some_unreliable_action(...):
    ""...""

class MyUser(AbstractBaseUser):
    @cached_property
    def public_phones(self):
        return self.phones.filter(public=True)
Ease debugging:.. code:: pythonsquares = {tap(x, 'x'): tap(x * x, 'x^2') for x in [3, 4]}
# x: 3
# x^2: 9
# ...

@print_exits
def some_func(...):
    ""...""

@log_calls(log.info, errors=False)
@log_errors(log.exception)
def some_suspicious_function(...):
    ""...""

with print_durations('Creating models'):
    Model.objects.create(...)
    # ...
# 10.2 ms in Creating models
And _.Dive inFuncy is an embodiment of ideas I explain in several essays:Running testsTo run the tests using your default python:::pip install -r test_requirements.txt
py.test
To fully run  you need all the supported pythons to be installed. These are3.4+ and PyPy3. You can run it for particular environment even in absenseof all of the above::tox -e py310
tox -e pypy3
tox -e lint
.. |Build Status| image:: https://github.com/Suor/funcy/actions/workflows/test.yml/badge.svg:target: https://github.com/Suor/funcy/actions/workflows/test.yml?query=branch%3Amaster"
https://github.com/python/cpython,The Python programming language,"This is Python version 3.13.0 alpha 1.. image:: https://github.com/python/cpython/workflows/Tests/badge.svg:alt: CPython build status on GitHub Actions:target: https://github.com/python/cpython/actions.. image:: https://dev.azure.com/python/cpython/_apis/build/status/Azure%20Pipelines%20CI?branchName=main:alt: CPython build status on Azure DevOps:target: https://dev.azure.com/python/cpython/_build/latest?definitionId=4&branchName=main.. image:: https://img.shields.io/badge/discourse-join_chat-brightgreen.svg:alt: Python Discourse chat:target: https://discuss.python.org/Copyright © 2001-2023 Python Software Foundation.  All rights reserved.See the end of this file for further copyright and license information... contents::General InformationContributing to CPythonFor more complete instructions on contributing to CPython development,see the _... _Developer Guide: https://devguide.python.org/Using PythonInstallable Python kits, and information about using Python, are available at_... _python.org: https://www.python.org/Build InstructionsOn Unix, Linux, BSD, macOS, and Cygwin::./configure
make
make test
sudo make install
This will install Python as .You can pass many options to the configure script; run to find out more.  On macOS case-insensitive file systems and on Cygwin,the executable is called ; elsewhere it's just .Building a complete Python installation requires the use of variousadditional third-party libraries, depending on your build platform andconfigure options.  Not all standard library modules are buildable oruseable on all platforms.  Refer to the_section of the _ for current detailed information ondependencies for various Linux distributions and macOS.On macOS, there are additional configure and build options relatedto macOS framework and universal builds.  Refer to _.On Windows, see _.To build Windows installer, see _.If you wish, you can create a subdirectory and invoke configure from there.For example::mkdir debug
cd debug
../configure --with-pydebug
make
make test
(This will fail if you also built at the top-level directory.  You should doa  at the top-level first.)To get an optimized build of Python, before you run .  This sets the default make targets up to enableProfile Guided Optimization (PGO) and may be used to auto-enable Link TimeOptimization (LTO) on some platforms.  For more details, see the sectionsbelow.Profile Guided Optimization^^^^^^^^^^^^^^^^^^^^^^^^^^^PGO takes advantage of recent versions of the GCC or Clang compilers.  If used,either via  or by manually running regardless of configure flags, the optimized buildprocess will perform the following steps:The entire Python directory is cleaned of temporary files that may haveresulted from a previous compilation.An instrumented version of the interpreter is built, using suitable compilerflags for each flavor. Note that this is just an intermediary step.  Thebinary resulting from this step is not good for real-life workloads as it hasprofiling instructions embedded inside.After the instrumented interpreter is built, the Makefile will run a trainingworkload.  This is necessary in order to profile the interpreter's execution.Note also that any output, both stdout and stderr, that may appear at this stepis suppressed.The final step is to build the actual interpreter, using the informationcollected from the instrumented one.  The end result will be a Python binarythat is optimized; suitable for distribution or production installation.Link Time Optimization^^^^^^^^^^^^^^^^^^^^^^Enabled via configure's  flag.  LTO takes advantage of theability of recent compiler toolchains to optimize across the otherwisearbitrary  file boundary when building final executables or sharedlibraries for additional performance gains.What's NewWe have a comprehensive overview of the changes in the _ document.  For a moredetailed change log, read , but a full.If you want to install multiple versions of Python, see the section belowentitled ""Installing multiple versions"".Documentation_ is online,updated daily.It can also be downloaded in many formats for faster access.  The documentationis downloadable in HTML, PDF, and reStructuredText formats; the latter versionis primarily for documentation authors, translators, and people with specialformatting requirements.For information about building Python's documentation, refer to _.Converting From Python 2.x to 3.xSignificant backward incompatible changes were made for the release of Python3.0, which may cause programs written for Python 2 to fail when run with Python3.  For more information about porting your code from Python 2 to Python 3, seethe _.TestingTo test the interpreter, type  in the top-level directory.  Thetest set produces some output.  You can generally ignore the messages aboutskipped tests due to optional features which can't be imported.  If a messageis printed about a failed test or a traceback or core dump is produced,something is wrong.By default, tests are prevented from overusing resources like disk space andmemory.  To enable these tests, run .If any tests fail, you can re-run the failing test(s) in verbose mode.  Forexample, if  and  failed, you can run::make test TESTOPTS=""-v test_os test_gdb""
If the failure persists and appears to be a problem with Python rather thanyour environment, you can _ and include relevant output fromthat command to show the issue.See _for more on running tests.Installing multiple versionsOn Unix and Mac systems if you intend to install multiple versions of Pythonusing the same installation prefix ( argument to the configurescript) you must take care that your primary python executable is notoverwritten by the installation of a different version.  All files anddirectories installed using  contain the major and minorversion and can thus live side-by-side.   also creates which refers to .  If youintend to install multiple versions using the same prefix you must decide whichversion (if any) is your ""primary"" version.  Install that version using .  Install all other versions using .For example, if you want to install Python 2.7, 3.6, and 3.13 with 3.13 being theprimary version, you would execute  in your 3.13 build directoryand  in the others.Release ScheduleSee :pep: for Python 3.13 release details.Copyright and License InformationCopyright © 2001-2023 Python Software Foundation.  All rights reserved.Copyright © 2000 BeOpen.com.  All rights reserved.Copyright © 1995-2001 Corporation for National Research Initiatives.  Allrights reserved.Copyright © 1991-1995 Stichting Mathematisch Centrum.  All rights reserved.See the _ forinformation on the history of this software, terms & conditions for usage, and aDISCLAIMER OF ALL WARRANTIES.This Python distribution contains no GNU General Public License (GPL) code,so it may be used in proprietary projects.  There are interfaces to some GNUcode but these are entirely optional.All trademarks referenced herein are property of their respective holders."
https://github.com/lauris/awesome-scala,"A community driven list of useful Scala libraries, frameworks and software.","Awesome Scala A community driven list of useful Scala libraries, frameworks and software. This is not a catalog of all the libraries, just a starting point for your explorations. Inspired by . Other amazingly awesome lists can be found in the  list.Also awesome is , the searchable, tagged, and centralized index of Scala libraries.Projects with over 500 stargazers are in bold.ContributingYour contributions are always welcome! Please submit a pull request or create an issue to add a new framework, library or software to the list. Do not submit a project that hasn’t been updated in the past 6 months or is not awesome.Don't modify  in your pull request. It is automatically generated. Modify  instead.Table of ContentsArchive and CompressionName | Description | GitHub Activity---- | ----------- | --------------- | SevenZip library for Scala, easy to use.  |  Artificial IntelligenceName | Description | GitHub Activity---- | ----------- | --------------- | Typesafe, purely functional Computational Intelligence |  DatabaseDatabase access libraries in Scala.Name | Description | GitHub Activity---- | ----------- | --------------- | akka-persistence-gcp-datastore is a journal and snapshot store plugin for akka-persistence using google cloud firestore in datastore mode.  |   | The Anorm database library |   | Casbah is now officially end-of-life (EOL). |   | Clickhouse Scala Client with Reactive Streams support |   | The Couchbase Monorepo for JVM Clients: Java, Scala, io-core… |   | A purely functional Scala client for CouchDB |   | Functional JDBC layer for Scala. |   | Elasticsearch Scala Client - Reactive, Non Blocking, Type Safe, HTTP Client |   | Scala etcd client implementing V3 APIs |   | PostgreSQL protocol support for Finagle |   | A Future-free Fs2 native pure FP Redis client |   | None |   | A Persistence Framework for Scala and NoSQL |   | Light-weight convenience wrapper around Lucene to simplify complex tasks and add Scala sugar. |   | A Scala ORM library |   | Reactive type-safe Scala driver for SQL databases |   | Scala lightweight, type-safe, asynchronous driver for neo4j  |   | Schema safe, type-safe, reactive Scala driver for Cassandra/Datastax Enterprise |   | Idiomatic, typesafe, and reactive Scala client for Apache Pulsar |   | Compile-time Language Integrated Queries for Scala |   | New ReactiveCouchbase driver using reactive-streams |   | :leaves: Non-blocking, Reactive MongoDB Driver for Scala |   | Non-blocking, Reactive Redis driver for Scala (with Sentinel support) |   | Performant database access in Scala |   | Salat is a simple serialization library for case classes. |   | Scala GraphQL implementation |   | ActiveRecord-like ORM library for Scala |   | Type-safe data migration tool for Slick, Git and beyond. |   | A scala library for connecting to a redis server, or a cluster of redis nodes using consistent hashing on the client side. |   | scala SQL api |   | Type-Safe framework for defining, modifying, and querying SQL databases |   | A tidy SQL-based DB access library for Scala developers. This library naturally wraps JDBC APIs and provides you easy-to-use APIs. |   | Simpler DynamoDB access for Scala |   | Non-blocking, ultra-fast Scala Redis client built on top of Akka IO, used in production at Livestream |   | Scala + Druid: Scruid. A library that allows you to compose queries in Scala, and parse the result back into typesafe classes. |   | Memcached client for Scala |   | Slick (Scala Language Integrated Connection Kit) is a modern database query and access library for Scala |   | Slick extensions for PostgreSQL |   | A Scala DSL for talking with databases with minimum verbosity and maximum type safety |   | Non-blocking asynchronous domain-customizable database query language for Scala and Scala.js against the Datomic database. |   | A ZIO-based redis client |   | A data access library for Scala + Postgres. |   | Highly available distributed strong eventual consistent and sequentially consistent storage with feeds and search |  MessagingName | Description | GitHub Activity---- | ----------- | --------------- | The Opinionated RabbitMQ Library for Scala and Akka |  Graphical User InterfacesLibraries for creation of graphical user interfacesName | Description | GitHub Activity---- | ----------- | --------------- | ScalaFX simplifies creation of JavaFX-based user interfaces in Scala |  Web FrameworksScala frameworks for web development.Name | Description | GitHub Activity---- | ----------- | --------------- | Tiny High Performance HTTP Server for Scala  |   | A lightweight framework for writing REST services in Scala. |   | Cask: a Scala HTTP micro-framework |   | I/O and Microservice library for Scala |   | Fast, testable, Scala services built on TwitterServer and Finagle |   | Lift Framework |   | Async lightweight Scala web framework |   | Play Framework |   | A module for the Play Framework to build highly modular applications |   | A simple FRP library and a web UI framework built on it |   | Facebook's React on Scala.JS |   | Tiny Scala high-performance, async web framework, inspired by Sinatra |   | :monorail: ""Scala on Rails"" - A full-stack web app framework for rapid development in Scala |   | A toolkit for servicing HTTP requests in Scala |   | Async and clustered Scala web framework and HTTP(S) server |   | Next generation user interface and application development in Scala and Scala.js for web, mobile, and desktop. |  Reactive Web FrameworksScala libraries for Reactive Web developmentName | Description | GitHub Activity---- | ----------- | --------------- | Reactive data-binding for Scala |   | Single Page Applications running on the server side. |   | Scala framework for building beautiful and maintainable web applications. |   | Vert.x for Scala |  Data Binding and ValidationScala libraries for data binding and validationName | Description | GitHub Activity---- | ----------- | --------------- | Accord: A sane validation library for Scala |   | Minimal, idiomatic, customizable validation Scala library. |   | Scala library for boilerplate-free validation |   | If you don't agree with the data |   | Scala validation library |  i18nScala libraries for i18n.Name | Description | GitHub Activity---- | ----------- | --------------- | Scala compiler plugin that acts like GNU xgettext command to extract i18n strings in Scala source code files to Gettext .po file |   | GNU Gettext .po file loader for Scala |  AuthenticationLibraries for implementing authentications schemes.Name | Description | GitHub Activity---- | ----------- | --------------- | Web & mobile client-side akka-http sessions, with optional JWT support |   | Scala library to sign HTTP requests to AWS services. |   | An implementation of an OAuth2 server designed for mocking/testing |   | Simple play module for authenticating against Google |   | Security library for Play framework 2 in Java and Scala: OAuth, CAS, SAML, OpenID Connect, LDAP, JWT... |   | Play2.x Authentication and Authorization module |   | OAuth 2.0 server-side implementation written in Scala |   | A module that provides OAuth, OAuth2 and OpenID authentication for Play Framework applications |  CryptographyCryptography and Encryption Libraries.Name | Description | GitHub Activity---- | ----------- | --------------- | Cryptographic primitives for Scala |   | Type-safe general-cryptography library - https://jmcardon.github.io/tsec/ |   | Extensible JOSE library for Scala |  TestingLibraries for code testing.Name | Description | GitHub Activity---- | ----------- | --------------- | Scala DSL for testing HTTP JSON API |   | Modern Load Testing as Code |   | The super light testing library for Scala and Scala.js |   | Mockito for Scala language |   | Scala testing library with actionable errors and extensible APIs |   | Property-based testing for Scala |   | Microbenchmarking and performance regression testing framework for the JVM platform. |   | Native Scala mocking framework |   | property based testing library for Scala |   | A testing tool for Scala and Java developers |   | Connect a Scala REPL to running JVM processes without any prior setup |   | Software Specifications for Scala |   | Mutation testing for Scala |   | A test framework that runs everything in parallel.  |   | Docker containers for testing in scala |   | A simple testing framework for Scala |  JSONLibraries for work with json.Name | Description | GitHub Activity---- | ----------- | --------------- | Purely functional JSON parser and library in scala. |   | Efficient CBOR and JSON (de)serialization in Scala |   | Yet another JSON library for Scala |   | A scala diff/patch library for Json |   | Add-on module for Jackson (https://github.com/FasterXML/jackson) to support Scala-specific datatypes |   | Jawn is for parsing jay-sawn (JSON) |   | JSON library |   | Scala macros for compile-time generation of safe and ultra-fast JSON codecs |   | Persist-Json, a Fast Json Parser Written in Scala |   | JSON typeclasses that know the difference between null and absent fields |   | The Play JSON library |   | ABANDONED Pure Scala serialization library with annotations |   | sbt plugin that generates Scala case classes for easy, statically typed and implicit access of JSON data e.g. from API responses |   | Scala support library for integrating the JSON API spec with Spray, Play! or Circe |   | Fast JSON parser/generator for Scala |   | A lightweight, clean and simple JSON implementation in Scala |   | Fast, secure JSON library with tight ZIO integration. |  YAMLLibraries for work with YAML.Name | Description | GitHub Activity---- | ----------- | --------------- | Scala wrapper for SnakeYAML |  CSVLibraries for work with CSV.Name | Description | GitHub Activity---- | ----------- | --------------- | Scala Library for Reading Flat File Data (CSV/TSV/XLS/XLSX) |   | CSV handling library for Scala |   | CSV Reader/Writer for Scala |   | Functional, stream-based CSV processor for Scala |  SerializationLibraries for serializing and deserializing data for storage or transport.Name | Description | GitHub Activity---- | ----------- | --------------- | Scala code generator for Avro schemas. |   | Efficient CBOR and JSON (de)serialization in Scala |   | Avro schema generation and serialization / deserialization for Scala |   | Scala extensions for the Kryo serialization library |   | MessagePack serializer implementation for Scala / msgpack.org[Scala] |   | Protocol buffer compiler for Scala. |   | Scala combinator library for working with binary data |   | A Thrift parser/generator |   | uPickle: a simple, fast, dependency-free JSON & Binary (MessagePack) serialization library for Scala |   | Lightweight and fast serialization library for Scala 2/3 based on Protocol Buffers with macros |  Science and Data AnalysisLibraries for scientific computing, data analysis and numerical processing.Name | Description | GitHub Activity---- | ----------- | --------------- | Abstract Algebra for Scala |   | Axle Domain Specific Language for Scientific Cloud Computing and Visualization |   | Building Large-Scale AI Applications for Distributed Big Data |   | Breeze is a numerical processing library for Scala. |   | C4E, a JVM friendly library written in Scala for both local  and distributed (Spark) Clustering. |   | :cake: doddle-model: machine learning in Scala. |   | Figaro Programming Language and Core Libraries |   | A dimensional analysis library based on dependent types |   | LoMRF is an open-source implementation of Markov Logic Networks |   | Purely functional genetic algorithms for multi-objective optimisation |   | Machine Learning framework for Spark | | N-dimensional arrays in Scala 3. Think NumPy ndarray, but type-safe over shapes, array/axis labels & numeric data types |   | numsca is numpy for scala |   | An ONNX (Open Neural Network eXchange) API and backend for typeful, functional deep learning in Scala 3 |   | Workflow engine for exploration of simulation models using high throughput computing |   | Optimus is a mathematical programming library for Scala. |   | a Scala toolkit for solving Operations Research problems | | Rings: efficient JVM library for polynomial rings |   | Statistical Machine Intelligence & Learning Engine |   | Interactive and Reactive Data Science using Scala and Spark. |   | Powerful new number types and numeric abstractions for Scala. |   | The Scala API for Quantities, Units of Measure and Dimensional Analysis |   | A group of neural-network libraries for functional and mainstream languages |   | TensorFlow API for the Scala Programming Language |   | Web-based notebook that enables data-driven, interactive data analytics and collaborative documents with SQL, Scala and more. |   | State of the Art Natural Language Processing |  Big DataName | Description | GitHub Activity---- | ----------- | --------------- | CPU and GPU-accelerated Machine Learning Library |   | Apache Flink |   | Scala library for accessing various file, batch systems, job schedulers and grid middlewares. |   | Mirror of Apache Kafka |   | Alpakka Kafka connector - Alpakka is a Reactive Enterprise Integration library for Java and Scala, based on Reactive Streams and Akka. |   | A Scala API for Cascading |   | Schema registry for CSV, TSV, JSON, AVRO and Parquet schema. Supports schema inference and GraphQL API. |   | A Scala API for Apache Beam and Google Cloud Dataflow. |   | A Scala wrapper for  which provides a framework for writing, testing, and running MapReduce pipelines. | | Apache Spark - A unified analytics engine for large-scale data processing |   | Spark package to ""plug"" holes in data using SQL based rules ⚡️ 🔌  |   | Real Time Analytics and Data Pipelines based on Spark Streaming |   | Streaming MapReduce with Scalding and Storm |   | The missing MatPlotLib for Scala + Spark |   | A schema-aware Scala library for data transformation |  Command Line InterfacesLibraries for creation of command line interfacesName | Description | GitHub Activity---- | ----------- | --------------- | Command Line Interface Scala Toolkit |   | A composable command-line parser for Scala. |   | A small, convenient, dependency-free library for command-line argument parsing in Scala |   | a simple Scala CLI parsing library |   | command line options parsing for Scala |  Image processing and image analysis2D and 3D image processing and image analysisName | Description | GitHub Activity---- | ----------- | --------------- | Image comparison by hash codes |   | Scalable Image Analysis and Shape Modelling |   | Java, Scala and Kotlin image processing library |  Sound processing and musicName | Description | GitHub Activity---- | ----------- | --------------- | Chromaprint/AcoustID audio fingerprinting for the JVM |   | A Scala sound synthesis library based on SuperCollider. |  Functional Reactive ProgrammingEvent streams, signals, observables, etc.Name | Description | GitHub Activity---- | ----------- | --------------- | Compositional, streaming I/O library for Scala |   | Iteratees for Cats |   | Asynchronous, Reactive Programming for Scala and Scala.js. |   | A concurrent reactive programming framework. |   | A scala extension for Project Reactor's Flux and Mono |   | REScala - reactive programming in OO applications |   | RxScala – Reactive Extensions for Scala – a library for composing asynchronous and event-based programs using observable sequences |   | An experimental library for Functional Reactive Programming in Scala |   | ZIO — A type-safe, composable library for async and concurrent programming in Scala |   | SynapseGrid is a framework for constructing dynamic low latency data flow systems. |   | Vert.x for Scala |   | A small and effective event-handling library for Scala |  Modularization and Dependency InjectionModularization of applications, dependency injection, etc.Name | Description | GitHub Activity---- | ----------- | --------------- | Essential Building Blocks for Scala |   | Productivity-oriented collection of lightweight fancy stuff for Scala toolchain |   | Lightweight and Nonintrusive Scala Dependency Injection Library |   | Scala extensions for Google Guice |   | Lightweight Scala Dependency Injection Library |   | Scala classpath scanner |   | Scala Uniquely Bound Classes Under Traits |   | Incredibly simple DI Scala library. |  Distributed SystemsLibraries and frameworks for writing distributed applications.Name | Description | GitHub Activity---- | ----------- | --------------- | Build highly concurrent, distributed, and resilient message-driven applications on the JVM |   | A distributed tracing extension for Akka. Provides integration with Play framework, Spray and Akka HTTP. |   | Platform to build distributed, scalable, enterprise-wide business applications |   | Distributed NoSQL Database |   | A fault tolerant, protocol-agnostic RPC system |   | Library to register and lookup actors by names in an Akka cluster |   | Reactive Microservices for the JVM |   | A purely functional library to build distributed and event-driven systems |   | Minimal, type-safe RPC Scala library. |   | A foundational framework for distributed programming. |  ExtensionsScala extensions.Name | Description | GitHub Activity---- | ----------- | --------------- | Scala Scripting |   | Simple, safe and intuitive Scala I/O |   | Blindsight is a Scala logging API with DSL based structured logging, fluent logging, semantic logging, flow logging, and context aware logging. |   | Cassovary is a simple big graph processing library for the JVM |   | Lightweight, modular, and extensible library for functional programming. |   | Scala library for boilerplate-free, type-safe data transformations |   | A JSR-310 port of nscala_time |   | A framework to create embedded Domain-Specific Languages in Scala |   | A macro library that converts native imperative syntax to scalaz's monadic expressions |   | Eff monad for cats - https://atnos-org.github.io/eff |   | A library that toggles Scala code at compile-time, like #if in C/C++ |   | A type-safe, reflection-free, powerful enumeration implementation for Scala with exhaustive pattern match warnings and helpful integrations. |   | Easy way to create Free Monad using Scala macros with first-class Intellij support. |   | Practical effect composition library based on abstract wrapping type and the free monad |   | A cohesive & pragmatic framework of FP centric Scala libraries |   | A mini Scala utility library |   | Lamma schedule generator for Scala is a professional schedule generation library for periodic schedules like fixed income coupon payment, equity deravitive fixing date generation etc.  |   | Large off-heap arrays and mmap files for Scala and Java |   | High-performance SLF4J wrapper for Scala. |   | Productivity-oriented collection of lightweight fancy stuff for Scala toolchain |   | Optics library for Scala |   | A new Scala wrapper for Joda Time based on scala-time |   | Modify deeply nested case class fields |   | Labeled records for Scala based on structural refinement types and macros. |   | Refinement types for Scala |   | An asynchronous programming facility for Scala |   | Graph for Scala is intended to provide basic graph functionality seamlessly fitting into the Scala Collection Library. Like the well known members of scala.collection, Graph for Scala is an in-memory graph library aiming at editing and traversing graphs, finding cycles etc. in a user-friendly way. |   | Convenient and performant logging library for Scala wrapping SLF4J. |   | Library to read, analyze, transform and generate Scala programs |   | Small library of utilities related to quality that helps keeping code clear and correct. | | Principled Functional Programming in Scala |   | The fastest logging library in the world. Built from scratch in Scala and programmatically configurable. |   | Generic programming for Scala |   | First class syntax support for type classes in Scala |   | Squid – type-safe metaprogramming and compilation framework for Scala |   | tinylog is a lightweight logging framework for Java, Kotlin, Scala, and Android |   | Wonderful reusable code from Twitter |  MiscProjects that don't fit into any specific category.Name | Description | GitHub Activity---- | ----------- | --------------- | Library of vote-counting algorithms for elections. | | Scala Scripting |   | Non-blocking AWS SDK for Scala exposing strongly-typed APIs built on top of http4s, fs2 and cats |   | Simple project to quickly start developing a Scala-based microservice or web application, without the need to write login, user registration etc. |   | A scala implementation of the Lightning Network. |   | Scala/Scala.js library for manipulating Fancy Ansi colored strings |   | ASCII-art banners in Scala |   | fs2 utilities to interact with AWS |   | A lean, functional library for Google Cloud Services in Scala |   | This API is a wrapper for the google java libraries. Currently mapping Admin Directory, Drive, and Calendar. |   | Mailgun API implementation in Scala |   | A zero-dependency Scala library for managing resources monadically |   | Scala command-line wrapper around ffmpeg, ffprobe, ImageMagick, and other tools relating to media. |   | Miniboxing is a program transformation that improves the performance of Scala generics when used with primitive types. It can speed up generic collections by factors between 1.5x and 22x, while maintaining bytecode duplication to a minimum. You can easily add miniboxing to your sbt project: |   | A chess library that runs on the server (Scala) and on the browser (ScalaJS). |   | Swagger spec generator for play framework |   | Pretty-printing value, types and type-signatures in Scala |   | A boilerplate-free library for loading configuration files |   | A neat little tool to build presentations using the Scala REPL |   | Remote shell access via SSH for your Scala applications |   | A library-based Software Transactional Memory (STM) for Scala, coupled with transactional sets and maps |   | An experimental automated theorem prover. | | A scala chassis to get your applications and services bootstrapped quickly |   | Efficient diffing in Scala |   | Scala library that provides an enumeration of ISO 3166 codes for countries, along with their subdivisions. |  AndroidScala libraries and wrappers for Android development.Name | Description | GitHub Activity---- | ----------- | --------------- | An easy-to-use sbt plugin for working with all Android projects |   | Scaloid makes your Android code easy to understand and maintain. |  HTTPScala libraries and wrappers for HTTP clients.Name | Description | GitHub Activity---- | ----------- | --------------- | The Streaming-first HTTP server/module of Akka |   | Scala wrapper for the Java AsyncHttpClient. |   | Scala combinator library for building Finagle HTTP services |   | Implement fast, type-safe HTTP webservices for Finagle |   | A minimal, idiomatic Scala interface for HTTP |   | Manages installation, updating, downloading, launching, error reporting, and more for your application. |   | An HTTP Server and Client library for Scala. |   | A Scala port of the popular Python Requests HTTP client: flexible, intuitive, and straightforward to use. |   | Unified Scala.js + Scala HTTP client API |   | Simple scala wrapper for HttpURLConnection.  OAuth included. |   | scalaxb is an XML data binding tool for Scala. |   | The Scala HTTP client you always wanted! |   | Declarative, type-safe web endpoints library |   | Describe HTTP endpoints in Scala and derive clients, servers, and documentation |   | Fast, efficient, pure-functional, effect-free websocket, http and udp server, http client and telegram bot |  Semantic WebScala libraries for interactions with the Web of Data, and other RDF tools.Name | Description | GitHub Activity---- | ----------- | --------------- | Banana RDF |   | A Scala DSL for programming with the OWL API. |  Metrics and MonitoringScala libraries for gathering metrics and monitoring applications.Name | Description | GitHub Activity---- | ----------- | --------------- | The scala API for Dropwizard's Metrics. |  ParsingScala libraries for creating parsers.Name | Description | GitHub Activity---- | ----------- | --------------- | friendly little parsers |   | Writing Fast Parsers Fast in Scala |   | A macro-based PEG parser generator for Scala 2.10+ |   | simple combinator-based parsing for Scala. formerly part of the Scala standard library, now a separate community-maintained module |   | A parsing library for the cats ecosystem |   | LL(1) parser combinators in Scala |  Sbt pluginsSbt plugins to make your life easier.Name | Description | GitHub Activity---- | ----------- | --------------- | Desugaring scala  without implicit s |   | Pure Scala Artifact Fetching |   | Typechecked markdown documentation for Scala |   | An Sbt plugin that fills apiMappings for common Scala libraries. |   | A port of apidocjs https://apidocjs.com to sbt, to document REST Api |   | Deploy über-JARs. Restart processes. (port of codahale/assembly-sbt) |   | I know this because build.sbt knows this. |   | sbt plugin to automate Sonatype releases from GitHub Actions |   | SBT Plugin for OWASP DependencyCheck. Monitor your dependencies and report if there are any publicly known vulnerabilities (e.g. CVEs). :rainbow: |   | Create Docker images directly from sbt |   | Doctest for scala |   | git, site and ghpages support for sbt projects. |   | sbt plugin to roll the Git history |   | sbt-header is an sbt plugin for creating file headers, e.g. copyright headers |   | Sbt plugin for rendering Scala objects to files. And more! |   | SBT plugin for tweaking various IDE settings |   | ""Trust no one, bench everything."" - sbt plugin for JMH (Java Microbenchmark Harness) |   | An sbt plugin to create awesome microsites for your project |   | A tool for catching binary incompatibility in Scala |   | sbt Native Packager |   | A sbt plugin for creating distributable Scala packages. |   | PGP plugin for sbt |   | A release plugin for sbt |   | An SBT plugin for dangerously fast development turnaround in Scala |   | A Sbt plugin that configures source mapping for Scala.js projects hosted on Github |   | sbt plugin for Scalafmt |   | sbt plugin for scoverage |   | Site generation for sbt |   | A sbt plugin for publishing Scala/Java projects to the Maven central. |   | sbt plugin to create a unified Scaladoc or Javadoc API document across multiple subprojects. |   | sbt plugin that can check Maven and Ivy repositories for dependency updates |   | Plugin for sbt to create Eclipse project definitions |   | Good advice for Scala compiler errors |   | better implicit errors for scala |   | doc/tutorial generator for scala |   | Servlet support for sbt |   | SBT deploy plugin |   | A git plugin for SBT |   | Use git-describe as a version and run git commands inside SBT shell |  XML / HTMLXML and HTML generation and processingName | Description | GitHub Activity---- | ----------- | --------------- | A Scala library for scraping content from HTML pages |   | XML Streaming for Scala including FS2/cats support |  MarkdownName | Description | GitHub Activity---- | ----------- | --------------- | Text Markup Transformer for sbt and Scala applications, transforming Markdown and reStructuredText to HTML, EPUB and PDF |  JavaScriptJavaScript generation and interop libraries.Name | Description | GitHub Activity---- | ----------- | --------------- | Web user interface for ScalaFiddle |   | Scala.js, the Scala to JavaScript compiler |  SchedulingName | Description | GitHub Activity---- | ----------- | --------------- | Quartz Extension and utilities for cron-style scheduling in Akka |  TemplatingWeb templating engines.Name | Description | GitHub Activity---- | ----------- | --------------- | A lightweight, logicless templating engine, written in Scala and inspired by Mustache |   | ScalaTags is a small XML/HTML construction library for Scala.  |   | Scalate is a Scala based template engine which supports HAML, Mustache and JSP, Erb and Velocity style syntaxes. |   | Twirl is Play's default template engine |   | Typesafe HTML templates in pure Scala. Static site generator included! Play and ScalaJS support! |  ToolsName | Description | GitHub Activity---- | ----------- | --------------- | Macro based print debugging. Locates log statements in your IDE. |   | Bloop is a build server and CLI tool to compile, test and run Scala fast from any editor or build tool. |   | Codacy tool for Scalameta |   | Pure Scala Artifact Fetching |   | Dregex is a JVM library that implements a regular expression engine using deterministic finite automata (DFA). It supports some Perl-style features and yet retains linear matching time, and also offers set operations. |   | Scala macro that generates ultra-fast string interpolators. |   | Extremely fast string formatting |   | A Git platform powered by Scala with easy installation, high extensibility & GitHub API compatibility |   | a command line tool to apply templates defined on GitHub  |   | Scala language server with rich IDE features 🚀  |   | Your shiny new Java/Scala build tool! |   | Macro based print debugging. Locates debug statements in your IDE. Supports logging. |   | sbt, the interactive build tool |   | Refactoring and linting tool for Scala |   | Code formatter for Scala | | Scala source code formatter |   | scalastyle |   | Programmable, Typesafe Document Generation |   | Scala compiler plugin for static code analysis |   | Flexible Scala code linting tool |   | URL (de)construct. Withers. |   | ScalaJS frontend router |   | Scala2PlantUML generates PlantUML diagrams from Scala code. |  GeospatialLibraries to aid with geospatial calculations and artifacts.Name | Description | GitHub Activity---- | ----------- | --------------- | GeoTrellis is a geographic data processing engine for high performance applications. |   | Scala and Spark library focused on reading OpenStreetMap Pbf files. |   | RTree2D is a 2D immutable R-tree for ultra-fast nearest and intersection queries in plane and spherical coordinates |   | LocationTech SFCurve is a Scala library for the creation, transformation, and querying of space-filling curves  |   | a scala library with primitives to build applications using the SpatioTemporal Asset Catalogs specification |   | A STAC/OGC API Features Web Service |  DevopsDevOps related tools and libraries.Name | Description | GitHub Activity---- | ----------- | --------------- | A Scala Kubernetes client library |  Learning ScalaNice books, blogs and other resources to learn ScalaBooksExercisesTutorials and coursesCommercial coursesCommunity Members' BlogsCompany BlogsPodcastsCommunitiesMisc."
https://github.com/google/grr,GRR Rapid Response: remote live forensics for incident response,"GRR is a python client (agent) that is installed on target systems, andpython server infrastructure that can manage and talk to clients.DocumentationPlease visit our  if you want to know more about GRR.Contact UsScreenshots"
https://github.com/detailyang/awesome-cheatsheet,:beers: awesome cheatsheet,"Awesome Cheatsheet      Inspired by   and improved by these [<marko.inline.RawText object at 0x000001592FDC70C8>].If you see a link here is not fit, you can fix it or provide a better link by submitting a Table of ContentsPlatformsProgramming LanguagesSoftware TestingFront-End DevelopmentBack-End DevelopmentBig DataDatabasesTheoryEditorsToolsMediaSecurityProject ManagementMiscellaneous"
https://github.com/falconry/falcon,"The no-magic web data plane API and microservices framework for Python developers, with a focus on reliability, correctness, and performance at scale.",".. raw:: html<a href=""https://falconframework.org"" target=""_blank"">
<img
    src=""https://raw.githubusercontent.com/falconry/falcon/master/logo/banner.jpg""
    alt=""Falcon web framework logo""
    style=""width:100%""
>
</a>
|Build Status| |Docs| |codecov.io| |Blue|The Falcon Web Framework__ is a minimalist ASGI/WSGI framework forbuilding mission-critical REST APIs and microservices, with a focus onreliability, correctness, and performance at scale.When it comes to building HTTP APIs, other frameworks weigh you down with tonsof dependencies and unnecessary abstractions. Falcon cuts to the chase with aclean design that embraces HTTP and the REST architectural style.Falcon apps work with any _or _ server, and run like achamp under CPython 3.7+ and PyPy 3.7+.Quick LinksWhat People are Saying""Falcon is rock solid and it's fast.""""We have been using Falcon as a replacement for [another framework] andwe simply love the performance (three times faster) and code base size (easilyhalf of our [original] code).""""I'm loving #falconframework! Super clean and simple, I finallyhave the speed and flexibility I need!""""Falcon looks great so far. I hacked together a quick test for atiny server of mine and was ~40% faster with only 20 minutes ofwork.""""I feel like I'm just talking HTTP at last, with nothing in themiddle. Falcon seems like the requests of backend.""""The source code for Falcon is so good, I almost prefer it todocumentation. It basically can't be wrong.""""What other framework has integrated support for 786 TRY IT NOW ?""FeaturesFalcon tries to do as little as possible while remaining highly effective... Patron list starts here. For Python package, we substitute this section with:Support Falcon DevelopmentA Big Thank You to Our Patrons!.. raw:: html<p>
<a href=""https://www.govcert.lu/"" target=""_blank""><img src=""https://falconframework.org/assets/govcert.png"" height=""60"" alt=""CERT Gouvernemental Luxembourg"" ></a>
 </p>

<p>
    <a href=""https://www.kontrolnaya-rabota.ru/s/"" target=""_blank""><img src=""https://falconframework.org/assets/rabota.jpg"" height=""30"" alt=""Examination RU"" style=""margin-right: 10px""></a>

    <a href=""https://www.pnk.sh/python-falcon"" target=""_blank""><img src=""https://falconframework.org/assets/paris.svg"" height=""30"" alt=""Paris Kejser"" style=""margin-right: 10px""></a>

    <a href=""https://www.algolia.com"" target=""_blank"" style=""margin-right: 10px""><img src=""https://falconframework.org/assets/algolia.svg"" height=""30"" alt=""Algolia""></a>

    <a href=""https://www.salesforce.com"" target=""_blank""><img src=""https://falconframework.org/assets/salesforce.svg"" height=""30"" alt=""Salesforce""></a>
</p>

<p>
    <a href=""https://www.misaka.io"" target=""_blank"" style=""margin-right: 10px""><img src=""https://falconframework.org/assets/misaka.svg"" height=""30"" alt=""Misaka Network""></a>
    <a href=""https://github.com/LikaloLLC"" target=""_blank"" style=""margin-right: 10px""><img src=""https://falconframework.org/assets/likalo.png"" height=""30"" alt=""Likalo""></a>
</p>
.. Patron list ends here (see the comment above this section).Has Falcon helped you make an awesome app? Show your support today with a one-time donation or by becoming a patron. Supporters get cool gear, an opportunity to promote their brand to Python developers, andprioritized support.Thanks!How is Falcon Different?Perfection is finally attained not when there is no longer anything
to add, but when there is no longer anything to take away.

*- Antoine de Saint-Exupéry*
We designed Falcon to support the demanding needs of large-scalemicroservices and responsive app backends. Falcon complements moregeneral Python web frameworks by providing bare-metal performance,reliability, and flexibility wherever you need it.Reliable. We go to great lengths to avoid introducing breaking changes, andwhen we do they are fully documented and only introduced (in the spirit of_) with a major version increment. The code isrigorously tested with numerous inputs and we require 100% coverage at alltimes. Falcon has no dependencies outside the standard library, helpingminimize your app's attack surface while avoiding transitive bugs and breakingchanges.Debuggable. Falcon eschews magic. It's easy to tell which inputs lead towhich outputs. Unhandled exceptions are never encapsulated or masked.Potentially surprising behaviors, such as automatic request body parsing, arewell-documented and disabled by default. Finally, when it comes to theframework itself, we take care to keep logic paths simple and understandable.All this makes it easier to reason about the code and to debug edge cases inlarge-scale deployments.Fast. Same hardware, more requests. Falcon turns around requestssignificantly faster than other popular Python frameworks like Django andFlask. For an extra speed boost, Falcon compiles itself with Cython whenavailable, and also works well with _. Considering amove to another programming language? Benchmark with Falcon+PyPy first!Flexible. Falcon leaves a lot of decisions and implementation details toyou, the API developer. This gives you a lot of freedom to customize and tuneyour implementation. It also helps you understand your apps at a deeper level,making them easier to tune, debug, and refactor over the long run. Falcon'sminimalist design provides space for Python community members to independentlyinnovate on _.Who's Using Falcon?Falcon is used around the world by a growing number of organizations,including:If you are using the Falcon framework for a community or commercialproject, please consider adding your information to our wiki under_CommunityA number of Falcon add-ons, templates, and complementary packages areavailable for use in your projects. We've listed several of these on the_ as a startingpoint, but you may also wish to search PyPI for additional resources.The Falconry community on Gitter is a great place to ask questions andshare your ideas. You can find us in . We also have a room for discussingthe design and development of the framework itself.Per our_,we expect everyone who participates in community discussions to actprofessionally, and lead by example in encouraging constructivediscussions. Each individual in the community is responsible forcreating a positive, constructive, and productive culture.InstallationPyPy^^^^__ is the fastest way to run your Falcon app.PyPy3.7+ is supported as of PyPy v7.3.4+... code:: bash$ pip install falcon
Or, to install the latest beta or release candidate, if any:.. code:: bash$ pip install --pre falcon
CPython^^^^^^^Falcon also fully supports__ 3.7+.The latest stable version of Falcon can be installed directly from PyPI:.. code:: bash$ pip install falcon
Or, to install the latest beta or release candidate, if any:.. code:: bash$ pip install --pre falcon
In order to provide an extra speed boost, Falcon can compile itself withCython. Wheels containing pre-compiled binaries are available from PyPI forseveral common platforms. However, if a wheel for your platform of choice is notavailable, you can install the source distribution. The installation processwill automatically try to cythonize Falcon for your environment, falling back toa normal pure-Python install if any issues are encountered during thecythonization step:.. code:: bash$ pip install --no-binary :all: falcon
If you want to verify that Cython is being invoked, simplypass the verbose flag  to pip in order to echo the compilation commands.The cythonization step is only active when using the  Pythonimplementation, so installing using  will skip it.If you want to skip Cython compilation step and installthe pure-Python version directly you can set the environment variable to a non empty value before install:.. code:: bash$ FALCON_DISABLE_CYTHON=Y pip install -v --no-binary :all: falcon
Please note that  is required to be able to install Falcon fromsource.Installing on OS XXcode Command Line Tools are required to compile Cython. Install themwith this command:.. code:: bash$ xcode-select --install
The Clang compiler treats unrecognized command-line options aserrors, for example:.. code:: bashclang: error: unknown argument: '-mno-fused-madd' [-Wunused-command-line-argument-hard-error-in-future]
You might also see warnings about unused functions. You can work aroundthese issues by setting additional Clang C compiler flags as follows:.. code:: bash$ export CFLAGS=""-Qunused-arguments -Wno-unused-function""
Dependencies^^^^^^^^^^^^Falcon does not require the installation of any other packages, although ifCython has been installed into the environment, it will be used to optimizethe framework as explained above.WSGI ServerFalcon speaks _ (or_; see also below). In order toserve a Falcon app, you will need a WSGI server. Gunicorn and uWSGI are some ofthe more popular ones out there, but anything that can load a WSGI app will do... code:: bash$ pip install [gunicorn|uwsgi]
ASGI ServerIn order to serve a Falcon ASGI app, you will need an ASGI server. Uvicornis a popular choice:.. code:: bash$ pip install uvicorn
Source CodeFalcon _, making thecode easy to browse, download, fork, etc. Pull requests are always welcome! Also,please remember to star the project if it makes you happy. :)Once you have cloned the repo or downloaded a tarball from GitHub, youcan install Falcon like this:.. code:: bash$ cd falcon
$ pip install .
Or, if you want to edit the code, first fork the main repo, clone the forkto your desktop, and then run the following to install it using symboliclinking, so that when you change your code, the changes will be automagicallyavailable to your app without having to reinstall the package:.. code:: bash$ cd falcon
$ pip install -e .
You can manually test changes to the Falcon framework by switching to thedirectory of the cloned repo and then running pytest:.. code:: bash$ cd falcon
$ pip install -r requirements/tests
$ pytest tests
Or, to run the default set of tests:.. code:: bash$ pip install tox && tox
See also the _file for a full list of available environments.Read the DocsThe docstrings in the Falcon code base are quite extensive, and werecommend keeping a REPL running while learning the framework so thatyou can query the various modules and classes as you have questions.Online docs are available at: https://falcon.readthedocs.ioYou can build the same docs locally as follows:.. code:: bash$ pip install tox && tox -e docs
Once the docs have been built, you can view them by opening the followingindex page in your browser. On OS X it's as simple as::$ open docs/_build/html/index.html
Or on Linux:.. code:: bash$ xdg-open docs/_build/html/index.html
Getting StartedHere is a simple, contrived example showing how to create a Falcon-basedWSGI app (the ASGI version is included further down):.. code:: python# examples/things.py

# Let's get this party started!
from wsgiref.simple_server import make_server

import falcon


# Falcon follows the REST architectural style, meaning (among
# other things) that you think in terms of resources and state
# transitions, which map to HTTP verbs.
class ThingsResource:
    def on_get(self, req, resp):
        """"""Handles GET requests""""""
        resp.status = falcon.HTTP_200  # This is the default status
        resp.content_type = falcon.MEDIA_TEXT  # Default is JSON, so override
        resp.text = ('\nTwo things awe me most, the starry sky '
                     'above me and the moral law within me.\n'
                     '\n'
                     '    ~ Immanuel Kant\n\n')


# falcon.App instances are callable WSGI apps...
# in larger applications the app is created in a separate file
app = falcon.App()

# Resources are represented by long-lived class instances
things = ThingsResource()

# things will handle all requests to the '/things' URL path
app.add_route('/things', things)

if __name__ == '__main__':
    with make_server('', 8000, app) as httpd:
        print('Serving on port 8000...')

        # Serve until process is killed
        httpd.serve_forever()
You can run the above example directly using the included wsgiref server:.. code:: bash$ pip install falcon
$ python things.py
Then, in another terminal:.. code:: bash$ curl localhost:8000/things
The ASGI version of the example is similar:.. code:: python# examples/things_asgi.py

import falcon
import falcon.asgi


# Falcon follows the REST architectural style, meaning (among
# other things) that you think in terms of resources and state
# transitions, which map to HTTP verbs.
class ThingsResource:
    async def on_get(self, req, resp):
        """"""Handles GET requests""""""
        resp.status = falcon.HTTP_200  # This is the default status
        resp.content_type = falcon.MEDIA_TEXT  # Default is JSON, so override
        resp.text = ('\nTwo things awe me most, the starry sky '
                     'above me and the moral law within me.\n'
                     '\n'
                     '    ~ Immanuel Kant\n\n')


# falcon.asgi.App instances are callable ASGI apps...
# in larger applications the app is created in a separate file
app = falcon.asgi.App()

# Resources are represented by long-lived class instances
things = ThingsResource()

# things will handle all requests to the '/things' URL path
app.add_route('/things', things)
You can run the ASGI version with uvicorn or any other ASGI server:.. code:: bash$ pip install falcon uvicorn
$ uvicorn things_asgi:app
A More Complex Example (WSGI)Here is a more involved example that demonstrates reading headers and queryparameters, handling errors, and working with request and response bodies.Note that this example assumes that the_ package has been installed.(For the equivalent ASGI app, see: _)... code:: python# examples/things_advanced.py

import json
import logging
import uuid
from wsgiref import simple_server

import falcon
import requests


class StorageEngine:

    def get_things(self, marker, limit):
        return [{'id': str(uuid.uuid4()), 'color': 'green'}]

    def add_thing(self, thing):
        thing['id'] = str(uuid.uuid4())
        return thing


class StorageError(Exception):

    @staticmethod
    def handle(ex, req, resp, params):
        # TODO: Log the error, clean up, etc. before raising
        raise falcon.HTTPInternalServerError()


class SinkAdapter:

    engines = {
        'ddg': 'https://duckduckgo.com',
        'y': 'https://search.yahoo.com/search',
    }

    def __call__(self, req, resp, engine):
        url = self.engines[engine]
        params = {'q': req.get_param('q', True)}
        result = requests.get(url, params=params)

        resp.status = str(result.status_code) + ' ' + result.reason
        resp.content_type = result.headers['content-type']
        resp.text = result.text


class AuthMiddleware:

    def process_request(self, req, resp):
        token = req.get_header('Authorization')
        account_id = req.get_header('Account-ID')

        challenges = ['Token type=""Fernet""']

        if token is None:
            description = ('Please provide an auth token '
                           'as part of the request.')

            raise falcon.HTTPUnauthorized(title='Auth token required',
                                          description=description,
                                          challenges=challenges,
                                          href='http://docs.example.com/auth')

        if not self._token_is_valid(token, account_id):
            description = ('The provided auth token is not valid. '
                           'Please request a new token and try again.')

            raise falcon.HTTPUnauthorized(title='Authentication required',
                                          description=description,
                                          challenges=challenges,
                                          href='http://docs.example.com/auth')

    def _token_is_valid(self, token, account_id):
        return True  # Suuuuuure it's valid...


class RequireJSON:

    def process_request(self, req, resp):
        if not req.client_accepts_json:
            raise falcon.HTTPNotAcceptable(
                description='This API only supports responses encoded as JSON.',
                href='http://docs.examples.com/api/json')

        if req.method in ('POST', 'PUT'):
            if 'application/json' not in req.content_type:
                raise falcon.HTTPUnsupportedMediaType(
                    title='This API only supports requests encoded as JSON.',
                    href='http://docs.examples.com/api/json')


class JSONTranslator:
    # NOTE: Normally you would simply use req.media and resp.media for
    # this particular use case; this example serves only to illustrate
    # what is possible.

    def process_request(self, req, resp):
        # req.stream corresponds to the WSGI wsgi.input environ variable,
        # and allows you to read bytes from the request body.
        #
        # See also: PEP 3333
        if req.content_length in (None, 0):
            # Nothing to do
            return

        body = req.stream.read()
        if not body:
            raise falcon.HTTPBadRequest(title='Empty request body',
                                        description='A valid JSON document is required.')

        try:
            req.context.doc = json.loads(body.decode('utf-8'))

        except (ValueError, UnicodeDecodeError):
            description = ('Could not decode the request body. The '
                           'JSON was incorrect or not encoded as '
                           'UTF-8.')

            raise falcon.HTTPBadRequest(title='Malformed JSON',
                                        description=description)

    def process_response(self, req, resp, resource, req_succeeded):
        if not hasattr(resp.context, 'result'):
            return

        resp.text = json.dumps(resp.context.result)


def max_body(limit):

    def hook(req, resp, resource, params):
        length = req.content_length
        if length is not None and length > limit:
            msg = ('The size of the request is too large. The body must not '
                   'exceed ' + str(limit) + ' bytes in length.')

            raise falcon.HTTPPayloadTooLarge(
                title='Request body is too large', description=msg)

    return hook


class ThingsResource:

    def __init__(self, db):
        self.db = db
        self.logger = logging.getLogger('thingsapp.' + __name__)

    def on_get(self, req, resp, user_id):
        marker = req.get_param('marker') or ''
        limit = req.get_param_as_int('limit') or 50

        try:
            result = self.db.get_things(marker, limit)
        except Exception as ex:
            self.logger.error(ex)

            description = ('Aliens have attacked our base! We will '
                           'be back as soon as we fight them off. '
                           'We appreciate your patience.')

            raise falcon.HTTPServiceUnavailable(
                title='Service Outage',
                description=description,
                retry_after=30)

        # NOTE: Normally you would use resp.media for this sort of thing;
        # this example serves only to demonstrate how the context can be
        # used to pass arbitrary values between middleware components,
        # hooks, and resources.
        resp.context.result = result

        resp.set_header('Powered-By', 'Falcon')
        resp.status = falcon.HTTP_200

    @falcon.before(max_body(64 * 1024))
    def on_post(self, req, resp, user_id):
        try:
            doc = req.context.doc
        except AttributeError:
            raise falcon.HTTPBadRequest(
                title='Missing thing',
                description='A thing must be submitted in the request body.')

        proper_thing = self.db.add_thing(doc)

        resp.status = falcon.HTTP_201
        resp.location = '/%s/things/%s' % (user_id, proper_thing['id'])

# Configure your WSGI server to load ""things.app"" (app is a WSGI callable)
app = falcon.App(middleware=[
    AuthMiddleware(),
    RequireJSON(),
    JSONTranslator(),
])

db = StorageEngine()
things = ThingsResource(db)
app.add_route('/{user_id}/things', things)

# If a responder ever raises an instance of StorageError, pass control to
# the given handler.
app.add_error_handler(StorageError, StorageError.handle)

# Proxy some things to another service; this example shows how you might
# send parts of an API off to a legacy system that hasn't been upgraded
# yet, or perhaps is a single cluster that all data centers have to share.
sink = SinkAdapter()
app.add_sink(sink, r'/search/(?P<engine>ddg|y)\Z')

# Useful for debugging problems in your API; works with pdb.set_trace(). You
# can also use Gunicorn to host your app. Gunicorn can be configured to
# auto-restart workers when it detects a code change, and it also works
# with pdb.
if __name__ == '__main__':
    httpd = simple_server.make_server('127.0.0.1', 8000, app)
    httpd.serve_forever()
Again this code uses wsgiref, but you can also run the above example usingany WSGI server, such as uWSGI or Gunicorn. For example:.. code:: bash$ pip install requests gunicorn
$ gunicorn things:app
On Windows you can run Gunicorn and uWSGI via WSL, or you might try Waitress:.. code:: bash$ pip install requests waitress
$ waitress-serve --port=8000 things:app
To test this example, open another terminal and run:.. code:: bash$ http localhost:8000/1/things authorization:custom-token
You can also view the application configuration from the CLI via the script that is bundled with the framework:.. code:: bashfalcon-inspect-app things_advanced:app
A More Complex Example (ASGI)Here's the ASGI version of the app from above. Note that it uses the_ package in lieu of_... code:: python# examples/things_advanced_asgi.py

import json
import logging
import uuid

import falcon
import falcon.asgi
import httpx


class StorageEngine:

    async def get_things(self, marker, limit):
        return [{'id': str(uuid.uuid4()), 'color': 'green'}]

    async def add_thing(self, thing):
        thing['id'] = str(uuid.uuid4())
        return thing


class StorageError(Exception):

    @staticmethod
    async def handle(ex, req, resp, params):
        # TODO: Log the error, clean up, etc. before raising
        raise falcon.HTTPInternalServerError()


class SinkAdapter:

    engines = {
        'ddg': 'https://duckduckgo.com',
        'y': 'https://search.yahoo.com/search',
    }

    async def __call__(self, req, resp, engine):
        url = self.engines[engine]
        params = {'q': req.get_param('q', True)}

        async with httpx.AsyncClient() as client:
            result = await client.get(url, params=params)

        resp.status = result.status_code
        resp.content_type = result.headers['content-type']
        resp.text = result.text


class AuthMiddleware:

    async def process_request(self, req, resp):
        token = req.get_header('Authorization')
        account_id = req.get_header('Account-ID')

        challenges = ['Token type=""Fernet""']

        if token is None:
            description = ('Please provide an auth token '
                           'as part of the request.')

            raise falcon.HTTPUnauthorized(title='Auth token required',
                                          description=description,
                                          challenges=challenges,
                                          href='http://docs.example.com/auth')

        if not self._token_is_valid(token, account_id):
            description = ('The provided auth token is not valid. '
                           'Please request a new token and try again.')

            raise falcon.HTTPUnauthorized(title='Authentication required',
                                          description=description,
                                          challenges=challenges,
                                          href='http://docs.example.com/auth')

    def _token_is_valid(self, token, account_id):
        return True  # Suuuuuure it's valid...


class RequireJSON:

    async def process_request(self, req, resp):
        if not req.client_accepts_json:
            raise falcon.HTTPNotAcceptable(
                description='This API only supports responses encoded as JSON.',
                href='http://docs.examples.com/api/json')

        if req.method in ('POST', 'PUT'):
            if 'application/json' not in req.content_type:
                raise falcon.HTTPUnsupportedMediaType(
                    description='This API only supports requests encoded as JSON.',
                    href='http://docs.examples.com/api/json')


class JSONTranslator:
    # NOTE: Normally you would simply use req.get_media() and resp.media for
    # this particular use case; this example serves only to illustrate
    # what is possible.

    async def process_request(self, req, resp):
        # NOTE: Test explicitly for 0, since this property could be None in
        # the case that the Content-Length header is missing (in which case we
        # can't know if there is a body without actually attempting to read
        # it from the request stream.)
        if req.content_length == 0:
            # Nothing to do
            return

        body = await req.stream.read()
        if not body:
            raise falcon.HTTPBadRequest(title='Empty request body',
                                        description='A valid JSON document is required.')

        try:
            req.context.doc = json.loads(body.decode('utf-8'))

        except (ValueError, UnicodeDecodeError):
            description = ('Could not decode the request body. The '
                           'JSON was incorrect or not encoded as '
                           'UTF-8.')

            raise falcon.HTTPBadRequest(title='Malformed JSON',
                                        description=description)

    async def process_response(self, req, resp, resource, req_succeeded):
        if not hasattr(resp.context, 'result'):
            return

        resp.text = json.dumps(resp.context.result)


def max_body(limit):

    async def hook(req, resp, resource, params):
        length = req.content_length
        if length is not None and length > limit:
            msg = ('The size of the request is too large. The body must not '
                   'exceed ' + str(limit) + ' bytes in length.')

            raise falcon.HTTPPayloadTooLarge(
                title='Request body is too large', description=msg)

    return hook


class ThingsResource:

    def __init__(self, db):
        self.db = db
        self.logger = logging.getLogger('thingsapp.' + __name__)

    async def on_get(self, req, resp, user_id):
        marker = req.get_param('marker') or ''
        limit = req.get_param_as_int('limit') or 50

        try:
            result = await self.db.get_things(marker, limit)
        except Exception as ex:
            self.logger.error(ex)

            description = ('Aliens have attacked our base! We will '
                           'be back as soon as we fight them off. '
                           'We appreciate your patience.')

            raise falcon.HTTPServiceUnavailable(
                title='Service Outage',
                description=description,
                retry_after=30)

        # NOTE: Normally you would use resp.media for this sort of thing;
        # this example serves only to demonstrate how the context can be
        # used to pass arbitrary values between middleware components,
        # hooks, and resources.
        resp.context.result = result

        resp.set_header('Powered-By', 'Falcon')
        resp.status = falcon.HTTP_200

    @falcon.before(max_body(64 * 1024))
    async def on_post(self, req, resp, user_id):
        try:
            doc = req.context.doc
        except AttributeError:
            raise falcon.HTTPBadRequest(
                title='Missing thing',
                description='A thing must be submitted in the request body.')

        proper_thing = await self.db.add_thing(doc)

        resp.status = falcon.HTTP_201
        resp.location = '/%s/things/%s' % (user_id, proper_thing['id'])


# The app instance is an ASGI callable
app = falcon.asgi.App(middleware=[
    # AuthMiddleware(),
    RequireJSON(),
    JSONTranslator(),
])

db = StorageEngine()
things = ThingsResource(db)
app.add_route('/{user_id}/things', things)

# If a responder ever raises an instance of StorageError, pass control to
# the given handler.
app.add_error_handler(StorageError, StorageError.handle)

# Proxy some things to another service; this example shows how you might
# send parts of an API off to a legacy system that hasn't been upgraded
# yet, or perhaps is a single cluster that all data centers have to share.
sink = SinkAdapter()
app.add_sink(sink, r'/search/(?P<engine>ddg|y)\Z')
You can run the ASGI version with any ASGI server, such as uvicorn:.. code:: bash$ pip install falcon httpx uvicorn
$ uvicorn things_advanced_asgi:app
ContributingThanks for your interest in the project! We welcome pull requests fromdevelopers of all skill levels. To get started, simply fork the master branchon GitHub to your personal account and then clone the fork into yourdevelopment environment.If you would like to contribute but don't already have something in mind,we invite you to take a look at the issues listed under our_.If you see one you'd like to work on, please leave a quick comment so that we don'tend up with duplicated effort. Thanks in advance!Please note that all contributors and maintainers of this project are subject to our _.Before submitting a pull request, please ensure you have added/updatedthe appropriate tests (and that all existing tests still pass with yourchanges), and that your coding style follows PEP 8 and doesn't causepyflakes to complain.Commit messages should be formatted using __.Comments follow __,with the additional requirement of prefixing inline comments using yourGitHub nick and an appropriate prefix:The core Falcon project maintainers are:Please don't hesitate to reach out if you have any questions, or just need alittle help getting started. You can find us in_ on Gitter.See also: __LegalCopyright 2013-2023 by Individual and corporate contributors asnoted in the individual source files.Licensed under the Apache License, Version 2.0 (the ""License""); you maynot use any portion of the Falcon framework except in compliance withthe License. Contributors agree to license their work under the sameLicense. You may obtain a copy of the License athttp://www.apache.org/licenses/LICENSE-2.0Unless required by applicable law or agreed to in writing, softwaredistributed under the License is distributed on an ""AS IS"" BASIS,WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.See the License for the specific language governing permissions andlimitations under the License... |Docs| image:: https://readthedocs.org/projects/falcon/badge/?version=stable:target: https://falcon.readthedocs.io/en/stable/?badge=stable:alt: Falcon web framework docs.. |Build Status| image:: https://github.com/falconry/falcon/workflows/Run%20tests/badge.svg:target: https://github.com/falconry/falcon/actions?query=workflow%3A%22Run+tests%22.. |codecov.io| image:: https://codecov.io/gh/falconry/falcon/branch/master/graphs/badge.svg:target: http://codecov.io/gh/falconry/falcon.. |Blue| image:: https://img.shields.io/badge/code%20style-blue-blue.svg:target: https://blue.readthedocs.io/:alt: code style: blue"
https://github.com/emre/storm,Manage your SSH like a boss.,"storm is a command line tool to manage your ssh connections.featuresdependenciesOn Debian systems, install header files and a static library for Python (python3.4-dev or python2.7-dev)On Ubuntu 16.04, you need install libssl-dev and libffi-dev (sudo apt-get install libssl-dev libffi-dev)installation$ [sudo] pip install stormssh
or if you like 90s:$ [sudo] easy_install stormssh
or if you like homebrew:$ brew install stormssh
or if prefer using a package manager in your distro:| Distro        | Package| ------------- |---------------|| Archlinux     | python-stormssh || Opensuse      | python-stormssh || Void Linux    | python-stormssh |troubleshooting installationclang: error: unknown argument: '-mno-fused-madd'

error: command 'cc' failed with exit status 1
See . If the issue persists, see also  .usage & documentationhttp://stormssh.readthedocs.org/en/master/screensweb ui"
https://github.com/tornadoweb/tornado,"Tornado is a Python web framework and asynchronous networking library, originally developed at FriendFeed.","Tornado Web Server.. image:: https://badges.gitter.im/Join%20Chat.svg:alt: Join the chat at https://gitter.im/tornadoweb/tornado:target: https://gitter.im/tornadoweb/tornado?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge_ is a Python web framework andasynchronous networking library, originally developed at .  By using non-blocking network I/O, Tornado,_, and otherapplications that require a long-lived connection to each user.Hello, worldHere is a simple ""Hello, world"" example web app for Tornado:.. code-block:: pythonimport asyncio
import tornado

class MainHandler(tornado.web.RequestHandler):
    def get(self):
        self.write(""Hello, world"")

def make_app():
    return tornado.web.Application([
        (r""/"", MainHandler),
    ])

async def main():
    app = make_app()
    app.listen(8888)
    await asyncio.Event().wait()

if __name__ == ""__main__"":
    asyncio.run(main())
This example does not use any of Tornado's asynchronous features; forthat see this _.DocumentationDocumentation and links to additional resources are available athttps://www.tornadoweb.org"
https://github.com/donnemartin/awesome-aws,"A curated list of awesome Amazon Web Services (AWS) libraries, open source repos, guides, blogs, and other resources.  Featuring the Fiery Meter of AWSome.","Awesome AWS A curated list of awesome AWS libraries, open source repos, guides, blogs, and other resources.Inspired by the  list.The Fiery Meter of AWSomeRepos not on  can still be awesome, see . Python Module  The Python module  regularly scans repos on  to maintain the accuracy of the .ContributingContributions are welcome!Review the .Also check out the .IndexSDKs and SamplesAWS and community SDKs with samples and docs, grouped by language.Android SDKC++ SDKThe C++ SDK is a labs project with limited docs and/or samples.Clojure SDKThe Clojure SDK is a community project with limited docs and/or samples.)Go SDKRelated Repos:iOS SDKIoT SDKThe IoT SDK is a labs project with limited docs and/or samples.Java SDKJavaScript SDKRelated Repos:Haskell SDKRelated Repos:The Haskell SDK is a community project with limited docs and/or samples.Perl SDKThe Perl SDK is a community project.PHP SDKRelated Repos:Python SDKRelated Repos:Ruby SDKRelated Repos:Rust SDKThe Rust SDK is a community project with limited docs and/or samples.Scala SDKRelated Repos:The Scala SDK is a labs project with limited docs and/or samples.Unity SDKXamarin SDKThe Xamarin SDK is a labs project with limited docs and/or samples..NET SDKCommand Line ToolsAWS and community command line tools with samples and docs.Universal Command Line InterfaceRelated Repos:Windows PowerShellIDE ToolkitsOfficial IDE toolkits with samples and docs.Eclipse ToolkitVisual Studio ToolkitOpen Source ReposAWS and community open source projects, grouped by service.  See API GatewayAWS Repos:Community Repos:CLIAWS Repos:Community Repos:CloudFormationAWS Repos:Community Repos:CloudSearchAWS Repos:Community Repos:CloudTrailAWS Repos:Community Repos:CloudWatchAWS Repos:Community Repos:Code DeployAWS Repos:Community Repos:Code PipelineAWS Repos:Community Repos:CognitoAWS Repos:Community Repos:Data PipelineAWS Repos:Community Repos:Device FarmAWS Repos:Community Repos:DynamoDBAWS Repos:Community Repos:Elastic BeanstalkAWS Repos:Community Repos:Elastic Compute CloudAWS Repos:Community Repos:Elastic Container ServiceAWS Repos:Community Repos:Elastic File SystemAWS Repos:Community Repos:Elastic MapReduceAWS Repos:Community Repos:Elastic SearchAWS Repos:Community Repos:ElasticacheAWS Repos:Community Repos:GlacierCommunity Repos:KinesisAWS Repos:Community Repos:LambdaAWS Repos:Community Repos:Machine LearningAWS Repos:Community Repos:Mobile AnalyticsAWS Repos:Community Repos:OpsWorksAWS Repos:Community Repos:RedshiftAWS Repos:Community Repos:Route 53AWS Repos:Community Repos:S3Community Repos:SESCommunity Repos:Simple WorkflowAWS Repos:Community Repos:SimpleDBCommunity Repos:SNSAWS Repos:Community Repos:SQSAWS Repos:Community Repos:DataAWS Repos:Community Repos:DevOpsCommunity Repos:SecurityAWS Repos:Community Repos:Accompanying ReposAWS Repos:Repos Accompanying Blogs, Training Events, and Conferences.Community Repos:Miscellaneous ReposAWS Repos:Community Repos:Guides, Books, Documentation, and TrainingHow-to's, training, whitepapers, docs, and case studies.Getting Started GuidesAWS Guides:Community Guides:General GuidesAWS Guides:Community Guides:BooksWhitepapersDocumentationTrainingCase Studies: Powered by AWSSocialBlogs, discussion groups, conferences, and social media.BlogsAWS Blogs:Community Blogs:Twitter InfluencersAWS Tweeps:Community Tweeps:Facebook PagesAWS Pages:Community Pages:YouTube ChannelsAWS Channels:Community Channels:LinkedIn GroupsAWS Page:Community Groups:SubredditsConferencesAWS Conferences:Community Conferences:Latest KPIs and StatsLatest key performance indicators and other interesting stats.Appendix of Core ServicesAppendix of official services, grouped by service category.Services in Plain EnglishCompute ServicesNetworking ServicesEnterprise ApplicationsAnalytics ServicesArtificial IntelligenceManagement ToolsSecurity and Identity ServicesInternet of Things ServiceMobile ServicesStorage and Content Delivery ServicesDatabasesApplication ServicesDeveloper ToolsMiscellaneous ServicesCreditsCheck out the .Other Awesome ListsOther awesome lists can be found in  and .Contact InfoFeel free to contact me to discuss any issues, questions, or comments.My contact info can be found on my .LicenseI am providing code and resources in this repository to you under an open source license.  Because this is my personal repository, the license you receive to my code and resources is from me and not my employer (Facebook).Copyright 2017 Donne Martin

Creative Commons Attribution 4.0 International License (CC BY 4.0)

http://creativecommons.org/licenses/by/4.0/
"
https://github.com/Netflix/flamescope,FlameScope is a visualization tool for exploring different time ranges as Flame Graphs.,"FlameScopeFlameScope is a visualization tool for exploring different time ranges as Flame Graphs, allowing quick analysis of performance issues such as perturbations, variance, single-threaded execution, and more.FlameScope begins by displaying the input data as an interactive subsecond-offset heat map. This shows patterns in the data. You can then select a time range to highlight on different patterns, and a flame graph will be generated just for that time range.DisclaimerFlameScope is in early stages of development and under constant change, so bugs and issues are expected. We count on your support to find and report them!Installation / InstructionsThe quickest way to get started is to run the pre-built client bundle:$ git clone https://github.com/Netflix/flamescope
$ cd flamescope
$ pip install -r requirements.txt
$ python run.py
(Note python3 is assumed, python2 may work)Then browse to http://127.0.0.1:5000/, and you can begin exploring profiles from the  directory. You can add new profiles to that directory, collected using Linux . Here are instructions for a generic CPU profile at 49 Hertz for 120 seconds:$ sudo perf record -F 49 -a -g -- sleep 120
$ sudo perf script --header > stacks.myproductionapp.2018-03-30_01
$ gzip stacks.myproductionapp.2018-03-30_01	# optional
If you are profiling C++ code, you may want to pipe stacks through  to get readable frames.There are extra steps to fetch stacks correctly for some runtimes, depending on the runtime. For example, we've previously published Java steps in : java needs to be running with the -XX:+PreserveFramePointer option, and  must be run immediately after the  to dump a JIT symbol table in /tmp.FlameScope can visualize any Linux  output that includes stack traces, including page faults, context switches, and other events. See the References section below for documentation.FlameScope is composed of two main components, the Python backend, and a React client interface. A pre-built client bundle is distributed with the backend, so the quickest way to get started is to install the Python requirements and start the application, as described earlier.Although not necessary, we strongly suggest using  to isolate your Python environment.By default, FlameScope will load a list of files from the  directory, which includes a two profile examples.Configuration OptionsFlameScope configuration file can be found in .DEBUG = True # run the web server in debug mode
PROFILE_DIR = 'examples' # path where flamescope will look for profiles
HOST = '127.0.0.1' # web server host
PORT = 5000 # web server port
JSONIFY_PRETTYPRINT_REGULAR = False # pretty print api json responses
Building Client from SourceIn order to build the client application from source, the following command line tools must be installed:Once those tools are available, you will be able to install the project dependencies and generate a build.$ npm install
$ npm run webpack
The  command will generate a new build under . This directory is exposed by the Python web server.Webpack can also watch and recompile files whenever they change. To build and start the watch task, run the following command:$ npm run webpack-watch
Building a Docker ImageFlameScope provides a Dockerfile to build a Docker image:$ cd flamescope
$ docker build -t flamescope .
The container expects the profiles to be bind-mounted into  and listens on port 5000. To view profiles from , start the container with the following command:$ docker run --rm -it -v /tmp/profiles:/profiles:ro -p 5000:5000 flamescope
Then access FlameScope on References"
https://github.com/bbfamily/abu,阿布量化交易系统(股票，期权，期货，比特币，机器学习) 基于python的开源量化交易，量化投资架构,"索引| 内容 | 位置 || ------| ------ || 阿布量化系统源代码 | abupy目录 || 阿布量化使用教程 | abupy_lecture目录 || 阿布量化非编程界面操作 | abupy_ui目录 || 《量化交易之路》示例代码 | ipython／python目录|| 《机器学习之路》示例代码 | https://github.com/maxmon/abu_ml | 🏆 特点支持的投资市场:工程设计目标：阿布量化综合AI大数据系统, K线形态系统, 缠论，波浪理论，谐波理论，突破，整理形态分析（头肩形态，三头，三角，旗形，楔形，矩形），经典指标系统, 走势趋势分析系统, 时间序列维度系统, 统计概率系统, 传统均线系统对投资品种进行深度量化分析, 彻底跨越用户复杂的代码量化阶段, 更适合普通人群使用, 迈向量化2.0时代.上述系统中结合上百种子量化模型, 如: 金融时间序列损耗模型, 深度形态质量评估模型, 多空形态组合评定模型, 多头形态止损策略模型, 空头形态回补策略模型, 大数据K线形态历史组合拟合模型, 交易持仓心态模型, 多巴胺量化模型, 惯性残存阻力支撑模型, 多空互换报复概率模型, 强弱对抗模型, 趋势角度变化率模型, 联动分析模型, 时间序列的过激反应模型, 迟钝报复反应模型, 趋势启动速度模型, 配对对冲模型等.阿布量化针对AI人工智能从底层开发算法, 构建适合量化体系的人工智能AI系统, 训练了数个从不同角度识别量化特征的评分模型，整体上分为三个系别：物理模型组、多巴胺生物模型组、量化形态模型组。不同系别模型群从不同角度(主要物理交易实体分析、人群心理、图表等三个方向)评估走势，系别的模型群是由若干个独有的识别算法和参数遗传淘汰，组成族群，加权投票评分. 阿布量化结合了传统基于代码策的量化系统, 对未来择时信号发出时机的预判, 系统基于数百种简单种子交易策略，衍生出更多的量化交易策略新策略在这些种子基础上不断自我学习、自我成长，不断分裂，适者生存，淘汰选择机制下繁衍，目前应用的量化买入卖出信号策略共计18496种。阿布量化结合多种量化分析数据构建了数百种量化应用,如: AI高能预警, AI高光时刻, 智能预测涨跌幅, 下跌五浪量化, 上涨五浪量化, 缠论，波浪理论，谐波理论，突破，整理形态分析（头肩形态，三头，三角，旗形，楔形，矩形），阻力支撑强度分析, 上升三角形突破, 下降三角形, 三重底 (头肩底), 三重顶 (头肩顶), 圆弧顶, 圆弧底, 乌云盖顶形态, 上升三部曲形态, 好友反攻形态, 单针探底形态, 射击之星形态, 多方炮形态, 上涨镊子线, 向上突破箱体, 跳空突破缺口, 黄金分割线量化, 趋势跟踪信号, 均值回复信号, 止损风险控制量化, 止盈利润保护量化, 综合指标分析等.安装部署推荐使用Anaconda部署Python环境，详见 测试import abupy
使用文档1：择时策略的开发择时策略决定什么时候买入投资品，回测告诉我们这种策略在历史数据中的模拟收益如何。2: 择时策略的优化通过止盈止损保护策略产生的利润，控制风险。3: 滑点策略与交易手续费考虑应用交易策略时产生的成交价格偏差及手续费。| type | date | symbol | commission || ------| ------ | ------ | ------ || buy | 20150423 | usTSLA | 8.22 || buy | 20150428 | usTSLA | 7.53 || sell | 20150622 | usTSLA | 8.22 || buy | 20150624 | usTSLA | 7.53 || sell | 20150706 | usTSLA | 7.53 || sell | 20150708 | usTSLA | 7.53 || buy | 20151230 | usTSLA | 7.22 || sell | 20160105 | usTSLA | 7.22 || buy | 20160315 | usTSLA | 5.57 || sell | 20160429 | usTSLA | 5.57 |4: 多支股票择时回测与仓位管理针对多支股票实现择时策略，通过仓位管理策略控制风险。5: 选股策略的开发一个好的策略需要一个好的标的。6: 回测结果的度量正确的度量引领着正确的前进方向。7: 寻找策略最优参数和评分通过定制的评分机制，寻找一个策略最合理的参数，比如：应该考虑多少天的均线？8: A股市场的回测9: 港股市场的回测10: 比特币, 莱特币的回测11: 期货市场的回测12: 机器学习与比特币示例如何在投资品的量化交易中正确使用机器学习技术？13: 量化技术分析应用技术分析三大假设：市场行为涵盖一切；价格沿趋势移动；历史会重演。14: 量化相关性分析应用相似的投资品数据的背后，往往是相似行为模式的投资人群。15: 量化交易和搜索引擎搜索策略生成的失败交易，由裁判拦截住冲动的交易者。19: 数据源abu支持股票、期货、数字货币等多种金融投资品的行情和交易，并具有高度可定制性。关注阿布量化微信公众号: abu_quantLicense"
https://github.com/LazoCoder/Pokemon-Terminal,Pokemon terminal themes.,"﻿# Pokemon-TerminalFeaturesInstallationInstall Python 3.7 or higher:Get a compatible terminal emulator:You can then proceed with one of the following methods for installation:pipLinux users: Your distro might include  in a different package than Python, make sure to have that installed.Run .If you want a system-wide install, run the command as superuser or administrator.If you want a per-user install, append the  flag.You might want to add the following directories to your  on a per-user install, to be able to call  and  everywhere:When the command completes, it's installed and ready to go!npmObviously requires to have  installed.You can install in any (npm-supported) OS using . That's it, you're done!Make sure you also have Python installed,  won't automagically do that for you.DistutilsThis doesn't works on Microsoft Store installations of Python.You can clone or  this repo, and run  at the root of the repo.If you want a system-wide install, run the command as superuser or administrator.If you want a per-user install, append the  flag. Look at the pip directives to add a per-user install to your .Usageusage: pokemon [-h] [-n NAME]
               [-r [{kanto,johto,hoenn,sinnoh,unova,kalos} [{kanto,johto,hoenn,sinnoh,unova,kalos} ...]]]
               [-l [0.xx]] [-d [0.xx]]
               [-t [{normal,fire,fighting,water,flying,grass,poison,electric,ground,psychic,rock,ice,bug,dragon,ghost,dark,steel,fairy} [{normal,fire,fighting,water,flying,grass,poison,electric,ground,psychic,rock,ice,bug,dragon,ghost,dark,steel,fairy} ...]]]
               [-ne] [-e] [-ss [X]] [-w] [-v] [-dr] [-c]
               [id]

Set a pokemon to the current terminal background or wallpaper

positional arguments:
  id                    Specify the wanted pokemon ID or the exact (case
                        insensitive) name

optional arguments:
  -h, --help            show this help message and exit
  -c, --clear           Clears the current pokemon from terminal background
                        and quits.

Filters:
  Arguments used to filter the list of pokemons with various conditions that
  then will be picked

  -n NAME, --name NAME  Filter by pokemon which name contains NAME
  -r [{kanto,johto,hoenn,sinnoh,unova,kalos} [{kanto,johto,hoenn,sinnoh,unova,kalos} ...]], --region [{kanto,johto,hoenn,sinnoh,unova,kalos} [{kanto,johto,hoenn,sinnoh,unova,kalos} ...]]
                        Filter the pokemons by region
  -l [0.xx], --light [0.xx]
                        Filter out the pokemons darker (lightness threshold
                        lower) then 0.xx (default is 0.7)
  -d [0.xx], --dark [0.xx]
                        Filter out the pokemons lighter (lightness threshold
                        higher) then 0.xx (default is 0.42)
  -t [{normal,fire,fighting,water,flying,grass,poison,electric,ground,psychic,rock,ice,bug,dragon,ghost,dark,steel,fairy} [{normal,fire,fighting,water,flying,grass,poison,electric,ground,psychic,rock,ice,bug,dragon,ghost,dark,steel,fairy} ...]], --type [{normal,fire,fighting,water,flying,grass,poison,electric,ground,psychic,rock,ice,bug,dragon,ghost,dark,steel,fairy} [{normal,fire,fighting,water,flying,grass,poison,electric,ground,psychic,rock,ice,bug,dragon,ghost,dark,steel,fairy} ...]]
                        Filter the pokemons by type.
  -ne, --no-extras      Excludes extra pokemons (from the extras folder)
  -e, --extras          Excludes all non-extra pokemons

Misc:
  -ss [X], --slideshow [X]
                        Instead of simply choosing a random pokemon from the
                        filtered list, starts a slideshow (with X minutes of
                        delay between pokemon) in the background with the
                        pokemon that matched the filters
  -w, --wallpaper       Changes the desktop wallpaper instead of the terminal
                        background
  -v, --verbose         Enables verbose output
  -dr, --dry-run        Implies -v and doesn't actually changes either
                        wallpaper or background after the pokemon has been
                        chosen

Not setting any filters will get a completely random pokemon
Example:Tips, tricks and common issuesiTerm2 settingsI highly suggest making the font colors black and the terminal window transparent. Some of the images have both light and dark colours and so it can be difficult to see the text sometimes. Transparency resolves this issue. Since Pokemon-Terminal only changes the background, the transparency must be done manually:The result should look like this:ConEmu settingsWindows Terminal settingsYou can, like in iTerm2, enable transparency. Simply press the down arrow in the tab bar and click settings. Once the JSON file opens, add the following settings under the  section:""backgroundImageOpacity"": 0.5,
""useAcrylic"": true,
""acrylicOpacity"": 0.0
The result should look like this:Adding Custom ImagesThe folder  is for adding custom images. You can manually add backgrounds to this folder and they will be visible to the program. Only JPG format is supported. To see a list of all the custom backgrounds type:$ pokemon -e -dr
Alternatively, you can delete images from this folder and it will not break the program. These are some custom backgrounds:Solutions for Common IssuesSavingiTerm2To save a background you will need to setup a startup command in the profile:ConEmuAfter setting your desired pokemon, from the menu under the symbol at left of title bar, navigate to Settings > Main > Background and click Save Settings.TerminologyTerminology already saves it automatically, just untick ""temporary"" in the settings after setting your desired Pokemon:To show a random Pokemon each session:That will simply pick a completely random Pokemon each session, but the  line is simply calling the app, so you can still filter with regions, darkness, and etc. like you normally would, or you can also reset to a preset Pokemon every time you start.Notes & Credits"
https://github.com/wangshub/wechat_jump_game,微信《跳一跳》Python 辅助,"教你用 Python 来玩微信跳一跳   游戏模式可能刚开始上手的时候，因为时间距离之间的关系把握不恰当，只能跳出几个就掉到了台子下面。如果能利用图像识别精确测量出起始和目标点之间测距离，就可以估计按压的时间来精确跳跃。原理说明由于微信检测非常严厉，这里的防禁代码可能已经不起作用，主要供学习用途adb shell screencap -p /sdcard/autojump.png
adb pull /sdcard/autojump.png .
adb shell input swipe x y x y time(ms)
使用教程相关软件工具安装和使用步骤请参考 获取源码- git clone https://github.com/wangshub/wechat_jump_game.git

非常推荐使用Python3，避免编码及import问题PR 要求请选择 merge 进 master 分支，并且标题写上简短描述，例子[优化] 使用PEP8优化代码版本说明FAQ更新日志开发者列表交流"
https://github.com/simonw/datasette,An open source multi-tool for exploring and publishing data,"An open source multi-tool for exploring and publishing dataDatasette is a tool for exploring and publishing data. It helps people take data of any shape or size and publish that as an interactive, explorable website and accompanying API.Datasette is aimed at data journalists, museum curators, archivists, local governments, scientists, researchers and anyone else who has data that they wish to share with the world., watch  or try it out by .Want to stay up-to-date with the project? Subscribe to the  for tips, tricks and news on what's new in the Datasette ecosystem.InstallationIf you are on a Mac,  is the easiest way to install Datasette:brew install datasette
You can also install it using  or :pip install datasette
Datasette requires Python 3.8 or higher. We also have  covering other options such as Docker.Basic usagedatasette serve path/to/database.db
This will start a web server on port 8001 - visit http://localhost:8001/ to access the web interface. is the default subcommand, you can omit it if you like.Use Chrome on OS X? You can run datasette against your browser history like so: datasette ~/Library/Application\ Support/Google/Chrome/Default/History --nolock
Now visiting http://localhost:8001/History/downloads will show you a web interface to browse your downloads data:metadata.jsonIf you want to include licensing and source information in the generated datasette website you can do so using a JSON file that looks something like this:{
    ""title"": ""Five Thirty Eight"",
    ""license"": ""CC Attribution 4.0 License"",
    ""license_url"": ""http://creativecommons.org/licenses/by/4.0/"",
    ""source"": ""fivethirtyeight/data on GitHub"",
    ""source_url"": ""https://github.com/fivethirtyeight/data""
}
Save this in  and run Datasette like so:datasette serve fivethirtyeight.db -m metadata.json
The license and source information will be displayed on the index page and in the footer. They will also be included in the JSON produced by the API.datasette publishIf you have  or  configured, Datasette can deploy one or more SQLite databases to the internet with a single command:datasette publish heroku database.db
Or:datasette publish cloudrun database.db
This will create a docker image containing both the datasette application and the specified SQLite database files. It will then deploy that image to Heroku or Cloud Run and give you a URL to access the resulting website and API.See  in the documentation for more details.Datasette Lite is Datasette packaged using WebAssembly so that it runs entirely in your browser, no Python web application server required. Read more about that in the ."
https://github.com/googleapis/google-cloud-python,Google Cloud Client Library for Python,"Google Cloud Python ClientPython idiomatic clients for _ services... _Google Cloud Platform: https://cloud.google.com/Stability levelsThe _ on PyPI indicates the current stabilityof a package... _development status classifier: https://pypi.org/classifiers/General AvailabilityGA (general availability) indicates that the client library for aparticular service is stable, and that the code surface will not change inbackwards-incompatible ways unless either absolutely necessary (e.g. becauseof critical security issues) or with an extensive deprecation period.Issues and requests against GA libraries are addressed with the highestpriority.GA libraries have development status classifier ... note::Sub-components of GA libraries explicitly marked as beta in the
import path (e.g. ``google.cloud.language_v1beta2``) should be considered
to be beta.
Beta SupportBeta indicates that the client library for a particular service ismostly stable and is being prepared for release. Issues and requestsagainst beta libraries are addressed with a higher priority.Beta libraries have development status classifier .Alpha SupportAlpha indicates that the client library for a particular service isstill a work-in-progress and is more likely to get backwards-incompatibleupdates. See _ for more details.Alpha libraries have development status classifier .If you need support for other Google APIs, check out the_... _Google APIs Python Client library: https://github.com/google/google-api-python-clientLibraries.. This table is generated, see synth.py for details... API_TABLE_START.. list-table:::header-rows: 1.. |PyPI-google-cloud-aiplatform| image:: https://img.shields.io/pypi/v/google-cloud-aiplatform.svg:target: https://pypi.org/project/google-cloud-aiplatform.. |PyPI-google-cloud-appengine-admin| image:: https://img.shields.io/pypi/v/google-cloud-appengine-admin.svg:target: https://pypi.org/project/google-cloud-appengine-admin.. |PyPI-google-cloud-asset| image:: https://img.shields.io/pypi/v/google-cloud-asset.svg:target: https://pypi.org/project/google-cloud-asset.. |PyPI-google-cloud-automl| image:: https://img.shields.io/pypi/v/google-cloud-automl.svg:target: https://pypi.org/project/google-cloud-automl.. |PyPI-google-cloud-bigquery| image:: https://img.shields.io/pypi/v/google-cloud-bigquery.svg:target: https://pypi.org/project/google-cloud-bigquery.. |PyPI-google-cloud-bigquery-storage| image:: https://img.shields.io/pypi/v/google-cloud-bigquery-storage.svg:target: https://pypi.org/project/google-cloud-bigquery-storage.. |PyPI-google-cloud-bigtable| image:: https://img.shields.io/pypi/v/google-cloud-bigtable.svg:target: https://pypi.org/project/google-cloud-bigtable.. |PyPI-google-cloud-binary-authorization| image:: https://img.shields.io/pypi/v/google-cloud-binary-authorization.svg:target: https://pypi.org/project/google-cloud-binary-authorization.. |PyPI-google-cloud-build| image:: https://img.shields.io/pypi/v/google-cloud-build.svg:target: https://pypi.org/project/google-cloud-build.. |PyPI-google-cloud-common| image:: https://img.shields.io/pypi/v/google-cloud-common.svg:target: https://pypi.org/project/google-cloud-common.. |PyPI-google-cloud-compute| image:: https://img.shields.io/pypi/v/google-cloud-compute.svg:target: https://pypi.org/project/google-cloud-compute.. |PyPI-google-cloud-containeranalysis| image:: https://img.shields.io/pypi/v/google-cloud-containeranalysis.svg:target: https://pypi.org/project/google-cloud-containeranalysis.. |PyPI-google-cloud-datastore| image:: https://img.shields.io/pypi/v/google-cloud-datastore.svg:target: https://pypi.org/project/google-cloud-datastore.. |PyPI-google-cloud-filestore| image:: https://img.shields.io/pypi/v/google-cloud-filestore.svg:target: https://pypi.org/project/google-cloud-filestore.. |PyPI-google-cloud-firestore| image:: https://img.shields.io/pypi/v/google-cloud-firestore.svg:target: https://pypi.org/project/google-cloud-firestore.. |PyPI-google-cloud-gke-hub| image:: https://img.shields.io/pypi/v/google-cloud-gke-hub.svg:target: https://pypi.org/project/google-cloud-gke-hub.. |PyPI-grafeas| image:: https://img.shields.io/pypi/v/grafeas.svg:target: https://pypi.org/project/grafeas.. |PyPI-grpc-google-iam-v1| image:: https://img.shields.io/pypi/v/grpc-google-iam-v1.svg:target: https://pypi.org/project/grpc-google-iam-v1.. |PyPI-google-cloud-kms| image:: https://img.shields.io/pypi/v/google-cloud-kms.svg:target: https://pypi.org/project/google-cloud-kms.. |PyPI-google-cloud-logging| image:: https://img.shields.io/pypi/v/google-cloud-logging.svg:target: https://pypi.org/project/google-cloud-logging.. |PyPI-google-cloud-monitoring-dashboards| image:: https://img.shields.io/pypi/v/google-cloud-monitoring-dashboards.svg:target: https://pypi.org/project/google-cloud-monitoring-dashboards.. |PyPI-google-cloud-ndb| image:: https://img.shields.io/pypi/v/google-cloud-ndb.svg:target: https://pypi.org/project/google-cloud-ndb.. |PyPI-google-cloud-os-login| image:: https://img.shields.io/pypi/v/google-cloud-os-login.svg:target: https://pypi.org/project/google-cloud-os-login.. |PyPI-db-dtypes| image:: https://img.shields.io/pypi/v/db-dtypes.svg:target: https://pypi.org/project/db-dtypes.. |PyPI-google-cloud-pubsub| image:: https://img.shields.io/pypi/v/google-cloud-pubsub.svg:target: https://pypi.org/project/google-cloud-pubsub.. |PyPI-google-cloud-pubsublite| image:: https://img.shields.io/pypi/v/google-cloud-pubsublite.svg:target: https://pypi.org/project/google-cloud-pubsublite.. |PyPI-google-cloud-service-management| image:: https://img.shields.io/pypi/v/google-cloud-service-management.svg:target: https://pypi.org/project/google-cloud-service-management.. |PyPI-google-cloud-spanner| image:: https://img.shields.io/pypi/v/google-cloud-spanner.svg:target: https://pypi.org/project/google-cloud-spanner.. |PyPI-django-google-spanner| image:: https://img.shields.io/pypi/v/django-google-spanner.svg:target: https://pypi.org/project/django-google-spanner.. |PyPI-google-cloud-speech| image:: https://img.shields.io/pypi/v/google-cloud-speech.svg:target: https://pypi.org/project/google-cloud-speech.. |PyPI-google-cloud-monitoring| image:: https://img.shields.io/pypi/v/google-cloud-monitoring.svg:target: https://pypi.org/project/google-cloud-monitoring.. |PyPI-google-cloud-storage| image:: https://img.shields.io/pypi/v/google-cloud-storage.svg:target: https://pypi.org/project/google-cloud-storage.. |PyPI-google-cloud-trace| image:: https://img.shields.io/pypi/v/google-cloud-trace.svg:target: https://pypi.org/project/google-cloud-trace.. |PyPI-google-cloud-translate| image:: https://img.shields.io/pypi/v/google-cloud-translate.svg:target: https://pypi.org/project/google-cloud-translate.. |PyPI-google-cloud-vision| image:: https://img.shields.io/pypi/v/google-cloud-vision.svg:target: https://pypi.org/project/google-cloud-vision.. |PyPI-bigframes| image:: https://img.shields.io/pypi/v/bigframes.svg:target: https://pypi.org/project/bigframes.. |PyPI-google-analytics-admin| image:: https://img.shields.io/pypi/v/google-analytics-admin.svg:target: https://pypi.org/project/google-analytics-admin.. |PyPI-google-analytics-data| image:: https://img.shields.io/pypi/v/google-analytics-data.svg:target: https://pypi.org/project/google-analytics-data.. |PyPI-google-cloud-audit-log| image:: https://img.shields.io/pypi/v/google-cloud-audit-log.svg:target: https://pypi.org/project/google-cloud-audit-log.. |PyPI-pandas-gbq| image:: https://img.shields.io/pypi/v/pandas-gbq.svg:target: https://pypi.org/project/pandas-gbq.. |PyPI-google-cloud-dns| image:: https://img.shields.io/pypi/v/google-cloud-dns.svg:target: https://pypi.org/project/google-cloud-dns.. |PyPI-google-cloud-dataflow-client| image:: https://img.shields.io/pypi/v/google-cloud-dataflow-client.svg:target: https://pypi.org/project/google-cloud-dataflow-client.. |PyPI-google-cloud-documentai-toolbox| image:: https://img.shields.io/pypi/v/google-cloud-documentai-toolbox.svg:target: https://pypi.org/project/google-cloud-documentai-toolbox.. |PyPI-google-cloud-error-reporting| image:: https://img.shields.io/pypi/v/google-cloud-error-reporting.svg:target: https://pypi.org/project/google-cloud-error-reporting.. |PyPI-google-cloud-run| image:: https://img.shields.io/pypi/v/google-cloud-run.svg:target: https://pypi.org/project/google-cloud-run.. |PyPI-google-cloud-runtimeconfig| image:: https://img.shields.io/pypi/v/google-cloud-runtimeconfig.svg:target: https://pypi.org/project/google-cloud-runtimeconfig.. |PyPI-sqlalchemy-bigquery| image:: https://img.shields.io/pypi/v/sqlalchemy-bigquery.svg:target: https://pypi.org/project/sqlalchemy-bigquery.. API_TABLE_END.. |ga| image:: https://img.shields.io/badge/support-GA-gold.svg:target: https://github.com/googleapis/google-cloud-python/blob/main/README.rst#general-availability.. |beta| image:: https://img.shields.io/badge/support-beta-orange.svg:target: https://github.com/googleapis/google-cloud-python/blob/main/README.rst#beta-support.. |alpha| image:: https://img.shields.io/badge/support-alpha-orange.svg:target: https://github.com/googleapis/google-cloud-python/blob/main/README.rst#alpha-supportExample Applications.. _getting-started-python: https://github.com/GoogleCloudPlatform/getting-started-python.. _tutorial: https://cloud.google.com/python.. _google-cloud-python-expenses-demo: https://github.com/GoogleCloudPlatform/google-cloud-python-expenses-demoAuthenticationWith  we try to make authentication as painless as possible.Check out the _ in our documentation to learn more... _Getting started with authentication: https://cloud.google.com/docs/authentication/getting-startedLicenseApache 2.0 - See _ for more information... _the LICENSE: https://github.com/googleapis/google-cloud-python/blob/main/LICENSE"
https://github.com/bitsadmin/wesng,Windows Exploit Suggester - Next Generation,"Windows Exploit Suggester - Next Generation (WES-NG)WES-NG is a tool based on the output of Windows'  utility which provides the list of vulnerabilities the OS is vulnerable to, including any exploits for these vulnerabilities. Every Windows OS between Windows XP and Windows 11, including their Windows Server counterparts, is supported.At the BITSADMIN blog an in-depth article on WES-NG is available: .UsageDemoCollectorThis GitHub repository regularly updates the database of vulnerabilities, so running  with the  parameter gets the latest version.If manual generation of the .csv file with hotfix information is required, use the scripts from the  folder to compile the database. Read the comments at the top of each script and execute them in the order as they are listed below. Executing these scripts will produce definitions.zip.The WES-NG collector pulls information from various sources:RationaleI developed WES-NG because while  worked excellently for operating systems in the Windows XP and Windows Vista era, GDSSecurity's Windows-Exploit-Suggester does not work for operating systems like Windows 11 and vulnerabilities published in recent years. This is because Microsoft replaced the Microsoft Security Bulletin Data Excel file [1] on which GDSSecurity's Windows-Exploit-Suggester is fully dependent, by the MSRC API [2]. The Microsoft Security Bulletin Data Excel file has not been updated since Q1 2017, so later operating systems and vulnerabilities cannot be detected. Thanks , for this great tool which has served many of us for so many years!BugsChangelogSee ImprovementsReferences[1] https://www.microsoft.com/download/details.aspx?id=36982[2] https://portal.msrc.microsoft.com/en-us/developer[3] https://nvd.nist.gov/vuln/data-feedsAuthored by Arris Huijgen ("
https://github.com/FederatedAI/FATE,An Industrial Grade Federated Learning Framework,"    | FATE (Federated AI Technology Enabler) is the world's first industrial grade federated learning open source framework to enable enterprises and institutions to collaborate on data while protecting data security and privacy.It implements secure computation protocols based on homomorphic encryption and multi-party computation (MPC).Supporting various federated learning scenarios, FATE now provides a host of federated learning algorithms, including logistic regression, tree-based algorithms, deep learning and transfer learning.FATE is an open source project hosted by Linux Foundation. The  sets forth the responsibilities and procedures for technical contribution to, and oversight of, the FATE (“Federated AI Technology Enabler”) Project. Getting StartedFATE can be deployed on a single host or on multiple nodes. Choose the deployment approach which matches your environment.Standalone deploymentCluster deploymentDeploying FATE to multiple nodes to achieve scalability, reliability and manageability.Quick StartRelated Repositories (Projects)DocumentationFATE DesignDeveloper ResourcesGovernance contains all the documents about how the community members coopearte with each other. Getting InvolvedContributingFATE is an inclusive and open community. We welcome developers who are interested in making FATE better! Contributions of all kinds are welcome. Please refer to the general  of all FATE projects and the contributing guideline of each repository.Mailing listJoin the FATE user , and stay connected with the community and learn about the latest news and information of the FATE project. Discussion and feedback of FATE project are welcome.Bugs or feature requestsFile bugs and features requests via the . If you need help, ask your questions via the mailing list.Contact emailsMaintainers: FedAI-maintainers @ groups.ioSecurity Response Committee: FATE-security @ groups.ioTwitterFollow us on twitter FAQhttps://github.com/FederatedAI/FATE/wikiLicense"
https://github.com/doccano/doccano,Open source annotation tool for machine learning practitioners.,"doccanodoccano is an open-source text annotation tool for humans. It provides annotation features for text classification, sequence labeling, and sequence to sequence tasks. You can create labeled data for sentiment analysis, named entity recognition, text summarization, and so on. Just create a project, upload data, and start annotating. You can build a dataset in hours.DemoTry the .DocumentationRead the documentation at .FeaturesUsageThere are three options to run doccano:pipTo install doccano, run:pip install doccano
By default, SQLite 3 is used for the default database. If you want to use PostgreSQL, install the additional dependencies:pip install 'doccano[postgresql]'
and set the  environment variable according to your PostgreSQL credentials:DATABASE_URL=""postgres://${POSTGRES_USER}:${POSTGRES_PASSWORD}@${POSTGRES_HOST}:${POSTGRES_PORT}/${POSTGRES_DB}?sslmode=disable""
After installation, run the following commands:# Initialize database.
doccano init
# Create a super user.
doccano createuser --username admin --password pass
# Start a web server.
doccano webserver --port 8000
In another terminal, run the command:# Start the task queue to handle file upload/download.
doccano task
Go to .DockerAs a one-time setup, create a Docker container as follows:docker pull doccano/doccano
docker container create --name doccano \
  -e ""ADMIN_USERNAME=admin"" \
  -e ""ADMIN_EMAIL=admin@example.com"" \
  -e ""ADMIN_PASSWORD=password"" \
  -v doccano-db:/data \
  -p 8000:8000 doccano/doccano
Next, start doccano by running the container:docker container start doccano
Go to .To stop the container, run . All data created in the container will persist across restarts.If you want to use the latest features, specify the  tag:docker pull doccano/doccano:nightly
Docker ComposeYou need to install Git and clone the repository:git clone https://github.com/doccano/doccano.git
cd doccano
Note for Windows developers: Be sure to configure git to correctly handle line endings or you may encounter  errors while running the services in future steps. Running with the git config options below will ensure your git directory correctly handles line endings.git clone https://github.com/doccano/doccano.git --config core.autocrlf=input
Then, create an  file with variables in the following format (see ):# platform settings
ADMIN_USERNAME=admin
ADMIN_PASSWORD=password
ADMIN_EMAIL=admin@example.com

# rabbit mq settings
RABBITMQ_DEFAULT_USER=doccano
RABBITMQ_DEFAULT_PASS=doccano

# database settings
POSTGRES_USER=doccano
POSTGRES_PASSWORD=doccano
POSTGRES_DB=doccano
After running the following command, access .docker-compose -f docker/docker-compose.prod.yml --env-file .env up
One-click Deployment| Service | Button ||---------|---|| AWS[^1]   |   || Heroku  |   |FAQSee the  for details.ContributionAs with any software, doccano is under continuous development. If you have requests for features, please file an issue describing your request. Also, if you want to see work towards a specific feature, feel free to contribute by working towards it. The standard procedure is to fork the repository, add a feature, fix a bug, then file a pull request that your changes are to be merged into the main repository and included in the next release.Here are some tips might be helpful. Citation@misc{doccano,
  title={{doccano}: Text Annotation Tool for Human},
  url={https://github.com/doccano/doccano},
  note={Software available from https://github.com/doccano/doccano},
  author={
    Hiroki Nakayama and
    Takahiro Kubo and
    Junya Kamura and
    Yasufumi Taniguchi and
    Xu Liang},
  year={2018},
}
ContactFor help and feedback, feel free to contact ."
https://github.com/prompt-toolkit/python-prompt-toolkit,Library for building powerful interactive command line applications in Python,"Python Prompt Toolkit|AppVeyor|  |PyPI|  |RTD|  |License|  |Codecov|.. image :: https://github.com/prompt-toolkit/python-prompt-toolkit/raw/master/docs/images/logo_400px.png is a library for building powerful interactive command line applications in Python.Read the _.Gallery_ is an interactivePython Shell, build on top of ... image :: https://github.com/prompt-toolkit/python-prompt-toolkit/raw/master/docs/images/ptpython.png_prompt_toolkit features could be a replacement for _, but it can be muchmore than that.Some features:Feel free to create tickets for bugs and feature requests, and create pullrequests if you have nice patches that you would like to share with others.Installation::pip install prompt_toolkit
For Conda, do:::conda install -c https://conda.anaconda.org/conda-forge prompt_toolkit
About Windows support is cross platform, and everything that you build on topshould run fine on both Unix and Windows systems. Windows support is best onrecent Windows 10 builds, for which the command line window supports vt100escape sequences. (If not supported, we fall back to using Win32 APIs for colorand cursor movements).It's worth noting that the implementation is a ""best effort of what ispossible"". Both Unix and Windows terminals have their limitations. But ingeneral, the Unix experience will still be a little better.For Windows, it's recommended to use either _ or _.Getting startedThe most simple example of the library would look like this:.. code:: pythonfrom prompt_toolkit import prompt

if __name__ == '__main__':
    answer = prompt('Give me some input: ')
    print('You said: %s' % answer)
For more complex examples, have a look in the  directory. Allexamples are chosen to demonstrate only one thing. Also, don't be afraid tolook at the source code. The implementation of the  function could bea good start.PhilosophyThe source code of  should be readable, concise andefficient. We prefer short functions focusing each on one task and for whichthe input and output types are clearly specified. We mostly prefer compositionover inheritance, because inheritance can result in too much functionality inthe same object. We prefer immutable objects where possible (objects don'tchange after initialization). Reusability is important. We absolutely refrainfrom having a changing global state, it should be possible to have multipleindependent instances of the same code in the same process. The architectureshould be layered: the lower levels operate on primitive operations and datastructures giving -- when correctly combined -- all the possible flexibility;while at the higher level, there should be a simpler API, ready-to-use andsufficient for most use cases. Thinking about algorithms and efficiency isimportant, but avoid premature optimization._Special thanks to.. |PyPI| image:: https://img.shields.io/pypi/v/prompt_toolkit.svg:target: https://pypi.python.org/pypi/prompt-toolkit/:alt: Latest Version.. |AppVeyor| image:: https://ci.appveyor.com/api/projects/status/32r7s2skrgm9ubva?svg=true:target: https://ci.appveyor.com/project/prompt-toolkit/python-prompt-toolkit/.. |RTD| image:: https://readthedocs.org/projects/python-prompt-toolkit/badge/:target: https://python-prompt-toolkit.readthedocs.io/en/master/.. |License| image:: https://img.shields.io/github/license/prompt-toolkit/python-prompt-toolkit.svg:target: https://github.com/prompt-toolkit/python-prompt-toolkit/blob/master/LICENSE.. |Codecov| image:: https://codecov.io/gh/prompt-toolkit/python-prompt-toolkit/branch/master/graphs/badge.svg?style=flat:target: https://codecov.io/gh/prompt-toolkit/python-prompt-toolkit/"
https://github.com/codemayq/chinese_chatbot_corpus,中文公开聊天语料库,"说明该库是对目前市面上已有的开源中文聊天语料的搜集和系统化整理工作该库搜集了包含共8个公开闲聊常用语料和短信，白鹭时代问答等语料。并对8个常见语料的数据进行了统一化规整和处理，达到直接可以粗略使用的目的。使用该项目，即可对所有的聊天语料进行一次性的处理和统一下载，不需要到处自己去搜集下载和分别处理各种不同的格式。环境python3处理过程将各个来源的语料按照其原格式进行提取，提取后进行繁体字转换，然后统一变成一轮一轮的对话。数据来源及说明语料名称 | 语料数量 | 语料来源说明 | 语料特点 | 语料样例 | 是否已分词---|---|---|---|---|---chatterbot | 560 | 开源项目 | 按类型分类，质量较高  | Q:你会开心的 A:幸福不是真正的可预测的情绪。 | 否douban（豆瓣多轮） | 352W | 来自北航和微软的paper, 开源项目 | 噪音相对较少，原本是多轮（平均7.6轮）  | Q:烟台 十一 哪 好玩 A:哪 都 好玩 · · · · | 是ptt（PTT八卦语料） | 77W（v1版本42W） | 开源项目，台湾PTT论坛八卦版 | 繁体，语料较生活化，有噪音  | Q:为什么乡民总是欺负国高中生呢QQ	A:如果以为选好科系就会变成比尔盖兹那不如退学吧  | 否qingyun（青云语料） | 10W | 某聊天机器人交流群 | 相对不错，生活化  | Q:看来你很爱钱 	 A:噢是吗？那么你也差不多了 | 否subtitle（电视剧对白语料） | 274W | 开源项目，来自爬取的电影和美剧的字幕 | 有一些噪音，对白不一定是严谨的对话，原本是多轮（平均5.3轮）  | Q:京戏里头的人都是不自由的	A:他们让人拿笼子给套起来了了 | 否tieba（贴吧论坛回帖语料） | 232W | 偶然找到的 | 多轮，有噪音  | Q:前排，鲁迷们都起床了吧	A:标题说助攻，但是看了那球，真是活生生的讽刺了 | 否weibo（微博语料） | 443W | 来自华为的paper | 仍有一些噪音  | Q:北京的小纯洁们，周日见。#硬汉摆拍清纯照# A:嗷嗷大湿的左手在干嘛，看着小纯洁撸么。 | 否xiaohuangji（小黄鸡语料） | 45W | 原人人网项目语料 | 有一些不雅对话，少量噪音 | Q:你谈过恋爱么	A:谈过，哎，别提了，伤心..。 | 否语料名称 | 语料原始URL（即出处，尊重原始版权）---|---chatterbot | https://github.com/gunthercox/chatterbot-corpus/tree/master/chatterbot_corpus/data/chinesedouban（豆瓣多轮） | https://github.com/MarkWuNLP/MultiTurnResponseSelectionptt（PTT八卦语料）| https://github.com/zake7749/Gossiping-Chinese-Corpusqingyun（青云语料） | 无subtitle（电视剧对白语料） | https://github.com/fateleak/dgk_lost_convtieba（贴吧论坛回帖语料）  | https://pan.baidu.com/s/1mUknfwy1nhSM7XzH8xi7gQ 密码:i4siweibo（微博语料）  | 61.93.89.94/Noah_NRM_Data/xiaohuangji（小黄鸡语料） | https://github.com/candlewill/Dialog_Corpus 使用方法下载语料阿里云盘https://www.aliyundrive.com/s/qXBdAYtz5j5提取码: 81aoGoogle Drivehttps://drive.google.com/file/d/1So-m83NdUHexfjJ912rQ4GItdLvnmJMD/view?usp=sharing将解压后的raw_chat_corpus文件夹放到当前目录下目录结构为raw_chat_corpus
-- language
-- process_pipelines
-- raw_chat_corpus
---- chatterbot-1k
---- douban-multiturn-100w
---- ....
-- main.py
-- ...
然后修改 config.py 中的 raw_chat_corpus_root 变量 为自己的目录，再执行main.py 脚本即可python main.py
或者python3 main.py
生成结果每个来源的语料分别生成一个独立的*.tsv文件，都放在新生成的clean_chat_corpus文件夹下。生成结果格式为 tsv格式，每行是一个样本，先是query，再是answerquery \t answer
结果的使用这个就根据每个人不同的情况自主使用即可个人对于聊天机器人方向实践也不是很多，以下一篇之前写的知乎专栏供参考《从产品完整性的角度浅谈chatbot》https://zhuanlan.zhihu.com/p/34927757文章粗略讲解了如下一些方面，介绍了聊天机器人在实际产品化过程中可能遇到的问题和解决办法。版权说明本项目为非商业项目，为纯搜集和汇总资料，如有侵权，请在issue下留言。@Misc{chinese-chatbot-corpus,
  title = {Chinese Chatbot Corpus},
  author = {codemayq},
  howpublished = {\url{https://github.com/codemayq/chinese_chatbot_corpus}},
  year = {2018}
}
"
https://github.com/keithito/tacotron,A TensorFlow implementation of Google's Tacotron speech synthesis with pre-trained model (unofficial),"TacotronAn implementation of Tacotron speech synthesis in TensorFlow.Audio SamplesRecent UpdatesBackgroundIn April 2017, Google published a paper, ,where they present a neural text-to-speech model that learns to synthesize speech directly from(text, audio) pairs. However, they didn't release their source code or training data. This is anindependent attempt to provide an open-source implementation of the model described in their paper.The quality isn't as good as Google's demo yet, but hopefully it will get there someday :-).Pull requests are welcome!Quick StartInstalling dependenciesUsing a pre-trained modelTrainingNote: you need at least 40GB of free disk space to train a model.Notes and Common IssuesOther Implementations"
https://github.com/jiangxufeng/v2rayL,v2ray linux GUI客户端，支持订阅、vemss、ss等协议，自动更新订阅、检查版本更新,"remark工作繁忙无法维护，工作繁忙无法维护，工作繁忙无法维护v2rayV2Ray 是 Project V 下的一个工具。Project V 包含一系列工具，帮助你打造专属的定制网络体系。而 V2Ray 属于最核心的一个。 简单地说，V2Ray 是一个与 Shadowsocks 类似的代理软件，但比Shadowsocks更具优势V2Ray 用户手册：https://www.v2ray.comV2Ray 项目地址：https://github.com/v2ray/v2ray-corev2rayLv2ray linux 客户端，使用pyqt5编写GUI界面，核心基于v2ray-core(v2ray-linux-64)开发环境：目前已实现以下功能：其中vmess支持websocket、mKcp、tcp目前程序可能存在一些bug但是没有测试出，若在使用过程中发现bug，请在issue中提交，以便改进。透明代理说明：透明代理设置参考v2ray教程：测试环境： 三台不同的机器(条件有限)测试时出现问题： 有些透明代理无法生效，导致代理失败。解决办法：在测试时发现多尝试启动几次(关闭，开启)或重启程序就可以正常使用后续会进一步深入优化这个问题，透明代理无法使用时可以关闭，不影响其正常使用使用使用前请注意所有命令请直接运行，避免导致出现权限问题所有命令请直接运行，避免导致出现权限问题使用脚本安装时下载的程序实在 + 的环境下打包的，因此在Python版本不一致的环境中可能会出现版本不兼容的问题解决方法(请先运行安装脚本)：在自己的电脑上重新打包程序，具体方法如下（参考）安装bash <(curl -s -L http://dl.thinker.ink/install.sh)
上述命令因解析文件下载直链服务可能出现500，因为也可通过下面方法安装可以下载从这：https://www.lanzous.com/iaynbud下载文件至本地后，解压运行./install.sh
deb包：可在中进行根据说明进行v2rayL的下载安装更新bash <(curl -s -L http://dl.thinker.ink/update.sh)
卸载bash <(curl -s -L http://dl.thinker.ink/uninstall.sh)
展示感谢UI界面设计来源：https://zmister.com/archives/477.html配置方面参考: https://github.com/2dust/v2rayNGdeb包提供：铜豌豆Linux https://www.atzlinux.com/allpackages.htm协议"
https://github.com/modin-project/modin,Modin: Scale your Pandas workflows by changing a single line of code,"| Dev Community & Support | Forums | Socials | Docs ||:---: | :---: | :---: | :---: ||  |  |  |  |What is Modin?Modin is a drop-in replacement for . While pandas issingle-threaded, Modin lets you instantly speed up your workflows by scaling pandas so it uses all of yourcores. Modin works especially well on larger datasets, where pandas becomes painfully slow or runs.By simply replacing the import statement, Modin offers users effortless speed and scale for their pandas workflows:In the GIFs below, Modin (left) and pandas (right) perform the same pandas operations on a 2GB dataset. The only difference between the two notebook examples is the import statement. The charts below show the speedup you get by replacing pandas with Modin based on the examples above. The example notebooks can be found . To learn more about the speedups you could get with Modin and try out some examples on your own, check out our  to try out some examples on your own!InstallationFrom PyPIModin can be installed with  on Linux, Windows and MacOS:pip install ""modin[all]"" # (Recommended) Install Modin with all of Modin's currently supported engines.
If you want to install Modin with a specific engine, we recommend:pip install ""modin[ray]"" # Install Modin dependencies and Ray.
pip install ""modin[dask]"" # Install Modin dependencies and Dask.
pip install ""modin[unidist]"" # Install Modin dependencies and Unidist.
Modin automatically detects which engine(s) you have installed and uses that for scheduling computation.From conda-forgeInstalling from  using will install Modin and four engines: , , and .conda install -c conda-forge modin-all
Each engine can also be installed individually (and also as a combination of several engines):conda install -c conda-forge modin-ray  # Install Modin dependencies and Ray.
conda install -c conda-forge modin-dask # Install Modin dependencies and Dask.
conda install -c conda-forge modin-unidist # Install Modin dependencies and Unidist.
conda install -c conda-forge modin-hdk # Install Modin dependencies and HDK.
To speed up conda installation we recommend using libmamba solver. To do this install it in a base environment:conda install -n base conda-libmamba-solver
and then use it during istallation either like:conda install -c conda-forge modin-ray modin-hdk --experimental-solver=libmamba
or starting from conda 22.11 and libmamba solver 22.12 versions:conda install -c conda-forge modin-ray modin-hdk --solver=libmamba
Choosing a Compute EngineIf you want to choose a specific compute engine to run on, you can set the environmentvariable  and Modin will do computation with that engine:export MODIN_ENGINE=ray  # Modin will use Ray
export MODIN_ENGINE=dask  # Modin will use Dask
export MODIN_ENGINE=unidist # Modin will use Unidist
If you want to choose the Unidist engine, you should set the additional environmentvariable , because currently Modin only supports Unidist on MPI:export UNIDIST_BACKEND=mpi # Unidist will use MPI backend
This can also be done within a notebook/interpreter before you import Modin:import modin.config as modin_cfg
import unidist.config as unidist_cfg

modin_cfg.Engine.put(""ray"")  # Modin will use Ray
modin_cfg.Engine.put(""dask"")  # Modin will use Dask

modin_cfg.Engine.put('unidist') # Modin will use Unidist
unidist_cfg.Backend.put('mpi') # Unidist will use MPI backend
Check  for HDK engine setup.Note: You should not change the engine after your first operation with Modin as it will result in undefined behavior.Which engine should I use?On Linux, MacOS, and Windows you can install and use either Ray, Dask or Unidist. There is no knowledge requiredto use either of these engines as Modin abstracts away all of the complexity, so feelfree to pick either!On Linux you also can choose , which is an experimentalengine based on  and included in the,which is a part of .Pandas API Coverage| pandas Object     | Modin's Ray Engine Coverage                                                          | Modin's Dask Engine Coverage | Modin's Unidist Engine Coverage ||-------------------|:------------------------------------------------------------------------------------:|:---------------:|:---------------:||     |  |  |  ||        |  |  | |      | ✅                                               | ✅ | ✅ ||    | ✅                                               | ✅ | ✅ ||  | ✅                                               | ✅ | ✅ ||      | ✅                                               | ✅ | ✅ ||  | ✅                                               | ✅ | ✅ ||    | ✅                                               | ✅ | ✅ ||     |                                          |  |  ||  |  |  |  |More about ModinFor the complete documentation on Modin, visit our  page.Scale your pandas workflow by changing a single line of code.Note: In local mode (without a cluster), Modin will create and manage a local (Dask or Ray) cluster for the execution.To use Modin, you do not need to specify how to distribute the data, or even know how manycores your system has. In fact, you can continue using your previouspandas notebooks while experiencing a considerable speedup from Modin, even on a singlemachine. Once you've changed your import statement, you're ready to use Modin just likeyou would with pandas!Faster pandas, even on your laptopThe  DataFrame is an extremely light-weight parallel DataFrame.Modin transparently distributes the data and computation so that you can continue using the same pandas APIwhile working with more data faster. Because it is so light-weight,Modin provides speed-ups of up to 4x on a laptop with 4 physical cores.In pandas, you are only able to use one core at a time when you are doing computation ofany kind. With Modin, you are able to use all of the CPU cores on your machine. Even with atraditionally synchronous task like , we see large speedups by efficientlydistributing the work across your entire machine.import modin.pandas as pd

df = pd.read_csv(""my_dataset.csv"")
Modin can handle the datasets that pandas can'tOften data scientists have to switch between different toolsfor operating on datasets of different sizes. Processing large dataframes with pandasis slow, and pandas does not support working with dataframes that are too large to fitinto the available memory. As a result, pandas workflows that work wellfor prototyping on a few MBs of data do not scale to tens or hundreds of GBs (depending on the sizeof your machine). Modin supports operating on data that does not fit in memory, so that you can comfortablywork with hundreds of GBs without worrying about substantial slowdown or memory errors.With and support, Modin is a DataFrame library with both great single-node performance and highscalability in a cluster.Modin ArchitectureWe designed to be modular so we can plug in different components as they develop and improve:Other ResourcesGetting Started with ModinModin CommunityLearn More about ModinGetting Involved[<marko.inline.CodeSpan object at 0x000001592FF3C3C8>, <marko.inline.RawText object at 0x000001592FF3CD48>]For more information on how to contribute to Modin, check out the.License"
https://github.com/ecthros/uncaptcha,Defeating Google's audio reCaptcha with 85% accuracy. ,"uncaptchaDefeating Google's audio reCaptcha system with 85% accuracy. DisclaimerunCaptcha is intended to be a proof of concept.  As of the time of , we found it to successfully solve reCaptcha's audio challenges with 85% success.  Since that time, reCaptcha appears to include some additional protections that limit unCaptcha's success. [<marko.inline.RawText object at 0x000001592FDE9E48>]For instance, Google has also improved their browser automation detection. This means that Selenium cannot be used in its current state to get captchas from Google. This may lead to Google sending odd audio segments back to the end user.  Additionally, we have observed that some audio challenges include not only digits, but small snippets of spoken text.We encourage you to be careful when doing research in this field, to be mindful of local, state, and federal law, and to responsibly disclose any potential vulnerabilities to Google immediately.Additionally, we have removed our API keys from all the necessary queries. If you are looking to recreate some of the work or are doing your own research in this area, you will need to acquire API keys from each of the six services used. These keys are delineated in our files by a long string of the character 'X'. InspirationAcross the Internet, hundreds of thousands of sites rely on Google's reCaptcha system for defense against bots (in fact, Devpost uses reCaptcha when creating a new account). After a Google research team demonstrated a near  of the text reCaptcha in 2012, the reCaptcha system evolved to rely on audio and image challenges, historically more difficult challenges for automated systems to solve. Google has continually iterated on its design, releasing a newer and more powerful version as recently as just this year. Successfully demonstrating a defeat of this captcha system spells significant vulnerability for hundreds of thousands of popular sites. What it doesOur unCaptcha system has attack capabilities written for the audio captcha. Using browser automation software, we can interact with the target website and engage with the captcha, parsing out the necessary elements to begin the attack. We rely primarily on the audio captcha attack - by properly identifying spoken numbers, we can pass the reCaptcha programmatically and fool the site into thinking our bot is a human. Specifically, unCaptcha targets the popular site Reddit by going through the motions of creating a new user, although unCaptcha stops before creating the user to mitigate the impact on Reddit.BackgroundGoogle's reCaptcha system uses an advanced risk analysis system to determine programmatically how likely a given user is to be a human or a bot. It takes into account your cookies (and by extension, your interaction with other Google services), the speed at which challenges are solved, mouse movements, and (obviously) how successfully you solve the given task. As the system gets increasingly suspicious, it delivers increasingly difficult challenges, and requires the user to solve more of them. Researchers have already identified minor weaknesses with the reCaptcha system - 9 days of legitimate (ish) interaction with Google's services is usually enough to lower the system's suspicion level significantly.How it worksThe format of the audio captcha is a varied-length series of numbers spaced out read aloud at varied speeds, pitches, and accents through background noise. To attack this captcha, the audio payload is identified on the page, downloaded, and automatically split by locations of speech. From there, each number audio bit is uploaded to 6 different free, online audio transcription services (IBM, Google Cloud, Google Speech Recognition, Sphinx, Wit-AI, Bing Speech Recognition), and these results are collected. We ensemble the results from each of these to probabilistically enumerate the most likely string of numbers with a predetermined heuristic. These numbers are then organically typed into the captcha, and the captcha is completed. From testing, we have seen 92%+ accuracy in individual number identification, and 85%+ accuracy in defeating the audio captcha in its entirety. InstallationFirst, install python dependencies:$ pip install -r requirements.txt
Make sure you also have sox, ffmpeg, and selenium installed! $ apt-get install sox ffmpeg selenium
Then, to kick off the PoC:$ python main.py --audio --reddit
This opens reddit.com, interacts with the page to go to account signup, generates a fake username, email, password, and then attacks the audio captcha. Once the captcha is completed (whether it passed or not), the browser exits. To learn morePlease read our paper, located , for more information. Additionally, you can visit our website , or check out the original .ExampleContributors"
https://github.com/edgedb/edgedb,"A graph-relational database with declarative schema, built-in migration system, and a next-generation query language","Schema is the foundation of your application. It should be something you canread, write, and understand.Forget foreign keys; tabular data modeling is a relic of an older age, and itwith modern languages. Instead, EdgeDB thinks about schema the same way you do:as object types containing properties connected by links.type Person {
  required property name -> str;
}

type Movie {
  required property title -> str;
  multi link actors -> Person;
}
This example is intentionally simple, but EdgeDB supports everything you'dexpect from your database: a strict type system, indexes, constraints, computedproperties, stored procedures...the list goes on. Plus it gives you some shinynew features too: link properties, schema mixins, and best-in-class JSONsupport. Read the for details.EdgeDB's super-powered query language EdgeQL is designed as a ground-upredesign of SQL. EdgeQL queries produce rich, structured objects, not flatlists of rows. Deeply fetching related objects is painless...bye, bye, JOINs.select Movie {
  title,
  actors: {
    name
  }
}
filter .title = ""The Matrix""
EdgeQL queries are also composable; you can use one EdgeQL query as anexpression inside another. This property makes things like subqueries andnested mutations a breeze.insert Movie {
  title := ""The Matrix Resurrections"",
  actors := (
    select Person
    filter .name in {
      'Keanu Reeves',
      'Carrie-Anne Moss',
      'Laurence Fishburne'
    }
  )
}
There's a lot more to EdgeQL: a comprehensive standard library, computedproperties, polymorphic queries,  blocks, transactions, and much more.Read the  for the fullpicture.While EdgeDB solves the same problems as ORM libraries, it's so much more. It'sa full-fledged database with a, a, a indifferent languages, a, and—coming soon—acloud hosting platform. The goal is to rethink every aspect of how developersmodel, migrate, manage, and query their database.Here's a taste-test of EdgeDB's next-level developer experience: you caninstall our CLI, spin up an instance, and open an interactive EdgeQL shell withjust three commands.$ curl --proto '=https' --tlsv1.2 -sSf https://sh.edgedb.com | sh
$ edgedb project init
$ edgedb
edgedb> select ""Hello world!""
Windows users: use this Powershell command to install the CLI.PS> iwr https://ps1.edgedb.com -useb | iex
Get startedTo start learning about EdgeDB, check out the following resources:ContributingPRs are always welcome! To get started, follow to build EdgeDB fromsource on your local machine.LicenseThe code in this repository is developed and distributed under theApache 2.0 license. See  for details."
https://github.com/belluzj/fantasque-sans,A font family with a great monospaced variant for programmers.,"Fantasque Sans MonoA programming font, designed with functionality in mind, and with somewibbly-wobbly handwriting-like fuzziness that makes it unassumingly cool. orsee .Previously known as Cosmic Sans Neue Mono. Itappeared that , and thatpeople tended to extend their instinctive hatred of Comic Sans to this veryfont of mine (which of course can only be loved). Why the previous name?Here is my original explanation:Inspirational sources include Inconsolata and Monaco. I have also been usingConsolas a lot in my programming life, so it may have some points in common.Weights, variants and glyph coverageThe font includes a bold version, with the same metrics as the regular one.Both versions include the same ranges of characters : latin letters, someaccented glyphs (quite a lot), some greek letters, some arrows.Please note that I have not tested all of the glyphs I have drawn (some lettershave those two layers of crazy accents that I have never witnessed before), soit might look bad in some cases. Please report these problems: see next section.It also features a good italic version, which I designed in a fashion similarto Consolas' italic version, with new glyph designs, not just an added slant.Stylistic set(s): nondescript No ~~distractive~~ lovely loop.or see the for techniques to activate the stylistic set.Author and licenseCreated by Jany Belluz jany.belluz AT hotmail.frLicensed under the SIL Open Font License (see ).Please send me an e-mail or  if you stumble uponbad design or rendering problems (with screen shot if possible), or if you needmore characters, or if you want to compliment me (I love compliments).InstallationYou can and install it by hand. In the  variant, the looped lowercase  isreplaced with a straight version. The  variant is especiallyuseful for users of accented capitals. For more info, see the .Automatic installation on macOS with :brew tap homebrew/cask-fonts #You only need to do this once for cask-fonts
brew install --cask font-fantasque-sans-mono
Instructions for other platforms might follow.Building installable font filesThe build process requires:Run . You should see green stuff and some ""OK"" messages.If you are using Ubuntu, please note that the FontForge versionin the default Ubuntu repositories is much outdated at the time of this writing,and that .You are advised to install FontForge from(using  prior to the installation).Alternatively, you can always the latest prebuilt release of these fonts. will install the TTF fonts into your local  directoryand update the font cache. It comes in handy while modifying the font.Alternatively, if you'd like to build Fantasque without installing requireddependencies, a Dockerfile is provided. Run the following command, and thefonts will be built to the  directory.docker build -t fantasque .
docker run -v ""$(pwd)/Variants:/fantasque/Variants"" fantasque
WebfontsEach variant has a  folder which contains various font formats foruse on the web, along with the matching CSS font declarations. To use them,you must combine in the same folder:Versions."
https://github.com/django/django,The Web framework for perfectionists with deadlines.,"======DjangoDjango is a high-level Python web framework that encourages rapid developmentand clean, pragmatic design. Thanks for checking it out.All documentation is in the """" directory and online athttps://docs.djangoproject.com/en/stable/. If you're just getting started,here's how we recommend you read the docs:Docs are updated rigorously. If you find any problems in the docs, or thinkthey should be clarified in any way, please take 30 seconds to fill out aticket here: https://code.djangoproject.com/newticketTo get more help:To contribute to Django:To run Django's test suite:Supporting the Development of DjangoDjango's development depends on your contributions.If you depend on Django, remember to support the Django Software Foundation: https://www.djangoproject.com/fundraising/"
https://github.com/nvdv/vprof,Visual profiler for Python,"vprofvprof is a Python package providing rich and interactive visualizations forvarious Python program characteristics such as running time and memory usage.It supports Python 3.4+ and distributed under BSD license.The project is in active development and some of its features might not work asexpected.ScreenshotsContributingAll contributions are highly encouraged! You can add new features,report and fix existing bugs and write docs and tutorials.Feel free to open an issue or send a pull request!PrerequisitesDependencies to build  from source code: is required to build  from sources only.DependenciesAll Python and  module dependencies are listed in  and.Installation can be installed from PyPIpip install vprof
To build  from sources, clone this repository and executepython3 setup.py deps_install && python3 setup.py build_ui && python3 setup.py install
To install just  dependencies, runpython3 setup.py deps_install
Usagevprof -c <config> <src>
 is a combination of supported modes:Shows CPU flame graph for .Runs built-in Python profiler on  and displays results.Shows objects that are tracked by CPython GC and left in memory after codeexecution. Also shows process memory usage after execution of each line of .Displays all executed code of  with line run times and execution counts. can be Python source file (e.g. ) or path to package(e.g. ).To run scripts with arguments use double quotesvprof -c cmh ""testscript.py --foo --bar""
Modes can be combinedvprof -c cm testscript.py
 can also profile functions. In order to do this,launch  in remote mode:vprof -r
 will open new tab in default web browser and then wait for stats.To profile a function runfrom vprof import runner

def foo(arg1, arg2):
    ...

runner.run(foo, 'cmhp', args=(arg1, arg2), host='localhost', port=8000)
where  is profiling mode,  and  are hostname and port of server launched in remote mode. Obtained stats will be rendered in newtab of default web browser, opened by  command. can save profile stats to file and render visualizations frompreviously saved file.vprof -c cmh src.py --output-file profile.json
writes profile to file andvprof --input-file profile.json
renders visualizations from previously saved file.Check  for full list of supported parameters.To show UI help, press  when visualizations are displayed.Also you can check  directory for more profiling examples.Testingpython3 setup.py test_python && python3 setup.py test_javascript && python3 setup.py e2e_test
LicenseBSD"
https://github.com/dbader/schedule,Python job scheduling for humans.,"__.. image:: https://github.com/dbader/schedule/workflows/Tests/badge.svg:target: https://github.com/dbader/schedule/actions?query=workflow%3ATests+branch%3Amaster.. image:: https://coveralls.io/repos/dbader/schedule/badge.svg?branch=master:target: https://coveralls.io/r/dbader/schedule.. image:: https://img.shields.io/pypi/v/schedule.svg:target: https://pypi.python.org/pypi/schedulePython job scheduling for humans. Run Python functions (or any other callable) periodically using a friendly syntax.Usage.. code-block:: bash$ pip install schedule
.. code-block:: pythonimport schedule
import time

def job():
    print(""I'm working..."")

schedule.every(10).seconds.do(job)
schedule.every(10).minutes.do(job)
schedule.every().hour.do(job)
schedule.every().day.at(""10:30"").do(job)
schedule.every(5).to(10).minutes.do(job)
schedule.every().monday.do(job)
schedule.every().wednesday.at(""13:15"").do(job)
schedule.every().day.at(""12:42"", ""Europe/Amsterdam"").do(job)
schedule.every().minute.at("":17"").do(job)

def job_with_argument(name):
    print(f""I am {name}"")

schedule.every(10).seconds.do(job_with_argument, name=""Peter"")

while True:
    schedule.run_pending()
    time.sleep(1)
DocumentationSchedule's documentation lives at _.MetaDaniel Bader - _ - mail@dbader.orgInspired by _ article _ and the _ Ruby module.Distributed under the MIT license. See _ for more information.https://github.com/dbader/schedule"
https://github.com/rossant/awesome-math,A curated list of awesome mathematics resources,"Awesome Math A curated list of awesome mathematics resources.ContentsGeneral ResourcesLearning PlatformsLearn to LearnYoutube SeriesToolsQuestions and AnswersEncyclopediaBooksMagazinesBlogsMiscLecture NotesFoundations of MathematicsTransition To Pure Rigour MathSet TheoryLogicCategory TheoryType TheoryHomotopy Type TheorySurreal NumbersNumber TheoryAlgebraic Number TheoryAnalytic Number TheoryAlgebraAbstract AlgebraGroup TheoryLinear AlgebraRing TheoryGalois TheoryLie AlgebrasCombinatoricsGraph TheoryGeometry and TopologyDifferential GeometryAlgebraic GeometryAlgebraic StatisticsTopologyAlgebraic TopologyAnalysisReal AnalysisHarmonic AnalysisComplex AnalysisFunctional AnalysisMeasure TheoryOrdinary Differential EquationsPartial Differential EquationsProbability and StatisticsProbability TheoryStatisticsStatistical LearningStochastic processesNumerical AnalysisSignal processingMathematics for Computer ScienceMathematical BiologyMathematical PhysicsStudents Lecture NotesRelated Awesome ListsLicenseTo the extent possible under law,  has waived all copyright and related or neighboring rights to this work."
https://github.com/jiaaro/pydub,Manipulate audio with a simple and easy high level interface,"Pydub  Pydub lets you do stuff to audio in a way that isn't stupid.Stuff you might be looking for:QuickstartOpen a WAV filefrom pydub import AudioSegment

song = AudioSegment.from_wav(""never_gonna_give_you_up.wav"")
...or a mp3song = AudioSegment.from_mp3(""never_gonna_give_you_up.mp3"")
... or an ogg, or flv, or ogg_version = AudioSegment.from_ogg(""never_gonna_give_you_up.ogg"")
flv_version = AudioSegment.from_flv(""never_gonna_give_you_up.flv"")

mp4_version = AudioSegment.from_file(""never_gonna_give_you_up.mp4"", ""mp4"")
wma_version = AudioSegment.from_file(""never_gonna_give_you_up.wma"", ""wma"")
aac_version = AudioSegment.from_file(""never_gonna_give_you_up.aiff"", ""aac"")
Slice audio:# pydub does things in milliseconds
ten_seconds = 10 * 1000

first_10_seconds = song[:ten_seconds]

last_5_seconds = song[-5000:]
Make the beginning louder and the end quieter# boost volume by 6dB
beginning = first_10_seconds + 6

# reduce volume by 3dB
end = last_5_seconds - 3
Concatenate audio (add one file to the end of another)without_the_middle = beginning + end
How long is it?without_the_middle.duration_seconds == 15.0
AudioSegments are immutable# song is not modified
backwards = song.reverse()
Crossfade (again, beginning and end are not modified)# 1.5 second crossfade
with_style = beginning.append(end, crossfade=1500)
Repeat# repeat the clip twice
do_it_over = with_style * 2
Fade (note that you can chain operations because everything returnsan AudioSegment)# 2 sec fade in, 3 sec fade out
awesome = do_it_over.fade_in(2000).fade_out(3000)
Save the results (again whatever ffmpeg supports)awesome.export(""mashup.mp3"", format=""mp3"")
Save the results with tags (metadata)awesome.export(""mashup.mp3"", format=""mp3"", tags={'artist': 'Various artists', 'album': 'Best of 2011', 'comments': 'This album is awesome!'})
You can pass an optional bitrate argument to export using any syntax ffmpegsupports.awesome.export(""mashup.mp3"", format=""mp3"", bitrate=""192k"")
Any further arguments supported by ffmpeg can be passed as a list in a'parameters' argument, with switch first, argument second. Note that novalidation takes place on these parameters, and you may be limited by whatyour particular build of ffmpeg/avlib supports.# Use preset mp3 quality 0 (equivalent to lame V0)
awesome.export(""mashup.mp3"", format=""mp3"", parameters=[""-q:a"", ""0""])

# Mix down to two channels and set hard output volume
awesome.export(""mashup.mp3"", format=""mp3"", parameters=[""-ac"", ""2"", ""-vol"", ""150""])
DebuggingMost issues people run into are related to converting between formats usingffmpeg/avlib. Pydub provides a logger that outputs the subprocess calls tohelp you track down issues:>>> import logging

>>> l = logging.getLogger(""pydub.converter"")
>>> l.setLevel(logging.DEBUG)
>>> l.addHandler(logging.StreamHandler())

>>> AudioSegment.from_file(""./test/data/test1.mp3"")
subprocess.call(['ffmpeg', '-y', '-i', '/var/folders/71/42k8g72x4pq09tfp920d033r0000gn/T/tmpeZTgMy', '-vn', '-f', 'wav', '/var/folders/71/42k8g72x4pq09tfp920d033r0000gn/T/tmpK5aLcZ'])
<pydub.audio_segment.AudioSegment object at 0x101b43e10>
Don't worry about the temporary files used in the conversion. They're cleaned upautomatically.Bugs & QuestionsYou can file bugs in our ,and ask any technical questions on.We keep an eye on both.InstallationInstalling pydub is easy, but don't forget to install ffmpeg/avlib (the next section in this doc)pip install pydub
Or install the latest dev version from github (or replace  with a )…pip install git+https://github.com/jiaaro/pydub.git@master
-OR-git clone https://github.com/jiaaro/pydub.git
-OR-Copy the pydub directory into your python path. ZipDependenciesYou can open and save WAV files with pure python. For opening and saving non-wavfiles – like mp3 – you'll need  or.PlaybackYou can play audio if you have one of these installed (simpleaudio strongly recommended, even if you are installing ffmpeg/libav):from pydub import AudioSegment
from pydub.playback import play

sound = AudioSegment.from_file(""mysound.wav"", format=""wav"")
play(sound)
Getting ffmpeg set upYou may use libav or ffmpeg.Mac (using ):# libav
brew install libav

####    OR    #####

# ffmpeg
brew install ffmpeg
Linux (using aptitude):# libav
apt-get install libav-tools libavcodec-extra

####    OR    #####

# ffmpeg
apt-get install ffmpeg libavcodec-extra
Windows:Important Notes objects are Ogg exporting and default codecsThe Ogg specification () does not specifythe codec to use, this choice is left up to the user. Vorbis and Theora are justsome of a number of potential codecs (see page 3 of the rfc) that can be used for theencapsulated data.When no codec is specified exporting to  will default to using as a convenience. That is:from pydub import AudioSegment
song = AudioSegment.from_mp3(""test/data/test1.mp3"")
song.export(""out.ogg"", format=""ogg"")  # Is the same as:
song.export(""out.ogg"", format=""ogg"", codec=""libvorbis"")
Example UseSuppose you have a directory filled with mp4 and flv videos and you want to convert all of them to mp3 so you can listen to  them on your mp3 player.import os
import glob
from pydub import AudioSegment

video_dir = '/home/johndoe/downloaded_videos/'  # Path where the videos are located
extension_list = ('*.mp4', '*.flv')

os.chdir(video_dir)
for extension in extension_list:
    for video in glob.glob(extension):
        mp3_filename = os.path.splitext(os.path.basename(video))[0] + '.mp3'
        AudioSegment.from_file(video).export(mp3_filename, format='mp3')
How about another example?from glob import glob
from pydub import AudioSegment

playlist_songs = [AudioSegment.from_mp3(mp3_file) for mp3_file in glob(""*.mp3"")]

first_song = playlist_songs.pop(0)

# let's just include the first 30 seconds of the first song (slicing
# is done by milliseconds)
beginning_of_song = first_song[:30*1000]

playlist = beginning_of_song
for song in playlist_songs:

    # We don't want an abrupt stop at the end, so let's do a 10 second crossfades
    playlist = playlist.append(song, crossfade=(10 * 1000))

# let's fade out the end of the last song
playlist = playlist.fade_out(30)

# hmm I wonder how long it is... ( len(audio_segment) returns milliseconds )
playlist_length = len(playlist) / (1000*60)

# lets save it!
with open(""%s_minute_playlist.mp3"" % playlist_length, 'wb') as out_f:
    playlist.export(out_f, format='mp3')
License ()Copyright © 2011 James Robert, http://jiaaro.comPermission is hereby granted, free of charge, to any person obtaininga copy of this software and associated documentation files (the""Software""), to deal in the Software without restriction, includingwithout limitation the rights to use, copy, modify, merge, publish,distribute, sublicense, and/or sell copies of the Software, and topermit persons to whom the Software is furnished to do so, subject tothe following conditions:The above copyright notice and this permission notice shall beincluded in all copies or substantial portions of the Software.THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND,EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OFMERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE ANDNONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BELIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTIONOF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTIONWITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
https://github.com/hhatto/autopep8,A tool that automatically formats Python code to conform to the PEP 8 style guide.,"========autopep8.. image:: https://img.shields.io/pypi/v/autopep8.svg:target: https://pypi.org/project/autopep8/:alt: PyPI Version.. image:: https://github.com/hhatto/autopep8/workflows/Python%20package/badge.svg:target: https://github.com/hhatto/autopep8/actions:alt: Build status.. image:: https://codecov.io/gh/hhatto/autopep8/branch/main/graph/badge.svg:target: https://codecov.io/gh/hhatto/autopep8:alt: Code Coverageautopep8 automatically formats Python code to conform to the _ styleguide. It uses the pycodestyle_ utility to determine what parts of the codeneeds to be formatted. autopep8 is capable of fixing most of the formattingissues_ that can be reported by pycodestyle... _PEP 8: https://www.python.org/dev/peps/pep-0008/.. _issues: https://pycodestyle.readthedocs.org/en/latest/intro.html#error-codes.. contents::InstallationFrom pip::$ pip install --upgrade autopep8
Consider using the  option_... _option: https://pip.pypa.io/en/latest/user_guide/#user-installsRequirementsautopep8 requires pycodestyle_... _pycodestyle: https://github.com/PyCQA/pycodestyleUsageTo modify a file in place (with aggressive level 2)::$ autopep8 --in-place --aggressive --aggressive <filename>
Before running autopep8... code-block:: pythonimport math, sys;

def example1():
    ####This is a long comment. This should be wrapped to fit within 72 characters.
    some_tuple=(   1,2, 3,'a'  );
    some_variable={'long':'Long code lines should be wrapped within 79 characters.',
    'other':[math.pi, 100,200,300,9876543210,'This is a long string that goes on'],
    'more':{'inner':'This whole logical line should be wrapped.',some_tuple:[1,
    20,300,40000,500000000,60000000000000000]}}
    return (some_tuple, some_variable)
def example2(): return {'has_key() is deprecated':True}.has_key({'f':2}.has_key(''));
class Example3(   object ):
    def __init__    ( self, bar ):
     #Comments should have a space after the hash.
     if bar : bar+=1;  bar=bar* bar   ; return bar
     else:
                    some_string = """"""
		           Indentation in multiline strings should not be touched.
Only actual code should be reindented.
""""""
                    return (sys.path, some_string)
After running autopep8... code-block:: pythonimport math
import sys


def example1():
    # This is a long comment. This should be wrapped to fit within 72
    # characters.
    some_tuple = (1, 2, 3, 'a')
    some_variable = {
        'long': 'Long code lines should be wrapped within 79 characters.',
        'other': [
            math.pi,
            100,
            200,
            300,
            9876543210,
            'This is a long string that goes on'],
        'more': {
            'inner': 'This whole logical line should be wrapped.',
            some_tuple: [
                1,
                20,
                300,
                40000,
                500000000,
                60000000000000000]}}
    return (some_tuple, some_variable)


def example2(): return ('' in {'f': 2}) in {'has_key() is deprecated': True}


class Example3(object):
    def __init__(self, bar):
        # Comments should have a space after the hash.
        if bar:
            bar += 1
            bar = bar * bar
            return bar
        else:
            some_string = """"""
		           Indentation in multiline strings should not be touched.
Only actual code should be reindented.
""""""
            return (sys.path, some_string)
Options::usage: autopep8 [-h] [--version] [-v] [-d] [-i] [--global-config filename]
                [--ignore-local-config] [-r] [-j n] [-p n] [-a]
                [--experimental] [--exclude globs] [--list-fixes]
                [--ignore errors] [--select errors] [--max-line-length n]
                [--line-range line line] [--hang-closing] [--exit-code]
                [files [files ...]]

Automatically formats Python code to conform to the PEP 8 style guide.

positional arguments:
  files                 files to format or '-' for standard in

optional arguments:
  -h, --help            show this help message and exit
  --version             show program's version number and exit
  -v, --verbose         print verbose messages; multiple -v result in more
                        verbose messages
  -d, --diff            print the diff for the fixed source
  -i, --in-place        make changes to files in place
  --global-config filename
                        path to a global pep8 config file; if this file does
                        not exist then this is ignored (default:
                        ~/.config/pep8)
  --ignore-local-config
                        don't look for and apply local config files; if not
                        passed, defaults are updated with any config files in
                        the project's root directory
  -r, --recursive       run recursively over directories; must be used with
                        --in-place or --diff
  -j n, --jobs n        number of parallel jobs; match CPU count if value is
                        less than 1
  -p n, --pep8-passes n
                        maximum number of additional pep8 passes (default:
                        infinite)
  -a, --aggressive      enable non-whitespace changes; multiple -a result in
                        more aggressive changes
  --experimental        enable experimental fixes
  --exclude globs       exclude file/directory names that match these comma-
                        separated globs
  --list-fixes          list codes for fixes; used by --ignore and --select
  --ignore errors       do not fix these errors/warnings (default:
                        E226,E24,W50,W690)
  --select errors       fix only these errors/warnings (e.g. E4,W)
  --max-line-length n   set maximum allowed line length (default: 79)
  --line-range line line, --range line line
                        only fix errors found within this inclusive range of
                        line numbers (e.g. 1 99); line numbers are indexed at
                        1
  --hang-closing        hang-closing option passed to pycodestyle
  --exit-code           change to behavior of exit code. default behavior of
                        return value, 0 is no differences, 1 is error exit.
                        return 2 when add this option. 2 is exists
                        differences.
Featuresautopep8 fixes the following issues_ reported by pycodestyle_::E101 - Reindent all lines.
E11  - Fix indentation.
E121 - Fix indentation to be a multiple of four.
E122 - Add absent indentation for hanging indentation.
E123 - Align closing bracket to match opening bracket.
E124 - Align closing bracket to match visual indentation.
E125 - Indent to distinguish line from next logical line.
E126 - Fix over-indented hanging indentation.
E127 - Fix visual indentation.
E128 - Fix visual indentation.
E129 - Fix visual indentation.
E131 - Fix hanging indent for unaligned continuation line.
E133 - Fix missing indentation for closing bracket.
E20  - Remove extraneous whitespace.
E211 - Remove extraneous whitespace.
E22  - Fix extraneous whitespace around keywords.
E224 - Remove extraneous whitespace around operator.
E225 - Fix missing whitespace around operator.
E226 - Fix missing whitespace around arithmetic operator.
E227 - Fix missing whitespace around bitwise/shift operator.
E228 - Fix missing whitespace around modulo operator.
E231 - Add missing whitespace.
E241 - Fix extraneous whitespace around keywords.
E242 - Remove extraneous whitespace around operator.
E251 - Remove whitespace around parameter '=' sign.
E252 - Missing whitespace around parameter equals.
E26  - Fix spacing after comment hash for inline comments.
E265 - Fix spacing after comment hash for block comments.
E266 - Fix too many leading '#' for block comments.
E27  - Fix extraneous whitespace around keywords.
E301 - Add missing blank line.
E302 - Add missing 2 blank lines.
E303 - Remove extra blank lines.
E304 - Remove blank line following function decorator.
E305 - Expected 2 blank lines after end of function or class.
E306 - Expected 1 blank line before a nested definition.
E401 - Put imports on separate lines.
E402 - Fix module level import not at top of file
E501 - Try to make lines fit within --max-line-length characters.
E502 - Remove extraneous escape of newline.
E701 - Put colon-separated compound statement on separate lines.
E70  - Put semicolon-separated compound statement on separate lines.
E711 - Fix comparison with None.
E712 - Fix comparison with boolean.
E713 - Use 'not in' for test for membership.
E714 - Use 'is not' test for object identity.
E721 - Use ""isinstance()"" instead of comparing types directly.
E722 - Fix bare except.
E731 - Use a def when use do not assign a lambda expression.
W291 - Remove trailing whitespace.
W292 - Add a single newline at the end of the file.
W293 - Remove trailing whitespace on blank line.
W391 - Remove trailing blank lines.
W503 - Fix line break before binary operator.
W504 - Fix line break after binary operator.
W605 - Fix invalid escape sequence 'x'.
W690 - Fix various deprecated code (via lib2to3).
autopep8 also fixes some issues not found by pycodestyle_.autopep8 avoids fixing some issues found by pycodestyle_... _eradicate: https://github.com/myint/eradicateMore advanced usageBy default autopep8 only makes whitespace changes. Thus, by default, it doesnot fix  and . (Changing  to  maychange the meaning of the program if  has its  methodoverridden.) Nor does it correct deprecated code . To enable thesemore aggressive fixes, use the  option::$ autopep8 --aggressive <filename>
Use multiple  to increase the aggressiveness level. Forexample,  requires aggressiveness level 2 (since  could bechanged to either  or , but autopep8 chooses the former). will also shorten lines more aggressively. It will also removetrailing whitespace more aggressively. (Usually, we don't touch trailingwhitespace in docstrings and other multiline strings. And to do even moreaggressive changes to docstrings, use docformatter_.).. _docformatter: https://github.com/myint/docformatterTo enable only a subset of the fixes, use the  option. For example,to fix various types of indentation issues::$ autopep8 --select=E1,W1 <filename>
If the file being fixed is large, you may want to enable verbose progressmessages::$ autopep8 -v <filename>
Passing in  enables the following functionality:::$ autopep8 --experimental Disabling line-by-lineIt is possible to disable autopep8 untill it it turned back on again in the file, using  and then renabling . .. code-block:: python# autopep8: off
    [
        [23, 23, 13, 43],
        [32, 34, 34, 34],
        [56, 34, 34, 11],
        [10, 10, 10, 10],
    ]
# autopep8: on
     
 and  are also valid.Use as a moduleThe simplest way of using autopep8 as a module is via the function:>>> import autopep8
>>> autopep8.fix_code('x=       123\n')
'x = 123\n'
Or with options:>>> import autopep8
>>> autopep8.fix_code('print( 123 )\n',
...                   options={'ignore': ['E']})
'print( 123 )\n'
ConfigurationBy default, if  ( in Windowsenvironment) exists, it will be used as global configuration file.Alternatively, you can specify the global configuration file with the option.Also, if , ,  and  files existin the directory where the target file exists, it will be used as theconfiguration file., , and  can be used as a section.configuration file example::[pycodestyle]
max_line_length = 120
ignore = E501
pyproject.tomlautopep8 can also use .The section must be , and  takes precedenceover any other configuration files.configuration file example::[tool.autopep8]
max_line_length = 120
ignore = ""E501,W6""  # or [""E501"", ""W6""]
in-place = true
recursive = true
aggressive = 3
Usage with pre-commitautopep8 can be used as a hook for pre-commit_.To add autopep8 as a plugin, add this repo definition to your configuration:.. code-block:: yamlrepos:
-   repo: https://github.com/hhatto/autopep8
    rev: ...  # select the tag or revision you want, or run `pre-commit autoupdate`
    hooks:
    -   id: autopep8
.. _: https://pre-commit.comTestingTest cases are in . They can be run directly via or via tox_. The latter is useful fortesting against multiple Python interpreters. (We currently test againstCPython versions 3.7, 3.8, 3.9 and 3.10. We also test against PyPy.).. _: https://pypi.org/project/tox/Broad spectrum testing is available via . This script runsautopep8 against Python code and checks for correctness and completeness of thecode fixes. It can check that the bytecode remains identical. makes use of  to test against the latestreleased packages on PyPI.TroubleshootingIf you are using an ancient version of , you might encounter when trying to run . Tryupgrading  to workaround this  problem::$ pip install --upgrade setuptools
Use  if you are installing to the system.Links.. _PyPI: https://pypi.org/project/autopep8/.. _GitHub: https://github.com/hhatto/autopep8.. _: https://travis-ci.org/hhatto/autopep8.. _: https://coveralls.io/r/hhatto/autopep8"
https://github.com/snare/voltron,A hacky debugger UI for hackers,"VoltronVoltron is an extensible debugger UI toolkit written in Python. It aims to improve the user experience of various debuggers (LLDB, GDB, VDB and WinDbg) by enabling the attachment of utility views that can retrieve and display data from the debugger host. By running these views in other TTYs, you can build a customised debugger user interface to suit your needs.Voltron does not aim to be everything to everyone. It's not a wholesale replacement for your debugger's CLI. Rather, it aims to complement your existing setup and allow you to extend your CLI debugger as much or as little as you like. If you just want a view of the register contents in a window alongside your debugger, you can do that. If you want to go all out and have something that looks more like OllyDbg, you can do that too.Built-in views are provided for:The author's setup looks something like this:Any debugger command can be split off into a view and highlighted with a specified Pygments lexer:More screenshots are .SupportVoltron supports LLDB, GDB, VDB and WinDbg/CDB (via ) and runs on macOS, Linux and Windows.WinDbg support is still fairly new, please  if you have problems.The following architectures are supported:|         | lldb | gdb | vdb | windbg ||---------|------|-----|-----|--------|| x86     | ✓    | ✓   | ✓   | ✓      || x86_64  | ✓    | ✓   | ✓   | ✓      || arm     | ✓    | ✓   | ✓   | ✗      || arm64   | ✓    | ✗   | ✗   | ✗      || powerpc | ✗    | ✓   | ✗   | ✗      |InstallationNote: Only macOS and Debian derivatives are fully supported by the install script. It should hopefully not fail on other Linux distros, but it won't try to install package dependencies. If you're using another distro, have a look at  to work out what dependencies you might need to install before running it.Download the source and run the install script:$ git clone https://github.com/snare/voltron
$ cd voltron
$ ./install.sh
By default, the install script will install into the user's  directory. If you want to install into the system , use the  flag:$ ./install.sh -s
You can also install into a virtual environment (for LLDB only) like this:$ ./install.sh -v /path/to/venv -b lldb
If you are on Windows without a shell, have problems installing, or would prefer to install manually, please see the .Quick StartDocumentationSee the  on github.FAQQ. Why am I getting an  loading Voltron?A. You might have multiple versions of Python installed and have installed Voltron using the wrong one. See the more detailed .Q. ? ? ? ?A. All super great extensions for GDB. These tools primarily provide sets of additional commands for exploitation tasks, but each also provides a ""context"" display with a view of registers, stack, code, etc, like Voltron. These tools print their context display in the debugger console each time the debugger stops. Voltron takes a different approach by embedding an RPC server implant in the debugger and enabling the attachment of views from other terminals (or even web browsers, or now ), which allows the user to build a cleaner multi-window interface to their debugger. Voltron works great alongside all of these tools. You can just disable the context display in your GDB extension of choice and hook up some Voltron views, while still getting all the benefits of the useful commands added by these tools.Bugs and ErrataSee the  on github for more information or to submit issues.If you're experiencing an  loading Voltron, please ensure you've followed the  for your platform.LLDBOn older versions of LLDB, the  command must be run manually after loading the debug target, as a target must be loaded before Voltron's hooks can be installed. Voltron will attempt to automatically register its event handler, and it will inform the user if  is required.WinDbgMore information about WinDbg/CDB support .MiscThe authors primarily use Voltron with the most recent version of LLDB on macOS. We will try to test everything on as many platforms and architectures as possible before releases, but LLDB/macOS/x64 is going to be by far the most frequently-used combination. Hopefully Voltron doesn't set your pets on fire, but YMMV.LicenseSee the  file.If you use this and don't hate it, buy me a beer at a conference some time. This license also extends to other contributors -  definitely deserves a few beers for his contributions.CreditsThanks to my former employers Assurance and Azimuth Security for giving me time to spend working on this.Props to  for all his contributions to Voltron.'s gdbinit was the original inspiration for this project.Thanks to  for implementing the VDB support.Voltron now uses  for disassembly as well as the debugger hosts' internal disassembly mechanism.  is a powerful, open source, multi-architecture disassembler upon which the next generation of reverse engineering and debugging tools are being built. Check it out.Thanks to  for ongoing contributions."
https://github.com/deanishe/alfred-workflow,Full-featured library for writing Alfred 3 & 4 workflows,"Alfred-WorkflowA helper library in Python for authors of workflows for .Supports Alfred 3 and Alfred 4 on macOS 10.7+ (Python 2.7).Alfred-Workflow takes the grunt work out of writing a workflow by giving you the tools to create a fast and featureful Alfred workflow from an API, application or library in minutes.Always supports all current Alfred features.FeaturesAlfred 4+ featuresContentsInstallationNote: If you're new to Alfred workflows, check out in the docs.With pipYou can install Alfred-Workflow directly into your workflow with:# from your workflow directory
pip install --target=. Alfred-Workflow
You can install any other library available on the  the same way. See the  for more information.It is highly advisable to bundle all your workflow's dependencies with your workflow in this way. That way, it will ""just work"".From sourceYour workflow should look something like this:Your Workflow/
    info.plist
    icon.png
    workflow/
        __init__.py
        background.py
        notify.py
        Notify.tgz
        update.py
        version
        web.py
        workflow.py
    yourscript.py
    etc.
Alternatively, you can clone/download the Alfred-Workflow  and copy the  subdirectory to your workflow's root directory.UsageA few examples of how to use Alfred-Workflow.Workflow script skeletonSet up your workflow scripts as follows (if you wish to use the built-in error handling or  modification):#!/usr/bin/python
# encoding: utf-8

import sys

# Workflow3 supports Alfred 3's new features. The `Workflow` class
# is also compatible with Alfred 2.
from workflow import Workflow3


def main(wf):
    # The Workflow3 instance will be passed to the function
    # you call from `Workflow3.run`.
    # Not super useful, as the `wf` object created in
    # the `if __name__ ...` clause below is global...
    #
    # Your imports go here if you want to catch import errors, which
    # is not a bad idea, or if the modules/packages are in a directory
    # added via `Workflow3(libraries=...)`
    import somemodule
    import anothermodule

    # Get args from Workflow3, already in normalized Unicode.
    # This is also necessary for ""magic"" arguments to work.
    args = wf.args

    # Do stuff here ...

    # Add an item to Alfred feedback
    wf.add_item(u'Item title', u'Item subtitle')

    # Send output to Alfred. You can only call this once.
    # Well, you *can* call it multiple times, but subsequent calls
    # are ignored (otherwise the JSON sent to Alfred would be invalid).
    wf.send_feedback()


if __name__ == '__main__':
    # Create a global `Workflow3` object
    wf = Workflow3()
    # Call your entry function via `Workflow3.run()` to enable its
    # helper functions, like exception catching, ARGV normalization,
    # magic arguments etc.
    sys.exit(wf.run(main))
ExamplesCache data for 30 seconds:def get_web_data():
    return web.get('http://www.example.com').json()

def main(wf):
    # Save data from `get_web_data` for 30 seconds under
    # the key ``example``
    data = wf.cached_data('example', get_web_data, max_age=30)
    for datum in data:
        wf.add_item(datum['title'], datum['author'])

    wf.send_feedback()
WebGrab data from a JSON web API:data = web.get('http://www.example.com/api/1/stuff').json()
Post a form:r = web.post('http://www.example.com/',
             data={'artist': 'Tom Jones', 'song': ""It's not unusual""})
Upload a file:files = {'fieldname' : {'filename': ""It's not unusual.mp3"",
                        'content': open(""It's not unusual.mp3"", 'rb').read()}
}
r = web.post('http://www.example.com/upload/', files=files)
WARNING: As this module is based on Python 2's standard HTTP libraries, on old versions of OS X/Python, it does not validate SSL certificates when making HTTPS connections. If your workflow uses sensitive passwords/API keys, you should strongly consider using the  library upon which the  API is based.Keychain accessSave password:wf = Workflow()
wf.save_password('name of account', 'password1lolz')
Retrieve password:wf = Workflow()
wf.get_password('name of account')
DocumentationThe full documentation, including API docs and a tutorial, can be found at .Dash docsetThe documentation is also available as a .Licensing, thanksThe code and the documentation are released under the MIT and  licences respectively. See  for details.The documentation was generated using  and a modified version of the  theme by .Many of the cooler ideas in Alfred-Workflow were inspired by  by Zhaocai.The Keychain parser was based on  by Jason R. Coombs.ContributingAdding a workflow to the listIf you want to add a workflow to the , don't add it to the docs! The list is machine-generated from  and the  file. If your workflow is available on , it will be added on the next update. If not, please add it to , and submit a corresponding pull request.The list is not auto-updated, so if you've released a workflow and are keen to see it in this list, please  asking me to update the list.Bug reports, pull requestsPlease see .ContributorsWorkflows using Alfred-Workflow of some of the many workflows based on Alfred-Workflow."
https://github.com/andialbrecht/sqlparse,A non-validating SQL parser module for Python,"python-sqlparse - Parse SQL statements|buildstatus|_|coverage|_|docs|_|packageversion|_.. docincludebeginsqlparse is a non-validating SQL parser for Python.It provides support for parsing, splitting and formatting SQL statements.The module is compatible with Python 3.6+ and released under the terms of the_.Visit the project page at https://github.com/andialbrecht/sqlparse forfurther information about this project.Quick Start.. code-block:: sh$ pip install sqlparse.. code-block:: pythonLinksProject pagehttps://github.com/andialbrecht/sqlparseBug trackerhttps://github.com/andialbrecht/sqlparse/issuesDocumentationhttps://sqlparse.readthedocs.io/Online Demohttps://sqlformat.org/sqlparse is licensed under the BSD license.Parts of the code are based on pygments written by Georg Brandl and others.pygments-Homepage: http://pygments.org/.. |buildstatus| image:: https://github.com/andialbrecht/sqlparse/actions/workflows/python-app.yml/badge.svg.. _buildstatus: https://github.com/andialbrecht/sqlparse/actions/workflows/python-app.yml.. |coverage| image:: https://codecov.io/gh/andialbrecht/sqlparse/branch/master/graph/badge.svg.. _coverage: https://codecov.io/gh/andialbrecht/sqlparse.. |docs| image:: https://readthedocs.org/projects/sqlparse/badge/?version=latest.. _docs: https://sqlparse.readthedocs.io/en/latest/?badge=latest.. |packageversion| image:: https://img.shields.io/pypi/v/sqlparse?color=%2334D058&label=pypi%20package.. _packageversion: https://pypi.org/project/sqlparse"
https://github.com/openstack/openstack,Repository tracking all OpenStack repositories as submodules. Mirror of code maintained at opendev.org.,"OpenStackOpenStack is a collection of interoperable components that can be deployedto provide computing, networking and storage resources. Those infrastructureresources can then be accessed by end users through programmable APIs.This repository just represents OpenStack as a collection of git submodules.You can find the repositories for individual components at:https://opendev.org/openstackYou can learn more about the various components in OpenStack at:https://openstack.org/softwareTo learn more about how to contribute to OpenStack, please head to ourContributor portal: https://www.openstack.org/community/To learn more about how OpenStack is governed, you can visit:https://governance.openstack.org/Why this repository ?Our continuous integration system, Zuul, gates all of the contained projectsin an effective single timeline. This means that OpenStack, across all of theprojects, does already have a sequence of combinations that have beenexplicitly tested, but it's non-trivial to go from a single commit of aparticular project to the commits that were tested with it.Gerrit's submodule tracking feature will update a super project everytime a subproject is updated, so the specific sequence created by zuulwill be captured by the super project commits.This repo is intended to be used in a read-only manner. Any commit in thisrepo will get a collection of commits in the other repos that haveexplicitly been tested with each other, if that sort of thing is importantto you."
https://github.com/mementum/backtrader,Python Backtesting library for trading strategies,"backtrader.. image:: https://img.shields.io/pypi/v/backtrader.svg:alt: PyPi Version:scale: 100%:target: https://pypi.python.org/pypi/backtrader/..  .. image:: https://img.shields.io/pypi/dm/backtrader.svg:alt: PyPi Monthly Donwloads:scale: 100%:target: https://pypi.python.org/pypi/backtrader/.. image:: https://img.shields.io/pypi/l/backtrader.svg:alt: License:scale: 100%:target: https://github.com/backtrader/backtrader/blob/master/LICENSE.. image:: https://travis-ci.org/backtrader/backtrader.png?branch=master:alt: Travis-ci Build Status:scale: 100%:target: https://travis-ci.org/backtrader/backtrader.. image:: https://img.shields.io/pypi/pyversions/backtrader.svg:alt: Python versions:scale: 100%:target: https://pypi.python.org/pypi/backtrader/Yahoo API Note:[2018-11-16] After some testing it would seem that data downloads can beagain relied upon over the web interface (or API )TicketsThe ticket system is (was, actually) more often than not abused to ask foradvice about samples.For feedback/questions/... use the _Here a snippet of a Simple Moving Average CrossOver. It can be done in severaldifferent ways. Use the docs (and examples) Luke!::from datetime import datetimeimport backtrader as btclass SmaCross(bt.SignalStrategy):def init(self):sma1, sma2 = bt.ind.SMA(period=10), bt.ind.SMA(period=30)crossover = bt.ind.CrossOver(sma1, sma2)self.signal_add(bt.SIGNAL_LONG, crossover)cerebro = bt.Cerebro()cerebro.addstrategy(SmaCross)data0 = bt.feeds.YahooFinanceData(dataname='MSFT', fromdate=datetime(2011, 1, 1),todate=datetime(2012, 12, 31))cerebro.adddata(data0)cerebro.run()cerebro.plot()Including a full featured chart. Give it a try! This is included in the samplesas . Along it is  which can beparametrized from the command line.Features:Live Trading and backtesting platform written in Python.DocumentationThe blog:Read the full documentation at:List of built-in Indicators (122)Python 2/3 SupportInstallation is self-contained with no external dependencies (except if youwant to plot)From pypi:.. note:: The minimum matplotlib version is An example for IB Data Feeds/Trading:For other functionalities like: , , , checkthe dependencies in the documentation.From source:Version numberingX.Y.Z.I"
https://github.com/miguelgrinberg/python-socketio,Python Socket.IO server and client,"python-socketio Python implementation of the  realtime client and server.SponsorsThe following organizations are funding this project:  | |-|-Many individual sponsors also support this project through small ongoing contributions. Why not ?Version compatibilityThe Socket.IO protocol has been through a number of revisions, and some of theseintroduced backward incompatible changes, which means that the client and theserver must use compatible versions for everything to work.If you are using the Python client and server, the easiest way to ensure compatibilityis to use the same version of this package for the client and the server. If you areusing this package with a different client or server, then you must ensure theversions are compatible.The version compatibility chart below maps versions of this package to versionsof the JavaScript reference implementation and the versions of the Socket.IO andEngine.IO protocols.JavaScript Socket.IO version | Socket.IO protocol revision | Engine.IO protocol revision | python-socketio version-|-|-|-0.9.x | 1, 2 | 1, 2 | Not supported1.x and 2.x | 3, 4 | 3 | 4.x3.x and 4.x | 5 | 4 | 5.xResources"
https://github.com/cyrus-and/gdb-dashboard,Modular visual interface for GDB in Python,"GDB dashboardGDB dashboard is a standalone  file written using the  that enables a modular interface showing relevant information about the program being debugged. Its main goal is to reduce the number of GDB commands needed to inspect the status of current program thus allowing the developer to primarily focus on the control flow.QuickstartJust place  in your home directory, for example with:wget -P ~ https://git.io/.gdbinit
Optionally install  to enable syntax highlighting:pip install pygments
Then debug as usual, the dashboard will appear automatically every time the inferior program stops.Keep in mind that no GDB command has been redefined, instead all the features are available via the main  command (see ).Head to the  to learn how to perform the most important tasks."
https://github.com/scipy/scipy,SciPy library main repository,".. image:: https://raw.githubusercontent.com/scipy/scipy/main/doc/source/_static/logo.svg:target: https://scipy.org:width: 110:height: 110:align: left .. image:: https://img.shields.io/badge/powered%20by-NumFOCUS-orange.svg?style=flat&colorA=E1523D&colorB=007D8A:target: https://numfocus.org.. image:: https://img.shields.io/pypi/dm/scipy.svg?label=Pypi%20downloads:target: https://pypi.org/project/scipy/.. image:: https://img.shields.io/conda/dn/conda-forge/scipy.svg?label=Conda%20downloads:target: https://anaconda.org/conda-forge/scipy.. image:: https://img.shields.io/badge/stackoverflow-Ask%20questions-blue.svg:target: https://stackoverflow.com/questions/tagged/scipy.. image:: https://img.shields.io/badge/DOI-10.1038%2Fs41592--019--0686--2-blue:target: https://www.nature.com/articles/s41592-019-0686-2SciPy (pronounced ""Sigh Pie"") is an open-source software for mathematics,science, and engineering. It includes modules for statistics, optimization,integration, linear algebra, Fourier transforms, signal and image processing,ODE solvers, and more.SciPy is built to work withNumPy arrays, and provides many user-friendly and efficient numerical routines,such as routines for numerical integration and optimization. Together, theyrun on all popular operating systems, are quick to install, and are free ofcharge. NumPy and SciPy are easy to use, but powerful enough to be dependedupon by some of the world's leading scientists and engineers. If you need tomanipulate numbers on a computer and display or publish the results, giveSciPy a try!For the installation instructions, see __.Call for ContributionsWe appreciate and welcome contributions. Small improvements or fixes are always appreciated; issues labeled as ""goodfirst issue"" may be a good starting point. Have a look at __.Writing code isn’t the only way to contribute to SciPy. You can also:If you’re unsure where to start or how your skills fit in, reach out! You canask on the mailing list or here, on GitHub, by leaving acomment on a relevant issue that is already open.If you are new to contributing to open source, __ helps explain why, what,and how to get involved."
https://github.com/smicallef/spiderfoot,SpiderFoot automates OSINT for threat intelligence and mapping your attack surface.,"SpiderFoot is an open source intelligence (OSINT) automation tool. It integrates with just about every data source available and utilises a range of methods for data analysis, making that data easy to navigate. SpiderFoot has an embedded web-server for providing a clean and intuitive web-based interface but can also be used completely via the command-line.  It's written in Python 3 and MIT-licensed.FEATURESWANT MORE?Need more from SpiderFoot? Check out  for:See the full set of differences between SpiderFoot HX and the open source version .USESSpiderFoot can be used offensively (e.g. in a red team exercise or penetration test) for reconnaissance of your target or defensively to gather information about what you or your organisation might have exposed over the Internet.You can target the following entities in a SpiderFoot scan:SpiderFoot's 200+ modules feed each other in a publisher/subscriber model to ensure maximum data extraction to do things like:INSTALLING & RUNNINGTo install and run SpiderFoot, you need at least Python 3.7 and a number of Python libraries which you can install with . We recommend you install a packaged release since master will often have bleeding edge features and modules that aren't fully tested.Stable build (packaged release): wget https://github.com/smicallef/spiderfoot/archive/v4.0.tar.gz
 tar zxvf v4.0.tar.gz
 cd spiderfoot-4.0
 pip3 install -r requirements.txt
 python3 ./sf.py -l 127.0.0.1:5001
Development build (cloning git master branch): git clone https://github.com/smicallef/spiderfoot.git
 cd spiderfoot
 pip3 install -r requirements.txt
 python3 ./sf.py -l 127.0.0.1:5001
Check out the  and our  for more tutorials.COMMUNITYWhether you're a contributor, user or just curious about SpiderFoot and OSINT in general, we'd love to have you join our community! SpiderFoot now has a  for seeking help from the community, requesting features or just general OSINT chit-chat.WRITING CORRELATION RULESWe have a comprehensive write-up and reference of the correlation rule-set introduced in SpiderFoot 4.0 .Also take a look at the  file for a walk through. The existing  are also quite readable and good as starting points for additional rules.MODULES / INTEGRATIONSSpiderFoot has over 200 modules, most of which don't require API keys, and many of those that do require API keys have a free tier.| Name     | Description | Type   ||:---------| :-----------|:-------||Look up domain, phone and IP address information from AbstractAPI.|Tiered API|Check if a host/domain, IP address or netblock is malicious according to Abuse.ch.|Free API|Check if an IP address is malicious according to AbuseIPDB.com blacklist.|Tiered API|Check if a netblock or IP address is in the Abusix Mail Intelligence blacklist.|Tiered APIAccount Finder|Look for possible associated accounts on over 500 social and other websites such as Instagram, Reddit, etc.|Internal|Check if linked pages would be blocked by AdBlock Plus.|Tiered API|Check if a host would be blocked by AdGuard DNS.|Free API|Search Tor 'Ahmia' search engine for mentions of the target.|Free API|Check if an IP or netblock is malicious according to the AlienVault IP Reputation database.|Free API|Obtain information from AlienVault Open Threat Exchange (OTX)|Tiered API|Search for potential Amazon S3 buckets associated with the target and attempt to list their contents.|Free API|Search Apple iTunes for mobile apps.|Free API|Identifies historic versions of interesting files/pages from the Wayback Machine.|Free API|Queries ARIN registry for contact information.|Free API|Search for potential Azure blobs associated with the target and attempt to list their contents.|Free APIBase64 Decoder|Identify Base64-encoded strings in URLs, often revealing interesting hidden information.|Internal|Obtain network information from BGPView API.|Free APIBinary String Extractor|Attempt to identify strings in binary content.|Internal|Obtain information from BinaryEdge.io Internet scanning systems, including breaches, vulnerabilities, torrents and passive DNS.|Tiered API|Search Bing for hosts sharing the same IP.|Tiered API|Obtain information from bing to identify sub-domains and links.|Tiered APIBitcoin Finder|Identify bitcoin addresses in scraped webpages.|Internal|Check for Bitcoin addresses against the Bitcoin Who's Who database of suspect/malicious addresses.|Tiered API|Check Bitcoin addresses against the bitcoinabuse.com database of suspect/malicious addresses.|Free API|Queries blockchain.info to find the balance of identified bitcoin wallet addresses.|Free API|Check if a netblock or IP is malicious according to blocklist.de.|Free API|Searches BotScout.com's database of spam-bot IP addresses and e-mail addresses.|Tiered API|Check if a domain is malicious according to botvrij.eu.|Free API|Query BuiltWith.com's Domain API for information about your target's web technology stack, e-mail addresses and more.|Tiered API|Queries the C99 API which offers various data (geo location, proxy detection, phone lookup, etc).|Commercial API|Lookup US phone number location and reputation information.|Free API|Obtain host information from Censys.io.|Tiered API|Gather hostnames from historical certificates in crt.sh.|Free API|Gather information about SSL certificates from SSLMate CertSpotter API.|Tiered API|Check if a netblock or IP address is malicious according to Collective Intelligence Network Security (CINS) Army list.|Free API|Obtain information from CIRCL.LU's Passive DNS and Passive SSL databases.|Free API|Check if a host would be blocked by CleanBrowsing.org DNS content filters.|Free API|Check if a netblock or IP address is on CleanTalk.org's spam IP list.|Free API|Check for names, addresses, domains and more based on lookups of e-mail addresses on clearbit.com.|Tiered API|Check if a host would be blocked by CloudFlare DNS.|Free API|Check if a domain appears on CoinBlocker lists.|Free API|Searches for URLs found through CommonCrawl.org.|Free API|Check if a host would be blocked by Comodo Secure DNS.|Tiered APICompany Name Extractor|Identify company names in any obtained data.|InternalCookie Extractor|Extract Cookies from HTTP headers.|InternalCountry Name Extractor|Identify country names in any obtained data.|InternalCredit Card Number Extractor|Identify Credit Card Numbers in any data|Internal|Search Crobat API for subdomains.|Free APICross-Referencer|Identify whether other domains are associated ('Affiliates') of the target by looking for links back to the target site(s).|Internal|Search CRXcavator for Chrome extensions.|Free APICustom Threat Feed|Check if a host/domain, netblock, ASN or IP is malicious according to your custom feed.|Internal|Check if a host/domain or IP address is malicious according to CyberCrime-Tracker.net.|Free API|Check whether an email is disposable|Free API|Gather breach data from Dehashed API.|Commercial API|Search for potential Digital Ocean Spaces associated with the target and attempt to list their contents.|Free APIDNS Brute-forcer|Attempts to identify hostnames through brute-forcing common names and iterations.|InternalDNS Common SRV|Attempts to identify hostnames through brute-forcing common DNS SRV records.|Internal|Check if a host would be blocked by DNS for Family.|Free APIDNS Look-aside|Attempt to reverse-resolve the IP addresses next to your target to see if they are related.|InternalDNS Raw Records|Retrieves raw DNS records such as MX, TXT and others.|InternalDNS Resolver|Resolves hosts and IP addresses identified, also extracted from raw content.|InternalDNS Zone Transfer|Attempts to perform a full DNS zone transfer.|Internal|Query FarSight's DNSDB for historical and passive DNS data.|Tiered API|Passive subdomain enumeration using HackerTarget's DNSDumpster|Free API|Obtain Passive DNS information from Rapid7 Sonar Project using DNSGrep API.|Free API|Query the DroneBL database for open relays, open proxies, vulnerable servers, etc.|Free API|Query DuckDuckGo's API for descriptive information about your target.|Free APIE-Mail Address Extractor|Identify e-mail addresses in any obtained data.|Internal|Search EmailCrawlr for email addresses and phone numbers associated with a domain.|Tiered API|Look up e-mail addresses on email-format.com.|Free API|Search EmailRep.io for email address reputation.|Tiered API|Check if a netblock or IP address is malicious according to EmergingThreats.net.|Free APIError String Extractor|Identify common error messages in content like SQL errors, etc.|InternalEthereum Address Extractor|Identify ethereum addresses in scraped webpages.|Internal|Queries etherscan.io to find the balance of identified ethereum wallet addresses.|Free APIFile Metadata Extractor|Extracts meta data from documents and images.|Internal|Search Flickr for domains, URLs and emails related to the specified domain.|Free API|Look up IP address information from Focsec.|Tiered API|Check if an IP address is malicious according to FortiGuard Antispam.|Free API|Obtain threat information from Fraudguard.io|Tiered API|Obtain network information from F-Secure Riddler.io API.|Commercial API|Gather domain and e-mail information from FullContact.com API.|Tiered API|Identify domain attack surface using FullHunt API.|Tiered API|Identify associated public code repositories on Github.|Free API|Look up company information from Global Legal Entity Identifier Foundation (GLEIF).|Tiered API|Identifies potential physical addresses and latitude/longitude coordinates.|Tiered API|Search for potential Google Object Storage buckets associated with the target and attempt to list their contents.|Free API|Check if the URL is included on any of the Safe Browsing lists.|Free API|Obtain information from the Google Custom Search API to identify sub-domains and links.|Tiered API|Retrieve user information from Gravatar API.|Free API|Find bucket names matching the keyword extracted from a domain from Grayhat API.|Tiered API|Check if a netblock or IP address is malicious according to greensnow.co.|Free API|Search grep.app API for links and emails related to the specified domain.|Free API|Obtain IP enrichment data from GreyNoise Community API|Tiered API|Obtain IP enrichment data from GreyNoise|Tiered API|Check external vulnerability scanning/reporting service h1.nobbd.de to see if the target is listed.|Free API|Search HackerTarget.com for hosts sharing the same IP.|Free APIHash Extractor|Identify MD5 and SHA hashes in web content, files and more.|Internal|Check HaveIBeenPwned.com for hacked e-mail addresses identified in breaches.|Commercial APIHosting Provider Identifier|Find out if any IP addresses identified fall within known 3rd party hosting ranges, e.g. Amazon, Azure, etc.|Internal|Obtain information about domain names from host.io.|Tiered APIHuman Name Extractor|Attempt to identify human names in fetched content.|Internal|Check for e-mail addresses and names on hunter.io.|Tiered API|Search Hybrid Analysis for domains and URLs related to the target.|Free APIIBAN Number Extractor|Identify International Bank Account Numbers (IBANs) in any data.|Internal|Check iknowwhatyoudownload.com for IP addresses that have been using torrents.|Tiered API|Obtain information from IntelligenceX about identified IP addresses, domains, e-mail addresses and phone numbers.|Tiered APIInteresting File Finder|Identifies potential files of interest, e.g. office documents, zip files.|Internal|Check if an IP address is malicious according to SANS ISC.|Free API|Queries ipapi.co to identify geolocation of IP Addresses using ipapi.co API|Tiered API|Queries ipapi.com to identify geolocation of IP Addresses using ipapi.com API|Tiered API|Identifies the physical location of IP addresses identified using ipinfo.io.|Tiered API|Determine if target is malicious using IPQualityScore API|Tiered API|Query the ipregistry.co database for reputation and geo-location.|Tiered API|Identifies the physical location of IP addresses identified using ipstack.com.|Tiered API|Search JsonWHOIS.com for WHOIS records associated with a domain.|Tiered APIJunk File Finder|Looks for old/temporary and other similar files.|Internal|Obtain additional information about domain names and identified usernames.|Free API|Search Koodous for mobile apps.|Tiered API|Search LeakIX for host data leaks, open ports, software and geoip.|Free API|Searches Leak-Lookup.com's database of breaches.|Free API|Obtain information about any malicious activities involving IP addresses|Free API|Searches malwarepatrol.net's database of malicious URLs/IPs.|Tiered API|Search MetaDefender API for IP address and domain IP reputation.|Tiered API|Obtain Passive DNS information from PassiveDNS.mnemonic.no.|Free API|Check if an IP address is an open proxy according to multiproxy.org open proxy list.|Free API|Gather username and location from MySpace.com profiles.|Free API|Check whether an email is disposable|Tiered API|Search NetworksDB.io API for IP address and domain information.|Tiered API|Search NeutrinoAPI for phone location information, IP address information, and host reputation.|Tiered API|Lookup phone number location and carrier information from numverify.com.|Tiered API|Search Tor 'Onion City' search engine for mentions of the target domain using Google Custom Search.|Free API|Search Tor onionsearchengine.com for mentions of the target domain.|Free API|Check Onyphe data (threat list, geo-location, pastries, vulnerabilities)  about a given IP.|Tiered API|Check external vulnerability scanning/reporting service openbugbounty.org to see if the target is listed.|Free API|Look up company information from OpenCorporates.|Tiered API|Check if a host would be blocked by OpenDNS.|Free API|Resolves host names in the OpenNIC alternative DNS system.|Free API|Check if a host/domain is malicious according to OpenPhish.com.|Free API|Retrieves latitude/longitude coordinates for physical addresses from OpenStreetMap API.|Free APIPage Information|Obtain information about web pages (do they take passwords, do they contain forms, etc.)|Internal|PasteBin search (via Google Search API) to identify related content.|Tiered APIPGP Key Servers|Look up domains and e-mail addresses in PGP public key servers.|Internal|Check if a netblock or IP address is malicious according to PhishStats.|Free API|Check if a host/domain is malicious according to PhishTank.|Free APIPhone Number Extractor|Identify phone numbers in scraped webpages.|InternalPort Scanner - TCP|Scans for commonly open TCP ports on Internet-facing systems.|Internal|Query the Project Honey Pot database for IP addresses.|Free API|Search for hosts/subdomains using chaos.projectdiscovery.io|Commercial API|Check psbdmp.cc (PasteBin Dump) for potentially hacked e-mails and domains.|Free API|Obtain information from Pulsedive's API.|Tiered API|Check the QOMPLX punkspider.io service to see if the target is listed as vulnerable.|Free API|Check if a host would be blocked by Quad9 DNS.|Free API|Reverse Whois lookups using reversewhois.io.|Free API|Queries the RIPE registry (includes ARIN data) to identify netblocks and other info.|Free API|Obtain information from RiskIQ's (formerly PassiveTotal) Passive DNS and Passive SSL databases.|Tiered API|Search Robtex.com for hosts sharing the same IP.|Free API|Search searchcode for code repositories mentioning the target domain.|Free API|Obtain Passive DNS and other information from SecurityTrails|Tiered API|Queries seon.io to gather intelligence about IP Addresses, email addresses, and phone numbers|Commercial API|Obtain information from SHODAN about identified IP addresses.|Tiered APISimilar Domain Finder|Search various sources to identify similar looking domain names, for instance squatted domains.|Internal|Look up e-mail addresses on Skymem.|Free API|Gather name and location from SlideShare profiles.|Free API|Gather available email IDs from identified domains|Tiered API|Queries SocialLinks.io to gather intelligence from social media platforms and dark web.|Commercial API|Tries to discover the social media profiles for human names identified.|Tiered APISocial Network Identifier|Identify presence on social media networks such as LinkedIn, Twitter and others.|Internal|Query the SORBS database for open relays, open proxies, vulnerable servers, etc.|Free API|Check if a netblock or IP address is in the SpamCop database.|Free API|Check if a netblock or IP address is in the Spamhaus Zen database.|Free API|Obtain information about any malicious activities involving IP addresses found|Commercial API|Search SpyOnWeb for hosts sharing the same IP address, Google Analytics code, or Google Adsense code.|Tiered APISSL Certificate Analyzer|Gather information about SSL certificates used by the target's HTTPS sites.|Internal|Search StackOverflow for any mentions of a target domain. Returns potentially related information.|Tiered API|Check if a domain is malicious (malware or adware) according to Steven Black Hosts list.|Free APIStrange Header Identifier|Obtain non-standard HTTP headers returned by web servers.|InternalSubdomain Takeover Checker|Check if affiliated subdomains are vulnerable to takeover.|Internal|Passive subdomain enumeration using Sublist3r's API|Free API|Check if a netblock, IP address or domain is in the SURBL blacklist.|Free API|Check if a netblock or IP address is malicious according to TalosIntelligence.|Free API|Obtain phone number type from TextMagic API|Tiered API|Check if an IP address is malicious according to ThreatJammer.com|Tiered API|Obtain information from ThreatCrowd about identified IP addresses, domains and e-mail addresses.|Free API|Check if an IP address is malicious according to ThreatFox.|Free API|Obtain information from ThreatMiner's database for passive DNS and threat intelligence.|Free APITLD Searcher|Search all Internet TLDs for domains with the same name as the target (this can be very slow.)|Internal|Identify what Content Management System (CMS) might be used.|Tool|Identify bit-squatting, typo and other similar domains to the target using a local DNSTwist installation.|Tool|Scans for open NETBIOS nameservers on your target's network.|Tool|Identify what Operating System might be used.|Tool|Fast and customisable vulnerability scanner.|Tool|Fast scanner to find publicly exposed SNMP services.|Tool|Scanner detecting the use of JavaScript libraries with known vulnerabilities|Tool|Finds file leaks and other security problems on HTTP servers.|Tool|Identify various TLS/SSL weaknesses, including Heartbleed, CRIME and ROBOT.|Tool|Searches through git repositories for high entropy strings and secrets, digging deep into commit history.|Tool|Identify what web application firewall (WAF) is in use on the specified website.|Tool|Wappalyzer indentifies technologies on websites.|Tool|Identify what software is in use on the specified website.|Tool|Check if an IP adddress or netblock appears on the Tor Metrics exit node list.|Free API|Search Tor 'TORCH' search engine for mentions of the target domain.|Free API|Queries Trashpanda to gather intelligence about mentions of target in pastesites|Tiered API|Check whether an email is disposable|Free API|Obtain information from Twilio about phone numbers. Ensure you have the Caller Name add-on installed in Twilio.|Tiered API|Gather name and location from Twitter profiles.|Free API|Check if a netblock or IP address is in the UCEPROTECT database.|Free API|Search URLScan.io cache for domain information.|Free API|Gather user information from Venmo API.|Free API|Identify co-hosted websites and perform reverse Whois lookups using ViewDNS.info.|Tiered API|Obtain information from VirusTotal about identified IP addresses.|Tiered API|Check if an IP address or netblock is malicious according to VoIP Blacklist (VoIPBL).|Free API|Check if a domain or IP address is malicious according to VXVault.net.|Free APIWeb Analytics Extractor|Identify web analytics IDs in scraped webpages and DNS TXT records.|InternalWeb Framework Identifier|Identify the usage of popular web frameworks like jQuery, YUI and others.|InternalWeb Server Identifier|Obtain web server banners to identify versions of web servers being used.|InternalWeb Spider|Spidering of web-pages to extract content for searching.|Internal|Check web technology using WhatCMS.org API.|Tiered API|Reverse Whois lookups using Whoisology.com.|Commercial APIWhois|Perform a WHOIS look-up on domain names and owned netblocks.|Internal|Reverse Whois lookups using Whoxy.com.|Commercial API|Query WiGLE to identify nearby WiFi access points.|Free API|Search Wikileaks for mentions of domain names and e-mail addresses.|Free API|Identify edits to Wikipedia articles made from a given IP address or username.|Free API|Obtain IP reputation and passive DNS information from IBM X-Force Exchange.|Tiered API|Check if a host would be blocked by Yandex DNS.|Free API|Query the Zetalytics database for hosts on your target domain(s).|Tiered API|Search ZoneFiles.io Domain query API for domain information.|Tiered API|Check if a hostname/domain appears on the zone-h.org 'special defacements' RSS feed.|Free APIDOCUMENTATIONRead more at the , including more complete documentation, blog posts with tutorials/guides, plus information about .Latest updates announced on ."
https://github.com/pytest-dev/pytest,"The pytest framework makes it easy to write small tests, yet scales to support complex functional testing",".. image:: https://github.com/pytest-dev/pytest/raw/main/doc/en/img/pytest_logo_curves.svg:target: https://docs.pytest.org/en/stable/:align: center:height: 200:alt: pytest.. image:: https://img.shields.io/pypi/v/pytest.svg:target: https://pypi.org/project/pytest/.. image:: https://img.shields.io/conda/vn/conda-forge/pytest.svg:target: https://anaconda.org/conda-forge/pytest.. image:: https://img.shields.io/pypi/pyversions/pytest.svg:target: https://pypi.org/project/pytest/.. image:: https://codecov.io/gh/pytest-dev/pytest/branch/main/graph/badge.svg:target: https://codecov.io/gh/pytest-dev/pytest:alt: Code coverage Status.. image:: https://github.com/pytest-dev/pytest/workflows/test/badge.svg:target: https://github.com/pytest-dev/pytest/actions?query=workflow%3Atest.. image:: https://results.pre-commit.ci/badge/github/pytest-dev/pytest/main.svg:target: https://results.pre-commit.ci/latest/github/pytest-dev/pytest/main:alt: pre-commit.ci status.. image:: https://img.shields.io/badge/code%20style-black-000000.svg:target: https://github.com/psf/black.. image:: https://www.codetriage.com/pytest-dev/pytest/badges/users.svg:target: https://www.codetriage.com/pytest-dev/pytest.. image:: https://readthedocs.org/projects/pytest/badge/?version=latest:target: https://pytest.readthedocs.io/en/latest/?badge=latest:alt: Documentation Status.. image:: https://img.shields.io/badge/Discord-pytest--dev-blue:target: https://discord.com/invite/pytest-dev:alt: Discord.. image:: https://img.shields.io/badge/Libera%20chat-%23pytest-orange:target: https://web.libera.chat/#pytest:alt: Libera chatThe  framework makes it easy to write small tests, yetscales to support complex functional testing for applications and libraries.An example of a simple test:.. code-block:: python# content of test_sample.py
def inc(x):
    return x + 1


def test_answer():
    assert inc(3) == 5
To execute it::$ pytest
============================= test session starts =============================
collected 1 items

test_sample.py F

================================== FAILURES ===================================
_________________________________ test_answer _________________________________

    def test_answer():
>       assert inc(3) == 5
E       assert 4 == 5
E        +  where 4 = inc(3)

test_sample.py:5: AssertionError
========================== 1 failed in 0.04 seconds ===========================
Due to 's detailed assertion introspection, only plain  statements are used. See _ for more examples.FeaturesDocumentationFor full documentation, including installation, tutorials and PDF documents, please see https://docs.pytest.org/en/stable/.Bugs/RequestsPlease use the _ to submit bugs or request features.ChangelogConsult the __ page for fixes and enhancements of each version.Support pytest_ is an online funding platform for open and transparent communities.It provides tools to raise money and share your finances in full transparency.It is the platform of choice for individuals and companies that want to make one-time ormonthly donations directly to the project.See more details in the _... _Open Collective: https://opencollective.com.. _pytest collective: https://opencollective.com/pytestpytest for enterpriseAvailable as part of the Tidelift Subscription.The maintainers of pytest and thousands of other packages are working with Tidelift to deliver commercial support andmaintenance for the open source dependencies you use to build your applications.Save time, reduce risk, and improve code health, while paying the maintainers of the exact dependencies you use._Security^^^^^^^^pytest has never been associated with a security vulnerability, but in any case, to report asecurity vulnerability please use the _.Tidelift will coordinate the fix and disclosure.LicenseCopyright Holger Krekel and others, 2004.Distributed under the terms of the _ license, pytest is free and open source software... _: https://github.com/pytest-dev/pytest/blob/main/LICENSE"
https://github.com/chrippa/livestreamer,"Command-line utility that extracts streams from various services and pipes them into a video player of choice. No longer maintained, use streamlink or youtube-dl instead.","Livestreamer.. image:: http://img.shields.io/pypi/v/livestreamer.svg?style=flat-square:target: https://pypi.python.org/pypi/livestreamer.. image:: http://img.shields.io/pypi/dm/livestreamer.svg?style=flat-square:target: https://pypi.python.org/pypi/livestreamer.. image:: http://img.shields.io/travis/chrippa/livestreamer.svg?style=flat-square:target: http://travis-ci.org/chrippa/livestreamerOverviewLivestreamer is a _ that pipes video streamsfrom various services into a video player, such as . available for developers who want accessto the video stream data... _command-line utility: http://docs.livestreamer.io/cli.html.. _API: http://docs.livestreamer.io/api_guide.htmlFeaturesLivestreamer is built upon a plugin system which allows support for new servicesto be easily added. Currently most of the big streaming services are supported,such as:... and many more. A full list of plugins currently included can be foundon the _ page... _Plugins: http://docs.livestreamer.io/plugin_matrix.htmlQuickstartThe default behaviour of Livestreamer is to playback a stream in the defaultplayer (_)... sourcecode:: console# pip install livestreamer
$ livestreamer twitch.tv/day9tv best
[cli][info] Found matching plugin twitch for URL twitch.tv/day9tv
[cli][info] Opening stream: source
[cli][info] Starting player: vlc
For more in-depth usage and install instructions see the _... _User guide: http://docs.livestreamer.io/index.html#user-guideRelated softwareFeel free to add any Livestreamer related things tothe _.ContributingIf you wish to report a bug or contribute code, please take a lookat _ first."
https://github.com/HJLebbink/asm-dude,Visual Studio extension for assembly syntax highlighting and code completion in assembly files and the disassembly window,"AsmDude2AsmDude2 represents a natural evolution from its predecessor, AsmDude. While AsmDude served as asingle, all-encompassing plugin for VS2015/17/19, providing support for Assembly source code,AsmDude2 is built around a Language Server Protocol ()and a lightweight Visual Studio extension (for VS2022), drawing its functionality from this LSP.Transitioning from a Visual Studio 2019 extension to one compatible with Visual Studio 2022 wasn'tstraightforward. Many of the features from the older AsmDude have yet to be ported, and some maynever be. See the list of known issues and things still todo.This extension can be found in the or download latest installer . FeaturesSyntax highlighting and Code FoldingAsmDude2 offers support for the following architectures: the instruction sets of x86 and x64, as well asSSE, AVX, AVX2, Xeon-Phi (Knights Corner), and AVX-512 instructions.Most of the commonly used Masm directives covered, along with a selection of Nasm directives.Code DescriptionsWhen you hover over a mnemonic, you may receive a pop-up with descriptions. These descriptionscan be modified and added by updating the AsmDudeData.xml file, which will be located alongsidethe installed plugin binaries (.vsix). Finding the directory where plugins are installed can bea bit challenging; you might want to try a location like C:\Users\AppData\Local\Microsoft\VisualStudio\17.0\Extensions\AsmDude2\2.0.0.1\Server.I kindly encourage you to share any updates you make.Please note that the formatting of the pop-up does not display Markdown (see the known issues)Code CompletionWhile typing text, the completion lists will be refined to display the relevantlanguage keywords. This applies to all keywords. However, please be aware that code suggestionsmay not be flawless at this stage; only valid code completions should be proposed.Signature HelpSignature Help, also referred to as Parameter Info, presents the method's signature in a tooltip whena user enters the character marking the start of the parameter list (e.g., in C++, an opening parenthesis).As the user types a parameter and a parameter separator (usually a comma),the tooltip is refreshed to display the next parameter in bold.Disassembly Window in VSSyntax highlighting in the disassembly window. No QuickInfo tooltips yet (see known issues)Where is the Source (Are you sure this is not a honeypot?!)If you're reading this, you're probably an assembly programmer. However, if you're still interestedin some C#, or you're just being cautious, you can run the extension from the source code. To dothat, you'll need to have the Visual Studio 2022 SDK installed. To run the extension, press F5 orselect the 'Debug > Start Debugging' option from the menu. This will launch a new instance of VisualStudio under the experimental environment.Things For a next releaseFeature Requests: (desire something - let me know)Updates:Known issues"
https://github.com/spesmilo/electrum,Electrum Bitcoin Wallet,"Electrum - Lightweight Bitcoin clientLicence: MIT Licence
Author: Thomas Voegtlin
Language: Python (>= 3.8)
Homepage: https://electrum.org/
Getting started(If you've come here looking to simply run Electrum,Electrum itself is pure Python, and so are most of the required dependencies,but not everything. The following sections describe how to run from source, but hereis a TL;DR:$ sudo apt-get install libsecp256k1-dev
$ python3 -m pip install --user "".[gui,crypto]""
Not pure-python dependenciesIf you want to use the Qt interface, install the Qt dependencies:$ sudo apt-get install python3-pyqt5
For elliptic curve operations,is a required dependency:$ sudo apt-get install libsecp256k1-dev
Alternatively, when running from a cloned repository, a script is provided to buildlibsecp256k1 yourself:$ sudo apt-get install automake libtool
$ ./contrib/make_libsecp256k1.sh
Due to the need for fast symmetric ciphers, is required.Install from your package manager (or from pip):$ sudo apt-get install python3-cryptography
If you would like hardware wallet support,.Running from tar.gzIf you downloaded the official package (tar.gz), you can runElectrum from its root directory without installing it on yoursystem; all the pure python dependencies are included in the 'packages'directory. To run Electrum from its root directory, just do:$ ./run_electrum
You can also install Electrum on your system, by running this command:$ sudo apt-get install python3-setuptools python3-pip
$ python3 -m pip install --user .
This will download and install the Python dependencies used byElectrum instead of using the 'packages' directory.It will also place an executable named  in ,so make sure that is on your  variable.Development version (git clone)(For OS-specific instructions, see Check out the code from GitHub:$ git clone https://github.com/spesmilo/electrum.git
$ cd electrum
$ git submodule update --init
Run install (this should install dependencies):$ python3 -m pip install --user -e .
Create translations (optional):$ sudo apt-get install python3-requests gettext qttools5-dev-tools
$ ./contrib/pull_locale
Finally, to start Electrum:$ ./run_electrum
Run testsRun unit tests with :$ pytest electrum/tests -v
To run a single file, specify it directly like this:$ pytest electrum/tests/test_bitcoin.py -v
Creating BinariesContributingAny help testing the software, reporting or fixing bugs, reviewing pull requestsand recent changes, writing tests, or helping with outstanding issues is very welcome.Implementing new features, or improving/refactoring the codebase, is of coursealso welcome, but to avoid wasted effort, especially for larger changes,we encourage discussing these on the issue tracker or IRC first.Besides ,most communication about Electrum development happens on IRC, in the channel on Libera Chat. The easiest way to participate on IRC iswith the web client, ."
https://github.com/matrix-org/synapse,Synapse: Matrix homeserver written in Python/Twisted.,"=========================================================================Synapse |support| |development| |documentation| |license| |pypi| |python|Synapse is an open-source _ homeserver written andmaintained by the Matrix.org Foundation. We began rapid development in 2014,reaching v1.0.0 in 2019. Development on Synapse and the Matrix protocol itself continuesin earnest today.Briefly, Matrix is an open standard for communications on the internet, supportingfederation, encryption and VoIP. Matrix.org has more to say about the , and the  describes the technical details... contents::Installing and configurationThe Synapse documentation describes . We recommend using or _... _federation:Synapse has a variety of _which can be used to customise its behaviour after installation.There are additional details on how to _... _reverse-proxy:Using a reverse proxy with SynapseIt is recommended to put a reverse proxy such as,,, or_ in front of Synapse. One advantage ofdoing so is that it means that you can expose the default https port (443) toMatrix clients without needing to run Synapse with root privileges.For information on configuring one, see _.Upgrading an existing SynapseThe instructions for upgrading Synapse are in _.Please check these instructions as upgrading may require extra steps for someversions of Synapse... _the upgrade notes: https://matrix-org.github.io/synapse/develop/upgrade.htmlPlatform dependenciesSynapse uses a number of platform dependencies such as Python and PostgreSQL,and aims to follow supported upstream versions. See the_for more details.Security noteMatrix serves raw, user-supplied data in some APIs -- specifically the _... _content repository endpoints: https://matrix.org/docs/spec/client_server/latest.html#get-matrix-media-r0-download-servername-mediaidWhilst we make a reasonable effort to mitigate against XSS attacks (forinstance, by using _), a Matrix homeserver should not be hosted on adomain hosting other web applications. This especially applies to sharingthe domain with Matrix web clients and other sensitive applications likewebmail. Seehttps://developer.github.com/changes/2014-04-25-user-content-security for moreinformation... _CSP: https://github.com/matrix-org/synapse/pull/1021Ideally, the homeserver should not simply be on a different subdomain, but ona completely different _ (also known as top-level site oreTLD+1). This is because _ are still possible as long as the twoapplications share the same registered domain... _registered domain: https://tools.ietf.org/html/draft-ietf-httpbis-rfc6265bis-03#section-2.3.. _some attacks: https://en.wikipedia.org/wiki/Session_fixation#Attacks_using_cross-subdomain_cookieTo illustrate this with an example, if your Element Web or other sensitive webapplication is hosted on , you should ideally host Synapse on. Some amount of protection is offered by hosting on instead, so this is also acceptable in some scenarios.However, you should not host your Synapse on .Note that all of the above refers exclusively to the domain used in Synapse's setting. In particular, it has no bearing on the domainmentioned in MXIDs hosted on that server.Following this advice ensures that even if an XSS is found in Synapse, theimpact to other applications will be minimal.Testing a new installationThe easiest way to try out your new Synapse installation is by connecting to itfrom a web client.Unless you are running a test instance of Synapse on your local machine, ingeneral, you will need to enable TLS support before you can successfullyconnect from a client: see_.An easy way to get started is to login or register via Element athttps://app.element.io/#/login or https://app.element.io/#/register respectively.You will need to change the server you are logging into from and instead specify a Homeserver URL of (or just  if you are using a reverse proxy).If you prefer to use another client, refer to our_.If all goes well you should at least be able to log in, create a room, andstart sending messages... _:Registering a new user from a clientBy default, registration of new users via Matrix clients is disabled. To enableit:We strongly recommend using a CAPTCHA, particularly if your homeserver is exposed tothe public internet. Without it, anyone can freely register accounts on your homeserver.This can be exploited by attackers to create spambots targetting the rest of the Matrixfederation.Your new user name will be formed partly from the , and partlyfrom a localpart you specify when you create the account. Your name will takethe form of::@localpart:my.domain.name
(pronounced ""at localpart on my dot domain dot name"").As when logging in, you will need to specify a ""Custom server"".  Specify yourdesired  in the 'User name' box.Troubleshooting and supportThe _includes tips on dealing with some common problems. For more details, see_.For additional support installing or managing Synapse, please ask in the communitysupport room |room|_ (from a matrix.org account if necessary). We do not use GitHubissues for support requests, only for bug reports and feature requests... |room| replace:: .. _room: https://matrix.to/#/#synapse:matrix.org.. |docs| replace:: .. _docs: docsIdentity ServersIdentity servers have the job of mapping email addresses and other 3rd PartyIDs (3PIDs) to Matrix user IDs, as well as verifying the ownership of 3PIDsbefore creating that mapping.They are not where accounts or credentials are stored - these live on homeThis process is very security-sensitive, as there is obvious risk of spam if itis too easy to sign up for Matrix accounts or harvest 3PID data. In the longerterm, we hope to create a decentralised system to manage it (), but in the meantime,, whose roleis purely to authenticate and track 3PID logins and publish end-user publickeys.You can host your own copy of Sydent, but this will prevent you reaching otherusers in the Matrix ecosystem via their email address, and prevent them findingyou. We therefore recommend that you use one of the centralised identity serversat  or  for now.To reiterate: the Identity server will only be used if you choose to associatean email address with your account, or send an invite to another user via theiremail address.DevelopmentWe welcome contributions to Synapse from the community!The best place to get started is our., which includesinformation for Synapse developers as well as Synapse administrators.Developers might be particularly interested in:Alongside all that, join our developer community on Matrix:_, featuring real humans!.. |support| image:: https://img.shields.io/matrix/synapse:matrix.org?label=support&logo=matrix:alt: (get support on #synapse:matrix.org):target: https://matrix.to/#/#synapse:matrix.org.. |development| image:: https://img.shields.io/matrix/synapse-dev:matrix.org?label=development&logo=matrix:alt: (discuss development on #synapse-dev:matrix.org):target: https://matrix.to/#/#synapse-dev:matrix.org.. |documentation| image:: https://img.shields.io/badge/documentation-%E2%9C%93-success:alt: (Rendered documentation on GitHub Pages):target: https://matrix-org.github.io/synapse/latest/.. |license| image:: https://img.shields.io/github/license/matrix-org/synapse:alt: (check license in LICENSE file):target: LICENSE.. |pypi| image:: https://img.shields.io/pypi/v/matrix-synapse:alt: (latest version released on PyPi):target: https://pypi.org/project/matrix-synapse.. |python| image:: https://img.shields.io/pypi/pyversions/matrix-synapse:alt: (supported python versions):target: https://pypi.org/project/matrix-synapse"
https://github.com/pyqtgraph/pyqtgraph,Fast data visualization and GUI tools for scientific / engineering applications,"PyQtGraphA pure-Python graphics library for PyQt5/PyQt6/PySide2/PySide6Copyright 2023 PyQtGraph developersPyQtGraph is intended for use in mathematics / scientific / engineering applications.Despite being written entirely in python, the library is fast due to itsheavy leverage of numpy for number crunching, Qt's GraphicsView framework for2D display, and OpenGL for 3D display.RequirementsPyQtGraph has adopted .This project supports:Currently this means:Optional added functionalitiesThrough 3rd part libraries, additional functionality may be added to PyQtGraph, see the table below for a summary.| Library        | Added functionality ||----------------|-||       |  Image processing through  Data array filtering through   ||    |  3D graphics  Faster image processing Note: on macOS Big Sur only works with python 3.9.1+ ||        |  Export in hdf5 format  ||    |  Add a collection of perceptually uniform colormaps  ||  |  Export of PlotItem in matplotlib figure  Add matplotlib collection of colormaps  ||        |  CUDA-enhanced image processing  Note: On Windows, CUDA toolkit must be >= 11.1  ||       |  Faster image processing  || |  Jupyter Notebook support      |SupportInstallation MethodsDocumentationThe official documentation lives at The easiest way to learn PyQtGraph is to browse through the examples; run  to launch the examples application.Used ByHere is a partial listing of some of the applications that make use of PyQtGraph!Do you use PyQtGraph in your own project, and want to add it to the list?  Submit a pull request to update this listing!"
https://github.com/commaai/research,"dataset and code for 2016 paper ""Learning a Driving Simulator""","the people's commathe paperthe comma.ai driving dataset7 and a quarter hours of largely highway driving. Enough to train what we had in .ExamplesWe present two Machine Learning Experiments to showpossible ways to use this dataset:Downloading the dataset./get_data.sh
or get it at 45 GB compressed, 80 GB uncompresseddog/2016-01-30--11-24-51 (7.7G)
dog/2016-01-30--13-46-00 (8.5G)
dog/2016-01-31--19-19-25 (3.0G)
dog/2016-02-02--10-16-58 (8.1G)
dog/2016-02-08--14-56-28 (3.9G)
dog/2016-02-11--21-32-47 (13G)
dog/2016-03-29--10-50-20 (12G)
emily/2016-04-21--14-48-08 (4.4G)
emily/2016-05-12--22-20-00 (7.5G)
frodo/2016-06-02--21-39-29 (6.5G)
frodo/2016-06-08--11-46-01 (2.7G)
Dataset referenced on this page is copyrighted by comma.ai and published under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0 License. This means that you must attribute the work in the manner specified by the authors, you may not use this work for commercial purposes and if you alter, transform, or build upon this work, you may distribute the resulting work only under the same license.Dataset structureThe dataset consists of 10 videos clips of variable size recorded at 20 Hzwith a camera mounted on the windshield of an Acura ILX 2016. In parallel to the videoswe also recorded some measurements such as car's speed, acceleration,steering angle, GPS coordinates, gyroscope angles. See the full  list .These measurements are transformed into a uniform 100 Hz time base.The dataset folder structure is the following:+-- dataset
|   +-- camera
|   |   +-- 2016-04-21--14-48-08
|   |   ...
|   +-- log
|   |   +-- 2016-04-21--14-48-08
|   |   ...
All the files come in hdf5 format and are named with the time they were recorded.The camera dataset has shape  and  type.One of the  hdf5-datasets is called  and addresses the alignmentbetween camera frames and the other measurements.RequirementsHiringWant a job at ?Show us amazing stuff on this datasetCreditsRiccardo Biasini, George Hotz, Sam Khalandovsky, Eder Santana, and Niel van der Westhuizen"
https://github.com/jpadilla/pyjwt,JSON Web Token implementation in Python,"PyJWT.. image:: https://github.com/jpadilla/pyjwt/workflows/CI/badge.svg:target: https://github.com/jpadilla/pyjwt/actions?query=workflow%3ACI.. image:: https://img.shields.io/pypi/v/pyjwt.svg:target: https://pypi.python.org/pypi/pyjwt.. image:: https://codecov.io/gh/jpadilla/pyjwt/branch/master/graph/badge.svg:target: https://codecov.io/gh/jpadilla/pyjwt.. image:: https://readthedocs.org/projects/pyjwt/badge/?version=stable:target: https://pyjwt.readthedocs.io/en/stable/A Python implementation of . Original implementation was written by .Sponsor+--------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+| |auth0-logo| | If you want to quickly add secure token-based authentication to Python projects, feel free to check Auth0's Python SDK and free plan at _. |+--------------+-----------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+.. |auth0-logo| image:: https://user-images.githubusercontent.com/83319/31722733-de95bbde-b3ea-11e7-96bf-4f4e8f915588.pngInstallingInstall with pip:.. code-block:: console$ pip install PyJWT
Usage.. code-block:: pycon>>> import jwt
>>> encoded = jwt.encode({""some"": ""payload""}, ""secret"", algorithm=""HS256"")
>>> print(encoded)
eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb21lIjoicGF5bG9hZCJ9.4twFt5NiznN84AWoo1d7KO1T_yoc0Z6XOpOVswacPZg
>>> jwt.decode(encoded, ""secret"", algorithms=[""HS256""])
{'some': 'payload'}
DocumentationView the full docs online at https://pyjwt.readthedocs.io/en/stable/TestsYou can run tests from the project root after cloning with:.. code-block:: console$ tox
"
https://github.com/brennerm/PyTricks,Collection of less popular features and tricks for the Python programming language,IntentionCreating a knowledge base of unpopular Python built-in features to save a lot of unnecessary code.ContributeFeel free to use the PyTrickBase.txt as a starting point.RequirementsContact
https://github.com/biopython/biopython,Official git repository for Biopython (originally converted from CVS),".. image:: https://img.shields.io/pypi/v/biopython.svg?logo=pypi:alt: Biopython on the Python Package Index (PyPI):target: https://pypi.python.org/pypi/biopython.. image:: https://img.shields.io/conda/vn/conda-forge/biopython.svg?logo=conda-forge:alt: Biopython on the Conda package conda-forge channel:target: https://anaconda.org/conda-forge/biopython.. image:: https://results.pre-commit.ci/badge/github/biopython/biopython/master.svg:target: https://results.pre-commit.ci/latest/github/biopython/biopython/master:alt: pre-commit.ci status.. image:: https://img.shields.io/circleci/build/github/biopython/biopython.svg?logo=circleci:alt: Linux testing with CircleCI:target: https://app.circleci.com/pipelines/github/biopython/biopython.. image:: https://img.shields.io/appveyor/ci/biopython/biopython/master.svg?logo=appveyor:alt: Windows testing with AppVeyor:target: https://ci.appveyor.com/project/biopython/biopython/history.. image:: https://img.shields.io/github/actions/workflow/status/biopython/biopython/ci.yml?logo=github-actions:alt: GitHub workflow status:target: https://github.com/biopython/biopython/actions.. image:: https://img.shields.io/codecov/c/github/biopython/biopython/master.svg?logo=codecov:alt: Test coverage on CodeCov:target: https://codecov.io/github/biopython/biopython/.. image:: http://depsy.org/api/package/pypi/biopython/badge.svg:alt: Research software impact on Depsy:target: http://depsy.org/package/python/biopython.. image:: https://github.com/biopython/biopython/raw/master/Doc/images/biopython_logo_m.png:alt: The Biopython Project:target: http://biopython.orgBiopython README fileThe Biopython Project is an international association of developers of freelyavailable Python tools for computational molecular biology.Our user-centric documentation is hosted on https://biopython.org includingour _ and the main_(_).This README file is intended primarily for people interested in workingwith the Biopython source code, either one of the releases from thehttp://biopython.org website, or from our repository on GitHubhttps://github.com/biopython/biopythonThe _file summarises the changes in each release of Biopython.The Biopython package is open source software made available under generousterms. Please see the _ file forfurther details.If you use Biopython in work contributing to a scientific publication, we askthat you cite our application note (below) or one of the module specificpublications (listed on our website):Cock, P.J.A. et al. Biopython: freely available Python tools for computationalmolecular biology and bioinformatics. Bioinformatics 2009 Jun 1; 25(11) 1422-3https://doi.org/10.1093/bioinformatics/btp163 pmid:19304878For the impatientPython includes the package management system ""pip"" which should allow you toinstall Biopython (and its dependency NumPy if needed), upgrade or uninstallwith just one terminal command::pip install biopython
pip install --upgrade biopython
pip uninstall biopython
Since Biopython 1.70 we have provided pre-compiled binary wheel packages onPyPI for Linux, macOS and Windows. This means pip install should be quick,and not require a compiler.As a developer or potential contributor, you may wish to download, build andinstall Biopython yourself. This is described below.Python RequirementsWe currently recommend using Python 3.11 from http://www.python.orgBiopython is currently supported and tested on the following Pythonimplementations:Optional DependenciesBiopython requires NumPy (see http://www.numpy.org) which will be installedautomatically if you install Biopython with pip (see below for compilingBiopython yourself).Depending on which parts of Biopython you plan to use, there are a number ofother optional Python dependencies, which can be installed later if needed:In addition there are a number of useful third party tools you may wish toinstall such as standalone NCBI BLAST, EMBOSS or ClustalW.Installation From SourceWe recommend using the pre-compiled binary wheels available on PyPI using::pip install biopython
However, if you need to compile Biopython yourself, the following are requiredat compile time:Then either download and decompress our source code, or fetch it using git.Now change directory to the Biopython source code folder and run::pip install -e .
python setup.py test
sudo python setup.py install
Substitute  with your specific version if required, for example, or .To exclude tests that require an internet connection (and which may take along time), use the  option::python setup.py test --offline
If you need to do additional configuration, e.g. changing the installdirectory prefix, please type .TestingBiopython includes a suite of regression tests to check if everything isrunning correctly. To run the tests, go to the biopython source codedirectory and type::pip install -e .
python setup.py test
If you want to skip the online tests (which is recommended when doing repeatedtesting), use::python setup.py test --offline
Do not panic if you see messages warning of skipped tests::test_DocSQL ... skipping. Install MySQLdb if you want to use Bio.DocSQL.
This most likely means that a package is not installed.  You canignore this if it occurs in the tests for a module that you were notplanning on using.  If you did want to use that module, please installthe required dependency and re-run the tests.Some of the tests may fail due to network issues, this is often down tochance or a service outage. If the problem does not go away onre-running the tests, you can use the  option.There is more testing information in the Biopython Tutorial & Cookbook.Experimental codeBiopython 1.61 introduced a new warning, ,which is used to mark any experimental code included in the otherwisestable Biopython releases. Such 'beta' level code is ready for widertesting, but still likely to change, and should only be tried by earlyadopters in order to give feedback via the biopython-dev mailing list.We'd expect such experimental code to reach stable status within one or tworeleases, at which point our normal policies about trying to preservebackwards compatibility would apply.BugsWhile we try to ship a robust package, bugs inevitably pop up.  If you arehaving problems that might be caused by a bug in Biopython, it is possiblethat it has already been identified. Update to the latest release if you arenot using it already, and retry. If the problem persists, please search ourbug database and our mailing lists to see if it has already been reported(and hopefully fixed), and if not please do report the bug. We can't fixproblems we don't know about ;)Issue tracker: https://github.com/biopython/biopython/issuesIf you suspect the problem lies within a parser, it is likely that the dataformat has changed and broken the parsing code.  (The text BLAST and GenBankformats seem to be particularly fragile.)  Thus, the parsing code inBiopython is sometimes updated faster than we can build Biopython releases.You can get the most recent parser by pulling the relevant files (e.g. theones in  or ) from our git repository. However, becareful when doing this, because the code in github is not as well-testedas released code, and may contain new dependencies.In any bug report, please let us know:And also ideally:Contributing, Bug ReportsBiopython is run by volunteers from all over the world, with many types ofbackgrounds. We are always looking for people interested in helping with codedevelopment, web-site management, documentation writing, technicaladministration, and whatever else comes up.If you wish to contribute, please first read _ here,visit our web site http://biopython.org and join our mailing list:http://biopython.org/wiki/Mailing_listsDistribution Structure"
https://github.com/pypa/pip,The Python package installer,"pip - The Python Package Installer.. image:: https://img.shields.io/pypi/v/pip.svg:target: https://pypi.org/project/pip/:alt: PyPI.. image:: https://img.shields.io/pypi/pyversions/pip:target: https://pypi.org/project/pip:alt: PyPI - Python Version.. image:: https://readthedocs.org/projects/pip/badge/?version=latest:target: https://pip.pypa.io/en/latest:alt: Documentationpip is the _ for Python. You can use pip to install packages from the _ and other indexes.Please take a look at our documentation for how to install and use pip:We release updates regularly, with a new version every 3 months. Find more details in our documentation:If you find bugs, need help, or want to talk to the developers, please use our mailing lists or chat rooms:If you want to get involved head over to GitHub to get the source code, look at our development documentation and feel free to jump on the developer mailing lists and chat rooms:Code of ConductEveryone interacting in the pip project's codebases, issue trackers, chatrooms, and mailing lists is expected to follow the _... _package installer: https://packaging.python.org/guides/tool-recommendations/.. _Python Package Index: https://pypi.org.. _Installation: https://pip.pypa.io/en/stable/installation/.. _Usage: https://pip.pypa.io/en/stable/.. _Release notes: https://pip.pypa.io/en/stable/news.html.. _Release process: https://pip.pypa.io/en/latest/development/release-process/.. _GitHub page: https://github.com/pypa/pip.. _Development documentation: https://pip.pypa.io/en/latest/development.. _Issue tracking: https://github.com/pypa/pip/issues.. _Discourse channel: https://discuss.python.org/c/packaging.. _User IRC: https://kiwiirc.com/nextclient/#ircs://irc.libera.chat:+6697/pypa.. _Development IRC: https://kiwiirc.com/nextclient/#ircs://irc.libera.chat:+6697/pypa-dev.. _PSF Code of Conduct: https://github.com/pypa/.github/blob/main/CODE_OF_CONDUCT.md"
https://github.com/SirVer/ultisnips,UltiSnips - The ultimate snippet solution for Vim. Send pull requests to SirVer/ultisnips!,"UltiSnipsUltiSnips is the ultimate solution for snippets in Vim. It has many features,speed being one of them.In this demo I am editing a python file. I first expand the  snippet, thenthe  snippet. The completion menu comes from, UltiSnips alsointegrates with , and more. I canjump through placeholders and add text while the snippet inserts text in otherplaces automatically: when I add  as a base class,  getsupdated to call the base class constructor. When I add arguments to theconstructor, they automatically get assigned to instance variables. I theninsert my personal snippet for  debugging. Note that I left insert mode,inserted another snippet and went back to add an additional argument to and the class snippet was still active and added another instancevariable.The official home of UltiSnips is at .Please add pull requests and issues there.UltiSnips was started in Jun 2009 by @SirVer. In Dec 2015, maintenance washanded over to  who ran out of timein early 2017. Since Jun 2019, @SirVer is maintaining UltiSnips again on avery constraint time budget. If you can help triaging issues it would begreatly appreciated.Quick StartThis assumes you are using . Adaptfor your plugin manager of choice. Put this into your ."" Track the engine.
Plugin 'SirVer/ultisnips'

"" Snippets are separated from the engine. Add this if you want them:
Plugin 'honza/vim-snippets'

"" Trigger configuration. You need to change this to something other than <tab> if you use one of the following:
"" - https://github.com/Valloric/YouCompleteMe
"" - https://github.com/nvim-lua/completion-nvim
let g:UltiSnipsExpandTrigger=""<tab>""
let g:UltiSnipsJumpForwardTrigger=""<c-b>""
let g:UltiSnipsJumpBackwardTrigger=""<c-z>""

"" If you want :UltiSnipsEdit to split your window.
let g:UltiSnipsEditSplit=""vertical""
UltiSnips comes with comprehensive.As there are more options and tons of features I suggest you at least skim it.There are example uses for some power user features here:ScreencastsFrom a gentle introduction to really advanced in a few minutes: The blog postsof the screencasts contain more advanced examples of the things discussed in thevideos.Also the excellent  dedicated three episodes toUltiSnips:"
https://github.com/kennethreitz/records,SQL for Humans™,"Records: SQL for Humans™.. image:: https://img.shields.io/pypi/v/records.svg:target: https://pypi.python.org/pypi/records.. image:: https://travis-ci.org/kennethreitz/records.svg?branch=master:target: https://travis-ci.org/kennethreitz/records.. image:: https://img.shields.io/badge/SayThanks.io-☼-1EAEDB.svg:target: https://saythanks.io/to/kennethreitzRecords is a very simple, but powerful, library for making raw SQL queries.. image:: https://farm1.staticflickr.com/569/33085227621_7e8da49b90_k_d.jpgJust write SQL. No bells, no whistles. This common task can besurprisingly difficult with the standard tools available.This library strives to make this workflow as simple as possible,while providing an elegant interface to work with your query results.Database support includes RedShift, Postgres, MySQL, SQLite, Oracle, and MS-SQL (drivers not included).☤ The BasicsWe know how to write SQL, so let's send some to our database:.. code:: pythonimport records

db = records.Database('postgres://...')
rows = db.query('select * from active_users')    # or db.query_file('sqls/active-users.sql')
Grab one row at a time:.. code:: python>>> rows[0]
<Record {""username"": ""model-t"", ""active"": true, ""name"": ""Henry Ford"", ""user_email"": ""model-t@gmail.com"", ""timezone"": ""2016-02-06 22:28:23.894202""}>
Or iterate over them:.. code:: pythonfor r in rows:
    print(r.name, r.user_email)
Values can be accessed many ways: , , or .Fields with non-alphanumeric characters (like spaces) are also fully supported.Or store a copy of your record collection for later reference:.. code:: python>>> rows.all()
[<Record {""username"": ...}>, <Record {""username"": ...}>, <Record {""username"": ...}>, ...]
If you're only expecting one result:.. code:: python>>> rows.first()
<Record {""username"": ...}>
Other options include  and .☤ FeaturesRecords is proudly powered by _and _.☤ Data Export FunctionalityRecords also features full Tablib integration, and allows you to exportyour results to CSV, XLS, JSON, HTML Tables, YAML, or Pandas DataFrames with a single line of code.Excellent for sharing data with friends, or generating reports... code:: pycon>>> print(rows.dataset)
username|active|name      |user_email       |timezone
--------|------|----------|-----------------|--------------------------
model-t |True  |Henry Ford|model-t@gmail.com|2016-02-06 22:28:23.894202
...
Comma Separated Values (CSV).. code:: pycon>>> print(rows.export('csv'))
username,active,name,user_email,timezone
model-t,True,Henry Ford,model-t@gmail.com,2016-02-06 22:28:23.894202
...
YAML Ain't Markup Language (YAML).. code:: python>>> print(rows.export('yaml'))
- {active: true, name: Henry Ford, timezone: '2016-02-06 22:28:23.894202', user_email: model-t@gmail.com, username: model-t}
...
JavaScript Object Notation (JSON).. code:: python>>> print(rows.export('json'))
[{""username"": ""model-t"", ""active"": true, ""name"": ""Henry Ford"", ""user_email"": ""model-t@gmail.com"", ""timezone"": ""2016-02-06 22:28:23.894202""}, ...]
Microsoft Excel (xls, xlsx).. code:: pythonwith open('report.xls', 'wb') as f:
    f.write(rows.export('xls'))
    
    
Pandas DataFrame.. code:: python>>> rows.export('df')
    username  active       name        user_email                   timezone
0    model-t    True Henry Ford model-t@gmail.com 2016-02-06 22:28:23.894202
You get the point. All other features of Tablib are also available,so you can sort results, add/remove columns/rows, remove duplicates,transpose the table, add separators, slice data by column, and more.See the _for more details.☤ InstallationOf course, the recommended installation method is _::$ pipenv install records[pandas]
✨🍰✨
☤ Command-Line ToolAs an added bonus, a  command-line tool is automaticallyincluded. Here's a screenshot of the usage information:.. image:: http://f.cl.ly/items/0S14231R3p0G3w3A0x2N/Screen%20Shot%202016-02-13%20at%202.43.21%20AM.png:alt: Screenshot of Records Command-Line Interface.☤ Thank YouThanks for checking this library out! I hope you find it useful.Of course, there's always room for improvement. Feel free to _ so we can make Records better, stronger, faster."
https://github.com/ktbyers/netmiko,Multi-vendor library to simplify Paramiko SSH connections to network devices,"NetmikoMulti-vendor library to simplify CLI connections to network devicesWhy Netmiko?Network automation to screen-scraping devices is primarily concerned with gathering output from show commands and with making configuration changes.Netmiko aims to accomplish both of these operations and to do it across a very broad set of platforms. It seeks to do this while abstracting away low-level state control (i.e. eliminate low-level regex pattern matching to the extent practical).Getting StartedExamplesYou really should look here.Supported PlatformsInstallationTo install netmiko, simply us pip:$ pip install netmiko
API-DocumentationCommon Issues/FAQAnswers to some TutorialsGetting Started:Create a dictionary representing the device.Supported device_types can be found in , see CLASS_MAPPER keys.from netmiko import ConnectHandler

cisco_881 = {
    'device_type': 'cisco_ios',
    'host':   '10.10.10.10',
    'username': 'test',
    'password': 'password',
    'port' : 8022,          # optional, defaults to 22
    'secret': 'secret',     # optional, defaults to ''
}

Establish an SSH connection to the device by passing in the device dictionary.net_connect = ConnectHandler(**cisco_881)
Execute show commands.output = net_connect.send_command('show ip int brief')
print(output)
Interface                  IP-Address      OK? Method Status                Protocol
FastEthernet0              unassigned      YES unset  down                  down
FastEthernet1              unassigned      YES unset  down                  down
FastEthernet2              unassigned      YES unset  down                  down
FastEthernet3              unassigned      YES unset  down                  down
FastEthernet4              10.10.10.10     YES manual up                    up
Vlan1                      unassigned      YES unset  down                  down
Execute configuration change commands (will automatically enter into config mode)config_commands = [ 'logging buffered 20000',
                    'logging buffered 20010',
                    'no logging console' ]
output = net_connect.send_config_set(config_commands)
print(output)
pynet-rtr1#config term
Enter configuration commands, one per line.  End with CNTL/Z.
pynet-rtr1(config)#logging buffered 20000
pynet-rtr1(config)#logging buffered 20010
pynet-rtr1(config)#no logging console
pynet-rtr1(config)#end
pynet-rtr1#
API-DocumentationAPI DocumentationBelow are some of the particularly handy Classes/functions for easy reference:ContributingContributors are welcome.You can contribute to Netmiko in a variety of ways: answering questions on Slack (see below in Questions/Discussions), responding to issues, adding to the common issues, reporting/fixing bugs, or even adding your own device type.Before contributing a new vendor/platform device type, remember that any code added needs to be supported in some fashion. To add a vendor/platform you can follow the outline . Once you've worked on your first pass of your driver and have it functional, you'll need to include test data in order for it to be merged into develop, you can see the general flow of how to do that .For all code contributions, please ensure that you have ran  against the code or your code will fail the Travis CI build.Questions/DiscussionIf you find an issue with Netmiko, then you can open an issue on this projects issue page here: . Please make sure you've read through the common issues and examples prior to opening an issue. Please only open issues for bugs, feature requests, or other topics related to development of Netmiko. If you simply have a question, join us on Slack...If you have questions or would like to discuss Netmiko, a #netmiko channel exists in  workspace. To join, use . Once you have entered the workspace, then you can join the #netmiko channel.Kirk ByersPython for Network Engineershttps://pynet.twb-tech.com  "
https://github.com/Hironsan/BossSensor,Hide screen when boss is approaching.,"BossSensorHide your screen when your boss is approaching.DemoThe boss stands up. He is approaching.When he is approaching, the program fetches face images and classifies the image.If the image is classified as the Boss, it will monitor changes.RequirementsPut images into  and .UsageFirst, Train boss image.$ python boss_train.py
Second, start BossSensor. $ python camera_reader.py
InstallInstall OpenCV, PyQt4, Anaconda.conda create -n venv python=3.5
source activate venv
conda install -c https://conda.anaconda.org/menpo opencv3
conda install -c conda-forge tensorflow
pip install -r requirements.txt
Change Keras backend from Theano to TensorFlow. LicenceAuthor"
https://github.com/felixonmars/dnsmasq-china-list,Chinese-specific configuration to improve your favorite DNS server. Best partner for chnroutes.,"dnsmasq-china-listChinese-specific configuration to improve your favorite DNS server. Best partner for chnroutes.DetailsPlease don't add subdomains if the top domain is already in the list. This includes all .cn domains which are already matched by the  rule.UsageAutomatic Installation (recommended)You can save the installer and run it again to update the list regularly.Manual Installation# change the default DNS server to 202.96.128.86
make SERVER=202.96.128.86 dnsmasq
# generate unbound's configuration
make unbound
# generate bind's configuration
make bind
# full example of generating dnscrypt-proxy forwarding rules for Windows
make SERVER=101.6.6.6 NEWLINE=DOS dnscrypt-proxy
LicenseCopyright © 2015 Felix Yan <felixonmars@archlinux.org>
This work is free. You can redistribute it and/or modify it under the
terms of the Do What The Fuck You Want To Public License, Version 2,
as published by Sam Hocevar. See the LICENSE file for more details.
"
https://github.com/statsmodels/statsmodels,Statsmodels: statistical modeling and econometrics in Python,".. image:: docs/source/images/statsmodels-logo-v2-horizontal.svg:alt: Statsmodels logo|PyPI Version| |Conda Version| |License| |Azure CI Build Status||Codecov Coverage| |Coveralls Coverage| |PyPI downloads| |Conda downloads|About statsmodelsstatsmodels is a Python package that provides a complement to scipy forstatistical computations including descriptive statistics and estimationand inference for statistical models.DocumentationThe documentation for the latest release is athttps://www.statsmodels.org/stable/The documentation for the development version is athttps://www.statsmodels.org/dev/Recent improvements are highlighted in the release noteshttps://www.statsmodels.org/stable/release/Backups of documentation are available at https://statsmodels.github.io/stable/and https://statsmodels.github.io/dev/.Main FeaturesHow to get itThe main branch on GitHub is the most up to date codehttps://www.github.com/statsmodels/statsmodelsSource download of release tags are available on GitHubhttps://github.com/statsmodels/statsmodels/tagsBinaries and source distributions are available from PyPihttps://pypi.org/project/statsmodels/Binaries can be installed in Anacondaconda install statsmodelsGetting the latest codeInstalling the most recent nightly wheelThe most recent nightly wheel can be installed using pip.

.. code:: bash

   python -m pip install -i https://pypi.anaconda.org/scientific-python-nightly-wheels/simple statsmodels --upgrade --use-deprecated=legacy-resolver

Installing from sources
~~~~~~~~~~~~~~~~~~~~~~~

See INSTALL.txt for requirements or see the documentation

https://statsmodels.github.io/dev/install.html

Contributing
============
Contributions in any form are welcome, including:

* Documentation improvements
* Additional tests
* New features to existing models
* New models

https://www.statsmodels.org/stable/dev/test_notes

for instructions on installing statsmodels in *editable* mode.

License
=======

Modified BSD (3-clause)

Discussion and Development
==========================

Discussions take place on the mailing list

https://groups.google.com/group/pystatsmodels

and in the issue tracker. We are very interested in feedback
about usability and suggestions for improvements.

Bug Reports
===========

Bug reports can be submitted to the issue tracker at

https://github.com/statsmodels/statsmodels/issues

.. |Azure CI Build Status| image:: https://dev.azure.com/statsmodels/statsmodels-testing/_apis/build/status/statsmodels.statsmodels?branchName=main
   :target: https://dev.azure.com/statsmodels/statsmodels-testing/_build/latest?definitionId=1&branchName=main
.. |Codecov Coverage| image:: https://codecov.io/gh/statsmodels/statsmodels/branch/main/graph/badge.svg
   :target: https://codecov.io/gh/statsmodels/statsmodels
.. |Coveralls Coverage| image:: https://coveralls.io/repos/github/statsmodels/statsmodels/badge.svg?branch=main
   :target: https://coveralls.io/github/statsmodels/statsmodels?branch=main
.. |PyPI downloads| image:: https://img.shields.io/pypi/dm/statsmodels?label=PyPI%20Downloads
   :alt: PyPI - Downloads
   :target: https://pypi.org/project/statsmodels/
.. |Conda downloads| image:: https://img.shields.io/conda/dn/conda-forge/statsmodels.svg?label=Conda%20downloads
   :target: https://anaconda.org/conda-forge/statsmodels/
.. |PyPI Version| image:: https://img.shields.io/pypi/v/statsmodels.svg
   :target: https://pypi.org/project/statsmodels/
.. |Conda Version| image:: https://anaconda.org/conda-forge/statsmodels/badges/version.svg
   :target: https://anaconda.org/conda-forge/statsmodels/
.. |License| image:: https://img.shields.io/pypi/l/statsmodels.svg
   :target: https://github.com/statsmodels/statsmodels/blob/main/LICENSE.txt
"
https://github.com/dschep/ntfy,"🖥️📱🔔 A utility for sending notifications, on demand and when commands finish.","About |Version|_ |Docs|_ |Build|_ |WinBuild|_ |Coverage|_ |SayThanks|_.. |Version| image:: https://img.shields.io/pypi/v/ntfy.svg?logo=data%3Aimage/svg%2Bxml%3Bbase64%2CPHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNDAiIGhlaWdodD0iMjQwIj48cGF0aCBmaWxsPSIjMzY3MWEyIiBkPSJNNjIuNDc2IDMzLjRjMC0xNi4zNiA0LjM4Ni0yNS4yNiAyOC42MDctMjkuNDk4IDE2LjQ0NC0yLjg4IDM3LjUyOC0zLjI0MiA1Ny4xOTMgMCAxNS41MzMgMi41NjQgMjguNjA3IDE0LjEzNCAyOC42MDcgMjkuNDk3djUzLjk3YzAgMTUuODMtMTIuNjkzIDI4Ljc5NS0yOC42MDcgMjguNzk1SDkxLjA4M2MtMTkuNDEgMC0zNS43NyAxNi41NS0zNS43NyAzNS4yNnYyNS44OTVIMzUuNjVjLTE2LjYzNiAwLTI2LjMyLTExLjk5My0zMC4zODgtMjguNzc2LTUuNDktMjIuNTQ2LTUuMjU2LTM1Ljk4IDAtNTcuNTc0QzkuODE4IDcyLjEzNyAyNC4zNzUgNjIuMTk4IDQxLjAxIDYyLjE5OGg3OC42OHYtNy4yMDRINjIuNDc2VjMzLjR6Ii8%2BPHBhdGggZmlsbD0iI2ZmZDA0NiIgZD0iTTE3Ni44ODMgMjA2LjEyM2MwIDE2LjM2LTE0LjE5OCAyNC42NDQtMjguNjA3IDI4Ljc3Ni0yMS42NzggNi4yMy0zOS4wNzUgNS4yNzMtNTcuMTkzIDAtMTUuMTMtNC40MS0yOC42MDctMTMuNDE3LTI4LjYwNy0yOC43OHYtNTMuOTdjMC0xNS41MzMgMTIuOTQ3LTI4Ljc5OCAyOC42MDctMjguNzk4aDU3LjE5M2MxOS4wNSAwIDM1Ljc3LTE2LjQ2NSAzNS43Ny0zNS45OFY2Mi4xOTZoMjEuNDQ0YzE2LjY1NiAwIDI0LjQ5NiAxMi4zNzYgMjguNjA3IDI4Ljc3NyA1LjcyMiAyMi43OCA1Ljk3NiAzOS44MTcgMCA1Ny41NzQtNS43ODUgMTcuMjUtMTEuOTcyIDI4Ljc3Ny0yOC42MDcgMjguNzc3aC04NS44djcuMjA1aDU3LjE5M3YyMS41OXoiLz48L3N2Zz4%3D.. _Version: https://pypi.org/project/ntfy/.. |Docs| image:: http://readthedocs.org/projects/ntfy/badge/?version=latest.. _Docs: http://ntfy.readthedocs.org/en/stable/?badge=latest.. |Build| image:: https://img.shields.io/travis/dschep/ntfy/master.svg?logo=data%3Aimage/svg%2Bxml%3Bbase64%2CPHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB2aWV3Qm94PSItMTQyLjUgLTE0Mi41IDMyIDE2IiB3aWR0aD0iMzIiIGhlaWdodD0iMTYiPjxjaXJjbGUgcj0iOCIgY3g9Ii0xMzQuNSIgY3k9Ii0xMzQuNSIgZmlsbD0iI2RkNDgxNCIvPjxnIGlkPSJhIiB0cmFuc2Zvcm09Im1hdHJpeCguMDU2NDYgMCAwIC4wNTY0NiAtMTM0LjUgLTEzNC41KSIgZmlsbD0iI2ZmZiI%252BPGNpcmNsZSBjeD0iLTk2LjQiIHI9IjE4LjkiLz48cGF0aCBkPSJNLTQ1LjYgNjguNGMtMTYuNi0xMS0yOS0yOC0zNC00Ny44IDYtNSA5LjgtMTIuMyA5LjgtMjAuNnMtMy44LTE1LjctOS44LTIwLjZjNS0xOS44IDE3LjQtMzYuNyAzNC00Ny44bDEzLjggMjMuMkMtNDYtMzUuMi01NS4zLTE4LjctNTUuMyAwYzAgMTguNyA5LjMgMzUuMiAyMy41IDQ1LjJ6Ii8%252BPC9nPjx1c2UgeGxpbms6aHJlZj0iI2EiIHRyYW5zZm9ybT0icm90YXRlKDEyMCAtMTM0LjUgLTEzNC41KSIgd2lkdGg9IjEwMCUiIGhlaWdodD0iMTAwJSIvPjx1c2UgeGxpbms6aHJlZj0iI2EiIHRyYW5zZm9ybT0icm90YXRlKC0xMjAgLTEzNC41IC0xMzQuNSkiIHdpZHRoPSIxMDAlIiBoZWlnaHQ9IjEwMCUiLz48Y2lyY2xlIGN5PSItMTM0LjUiIGN4PSItMTM0LjUiIHI9IjgiIGZpbGw9IiNkZDQ4MTQiLz48ZyB0cmFuc2Zvcm09Im1hdHJpeCguMDU2NDYgMCAwIC4wNTY0NiAtMTM0LjUgLTEzNC41KSIgZmlsbD0iI2ZmZiI%252BPGNpcmNsZSByPSIxOC45IiBjeD0iLTk2LjQiLz48cGF0aCBkPSJNLTQ1LjYgNjguNGMtMTYuNi0xMS0yOS0yOC0zNC00Ny44IDYtNSA5LjgtMTIuMyA5LjgtMjAuNnMtMy44LTE1LjctOS44LTIwLjZjNS0xOS44IDE3LjQtMzYuNyAzNC00Ny44bDEzLjggMjMuMkMtNDYtMzUuMi01NS4zLTE4LjctNTUuMyAwYzAgMTguNyA5LjMgMzUuMiAyMy41IDQ1LjJ6Ii8%252BPC9nPjx1c2UgaGVpZ2h0PSIxMDAlIiB3aWR0aD0iMTAwJSIgdHJhbnNmb3JtPSJyb3RhdGUoMTIwIC0xMzQuNSAtMTM0LjUpIiB4bGluazpocmVmPSIjYSIvPjx1c2UgaGVpZ2h0PSIxMDAlIiB3aWR0aD0iMTAwJSIgdHJhbnNmb3JtPSJyb3RhdGUoLTEyMCAtMTM0LjUgLTEzNC41KSIgeGxpbms6aHJlZj0iI2EiLz48cGF0aCBkPSJNLTExMS41NTUtMTMwLjAzMWE4LjY5OCA4LjY5OCAwIDAgMS0uODYgMS41NDZjLS40NTMuNjQ1LS44MjMgMS4wOTItMS4xMDkgMS4zNC0uNDQyLjQwNy0uOTE2LjYxNS0xLjQyNC42MjctLjM2NCAwLS44MDQtLjEwNC0xLjMxNS0uMzE0LS41MTQtLjIxLS45ODUtLjMxMy0xLjQxNy0uMzEzLS40NTIgMC0uOTM3LjEwMy0xLjQ1Ni4zMTMtLjUyLjIxLS45MzguMzItMS4yNTguMzMtLjQ4Ny4wMjEtLjk3Mi0uMTkzLTEuNDU3LS42NDMtLjMwOS0uMjctLjY5Ni0uNzMyLTEuMTU5LTEuMzg3LS40OTctLjctLjkwNS0xLjUxLTEuMjI1LTIuNDM0LS4zNDMtLjk5OS0uNTE1LTEuOTY2LS41MTUtMi45MDIgMC0xLjA3Mi4yMzItMS45OTcuNjk2LTIuNzcyYTQuMDgyIDQuMDgyIDAgMCAxIDEuNDU3LTEuNDc0IDMuOTIgMy45MiAwIDAgMSAxLjk3LS41NTZjLjM4NyAwIC44OTQuMTIgMS41MjQuMzU1LjYyOC4yMzYgMS4wMzIuMzU1IDEuMjA5LjM1NS4xMzIgMCAuNTgtLjE0IDEuMzQtLjQxOC43MTgtLjI1OSAxLjMyNS0uMzY2IDEuODIxLS4zMjQgMS4zNDcuMTA5IDIuMzU4LjY0IDMuMDMgMS41OTUtMS4yMDMuNzMtMS43OTkgMS43NTItMS43ODcgMy4wNjIuMDEgMS4wMjEuMzgxIDEuODcgMS4xMDkgMi41NDUuMzMuMzEzLjY5OC41NTUgMS4xMDguNzI3LS4wODkuMjU4LS4xODMuNTA1LS4yODIuNzQyem0tMy4wODgtMTIuMTQ5YzAgLjgtLjI5MiAxLjU0Ny0uODc1IDIuMjM5LS43MDMuODIyLTEuNTUzIDEuMjk3LTIuNDc2IDEuMjIyYTIuNTAyIDIuNTAyIDAgMCAxLS4wMTgtLjMwM2MwLS43NjguMzM0LTEuNTkuOTI4LTIuMjYzLjI5Ni0uMzQuNjczLS42MjMgMS4xMy0uODQ5LjQ1Ny0uMjIyLjg4OS0uMzQ1IDEuMjk1LS4zNjYuMDExLjEwNy4wMTYuMjE0LjAxNi4zMnoiIGZpbGw9IiNmZmYiLz48L3N2Zz4%3D.. _Build: https://travis-ci.org/dschep/ntfy.. |WinBuild| image:: https://img.shields.io/appveyor/ci/dschep/ntfy/master.svg?logo=data%3Aimage%2Fsvg%2Bxml%3Bbase64%2CPHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZlcnNpb249IjEuMSIgd2lkdGg9IjEyOCIgaGVpZ2h0PSIxMjgiIHZpZXdCb3g9IjAgMCAxMjggMTI4Ij48ZyBmaWxsPSIjMUJBMUUyIiB0cmFuc2Zvcm09InNjYWxlKDgpIj48cGF0aCBkPSJNMCAyLjI2NWw2LjUzOS0uODg4LjAwMyA2LjI4OC02LjUzNi4wMzd6Ii8%2BPHBhdGggZD0iTTYuNTM2IDguMzlsLjAwNSA2LjI5My02LjUzNi0uODk2di01LjQ0eiIvPjxwYXRoIGQ9Ik03LjMyOCAxLjI2MWw4LjY3LTEuMjYxdjcuNTg1bC04LjY3LjA2OXoiLz48cGF0aCBkPSJNMTYgOC40NDlsLS4wMDIgNy41NTEtOC42Ny0xLjIyLS4wMTItNi4zNDV6Ii8%2BPC9nPjwvc3ZnPg==.. _WinBuild: https://ci.appveyor.com/project/dschep/ntfy.. |Coverage| image:: https://coveralls.io/repos/github/dschep/ntfy/badge.svg?branch=master.. _Coverage: https://coveralls.io/github/dschep/ntfy?brach=master.. |Requires| image:: https://requires.io/github/dschep/ntfy/requirements.svg?branch=master.. _Requires: https://requires.io/github/dschep/ntfy/requirements/?branch=master.. |SayThanks| image:: https://img.shields.io/badge/Say%20Thanks-!-1EAEDB.svg.. _SayThanks: https://saythanks.io/to/dschep brings notification to your shell. It can automatically providedesktop notifications when long running commands finish or it can sendpush notifications to your phone when a specific command finishes.Confused? This video demonstrates some of this functionality:.. image:: https://raw.githubusercontent.com/dschep/ntfy/master/docs/demo.gifQuickstart.. code:: shell$ sudo pip install ntfy
$ ntfy send test
# send a notification when the command `sleep 10` finishes
# this sends the message '""sleep 10"" succeeded in 0:10 minutes'
$ ntfy done sleep 10
$ ntfy -b pushover -o user_key t0k3n send 'Pushover test!'
$ ntfy -t 'ntfy' send ""Here's a custom notification title!""
$ echo -e 'backends: [""pushover""]\npushover: {""user_key"": ""t0k3n""}' > ~/.ntfy.yml
$ ntfy send ""Pushover via config file!""
$ ntfy done --pid 6379  # pid extra
$ ntfy send "":tada: ntfy supports emoji! :100:""  # emoji extra
# Enable shell integration
$ echo 'eval ""$(ntfy shell-integration)""' >> ~/.bashrc
InstallThe install technique in the quickstart is the suggested method of installation.It can be installed in a virtualenv, but with some caveats: Linux notificationsrequire  for the virtualenv and OS X notificationsdon't work at all.:penguin: NOTE: _require Python DBUS bindings. See _ for more info.Shell integration``ntfy`` has support for **automatically** sending notifications when long
running commands finish in bash and zsh. In bash it emulates zsh's preexec and
precmd functionality with `rcaloras/bash-preexec <https://github.com/rcaloras/bash-preexec>`_.
To enable it add the following to your ``.bashrc`` or ``.zshrc``:

.. code:: shell

    eval ""$(ntfy shell-integration)""

By default it will only send notifications for commands lasting longer than 10
seconds and if the terminal is focused. Terminal focus works on X11(Linux) and
with Terminal.app and iTerm2 on MacOS. Both options can be configured via the
``--longer-than`` and ``--foreground-too`` options.

To avoid unnecessary notifications when running interactive programs, programs
listed in ``AUTO_NTFY_DONE_IGNORE`` don't generate notifications. For example:

.. code:: shell

    export AUTO_NTFY_DONE_IGNORE=""vim screen meld""

Extras
~~~~~~
``ntfy`` has a few features that require extra dependencies.
    * ``ntfy done -p $PID`` requires installing as ``pip install ntfy[pid]``
    * `emoji <https://en.wikipedia.org/wiki/Emoji>`_ support requires installing as ``pip install ntfy[emoji]``
    * `XMPP <https://xmpp.org/>`_ support requires installing as ``pip install ntfy[xmpp]``
    * `Telegram <https://telegram.org/>`_ support requires installing as ``pip install ntfy[telegram]``
    * `Instapush <https://instapush.im/>`_ support requires installing as ``pip install ntfy[instapush]``
    * `Slack <https://slack.com/>`_ support requires installing as ``pip install ntfy[slack]``
    * `Slack Incoming webhook <https://slack.com/>`_ - simpler slack implementation that doesn't have additional dependencies
    * `Rocket.Chat <https://Rocket.Chat>`_ support requires installing as ``pip install ntfy[rocketchat]``

To install multiple extras, separate with commas: e.g., ``pip install ntfy[pid,emoji]``.

Configuring ``ntfy``
--------------------

``ntfy`` is configured with a YAML file stored at ``~/.ntfy.yml`` or in standard platform specific locations:

* Linux - ``~/.config/ntfy/ntfy.yml``
* macOS - ``~/Library/Application Support/ntfy/ntfy.yml``
* Windows - ``C:\Users\<User>\AppData\Local\dschep\ntfy.yml``

Backends
~~~~~~~~

The backends key specifies what backends to use by default. Each backend has
its own configuration, stored in a key of its own name. For example:

.. code:: yaml

    ---
    backends:
        - pushover
    pushover:
        user_key: hunter2
    pushbullet:
        access_token: hunter2
    simplepush:
        key: hunter2
    slack:
        token: slacktoken
        recipient: ""#slackchannel""
    xmpp:
         jid: ""user@gmail.com""
         password: ""xxxx""
         mtype: ""chat""
         recipient: ""me@jit.si""

If you want mulitple configs for the same backend type, you can specify any
name and then specify the backend with a backend key. For example:

.. code:: yaml

    ---
    pushover:
        user_key: hunter2
    cellphone:
        backend: pushover
        user_key: hunter2

See the backends below for available backends and options. As of v2.6.0 ``ntfy`` also supports
`3rd party backends <#3rd-party-backends>`_

`Pushover <https://pushover.net>`_ - ``pushover``
Required parameters:* Optional parameters:* * * * * *  - use your own application token*  - target a device, if omitted, notification is sent to all devices* * * _ - Required parameter:
    * ``access_token`` - Your Pushbullet access token, created at https://www.pushbullet.com/#settings/account

Optional parameters:
    * ``device_iden`` - a device identifier, if omited, notification is sent to all devices
    * ``email`` - send notification to pushbullet user with the specified email or send an email if they aren't a pushullet user

`Simplepush <https://simplepush.io>`_ - ``simplepush``
Required parameter:*  - Your Simplepush key, created by installing the Android App (no registration required) at https://simplepush.ioOptional parameters:*  - sets ringtone and vibration pattern for incoming notifications (can be defined in the simplepush app)XMPP - Requires parameters:
    * ``jid``
    * ``password``
    * ``recipient``
Optional parameters
    * ``hostname`` (if not from jid)
    * ``port``
    * ``path_to_certs``
    * ``mtype``

Requires extras, install like this: ``pip install ntfy[xmpp]``.

To verify the SSL certificates offered by a server:
path_to_certs = ""path/to/ca/cert""

Without dnspython library installed, you will need
to specify the server hostname if it doesn't match the jid.

Specify port if other than 5222.
NOTE: Ignored without specified hostname

NOTE: Google Hangouts doesn't support XMPP since 2017

`Telegram <https://telegram.org>`_ - ``telegram``
Requires extras, install like this: .Requires  to be installed as . This backend isconfigured the first time you will try to use it: ._ - Required parameter:
    * ``secret`` - The Pushjet service secret token, created with http://docs.pushjet.io/docs/creating-a-new-service

Optional parameters:
    * ``endpoint`` - custom Pushjet API endpoint
        (defaults to https://api.pushjet.io)
    * ``level`` - The importance level from 1(low) to 5(high)
    * ``link``

`Notifico <https://n.tkte.ch/>`_ - ``notifico``
Required parameter:*  - The webhook link, created at https://n.tkte.ch/(choose  service when creating the webhook)_ - Requires extras, install like this: ``pip install ntfy[slack]``.

Required parameter:
    * ``token`` - The Slack service secret token, either a legacy user token created at https://api.slack.com/custom-integrations/legacy-tokens or a token obtained by creating an app at https://api.slack.com/apps?new_app=1 with ``chat:write:bot`` scope and linking it to a workspace.
    * ``recipient`` - The Slack channel or user to send notifications to. If you use the ``#`` symbol the message is send to a Slack channel and if you use the ``@`` symbol the message is send to a Slack user.

`Slack Incoming Webhook <https://slack.com>`_ - ``slack_webhook``
Required parameter:*  - the URL of the incoming webhook*  - The Slack channel or user to send notifications to_ - Requires extras, install like this ``pip install ntfy[instapush]``.

Instapush does not support notification title.
It sends template-driven notifications, so you have to setup you events on the dashboard first.
The backend is called insta due to homonymy with the instapush python wrapper

Required parameters:
    * ``appid`` - The application id
    * ``secret`` - The application secret
    * ``event_name`` - The instapush event to be used
    * ``trackers`` - The array of trakers to use

Note on trackers:
Trackers are placeholders for events (a sort of notification template). If you defined more than one tracker in your event
you'll have to provide more messages. At the moment, the only way to do so is to separate each message with a colon (:) character.
You can also escape the separator character:
Example:

.. code:: shell

    ntfy -b insta send ""message1:message2""
    ntfy -b insta send ""message1:message2\:with\:colons""

`Prowl <https://www.prowlapp.com/>`_ - ``prowl``
Optional parameters:* * * * _ - Works via `dbus`, works with most DEs like Gnome, KDE, XFCE and with libnotify.

The following dependecies should be installed.

.. code:: shell

    $ sudo apt install python-dbus # on ubuntu/debian

You will need to install some font that supports emojis (in Debian `fonts-symbola` or Gentoo `media-fonts/symbola`).

Optional parameters:
    * ``icon`` - Specifies path to the notification icon, empty string for no icon.
    * ``urgency`` - Specifies the urgency level (low, normal, critical).
    * ``transient`` - Skip the history (exp: the Gnome message tray) (true, false).
    * ``soundfile`` - Specifies the notification sound file (e.g. /usr/share/sounds/notif.wav).
    * ``timeout`` - Specifies notification expiration time level (-1 - system default, 0 - never expire).

Windows Desktop Notifications - ``win32``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Uses ``pywin32``.

Mac OS X Notification Center - ``darwin``
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Requires ``ntfy`` to be installed globally (not in a virtualenv).

System log - ``systemlog``
~~~~~~~~~~~~~~~~~~~~~~~~~~
Uses the ``syslog`` core Python module, which is not available on Windows
platforms.

Optional parameters:
    * ``prio`` - Syslog priority level.  Default is ``ALERT``.  Possible values
      are:

      * EMERG
      * ALERT
      * CRIT
      * ERR
      * WARNING
      * NOTICE
      * INFO
      * DEBUG

    * ``facility`` - Syslog facility.  Default is ``LOCAL5``.  Possible values
      are:

      * KERN
      * USER
      * MAIL
      * DAEMON
      * AUTH
      * LPR
      * NEWS
      * UUCP
      * CRON
      * SYSLOG
      * LOCAL0
      * LOCAL1
      * LOCAL2
      * LOCAL3
      * LOCAL4
      * LOCAL5
      * LOCAL6
      * LOCAL7

    * ``fmt`` - Format of the message to be sent to the system logger.  The
      title and the message are specified using the following placeholders:

      * ``{title}``
      * ``{message}``

      Default is ``[{title}] {message}``.

`Termux:API <https://play.google.com/store/apps/details?id=com.termux.api&hl=en>`_ - ``termux``
Requires the app to be install from the Play store and the CLI utility beinstalled with ._ - Required parameter:
    * ``auth_token`` - Your private Pushalot auth token, found here https://pushalot.com/manager/authorizations

Optional parameters:
    * ``source`` - source of the notification
    * ``ttl`` - message expire time in minutes (time to live)
    * ``url`` - URL to include in the notifications
    * ``url_title`` - visible URL title (ignored if no url specified)
    * ``image`` - URL of image included in the notifications
    * ``important`` - mark notifications as important
    * ``silent`` - mark notifications as silent

`Rocket.Chat <https://rocket.chat>`_ - ``rocketchat``
Requires extras, install like this: .Required parameters:*  - URL of your Rocket.Chat instance*  - login username*  - login password*  - room/channel name to post in_ - Requires extras, install like this: ``pip install ntfy[matrix]``.

Required parameters:
    * ``url`` - URL of your homeserver instance
    * ``roomId`` - room to post in
    * ``userId`` - login userid
    * ``password`` - login password
    * ``token`` - access token

You must either specify ``token``, or ``userId`` and ``password``.


`Webpush <https://github.com/dschep/ntfy-webpush>`_ - ``ntfy_webpush``
Webpush support is provded by an external ntfy module, install like this: .Required parameters:For more info, see ntfy-webpush3rd party backendsTo use or implement your own backends, specify the full path of the module as your backend. The
module needs to contain a module with a function called ``notify`` with the following signature:

.. code:: python

    def notify(title, message, **kwargs):
        """"""
        kwargs contains retcode if using ntfy done or ntfy shell-integration
        and all options in your backend's section of the config
        """"""
        pass


Other options
~~~~~~~~~~~~~

Title is configurable with the `title` key in the config. Example:

.. code:: yaml

    ---
    title: Customized Title


Backends ToDo
~~~~~~~~~~~~~
-  `Airgram <http://www.airgramapp.com>`_
-  `Boxcar <https://boxcar.io>`_

Testing
-------

.. code:: shell

    python setup.py test

Contributors
------------
- `dschep <https://github.com/dschep>`_ - Maintainer & Lead Developer
- `danryder <https://github.com/danryder>`_ - XMPP Backend & emoji support
- `oz123 <https://github.com/oz123>`_ - Linux desktop notification improvements
- `schwert <https://github.com/schwert>`_ - PushJet support
- `rahiel <https://github.com/rahiel>`_ - Telegram support
- `tymm <https://github.com/tymm>`_ - Simplepush support
- `jungle-boogie <https://github.com/jungle-boogie>`_ - Documentation updates
- `tjbenator <https://github.com/tjbenator>`_ - Advanced Pushover options
- `mobiusklein <https://github.com/mobiusklein>`_ - Win32 Bugfix
- `rcaloras <https://github.com/rcaloras>`_ - Creator of `bash-prexec`, without which there woudn't be bash shell integration for `ntfy`
- `eightnoteight <https://github.com/eightnoteight>`_ - Notifico support
- `juanpabloaj <https://github.com/juanpabloaj>`_ - Slack support
- `giuseongit <https://github.com/giuseongit>`_ - Instapush support
- `jlesage <https://github.com/jlesage>`_ - Systemlog support
- `sambrightman <https://github.com/sambrightman>`_ - Prowl support
- `mlesniew <https://github.com/mlesniew>`_ - Pushalot support
- `webworxshop <https://github.com/webworxshop>`_ - Rocket.Chat support
- `rhabbachi <https://github.com/rhabbachi>`_ - transient option in Linux desktop notifications
- `Half-Shot <https://github.com/Half-Shot>`_ - Matrix support
"
https://github.com/seemoo-lab/opendrop,An open Apple AirDrop implementation written in Python,"OpenDrop: an Open Source AirDrop ImplementationOpenDrop is a command-line tool that allows sharing files between devices directly over Wi-Fi. Its unique feature is that it is protocol-compatible with Apple AirDrop which allows to share files with Apple devices running iOS and macOS.~~Currently (and probably also for the foreseeable future), OpenDrop only supports sending to Apple devices that are discoverable by everybody as the default contacts only mode requires .~~We support contacts-only devices by using extracted AirDrop credentials (keys and certificates) from macOS via our .DisclaimerOpenDrop is experimental software and is the result of reverse engineering efforts by the  project.Therefore, it does not support all features of AirDrop or might be incompatible with future AirDrop versions.OpenDrop is not affiliated with or endorsed by Apple Inc. Use this code at your own risk.RequirementsTo achieve compatibility with Apple AirDrop, OpenDrop requires the target platform to support a specific Wi-Fi link layer.In addition, it requires Python >=3.6 as well as several libraries.Apple Wireless Direct Link.As AirDrop exclusively runs over Apple Wireless Direct Link (AWDL), OpenDrop is only supported on macOS or on Linux systems running an open re-implementation of AWDL such as .Libraries.OpenDrop relies on a current version of .macOS ships with a rather old version, so you will need to install a newer version, for example, via :brew install libarchive
OpenDrop automatically sets  to look for the Homebrew version. You may need to update the variable yourself if you install the libraries differently.Linux distributions should ship with more up-to-date versions, so this won't be necessary.InstallationInstallation of the Python package  is straightforward using :pip3 install opendrop
You can also install the current development version by first cloning this repository, and then installing it via :git clone https://github.com/seemoo-lab/opendrop.git
pip3 install ./opendrop
UsageWe briefly explain how to send and receive files using .To see all command line options, run .Sending a File or a LinkSending a file is typically a two-step procedure. You first discover devices in proximity using the  command.Stop the process once you have found the receiver.$ opendrop find
Looking for receivers. Press Ctrl+C to stop ...
Found  index 0  ID eccb2f2dcfe7  name John’s iPhone
Found  index 1  ID e63138ac6ba8  name Jane’s MacBook Pro
You can then  a file (or link, see below) using $ opendrop send -r 0 -f /path/to/some/file
Asking receiver to accept ...
Receiver accepted
Uploading file ...
Uploading has been successful
Instead of the , you can also use  or .OpenDrop will try to interpret the input in the order (1) , (2) , and (3)  and fail if no match was found.Sending a web link. Since v0.13, OpenDrop supports sending web links, i.e., URLs, so that receiving Apple devices will immediately open their browser upon accepting.(Note that OpenDrop receivers still only support receiving regular files.)$ opendrop send -r 0 -f https://owlink.org --url
Receiving FilesReceiving is much easier. Simply use the  command. OpenDrop will accept all incoming files automatically and put received files in the current directory.$ opendrop receive
Current Limitations/TODOsOpenDrop is the result of a research project and, thus, has several limitations (non-exhaustive list below). I do not have the capacity to work on them myself but am happy to provide assistance if somebody else want to take them on.Our PapersAuthorsLicenseOpenDrop is licensed under the ."
https://github.com/tiangolo/full-stack-fastapi-postgresql,"Full stack, modern web application generator. Using FastAPI, PostgreSQL as database, Docker, automatic HTTPS and more.","Full Stack FastAPI and PostgreSQL - Base Project GeneratorGenerate a backend and frontend stack using Python, including interactive API documentation.Interactive API documentationAlternative API documentationDashboard LoginDashboard - Create UserFeaturesHow to use itGo to the directory where you want to create your project and run:pip install cookiecutter
cookiecutter https://github.com/tiangolo/full-stack-fastapi-postgresql
Generate passwordsYou will be asked to provide passwords and secret keys for several components. Open another terminal and run:openssl rand -hex 32
# Outputs something like: 99d3b1f01aa639e4a76f4fc281fc834747a543720ba4c8a8648ba755aef9be7f
Copy the contents and use that as password / secret key. And run that again to generate another secure key.Input variablesThe generator (cookiecutter) will ask you for some data, you might want to have at hand before generating the project.The input variables, with their default values (some auto generated) are:How to deployThis stack can be adjusted and used with several deployment options that are compatible with Docker Compose, but it is designed to be used in a cluster controlled with pure Docker in Swarm Mode with a Traefik main load balancer proxy handling automatic HTTPS certificates, using the ideas from DockerSwarm.rocks.Please refer to DockerSwarm.rocks to see how to deploy such a cluster in 20 minutes.More detailsAfter using this generator, your new project (the directory created) will contain an extensive  with instructions for development, deployment, etc. You can pre-read .Sibling project generatorsRelease NotesLatest Changes0.5.00.4.00.3.00.2.20.2.10.2.0<a href=""https://github.com/tiangolo/full-stack-fastapi-postgresql/pull/2"" target=""_blank"">:0.1.20.1.1Several bug fixes since initial publication, including:LicenseThis project is licensed under the terms of the MIT license."
https://github.com/localstack/localstack,💻 A fully functional local AWS cloud stack. Develop and test your cloud & Serverless apps offline,"Overview is a cloud service emulator that runs in a single container on your laptop or in your CI environment. With LocalStack, you can run your AWS applications or Lambdas entirely on your local machine without connecting to a remote cloud provider! Whether you are testing complex CDK applications or Terraform configurations, or just beginning to learn about AWS services, LocalStack helps speed up and simplify your testing and development workflow.LocalStack supports a growing number of AWS services, like AWS Lambda, S3, Dynamodb, Kinesis, SQS, SNS, and many more! The  supports additional APIs and advanced features. You can find a comprehensive list of supported APIs on our  page.LocalStack also provides additional features to make your life as a cloud developer easier! Check out LocalStack's  for more information.InstallationThe quickest way get started with LocalStack is by using the LocalStack CLI.It allows you to start and manage the LocalStack Docker container from your command line.Please make sure that you have a working  on your machine before moving on.Brew (MacOS or Linux with Homebrew)Install the LocalStack CLI by using our :brew install localstack/tap/localstack-cli
Binary download (MacOS, Linux, Windows)If you do not have Brew on your machine, you can directly download the pre-built LocalStack CLI binary for your system:Python package (MacOS, Linux, Windows)LocalStack is built with Python.You can directly install the LocalStack CLI in your Python environment using .PrerequisitesInstallationpython3 -m pip install localstack
This installs the  which is used to run the Docker image that hosts the LocalStack runtime. The  CLI for interacting with the local AWS services is installed separately. See the  for installation instructions.ExampleStart LocalStack inside a Docker container by running: % localstack start -d

     __                     _______ __             __
    / /   ____  _________ _/ / ___// /_____ ______/ /__
   / /   / __ \/ ___/ __ `/ /\__ \/ __/ __ `/ ___/ //_/
  / /___/ /_/ / /__/ /_/ / /___/ / /_/ /_/ / /__/ ,<
 /_____/\____/\___/\__,_/_//____/\__/\__,_/\___/_/|_|

 💻 LocalStack CLI 2.3.0

[20:22:20] starting LocalStack in Docker mode 🐳
[20:22:21] detaching
You can query the status of respective services on LocalStack by running:% localstack status services
┏━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓
┃ Service                  ┃ Status      ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩
│ acm                      │ ✔ available │
│ apigateway               │ ✔ available │
│ cloudformation           │ ✔ available │
│ cloudwatch               │ ✔ available │
│ config                   │ ✔ available │
│ dynamodb                 │ ✔ available │
...
To use SQS, a fully managed distributed message queuing service, on LocalStack, run:% awslocal sqs create-queue --queue-name sample-queue
{
    ""QueueUrl"": ""http://localhost:4566/000000000000/sample-queue""
}
Learn more about  and using them with LocalStack's  CLI.RunningYou can run LocalStack through the following options:UsageTo start using LocalStack, check out our documentation at .To use LocalStack with a graphical user interface, you can use the following UI clients:ReleasesPlease refer to  to see the complete list of changes for each release. For extended release notes, please refer to the .ContributingIf you are interested in contributing to LocalStack:We are thankful for all the contributions and feedback we receive.Get in touchTo get in touch with LocalStack team for bugs/feature requests, support questions or general discussions, please use:ContributorsWe are thankful to all the people who have contributed to this project.BackersWe are also grateful to all our backers who have donated to the project. You can become a backer on .SponsorsYou can also support this project by becoming a sponsor on . Your logo will show up here along with a link to your website.LicenseCopyright (c) 2017-2023 LocalStack maintainers and contributors.Copyright (c) 2016 Atlassian and others.This version of LocalStack is released under the Apache License, Version 2.0 (see ). By downloading and using this software you agree to the . To know about the external software we use, look at our  page."
https://github.com/aws/aws-sam-cli,"CLI tool to build, test, debug, and deploy Serverless applications using AWS SAM","AWS SAM CLI |  |  |  |  |  | The AWS Serverless Application Model (SAM) CLI is an open-source CLI tool that helps you develop serverless applications containing , , , , ,  and more. Some of the features it provides are:Recent blogposts and workshopsGet StartedTo get started with building SAM-based applications, use the SAM CLI. SAM CLI provides a Lambda-like executionenvironment that lets you locally build, test, debug, and deploy  applications.Next Steps: Learn to build a more complex serverless application.What is this Github repository? 💻This Github repository contains source code for SAM CLI. Here is the development team talking about this code:Related Repositories and ResourcesContribute to SAMWe love our contributors ❤️ We have over 100 contributors who have built various parts of the product.Read this  to learnmore about what it was like contributing to SAM.Depending on your interest and skill, you can help build the different parts of the SAM project;Enhance the SAM SpecificationMake pull requests, report bugs, and share ideas to improve the full SAM template specification.Source code is located on Github at .Read the to get started.Strengthen SAM CLIAdd new commands, enhance existing ones, report bugs, or request new features for the SAM CLI.Source code is located on Github at . Read the  toget started.Update SAM Developer Guide provides a comprehensive getting started guide and reference documentation.Source code is located on Github at .Read the  to getstarted.Join the SAM Community on Slack on Slack to collaborate with fellow community members and the AWS SAM team."
https://github.com/0voice/interview_internal_reference,2023年最新总结，阿里，腾讯，百度，美团，头条等技术面试题目，以及答案，专家出题人分析汇总。,"2023年最新总结，阿里，腾讯，百度，美团，头条等技术面试题目，以及答案，专家出题人分析汇总。持续更新中。10.1.6 redis 最适合的场景10.1.9 redis集群如何保证一致性？11.2.5 MongoDB中的命名空间是什么意思?11.2.6 哪些语言支持MongoDB?11.4.0 为什么要在MongoDB中用""Code""数据类型？11.4.1 为什么要在MongoDB中用""Regular Expression""数据类型？11.4.2 为什么在MongoDB中使用""Object ID""数据类型？11.5.0 如何使用""AND""或""OR""条件循环查询集合中的文档？11.5.6 Mongodb存储特性与内部原理?)13.1.9  ))安利术语：零声，专注于C/C++，Linux，Nginx，ZeroMQ，MySQL，Redis，fastdfs，MongoDB，ZK，流媒体，CDN，P2P，K8S，Docker，TCP/IP，协程，DPDK, SPDK, bpf/ebpf等等相关技术分享。本repo由零声的小伙伴推动以及所有提交patch的小伙伴（后面鸣谢部分）参与，共同完成。内容来源于互联网，本repo仅限于整理总结。零声交流群: 762073882, 点击进入 鸣谢感谢各位贡献patch的朋友， 还很多在issue里面出谋划策的朋友，为此衷心感谢。使得该repo能够在github趋势榜，持续一周时间问鼎排行榜。加入 gitter 讨论组https://gitter.im/im0voice/interview_internal_reference"
https://github.com/taki0112/UGATIT,Official Tensorflow implementation of U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation (ICLR 2020),"U-GAT-IT &mdash; Official TensorFlow Implementation (ICLR 2020): Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation | This repository provides the official Tensorflow implementation of the following paper:RequirementsPretrained modelDatasetWeb pageTelegram BotUsage├── dataset
   └── YOUR_DATASET_NAME
       ├── trainA
           ├── xxx.jpg (name, format doesn't matter)
           ├── yyy.png
           └── ...
       ├── trainB
           ├── zzz.jpg
           ├── www.png
           └── ...
       ├── testA
           ├── aaa.jpg 
           ├── bbb.png
           └── ...
       └── testB
           ├── ccc.jpg 
           ├── ddd.png
           └── ...
Train> python main.py --dataset selfie2anime
Test> python main.py --dataset selfie2anime --phase test
ArchitectureResultsAblation studyUser studyKernel Inception Distance (KID)CitationIf you find this code useful for your research, please cite our paper:@inproceedings{
Kim2020U-GAT-IT:,
title={U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation},
author={Junho Kim and Minjae Kim and Hyeonwoo Kang and Kwang Hee Lee},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=BJlZ5ySKPH}
}
Author, Minjae Kim, Hyeonwoo Kang, Kwanghee Lee"
https://github.com/UKPLab/sentence-transformers,Multilingual Sentence & Image Embeddings with BERT,"Sentence Transformers: Multilingual Sentence, Paragraph, and Image Embeddings using BERT & Co.This framework provides an easy method to compute dense vector representations for sentences, paragraphs, and images. The models are based on transformer networks like BERT / RoBERTa / XLM-RoBERTa etc. and achieve state-of-the-art performance in various tasks. Text is embedded in vector space such that similar text are closer and can efficiently be found using cosine similarity.We provide an increasing number of [<marko.inline.RawText object at 0x0000015930043748>] for more than 100 languages, fine-tuned for various use-cases.Further, this framework allows an easy  [<marko.inline.RawText object at 0x000001592FF47108>], to achieve maximal performance on your specific task.For the full documentation, see [<marko.inline.RawText object at 0x0000015930043A48>].The following publications are integrated in this framework:InstallationWe recommend Python 3.6 or higher, [<marko.inline.RawText object at 0x000001592FD92608>] or higher and [<marko.inline.RawText object at 0x000001592FD92AC8>] or higher. The code does not work with Python 2.7.Install with pipInstall the sentence-transformers with :pip install -U sentence-transformers
Install with condaYou can install the sentence-transformers with :conda install -c conda-forge sentence-transformers
Install from sourcesAlternatively, you can also clone the latest version from the  and install it directly from the source code:pip install -e .
PyTorch with CUDAIf you want to use a GPU / CUDA, you must install PyTorch with the matching CUDA Version. Follow for further details how to install PyTorch.Getting StartedSee  in our documenation. shows you how to use an already trained Sentence Transformer model to embed sentences for another task.First download a pretrained model.from sentence_transformers import SentenceTransformer
model = SentenceTransformer('all-MiniLM-L6-v2')
Then provide some sentences to the model.sentences = ['This framework generates embeddings for each input sentence',
    'Sentences are passed as a list of string.', 
    'The quick brown fox jumps over the lazy dog.']
sentence_embeddings = model.encode(sentences)
And that's it already. We now have a list of numpy arrays with the embeddings.for sentence, embedding in zip(sentences, sentence_embeddings):
    print(""Sentence:"", sentence)
    print(""Embedding:"", embedding)
    print("""")
Pre-Trained ModelsWe provide a large list of  for more than 100 languages. Some models are general purpose models, while others produce embeddings for specific use cases. Pre-trained models can be loaded by just passing the model name: .TrainingThis framework allows you to fine-tune your own sentence embedding methods, so that you get task-specific sentence embeddings. You have various options to choose from in order to get perfect sentence embeddings for your specific task. See  for an introduction how to train your own embedding models. We provide  how to train models on various datasets.Some highlights are:PerformanceOur models are evaluated extensively on 15+ datasets including challening domains like Tweets, Reddit, emails. They achieve by far the best performance from all available sentence embedding methods. Further, we provide several smaller models that are optimized for speed.Application ExamplesYou can use this framework for:and many more use-cases.For all examples, see .Citing & AuthorsIf you find this repository helpful, feel free to cite our publication :@inproceedings{reimers-2019-sentence-bert,
    title = ""Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks"",
    author = ""Reimers, Nils and Gurevych, Iryna"",
    booktitle = ""Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing"",
    month = ""11"",
    year = ""2019"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://arxiv.org/abs/1908.10084"",
}
If you use one of the multilingual models, feel free to cite our publication :@inproceedings{reimers-2020-multilingual-sentence-bert,
    title = ""Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation"",
    author = ""Reimers, Nils and Gurevych, Iryna"",
    booktitle = ""Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing"",
    month = ""11"",
    year = ""2020"",
    publisher = ""Association for Computational Linguistics"",
    url = ""https://arxiv.org/abs/2004.09813"",
}
Please have a look at  for our different publications that are integrated into SentenceTransformers.Contact person: , https://www.ukp.tu-darmstadt.de/Don't hesitate to send us an e-mail or report an issue, if something is broken (and it shouldn't be) or if you have further questions."
https://github.com/timesler/facenet-pytorch,Pretrained Pytorch face detection (MTCNN) and facial recognition (InceptionResnet) models,"Face Recognition Using PytorchYou can also read a translated version of this file | Python | 3.10 | 3.9 | 3.8 || :---: | :---: | :---: | :---: || Status |  |  |  |This is a repository for Inception Resnet (V1) models in pytorch, pretrained on VGGFace2 and CASIA-Webface.Pytorch model weights were initialized using parameters ported from David Sandberg's .Also included in this repo is an efficient pytorch implementation of MTCNN for face detection prior to inference. These models are also pretrained. To our knowledge, this is the fastest MTCNN implementation available.Table of contentsQuick startSee  and  for usage and implementation details.Pretrained modelsSee: The following models have been ported to pytorch (with links to download pytorch state_dict's):|Model name|LFW accuracy (as listed )|Training dataset|| :- | :-: | -: || (111MB)|0.9905|CASIA-Webface|| (107MB)|0.9965|VGGFace2|There is no need to manually download the pretrained state_dict's; they are downloaded automatically on model instantiation and cached for future use in the torch cache. To use an Inception Resnet (V1) model for facial recognition/identification in pytorch, use:from facenet_pytorch import InceptionResnetV1

# For a model pretrained on VGGFace2
model = InceptionResnetV1(pretrained='vggface2').eval()

# For a model pretrained on CASIA-Webface
model = InceptionResnetV1(pretrained='casia-webface').eval()

# For an untrained model with 100 classes
model = InceptionResnetV1(num_classes=100).eval()

# For an untrained 1001-class classifier
model = InceptionResnetV1(classify=True, num_classes=1001).eval()
Both pretrained models were trained on 160x160 px images, so will perform best if applied to images resized to this shape. For best results, images should also be cropped to the face using MTCNN (see below).By default, the above models will return 512-dimensional embeddings of images. To enable classification instead, either pass  to the model constructor, or you can set the object attribute afterwards with . For VGGFace2, the pretrained model will output logit vectors of length 8631, and for CASIA-Webface logit vectors of length 10575.Example notebooksComplete detection and recognition pipelineFace recognition can be easily applied to raw images by first detecting faces using MTCNN before calculating embedding or probabilities using an Inception Resnet model. The example code at  provides a complete example pipeline utilizing datasets, dataloaders, and optional GPU processing.Face tracking in video streamsMTCNN can be used to build a face tracking system (using the  method). A full face tracking example can be found at .Finetuning pretrained models with new dataIn most situations, the best way to implement face recognition is to use the pretrained models directly, with either a clustering algorithm or a simple distance metrics to determine the identity of a face. However, if finetuning is required (i.e., if you want to select identity based on the model's output logits), an example can be found at .Guide to MTCNN in facenet-pytorchThis guide demonstrates the functionality of the MTCNN module. Topics covered are:See the .Performance comparison of face detection packagesThis notebook demonstrates the use of three face detection packages:Each package is tested for its speed in detecting the faces in a set of 300 images (all frames from one video), with GPU support enabled. Performance is based on Kaggle's P100 notebook kernel. Results are summarized below.|Package|FPS (1080x1920)|FPS (720x1280)|FPS (540x960)||---|---|---|---||facenet-pytorch|12.97|20.32|25.50||facenet-pytorch (non-batched)|9.75|14.81|19.68||dlib|3.80|8.39|14.53||mtcnn|3.04|5.70|8.23|See the .The FastMTCNN algorithmThis algorithm demonstrates how to achieve extremely efficient face detection specifically in videos, by taking advantage of similarities between adjacent frames.See the .Running with dockerThe package and any of the example notebooks can be run with docker (or nvidia-docker) using:docker run --rm -p 8888:8888
    -v ./facenet-pytorch:/home/jovyan timesler/jupyter-dl-gpu \
    -v <path to data>:/home/jovyan/data
    pip install facenet-pytorch && jupyter lab 
Navigate to the examples/ directory and run any of the ipython notebooks.See  for docker container details.Use this repo in your own git projectTo use this code in your own git repo, I recommend first adding this repo as a submodule. Note that the dash ('-') in the repo name should be removed when cloning as a submodule as it will break python when importing:Alternatively, the code can be installed as a package using pip:Conversion of parameters from Tensorflow to PytorchSee: Note that this functionality is not needed to use the models in this repo, which depend only on the saved pytorch 's. Following instantiation of the pytorch model, each layer's weights were loaded from equivalent layers in the pretrained tensorflow models from .The equivalence of the outputs from the original tensorflow models and the pytorch-ported models have been tested and are identical:Passing test data through TF model

tensor([[-0.0142,  0.0615,  0.0057,  ...,  0.0497,  0.0375, -0.0838],
        [-0.0139,  0.0611,  0.0054,  ...,  0.0472,  0.0343, -0.0850],
        [-0.0238,  0.0619,  0.0124,  ...,  0.0598,  0.0334, -0.0852],
        [-0.0089,  0.0548,  0.0032,  ...,  0.0506,  0.0337, -0.0881],
        [-0.0173,  0.0630, -0.0042,  ...,  0.0487,  0.0295, -0.0791]])

Passing test data through PT model

tensor([[-0.0142,  0.0615,  0.0057,  ...,  0.0497,  0.0375, -0.0838],
        [-0.0139,  0.0611,  0.0054,  ...,  0.0472,  0.0343, -0.0850],
        [-0.0238,  0.0619,  0.0124,  ...,  0.0598,  0.0334, -0.0852],
        [-0.0089,  0.0548,  0.0032,  ...,  0.0506,  0.0337, -0.0881],
        [-0.0173,  0.0630, -0.0042,  ...,  0.0487,  0.0295, -0.0791]],
       grad_fn=<DivBackward0>)

Distance 1.2874517096861382e-06
In order to re-run the conversion of tensorflow parameters into the pytorch model, ensure you clone this repo with submodules, as the davidsandberg/facenet repo is included as a submodule and parts of it are required for the conversion.References"
https://github.com/jazzband/django-taggit,Simple tagging for django,"django-taggit.. image:: https://jazzband.co/static/img/badge.svg:target: https://jazzband.co/:alt: Jazzband.. image:: https://img.shields.io/pypi/pyversions/django-taggit.svg:target: https://pypi.org/project/django-taggit/:alt: Supported Python versions.. image:: https://img.shields.io/pypi/djversions/django-taggit.svg:target: https://pypi.org/project/django-taggit/:alt: Supported Django versions.. image:: https://github.com/jazzband/django-taggit/workflows/Test/badge.svg:target: https://github.com/jazzband/django-taggit/actions:alt: GitHub Actions.. image:: https://codecov.io/gh/jazzband/django-taggit/coverage.svg?branch=master:target: https://codecov.io/gh/jazzband/django-taggit?branch=masterThis is a _ project. By contributing you agreeto abide by the _ and follow the _. a simpler approach to tagging with Django.  Add  to your then just add a TaggableManager to your model and go:.. code:: pythonfrom django.db import models

from taggit.managers import TaggableManager


class Food(models.Model):
    # ... fields here

    tags = TaggableManager()
Then you can use the API like so:.. code:: pycon>>> apple = Food.objects.create(name=""apple"")
>>> apple.tags.add(""red"", ""green"", ""delicious"")
>>> apple.tags.all()
[<Tag: red>, <Tag: green>, <Tag: delicious>]
>>> apple.tags.remove(""green"")
>>> apple.tags.all()
[<Tag: red>, <Tag: delicious>]
>>> Food.objects.filter(tags__name__in=[""red""])
[<Food: apple>, <Food: cherry>]
Tags will show up for you automatically in forms and the admin. requires Django 3.2 or greater.For more info check out the _. And for questions about usage ordevelopment you can create an issue on Github (if your question is aboutusage please add the  tag)."
