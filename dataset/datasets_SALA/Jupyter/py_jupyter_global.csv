repo_url,readme,description
https://github.com/JasonJWilliamsNY/biocoding_2015,b'# biocoding_2015\nCoursework for BioCoding 2015 Course\n',Coursework for BioCoding 2015 Course
https://github.com/bollwyvl/es6-widget-example,"b'# ES6 Widget Example\n![](http://i.imgur.com/EtoyFIo.jpg)\n> [Discuss here!](https://github.com/ipython/ipywidgets/issues/14)\n\nAn evolving approach to creating a [Jupyter widget](https://github.com/ipython/ipywidgets) with [ES6 _ne_ ES2015 classes](http://www.ecma-international.org/ecma-262/6.0/#sec-class-definitions).\n\n## Challenges\n- [Backbone and ES6 Classes](http://benmccormick.org/2015/07/06/backbone-and-es6-classes-revisited/) have slightly different opinions, especially with respect to static properties\n\n## Process Tools\n- [x] [`npm run`](https://docs.npmjs.com/cli/run-script) for minimal automation tasks\n- [x] [`watch`](https://www.npmjs.com/package/watch) for live developing with continuous integration\n- [x] [`babel`](https://babeljs.io) for compilation\n  - [x] [`babel-preset-es2015`](https://www.npmjs.com/package/babel-preset-es2015) for un-opinonated use of experimental features\n  - [x] [`babel-plugin-transform-es2015-modules-amd`](https://www.npmjs.com/package/babel-plugin-transform-es2015-modules-amd) for `require.js`-compatible modules\n- [x] [`esdoc`](https://esdoc.org) for documentation\n- [x] [`eslint`](http://eslint.org) for code style\n- [ ] [`mocha`](https://mochajs.org) for tests and coverage\n- [ ] [`browserify`](http://browserify.org/) for (inevitable) packaging of external dependencies distributed on `npm`\n'",an evolving approach for writing Jupyter/IPython widgets with ES6/ES2015
https://github.com/k-sokolov/ensemble,b'isolation forest for credit card fraud detection ',Identifying fraudulent transactions and banknotes as anomaly detection and classification tasks
https://github.com/adbeyer23/MTA-Analysis-Project,"b'# MTA-Analysis-Project\n\nFirst Project done at Metis! In groups, we looked at MTA subway data to determine the best times and stations to target people for a fictional fundraising gala. This involved a lot of cleaning data, visualizing and analyzing it in different ways, and making assumptions about which groups of people would be most willing to give email information to our canvassers. \n'",Analyzing NYC subway data
https://github.com/chakpongchung/Spherical-Harmonics-Expansion,"b'spherical harmonics expansion and the inner product of rotational gradient operator\n========================================================================\n\n\nTUTORIAL\n--------\n\n\nhttps://nbviewer.jupyter.org/github/chakpongchung/Spherical-Harmonics-Expansion/blob/master/derivation.ipynb\n\n\n![Image of Yaktocat](http://stsdas.stsci.edu/download/mdroe/plotting/_images/entry1.png)\n\n\n\n\n We used it in nematic polymer simulation to find out the parameters that can coverge to  periodic pattern.\n\n\n\nThe core algorithm is implemented with Boost C++ Libraries and GNU Scientific Library(GSL) in C/C++. It can also be used with SymPy in Python since SciPy 2015.  \nhttp://docs.sympy.org/dev/aboutus.html#sympy-development-team\n\n\nTo get started, you can run the following commands:\n\n    tests$ bash ./run.sh\n\n\n\nLIBRARIES DEPENDENCIES:\n-----------------------\nUpdates:  \nstarting from c++17, Associated Legendre Polynomials(hence the function for Spherical Harmonics) are supported in C++17  .\n\nBoost C++ Libraries \nwww.boost.org/\n\nGSL - GNU Scientific Library - GNU Project\nwww.gnu.org/software/gsl/\n\nSymPy\nhttp://www.sympy.org/\n\n\n'",spherical-harmonics-expansion using boost and gsl
https://github.com/lila/spot_price_history,"b'# README for Spot Pricing project\n\n>This project provides the code basis for [The Data Science of Spot Pricing](https://medium.com/cloud-uprising/the-data-science-of-aws-spot-pricing-8bed655caed2). \n\nfiles:\n  bin/ - scripts\n  \tdownload.sh - bash script to download (by default 1 days worth) pricing dataset as json\n  data/ - datasets\n  notebooks - the ipython notebooks\n  \n\n## to start:\n\n1. startup ipython\n\n> % ipython notebook\n\n2. using the web interface, interact with the notebooks.\n\n## managing the dependencies\n\nsince the notebooks require a bunch of libraries, (pandas, matlabplot, etc), you can\nrun the following command to get all the requirements installed:\n\n> % pip install -r requirements.txt\n\n## Example notebook\n\nGo to [Example ipython notebook](https://github.com/lila/spot_price_history/blob/master/notebook/SpotPriceHistory.ipynb)\n\n'","IPython notebook that uses Boto, Pandas and MatPlotLib to show historical price data"
https://github.com/securesystemslab/zippy,"b""![zippy-logo-200-rounded.jpg](http://ssllab.org/zippy_logo.jpeg)\r\n# ZipPy [![Build Status](https://travis-ci.org/securesystemslab/zippy.svg?branch=master)](https://travis-ci.org/securesystemslab/zippy) #\r\n\r\n|                 | Standard JVM  | Graal JVM   |\r\n| :------------------: |:-------------:| :----------:|\r\n| Linux Ubuntu 14.04.5  | [![Build Status](https://badges.herokuapp.com/travis/securesystemslab/zippy?env=ZIPPY_JDK_TYPE=STANDARD_LINUX&label=Standard%20JVM)](https://travis-ci.org/securesystemslab/zippy)  | [![Build Status](https://badges.herokuapp.com/travis/securesystemslab/zippy?env=ZIPPY_JDK_TYPE=GRAALJVM_LINUX&label=Graal%20JVM)](https://travis-ci.org/securesystemslab/zippy) |\r\n| Mac OSX 10.12 | [![Build Status](https://badges.herokuapp.com/travis/securesystemslab/zippy?env=ZIPPY_JDK_TYPE=STANDARD_OSX&label=Standard%20JVM)](https://travis-ci.org/securesystemslab/zippy)  | [![Build Status](https://badges.herokuapp.com/travis/securesystemslab/zippy?env=ZIPPY_JDK_TYPE=GRAALJVM_OSX&label=Graal%20JVM)](https://travis-ci.org/securesystemslab/zippy) |\r\n\r\nZipPy is a fast and lightweight [Python 3](https://www.python.org/) implementation built using the [Truffle](http://openjdk.java.net/projects/graal/) framework. ZipPy leverages the underlying Java JIT compiler and compiles Python programs to highly optimized machine code at runtime.\r\n\r\nZipPy is currently maintained by [Secure Systems and Software Laboratory](https://ssllab.org) at the \xe2\x80\x8b[University of California, Irvine](http://www.uci.edu/).\r\n\r\n### Short instructions (Using Standard JDK):\r\n\r\n##### Prerequisites:\r\n\r\n1. Install the most recent [JDK 8](http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html)\r\n\r\n#### Getting ZipPy:\r\n\r\n1. Create a working directory ($ZIPPY_HOME)\r\n2. Clone mxtool:\r\n\r\n        $ cd $ZIPPY_HOME\r\n        $ git clone https://github.com/graalvm/mx.git\r\n\r\n3. Append the `mx` build tool directory to your `PATH`.\r\n\r\n        $ export PATH=$ZIPPY_HOME/mx:$PATH\r\n\r\n4. Clone ZipPy:\r\n\r\n        $ git clone https://github.com/securesystemslab/zippy.git\r\n\r\n5. Get all ZipPy's dependencies:\r\n\r\n        $ cd $ZIPPY_HOME/zippy\r\n        $ mx spull\r\n\r\n6. Create a file `$ZIPPY_HOME/zippy/mx.zippy/env` and add JDK path\r\n\r\n        JAVA_HOME=/path/to/jdk8\r\n        DEFAULT_VM=server\r\n\r\n> For instructions on [using Graal JVM (recommended)](https://github.com/securesystemslab/zippy/tree/master/doc/graal.md).\r\n\r\n> For more information please visit the [ZipPy Wiki](https://github.com/securesystemslab/zippy/wiki).\r\n\r\n\r\n### Build:\r\n\r\n    $ cd $ZIPPY_HOME/zippy\r\n    $ mx build\r\n\r\n### Run:\r\n\r\n    $ cd $ZIPPY_HOME/zippy\r\n    $ mx python <file.py>\r\n\r\n### Test:\r\n\r\n    $ cd $ZIPPY_HOME/zippy\r\n    $ mx junit\r\n\r\nFor more details and instructions for downloading and building the system, please visit the [ZipPy Wiki](https://github.com/securesystemslab/zippy/wiki).\r\n""",ZipPy is a Python3 interpreter on top of Truffle framework
https://github.com/Tiryoh/probrobo_note,b'# probrobo_note\nTest repository\n\n# References\n* [ryuichiueda/probrobo_practice](https://github.com/ryuichiueda/probrobo_practice)\n* [Kuwamai/probrobo_note](https://github.com/Kuwamai/probrobo_note)\n',Probabilistic Robotics Test Repository
https://github.com/cben/ansible_jupyter_kernel,"b'ansible_jupyter_kernel\n======================\n\nWIP Jupyter kernel for executing Ansible plays\n\nUsage\n-----\n\n    $ git clone https://github.com/cben/ansible_jupyter_kernel\n    $ cd ansible_jupyter_kernel\n    $ pip3 install . [--user]\n\n(can\'t `pip install ansible_jupyter_kernel`, not uploaded to PyPI yet)\n\nNote that ""develop mode"" `pip install -e .` does not copy `kernel.json` into the correct place.\nYou can manually create a directory `/usr/share/jupyter/kernels/ansible/`, `~/.local/share/jupyter/kernels/ansible/`, or `<virtualenv>/share/jupyter/kernels/ansible/` and copy `kernel.json` there.\n\nLicense\n-------\n\nGPL v3 or later, same as Ansible.\n\nPrior Art\n---------\n\nSome people have been using Ansible inside Jupyter:\n\n- https://github.com/NII-cloud-operation/Literate-computing-Basics\n  - https://www.youtube.com/watch?v=xyfdufiibQk\n- http://enakai00.hatenablog.com/entry/2016/04/22/204125\n- https://chusiang.gitbooks.io/automate-with-ansible/content/07.how-to-practive-the-ansible-with-jupyter1.html\n- https://www.slideshare.net/irix_jp/osc2016-kyoto-heat-ansible-jupyter\n- https://chusiang.gitbooks.io/automate-with-ansible/content/07.how-to-practive-the-ansible-with-jupyter1.html,\n  https://chusiang.gitbooks.io/automate-with-ansible/content/08.how-to-practive-the-ansible-with-jupyter2.html\n\nAFAICT all these are running a Python kernel and shelling out using `!ansible` syntax, or `%%writefile ...playbook.yml` followed by `!ansible-notebook`.\n\nThat might actually be more flexible (and less buggy) than a kernel that only runs ansible like I\'m doing here \xe2\x80\x94 and they clearly unlike me have lots of experience actually achieving things using Ansible inside Jupyter \xe2\x80\x94 but the purpose of this exercise was seeing what I can gain from writing a kernel...\n\n- But https://github.com/NII-cloud-operation also have some nbextensions\n'",[PyCon IL 2017] WIP Jupyter kernel for executing Ansible plays
https://github.com/ezdatascience/ezpython,"b'# python\n# used for saving python code\n# for now, it is ipython notebook\n'",ez's python repository
https://github.com/franpe1/starting_with_python_ver2,b'# starting_with_python_ver2\nExcersices and Data for Starting with Python Ver2 book\n',Excersices and Data for Starting with Python Ver2 book
https://github.com/coolcalbeans/PEPredictor,"b'PEPredictor\n===========\n\nThe project parsed through a data file containing 70 fundamental / momentum attributes (Returns, Volatility, RSI etc) for ~2200 US Equity Tickers and use KNN algorithm to find the nearest neighbor for a given stock and determines its Predicted fundamental ratios (P/E, P/B and P/S). Also generated a 95% confidence range for price using yearly volatility of the equity. Later improvements, as a part of the Data Science capstone project @ General Assembly, took advantage of the Pandas and Gradient Boosted Regression Techniques from Scikit learn. \n'","The project parsed through a data file containing 70 fundamental / momentum attributes (Returns, Volatility, RSI etc) for ~2200 US Equity Tickers and use KNN algorithm to find the nearest neighbor for a given stock and determines its Predicted fundamental ratios (P/E, P/B and P/S). Also generated a 95% confidence range for price using yearly volatility of the equity. Later improvements, as a part of the Data Science capstone project @ General Assembly, took advantage of the Pandas and Gradient Boosted Regression Techniques from Scikit learn. "
https://github.com/hfoffani/GAN_MNIST,b'# Simple Number Generation with GAN\n\nA GAN that generates numbers from MNIST data.\n\n\n### Instructions\n\nI recommend Anaconda to run this notebook.\nSpecifically Miniconda https://conda.io/miniconda.html\nFollow the instrunctions in that page and\ncreate a new environment using the file `environment.yml`\nthat is provided in this project.\n\n',A GAN that generates numbers from MNIST dataset
https://github.com/biocore/qiime-workshops,b'',Materials for biocore organized workshops
https://github.com/niharikabalachandra/Linear-regression-MiniProject,b'# Linear-regression\nLinear Regression using Boston Housing data set. The Boston Housing data set contains information about the housing values in suburbs of Boston. This dataset was originally taken from the StatLib library which is maintained at Carnegie Mellon University and is now available on the UCI Machine Learning Repository.\n',Linear Regression using Boston Housing data set. The Boston Housing data set contains information about the housing values in suburbs of Boston. This dataset was originally taken from the StatLib library which is maintained at Carnegie Mellon University and is now available on the UCI Machine Learning Repository.
https://github.com/vishal-keshav/Conv-neural-network,"b'# Conv-neural-network\nAn application of Tensorflow and caffee for text, image and speech recognition\n'",An application of convolutional neural network algorithms with caffee library and python interface for image and speech detection
https://github.com/RyosukeHonda/CarND-LaneLines-P1,"b'#**Finding Lane Lines on the Road** \n<img src=""laneLines_thirdPass.jpg"" width=""480"" alt=""Combined Image"" />\n\nWhen we drive, we use our eyes to decide where to go.  The lines on the road that show us where the lanes are act as our constant reference for where to steer the vehicle.  Naturally, one of the first things we would like to do in developing a self-driving car is to automatically detect lane lines using an algorithm.\n\nIn this project you will detect lane lines in images using Python and OpenCV.  OpenCV means ""Open-Source Computer Vision"", which is a package that has many useful tools for analyzing images.  \n\n**Step 1:** Getting setup with Python\n\nTo do this project, you will need Python 3 along with the numpy, matplotlib, and OpenCV libraries, as well as Jupyter Notebook installed. \n\nWe recommend downloading and installing the Anaconda Python 3 distribution from Continuum Analytics because it comes prepackaged with many of the Python dependencies you will need for this and future projects, makes it easy to install OpenCV, and includes Jupyter Notebook.  Beyond that, it is one of the most common Python distributions used in data analytics and machine learning, so a great choice if you\'re getting started in the field.\n\nChoose the appropriate Python 3 Anaconda install package for your operating system <A HREF=""https://www.continuum.io/downloads"" target=""_blank"">here</A>.   Download and install the package.\n\nIf you already have Anaconda for Python 2 installed, you can create a separate environment for Python 3 and all the appropriate dependencies with the following command:\n\n`>  conda create --name=yourNewEnvironment python=3 anaconda`\n\n`>  source activate yourNewEnvironment`\n\n**Step 2:** Installing OpenCV\n\nOnce you have Anaconda installed, first double check you are in your Python 3 environment:\n\n`>python`    \n`Python 3.5.2 |Anaconda 4.1.1 (x86_64)| (default, Jul  2 2016, 17:52:12)`  \n`[GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)] on darwin`  \n`Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.`  \n`>>>`   \n(Ctrl-d to exit Python)\n\nrun the following commands at the terminal prompt to get OpenCV:\n\n`> pip install pillow`  \n`> conda install -c https://conda.anaconda.org/menpo opencv3`\n\nthen to test if OpenCV is installed correctly:\n\n`> python`  \n`>>> import cv2`  \n`>>>`  \n(Ctrl-d to exit Python)\n\n**Step 3:** Installing moviepy  \n\nWe recommend the ""moviepy"" package for processing video in this project (though you\'re welcome to use other packages if you prefer).  \n\nTo install moviepy run:\n\n`>pip install moviepy`  \n\nand check that the install worked:\n\n`>python`  \n`>>>import moviepy`  \n`>>>`  \n(Ctrl-d to exit Python)\n\n**Step 4:** Opening the code in a Jupyter Notebook\n\nYou will complete this project in a Jupyter notebook.  If you are unfamiliar with Jupyter Notebooks, check out <A HREF=""https://www.packtpub.com/books/content/basics-jupyter-notebook-and-python"" target=""_blank"">Cyrille Rossant\'s Basics of Jupyter Notebook and Python</A> to get started.\n\nJupyter is an ipython notebook where you can run blocks of code and see results interactively.  All the code for this project is contained in a Jupyter notebook. To start Jupyter in your browser, run the following command at the terminal prompt (be sure you\'re in your Python 3 environment!):\n\n`> jupyter notebook`\n\nA browser window will appear showing the contents of the current directory.  Click on the file called ""P1.ipynb"".  Another browser window will appear displaying the notebook.  Follow the instructions in the notebook to complete the project.  \n'",Finding Lane Lines on the Road
https://github.com/zeroviscosity/d3-js-step-by-step,"b""## D3.js Step by Step\n\nCheck out the [tutorial](http://zeroviscosity.com/category/d3-js-step-by-step).\n\n#### NOTE: Examples now target D3 v4.\n\nHere's what changed in this repo from v3 to v4:\n\n* `d3.scale.category20b()` became `d3.scaleOrdinal(d3.schemeCategory20b)`\n* `d3.svg.arc()` became `d3.arc()`\n* `innerRadius()` needs to be explicitly defined so it is now `.innerRadius(0)` for the pie chart\n* `d3.layout.pie()` became `d3.pie()`\n""",http://zeroviscosity.com/category/d3-js-step-by-step
https://github.com/elhanarinc/deeplearning,b'# Ceng 783 Deep Learning\n\n* Check the website for information: **_http://www.kovan.ceng.metu.edu.tr/~sinan/DL/_**',Ceng 783 Deep Learning Assignments
https://github.com/patrickfuller/imolecule,"b'imolecule\n=========\n\nAn embeddable webGL molecule viewer and file format converter.\nhttp://patrickfuller.github.io/imolecule/\n\nExamples\n========\n\n * [IPython notebook](http://patrickfuller.github.io/imolecule/examples/ipython.html)\n * [metal-organic frameworks](http://patrickfuller.github.io/imolecule/examples/mof.html)\n\nIPython\n=======\n\nThe IPython notebook is an open-source tool poised to replace MATLAB in many\napplications. As a scientist of sorts, I\'m all about it. Therefore, I made\nhandles to use imolecule with the notebook. Install through pip:\n\n```\npip install imolecule\n```\n\nOpen a new notebook and test the setup by typing:\n\n```python\nimport imolecule\nimolecule.draw(""CC1(C(N2C(S1)C(C2=O)NC(=O)CC3=CC=CC=C3)C(=O)O)C"")\n```\n\ninto a notebook cell. This should convert, optimize and draw the specified\nSMILES structure (in this case, penicillin) into the notebook.\n\nNote that this requires Open Babel to function. If you do not have Open Babel,\nsee below for installation details.\n\nThe drawer can handle any format specified [here](http://openbabel.org/docs/2.3.1/FileFormats/Overview.html),\nand can be set up to better handle different use cases. Check out the docstrings\nassociated with the IPython interface for more.\n\nServer\n======\n\nIf you want to run the file format converter on your own computer, install the library with:\n\n```\npip install imolecule\n```\n\nThen run from the command line with:\n\n```\nimolecule\n````\n\nThe default site allows for loading molecules via a simple file drag-and-drop\ninterface.  Drag a file to anywhere in the browser and drop to load. This\ninterface communicates with openbabel via websocket, so most file formats should\nwork. Be sure to set the extensions of your files to their data type (ie. ""mol"",\n""pdb"", etc.) for format inference to work properly.\n\nIf you have an existing web server, tornado can be easily switched out for\nother libraries. If you want to use imolecule as a starting point for a\nbroader user interface, the server is written to be extensible. In both cases,\nread through the source code - it\'s surprisingly short.\n\nJavascript\n==========\n\nStart by downloading the minified javascript file:\n\n```\nwget https://raw.githubusercontent.com/patrickfuller/imolecule/master/imolecule/js/build/imolecule.min.js\n```\n\nInclude this file alongside [jQuery](http://jquery.com/) in your project, and then use with:\n\n```javascript\nimolecule.create(\'my-selector\');\nimolecule.draw(myMolecule);\n```\n\nwhere `\'my-selector\'` is where you want to place imolecule, and `myMolecule` is\nan object. See below for more on the object structure, or just check out the\nincluded examples. The `imolecule.create()` method takes a few optional parameters,\nspecifying a few common drawing and camera types.\n\n```javascript\noptions = {\n    drawingType: ""ball and stick"", // Can be ""ball and stick"", ""wireframe"", or ""space filling""\n    cameraType: ""perspective"", // Can be ""perspective"" or ""orthogonal""\n    shader: ""toon"" // three.js shader to use, can be ""toon"", ""basic"", ""phong"", or ""lambert""\n};\n```\n\nMolecule Data Format\n====================\n\nAt its core, imolecule takes input chemical structures as javascript objects.\nAs an example, consider benzene:\n\n```javascript\n{\n    atoms: [\n        { element: ""C"", location: [ -0.762160, 1.168557, 0.022754 ] },\n        { element: ""C"", location: [ 0.631044, 1.242862, -0.013022 ] },\n        { element: ""C"", location: [ 1.391783, 0.076397, -0.012244 ] },\n        { element: ""C"", location: [ 0.762101, -1.168506, 0.026080 ] },\n        { element: ""C"", location: [ -0.631044, -1.242903, -0.011791 ] },\n        { element: ""C"", location: [ -1.391806, -0.076430, -0.014083 ] },\n    ],\n    bonds: [\n        { atoms: [ 0, 1 ], order: 2 },\n        { atoms: [ 1, 2 ], order: 1 },\n        { atoms: [ 2, 3 ], order: 2 },\n        { atoms: [ 3, 4 ], order: 1 },\n        { atoms: [ 4, 5 ], order: 2 },\n        { atoms: [ 0, 5 ], order: 1 }\n    ]\n}\n```\n\nIf you want to make properly formatted JSON, you can use either `format_converter.py` as a script or run your own imolecule server to convert most chemical file formats to JSON.\n\nOpen Babel\n==========\n\n[Open Babel](http://openbabel.org/wiki/Main_Page) is an open-source library\nfor interconverting over 100 chemical file formats. imolecule uses Open Babel\nto convert input formats to JSON before drawing. Therefore, to use imolecule\nwith non-JSON formats, you will need Open Babel.\n\nOpen Babel is best installed through conda.\n\n```\nconda install -c conda-forge openbabel\n```\n\nIf you don\'t use conda, Open Babel can be installed from source. For more, read through the\n[Open Babel installation instructions](http://openbabel.org/docs/dev/Installation/install.html).\n\n```\ngit clone https://github.com/openbabel/openbabel\nmkdir build && cd build\ncmake ../openbabel -DPYTHON_BINDINGS=ON\nmake && make install\n```\n'",An embeddable webGL molecule viewer and file format converter.
https://github.com/nazmiasri95/Bidirectional-LSTM,b'# Bidirectional-LSTM\nExperimental on Bidirectional LSTM and comparison with LSTM\n',Experimental on Bidirectional LSTM and comparison with LSTM
https://github.com/tphinkle/pore_stats,"b'![pore_stats logo](https://github.com/tphinkle/pore_stats/blob/master/qt_app/resources/logo.png)\n\n\n## Contents\n1. Overview\n2. Event extraction\n3. Analysis\n\n## 1. Overview\npore_stats is a software library written in Python for analyzing [resistive pulse](https://en.wikipedia.org/wiki/Tunable_resistive_pulse_sensing) experimental data. The library consists of a GUI program written in PyQt for extracting pulses from the baseline, and modules for analyzing the extracted events.\n\n## 2. Extraction\n\n#### Feature highlights\n\n- Single events are detected and extracted automatically, even from signals with drifting or jagged baselines.\n\n![Events found in baseline](https://github.com/tphinkle/pore_stats/blob/master/qt_app/demo/full_view.png)\n\n- The program allows the user to change the parameters most relevant to the detection algorithm.\n\n- A low-pass filter can be used to reduce noise and find events that are buried in the baseline.\n\n![Hidden events](https://github.com/tphinkle/pore_stats/blob/master/qt_app/demo/filter_demo.png)\n\n#### Event validation\n\n- Detected events can be rejected after detection in one of three ways:\n\n1. Manual accept/reject decision making\n\t- Commands to scroll through events and accept/reject are bound to simple hot keys that make manual review of the events as fast as possible.\n\n2. Population slicing\n\t- A region-of-interest (ROI) square can be dragged on the amplitude-duration scatter plot to remove events from regions known to contain undesirable events (e.g., double events with amplitudes that are too large, spurious short-lived noise spikes that were detected as events, etc.)\n\n![Scatter plot view](https://github.com/tphinkle/pore_stats/blob/master/qt_app/demo/scatter_plot_view.png)\n\n\n\n3. Machine learning\n\t- Whenever the event data is saved, the raw data and decision for each event is saved to a separate file. The saved data for all the events constitutes a data set for training a model to make future accept/reject binary decisions on new events. Currently the training data is saved automatically, but the model must be trained manually. After training a model in scikit-learn, it must be pickled and placed in the correct directory for the GUI program to locate it. This method is functional, but will require some hacking to work; unlike the other two methods, this method doesn\'t work out of the box (for now).\n\n## 3. Analysis\n\n- The pore_stats event analysis libraries can automatically determine the event __amplitude__, __duration__, __local minima and maxima__, and __current levels__ for non-constant pulses.\n\n- Events are loaded in from the file produced by the event extraction program. An RP event is instantiated as an object of type RPEvent, a class that bundles the event\'s data and methods for performing calculations and transformations on the data.\n\n#### Gallery\n\nHere are some plots of the data created by using the pore_stats analysis library.\n\n<img src=""https://github.com/tphinkle/pore_stats/blob/master/qt_app/demo/analysis_gallery/HCT-116_multievent_7-10.png"" alt=""multievent"" height=""400""/> <img src=""https://github.com/tphinkle/pore_stats/blob/master/qt_app/demo/analysis_gallery/HCT-116_7-29_15um-20um_8-2_scatter.png"" alt=""scatter"" height=""400""/>\n\n<img src=""https://github.com/tphinkle/pore_stats/blob/master/qt_app/demo/analysis_gallery/event_durations.png"" alt=""Event durations"" height=""325""/> <img src=""https://github.com/tphinkle/pore_stats/blob/master/qt_app/demo/analysis_gallery/filtered_psmix.png"" alt=""filtered psmix"" height=""325""/>\n\n<img src=""https://github.com/tphinkle/pore_stats/blob/master/qt_app/demo/analysis_gallery/HCT-116_peak-distributions_7-29_pr0006.png"" alt=""Peak distributions"" height=""400""/> \n'","Event detection, extraction, and analysis in micro and nano resistive pulse experiments."
https://github.com/davidaknowles/tf_net,"b""## A custom CNN for the DREAM ENCODE challenge\n\nThis is a pretty standard convolutional neural net on genomic sequence with the following features added:\n* normalized per-base DNase I cuts for the + and - strand are concatenated onto the one hot encoding of sequence, to give a [sequence context] x 6 input matrix.\n* gene expression PCs are included as features to allow the model to interpolate between different cell types.\n* a three class ordinal likelihood is used for the Unbound/Ambiguous/Bound labels.\n* simultaneous analysis of the forward and reverse complement.\n* down-sampling of the negative set to speed up training (and accounting for by weighting the likelihood). \n\nFrom the round 2 leaderboard you can see performance is highly competitive for some TFs (e.g. MAX https://www.synapse.org/#!Synapse:syn6131484/wiki/402503) and less so for others (e.g. REST https://www.synapse.org/#!Synapse:syn6131484/wiki/402505).\n\nThe repo is intended to be fully self contained (save dependencies on synapseclient, pysam and pyDNase python packages), including programmatic download of challenge data, pre-processing, model fitting, prediction and submission.\n\n[METHODS.ipynb](METHODS.ipynb) goes through the math for the ordinal likelihood, negative set downsampling and forward/RC model. \n\n### Installation\n\nYou'll need the following python packages: pysam, pyDNase, scikit-learn (for performance metrics), synapseclient (for downloading the data and submitting), numpy, scipy, theano. \n\n### Usage\n\nThe script `run_all.sh` will in principle run all of these steps for you. Realistically you'll want to train each TF model (and probably do the DNase pre-processing) on a cluster since this is pretty time consuming (10ish hours). \n\n1. Set a data location, e.g. add something like the following to your ~/.bash_profile\n```\nexport DREAM_ENCODE_DATADIR=/myscratchspace/dream_encode/\n```\n\n2. Download the challenge data using `download_challenge_data.py`, but note you'll need to set your Synapse email/password in that script.\n\n3. [optional] Calculate gene expression PCs using `gene_expression_pca.R`. I included the output file, 'ge_pca.txt' so you don't strictly need to rerun this. If you do want to do this yourself you'll need the R packages irlba and foreach.\n\n4. Calculate DNase I cut counts using the `get_DNase_cuts.py` script. This converts the DNase I bams into an efficient numpy representation of cut counts saved in .npz files. The bam first need indexing (e.g. using samtools). `index.sh` will do this for you. \n\n5. Train models for each TF using `train.py`. This script includes outputting leaderboard and final submissions.\n\n6. Submit to Synapse using `submit.py`. Note you'll need to set up a folder in Synapse to use for this and set the id in the script. """,A custom conv net for the DREAM ENCODE challenge
https://github.com/mmt/deeprl22,"b""# Deep Reinforcement Learning Study Group\n\nThis repository is a clone of the Spring 2017 deep reinforcement\nlearning [class](http://rll.berkeley.edu/deeprlcourse/) at Berkeley.\nWe've cloned it for the purpose of having a collaborative study group\nwatching the lectures and working on the problem sets.\n\n# Resources.\n## Lecture Material\n\n- Google's deep learning Udacity [course](https://www.udacity.com/course/deep-learning--ud730) seems like a good introduction.\n\n- Professor Bertsekas's Dynamic Programming [lectures](https://www.youtube.com/playlist?list=PLiCLbsFQNFAxOmVeqPhI5er1LGf2-L9I4) seem like a good supplement to these lectures.\n\n## Papers\n\n- Yann Lecun's [paper](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf) discusses\n  tricks for normalization and initialization.\n\n## Blog Posts etc.\n\n- A [post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) describing RNNs.\n- A [post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) explaining LSTMs.\n- A pair of posts on [seq2seq](https://indico.io/blog/sequence-modeling-neuralnets-part1/)\n  and [attention](https://indico.io/blog/sequence-modeling-neuralnets-part1/).\n- A [post](http://karpathy.github.io/2016/05/31/rl/) giving general background on RL and diving into score function gradient estimators.\n\n## Books\n\n- [The Deep Learning Book](http://www.deeplearningbook.org/).\n\n## Software\n\n### Tensorflow\n\n- [Tutorials](https://www.tensorflow.org/tutorials/mandelbrot)\n\n- [Tensorboard](https://www.tensorflow.org/get_started/summaries_and_tensorboard) seems like a good way to instrument training algorithms.\n\n- [Supervisor](https://www.tensorflow.org/programmers_guide/supervisor)\n\n- [Threading and queues](https://www.tensorflow.org/programmers_guide/threading_and_queues)\n\n- [TF Slim](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim) might save time.\n\n### Other\n\n- [Pudb](https://pypi.python.org/pypi/pudb) seems like a useful debug tool.\n\n## Other courses\n- [Stanford CS class CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io) - another class more focused on convnets\n""",Study group for Deep Reinforcement Learning
https://github.com/Patrick-Woo/CarND-LaneLines-P1,"b'#**Finding Lane Lines on the Road** \n\n---\nThe goals / steps of this project are the following:\n* Make a pipeline that finds lane lines on the road\n* Reflect on your work in a written report\n\n\n[//]: # (Image References)\n\n\n<img width=""450"" height=""250"" src=""./test_images/solidWhiteCurve_modified.jpg""/>\n\n---\n\n### Reflection\n\n###1. Describe your pipeline. \nMy pipeline consisted of 5 steps:\n\n* Convert the images to grayscale<br>\n* Perform Gausian smoothing and apply Canny edge detection<br>\n* Select region of interest and mask other areas of the image<br>\n* Apply Hough Transform to detect lane lines<br>\n* Superimpose the lane lines on the original image<br>\n\nIn order to draw a single line on the left and right lanes, I modified the draw_lines() function by:\n\n* Calculated slope and center of each line. <br>\n* Then based on the slope, sort it into right or left lane line<br>\n* Calculate the average slope and the center of right and left lane<br>\n* Then using the Y coordinates, based on Region of Interest, figure out the X cordinates using the avg slope and center point of lane lines [equation: (y-y\') = M (x-x\')]\n\nThe explaination of this equation: (y-y\') = M (x-x\') is as follows:<br>\ny is equal with ymax and y\' is equal with y_avg.<br>\nAs (xmax,ymax) and (x_avg,y_avg) are two points that are located in the same line. <br>\nAs a result, the line has the unique M and b.<br>\n\ny=Mx+b,  b=y-Mx<br>\ny\'=Mx\'+b,\tb=y\'-Mx\'<br>\nb=b, y-Mx=y\'-Mx\'<br>\n(y-y\') = M (x-x\')<br>\n\nReplacing xmax,x_avg,y_max,y-avg to this equation, I get the result below:<br>\nymax-y_avg=M(xmax-x_avg)<br>\nxmax=x_avg-(y_avg-ymax)/slope_avg    <br>      \n\nThen put above xmax into draw_lines function and calculate the xmax and xmin cordinates for both right and left sides.\n \n\n\n\n###2. Identify potential shortcomings with your current pipeline\n\n\nPotential shortcomings of my approach:\n\n* The region of interest in Image masking is static, hence it can only work in specific scenarios\n\n* Slope conditions used for detecting right and left lanes do not work in case of a curve in the road\n\n\n###3. Suggest possible improvements to your pipeline\n\nPossible improvements to my approach:\n\n* Make the image mask selection dynamic, so that it could work in different scenarios\n\n* Modify the slope conditions, so that they work with curve in the road\n'",This is my first update.
https://github.com/balaganesh99/project,b'# project\nproject\n',project
https://github.com/todddangerfarr/mlnd_p4_train_a_smartcab_to_drive,"b'# Train a Smartcab How to Drive\n\nUdacity Machine Learning - Reinforcement Learning Project\nThe goal of this project was to implement a Q-Learning algorithm so that a smartcab agent can learn to navigate to a destination within a predefined time in a grid-world.\n\nThe methodology for the implementation of this code is found in the report found in the main directory of this repository.  \n\n## Install\n\nThis project requires Python 2.7 with the pygame library installed:\n\nhttps://www.pygame.org/wiki/GettingStarted\n\n## Code\n\nOpen `smartcab/agent.py` and implement `LearningAgent`. Follow `TODO`s for further instructions.\n\n## Run\n\nMake sure you are in the top-level project directory `smartcab/` (that contains this README). Then run:\n\n```python smartcab/agent.py```\n\nOR:\n\n```python -m smartcab.agent```\n\n## Automated Data Collection\n\nTo run the automated data collection uncomment the script according to single scenario or iterative.\n\n#### For iterative collection\n- Comment out the Single Scenario Function call\n- set the number of times for each iteration\n- choose ranges and steps for Gamma, Epsilon and Epsilon Decay\n\n```sh\n    gamma_values = [(x / 100.0) for x in  range(30, 91, 10)]\n    epsilon_values = [(x / 100.0) for x in range(50, 55, 10)]\n    epsilon_decay_values = [(x / 100.0) for x in range(90, 100, 3)]\n\n    number_of_times = 5\n        for i in range(0, number_of_times):\n        iterative_data_collection(gamma_values, epsilon_values, epsilon_decay_values, str(i))\n```\n\n- Run ```python automated_data_collection.py```\n\n#### For Single Scenario\n- Comment out the Iterative Data Collection Function Call\n- Set the variables for Gamma, Epsilon, Epsilon Decay and the number of times\n\n```sh\n    single_scenario_repeat_data_collection(0.50, 0.50, 0.99, 10)\n```\n\n- Run ```python automated_data_collection.py```\n'",Udacity Machine Learning NanoDegree Project 4 - Reinforcement Learning: Train a Smartcab to Drive 
https://github.com/rohitvarkey/Compose3D.jl,"b""# Compose3D\n\nCompose3D is a Julia package written to try and extend [Compose](http://composejl.org/) to 3-D. Currently, only WebGL backends using Three.JS are supported which allows you to draw 3D in a Jupyter notebook. The long term goal would be to integrate this package with the Compose package.\n\nPlease check the exp folder for some example IJulia notebooks.\n\n# Documentation\n\n### Contexts\n\nContexts are the things that you are able to draw. Contexts are created by specifying an origin point and the width, height and depth of the required context. \n\nYou can use the *Context* constructor to create a Context. \n\n* **Context(x0,y0,z0,width,height,depth)** - *This will return a Context created which has it's coordinate system relative to (x0,y0,z0) and a width of 'width', height of 'height', and depth of 'depth'.* \n\n### Geometries (Forms)\n\nGeometries look to provide the user with primitives for creation of 3-D shapes.\n\nCurrent primitives implemented are :\n\n   * Cubes\n   * Spheres\n   * Pyramids\n\nFunctions available for users to use to create such geometries are:\n\t\n* **cube(x0,y0,z0,side)** - *Returns a cube centered at (x0,y0,z0) and of the specified `side`*\n* **sphere(x0,y0,z0,radius)** - *Returns a sphere centered at (x0,y0,z0) with the specified radius.*\n* **pyramid(x0,y0,z0,baseLength, h)** - *Returns a square base pyramid of base length 'baseLength' with a corner at (x0,y0,z0) and the specified height 'h'.*\n\n### Materials (Properties)\n\nMaterials are to add properties to the 3D objects like color and texture maps. \n\nFunctions available currently are:\n\n* **mesh_color(color)** - `color` can be a string or any Color in Color.jl that can be converted to RGB. All geometries that are children of this node and are part of the subtree at the same level will be coloured with this color. \n\n### Compositions\n\n**Drawing things** is done by composing a root Context with other Contexts or Geometries.   \n\nCompositions work exactly like in Compose except for :\n\n* Measures of the parent Context is resolved by adding 'w','h' and 'd' rather than just providing numbers in Compose. \n* There is no support for using the width, height and the depth of the root box as of now like 'w','h' and 'd' do in Compose.\n* The root Context has to have absolute values rather than relative values.\n\nThe function to be used by the user is the *compose* function. \n\n* **compose(Context, Geometry)** - *Returns a new context after adding the geometry object to the context. The geometry's relative measures are converted to absolute measures based on the parent context.*\n* **compose(Context, Context)** - *Returns a new context after adding the context object to the parent context. The contexts relative measures are converted to absolute measures based on the parent context.*\n\nCompose can also take inputs in S-tree formats to build a tree. This saves the user from having to call compose again and again.\n\n### Measures\n\nCompose3D makes use of a similar measure system to Compose. The basic unit is of *'mm'*.\n\nAbsolute measure units can be used like :\n\n* cm - 10mm\n* inch - 25.4mm\n* pt - inch/72\n\nRelative measures can also be used where the units will be :\n\n* w - width\n* h - height\n* d - depth\n\nAbsolute and relative measures can be combined.\n  \n### Examples\n\n```julia\ncube1 = cube(0mm,0mm,0mm,1mm) # a cube of size 1 unit at the origin.\nsphere1 = sphere(-5mm,0mm,0mm,0.1w) # a sphere with radius 1/10th of the width of the parent context and centered at (-5,0,0).\npyramid1 = pyramid(0mm,0mm,0.4d,2mm,1mm) # a pyramid with a corner at (0,0, 0.4*depth of parent context) of base 2mm and height 1mm.\ncontext1 = Context(0mm,0mm,0mm,10mm,10mm,10mm) # context with origin at (0,0,0) and all dimensions of 10 units \ncontext2 = compose(context1,cube1) # Returns context with the cube.\ncontext3 = compose(context2, sphere1, pyramid1) # Returns a context with all the geometries. Notice how we added 2 children in one line.\n```\n\nThe drawing of a Sierpenski Pyramid can be done with the following code:\n```julia\nfunction sierpinski(n)\n    if n == 0\n        compose(Context(0w,0h,0d,1w,1h,1d),pyramid(0w,0h,0d,1w,1h))\n    else\n        t = sierpinski(n - 1)\n        compose(Context(0w,0h,0d,1w,1h,1d),\n        (Context(0w,0h,0d,(1/2)w,(1/2)h,(1/2)d), t),\n        (Context(0.5w,0h,0d,(1/2)w,(1/2)h,(1/2)d), t),\n        (Context(0.5w,0.5h,0d,(1/2)w,(1/2)h,(1/2)d), t),\n        (Context(0w,0.5h,0d,(1/2)w,(1/2)h,(1/2)d), t),\n        (Context(0.25w,0.25h,0.5d,(1/2)w,(1/2)h,(1/2)d), t))\n    end\nend\n\ncompose(Context(-5mm,-5mm,-5mm,10mm,10mm,10mm),sierpinski(0))\n```\n\n### Acknowledgements\n\n[pranavtbhat](https://www.github.com/pranavtbhat) for supplying the initial JavaScript code and getting this project started.\n""",A library to try and extend Compose.jl to 3D.
https://github.com/catherinedevlin/ipython-sql,"b'===========\nipython-sql\n===========\n\n:Author: Catherine Devlin, http://catherinedevlin.blogspot.com\n\nIntroduces a %sql (or %%sql) magic.\n\nLegacy project\n--------------\n\nIPython-SQL\'s functionality and maintenance have been eclipsed by JupySQL_, a fork maintained and developed by the Ploomber team.  Future work will be directed into JupySQL - please file issues there, as well!\n\nDescription\n-----------\n\nConnect to a database, using `SQLAlchemy URL`_ connect strings, then issue SQL\ncommands within IPython or IPython Notebook.\n\n.. image:: https://raw.github.com/catherinedevlin/ipython-sql/master/examples/writers.png\n   :width: 600px\n   :alt: screenshot of ipython-sql in the Notebook\n\nExamples\n--------\n\n.. code-block:: python\n\n    In [1]: %load_ext sql\n\n    In [2]: %%sql postgresql://will:longliveliz@localhost/shakes\n       ...: select * from character\n       ...: where abbrev = \'ALICE\'\n       ...:\n    Out[2]: [(u\'Alice\', u\'Alice\', u\'ALICE\', u\'a lady attending on Princess Katherine\', 22)]\n\n    In [3]: result = _\n\n    In [4]: print(result)\n    charid   charname   abbrev                description                 speechcount\n    =================================================================================\n    Alice    Alice      ALICE    a lady attending on Princess Katherine   22\n\n    In [4]: result.keys\n    Out[5]: [u\'charid\', u\'charname\', u\'abbrev\', u\'description\', u\'speechcount\']\n\n    In [6]: result[0][0]\n    Out[6]: u\'Alice\'\n\n    In [7]: result[0].description\n    Out[7]: u\'a lady attending on Princess Katherine\'\n\nAfter the first connection, connect info can be omitted::\n\n    In [8]: %sql select count(*) from work\n    Out[8]: [(43L,)]\n\nConnections to multiple databases can be maintained.  You can refer to\nan existing connection by username@database\n\n.. code-block:: python\n\n    In [9]: %%sql will@shakes\n       ...: select charname, speechcount from character\n       ...: where  speechcount = (select max(speechcount)\n       ...:                       from character);\n       ...:\n    Out[9]: [(u\'Poet\', 733)]\n\n    In [10]: print(_)\n    charname   speechcount\n    ======================\n    Poet       733\n\nIf no connect string is supplied, ``%sql`` will provide a list of existing connections;\nhowever, if no connections have yet been made and the environment variable ``DATABASE_URL``\nis available, that will be used.\n\nFor secure access, you may dynamically access your credentials (e.g. from your system environment or `getpass.getpass`) to avoid storing your password in the notebook itself. Use the `$` before any variable to access it in your `%sql` command.\n\n.. code-block:: python\n\n    In [11]: user = os.getenv(\'SOME_USER\')\n       ....: password = os.getenv(\'SOME_PASSWORD\')\n       ....: connection_string = ""postgresql://{user}:{password}@localhost/some_database"".format(user=user, password=password)\n       ....: %sql $connection_string\n    Out[11]: u\'Connected: some_user@some_database\'\n\nYou may use multiple SQL statements inside a single cell, but you will\nonly see any query results from the last of them, so this really only\nmakes sense for statements with no output\n\n.. code-block:: python\n\n    In [11]: %%sql sqlite://\n       ....: CREATE TABLE writer (first_name, last_name, year_of_death);\n       ....: INSERT INTO writer VALUES (\'William\', \'Shakespeare\', 1616);\n       ....: INSERT INTO writer VALUES (\'Bertold\', \'Brecht\', 1956);\n       ....:\n    Out[11]: []\n\n\nAs a convenience, dict-style access for result sets is supported, with the\nleftmost column serving as key, for unique values.\n\n.. code-block:: python\n\n    In [12]: result = %sql select * from work\n    43 rows affected.\n\n    In [13]: result[\'richard2\']\n    Out[14]: (u\'richard2\', u\'Richard II\', u\'History of Richard II\', 1595, u\'h\', None, u\'Moby\', 22411, 628)\n\nResults can also be retrieved as an iterator of dictionaries (``result.dicts()``)\nor a single dictionary with a tuple of scalar values per key (``result.dict()``)\n\nVariable substitution \n---------------------\n\nBind variables (bind parameters) can be used in the ""named"" (:x) style.\nThe variable names used should be defined in the local namespace.\n\n.. code-block:: python\n\n    In [15]: name = \'Countess\'\n\n    In [16]: %sql select description from character where charname = :name\n    Out[16]: [(u\'mother to Bertram\',)]\n\n    In [17]: %sql select description from character where charname = \'{name}\' \n    Out[17]: [(u\'mother to Bertram\',)]\n\nAlternately, ``$variable_name`` or ``{variable_name}`` can be \nused to inject variables from the local namespace into the SQL \nstatement before it is formed and passed to the SQL engine.\n(Using ``$`` and ``{}`` together, as in ``${variable_name}``, \nis not supported.)\n\nBind variables are passed through to the SQL engine and can only \nbe used to replace strings passed to SQL.  ``$`` and ``{}`` are \nsubstituted before passing to SQL and can be used to form SQL \nstatements dynamically.\n\nAssignment\n----------\n\nOrdinary IPython assignment works for single-line `%sql` queries:\n\n.. code-block:: python\n\n    In [18]: works = %sql SELECT title, year FROM work\n    43 rows affected.\n\nThe `<<` operator captures query results in a local variable, and\ncan be used in multi-line ``%%sql``:\n\n.. code-block:: python\n\n    In [19]: %%sql works << SELECT title, year\n        ...: FROM work\n        ...:\n    43 rows affected.\n    Returning data to local variable works\n\nConnecting\n----------\n\nConnection strings are `SQLAlchemy URL`_ standard.\n\nSome example connection strings::\n\n    mysql+pymysql://scott:tiger@localhost/foo\n    oracle://scott:tiger@127.0.0.1:1521/sidname\n    sqlite://\n    sqlite:///foo.db\n    mssql+pyodbc://username:password@host/database?driver=SQL+Server+Native+Client+11.0\n\n.. _`SQLAlchemy URL`: http://docs.sqlalchemy.org/en/latest/core/engines.html#database-urls\n\nNote that ``mysql`` and ``mysql+pymysql`` connections (and perhaps others)\ndon\'t read your client character set information from .my.cnf.  You need\nto specify it in the connection string::\n\n    mysql+pymysql://scott:tiger@localhost/foo?charset=utf8\n\nNote that an ``impala`` connection with `impyla`_  for HiveServer2 requires disabling autocommit::\n\n    %config SqlMagic.autocommit=False\n    %sql impala://hserverhost:port/default?kerberos_service_name=hive&auth_mechanism=GSSAPI\n\n.. _impyla: https://github.com/cloudera/impyla\n\nConnection arguments not whitelisted by SQLALchemy can be provided as\na flag with (-a|--connection_arguments)the connection string as a JSON string.\nSee `SQLAlchemy Args`_.\n\n    | %sql --connection_arguments {""timeout"":10,""mode"":""ro""} sqlite:// SELECT * FROM work;\n    | %sql -a \'{""timeout"":10, ""mode"":""ro""}\' sqlite:// SELECT * from work;\n\n.. _`SQLAlchemy Args`: https://docs.sqlalchemy.org/en/13/core/engines.html#custom-dbapi-args\n\nDSN connections\n~~~~~~~~~~~~~~~\n\nAlternately, you can store connection info in a \nconfiguration file, under a section name chosen to \nrefer to your database.\n\nFor example, if dsn.ini contains \n\n    | [DB_CONFIG_1] \n    | drivername=postgres \n    | host=my.remote.host \n    | port=5433 \n    | database=mydatabase \n    | username=myuser \n    | password=1234\n\nthen you can  \n\n    | %config SqlMagic.dsn_filename=\'./dsn.ini\'\n    | %sql --section DB_CONFIG_1 \n\nConfiguration\n-------------\n\nQuery results are loaded as lists, so very large result sets may use up\nyour system\'s memory and/or hang your browser.  There is no autolimit\nby default.  However, `autolimit` (if set) limits the size of the result\nset (usually with a `LIMIT` clause in the SQL).  `displaylimit` is similar,\nbut the entire result set is still pulled into memory (for later analysis);\nonly the screen display is truncated.\n\n.. code-block:: python\n\n   In [2]: %config SqlMagic\n   SqlMagic options\n   --------------\n   SqlMagic.autocommit=<Bool>\n       Current: True\n       Set autocommit mode\n   SqlMagic.autolimit=<Int>\n       Current: 0\n       Automatically limit the size of the returned result sets\n   SqlMagic.autopandas=<Bool>\n       Current: False\n       Return Pandas DataFrames instead of regular result sets\n   SqlMagic.column_local_vars=<Bool>\n       Current: False\n       Return data into local variables from column names\n   SqlMagic.displaycon=<Bool>\n       Current: False\n       Show connection string after execute\n   SqlMagic.displaylimit=<Int>\n       Current: None\n       Automatically limit the number of rows displayed (full result set is still\n       stored)\n   SqlMagic.dsn_filename=<Unicode>\n       Current: \'odbc.ini\'\n       Path to DSN file. When the first argument is of the form [section], a\n       sqlalchemy connection string is formed from the matching section in the DSN\n       file.\n   SqlMagic.feedback=<Bool>\n       Current: False\n       Print number of rows affected by DML\n   SqlMagic.short_errors=<Bool>\n       Current: True\n       Don\'t display the full traceback on SQL Programming Error\n   SqlMagic.style=<Unicode>\n       Current: \'DEFAULT\'\n       Set the table printing style to any of prettytable\'s defined styles\n       (currently DEFAULT, MSWORD_FRIENDLY, PLAIN_COLUMNS, RANDOM)\n\n   In[3]: %config SqlMagic.feedback = False\n\nPlease note: if you have autopandas set to true, the displaylimit option will not apply. You can set the pandas display limit by using the pandas ``max_rows`` option as described in the `pandas documentation <http://pandas.pydata.org/pandas-docs/version/0.18.1/options.html#frequently-used-options>`_.\n\nPandas\n------\n\nIf you have installed ``pandas``, you can use a result set\'s\n``.DataFrame()`` method\n\n.. code-block:: python\n\n    In [3]: result = %sql SELECT * FROM character WHERE speechcount > 25\n\n    In [4]: dataframe = result.DataFrame()\n\n\nThe ``--persist`` argument, with the name of a \nDataFrame object in memory, \nwill create a table name\nin the database from the named DataFrame.  \nOr use ``--append`` to add rows to an existing \ntable by that name.\n\n.. code-block:: python\n\n    In [5]: %sql --persist dataframe\n\n    In [6]: %sql SELECT * FROM dataframe;\n\n.. _Pandas: http://pandas.pydata.org/\n\nGraphing\n--------\n\nIf you have installed ``matplotlib``, you can use a result set\'s\n``.plot()``, ``.pie()``, and ``.bar()`` methods for quick plotting\n\n.. code-block:: python\n\n    In[5]: result = %sql SELECT title, totalwords FROM work WHERE genretype = \'c\'\n\n    In[6]: %matplotlib inline\n\n    In[7]: result.pie()\n\n.. image:: https://raw.github.com/catherinedevlin/ipython-sql/master/examples/wordcount.png\n   :alt: pie chart of word count of Shakespeare\'s comedies\n\nDumping\n-------\n\nResult sets come with a ``.csv(filename=None)`` method.  This generates\ncomma-separated text either as a return value (if ``filename`` is not\nspecified) or in a file of the given name.\n\n.. code-block:: python\n\n    In[8]: result = %sql SELECT title, totalwords FROM work WHERE genretype = \'c\'\n\n    In[9]: result.csv(filename=\'work.csv\')\n\nPostgreSQL features\n-------------------\n\n``psql``-style ""backslash"" `meta-commands`_ commands (``\\d``, ``\\dt``, etc.)\nare provided by `PGSpecial`_.  Example:\n\n.. code-block:: python\n\n    In[9]: %sql \\d\n\n.. _PGSpecial: https://pypi.python.org/pypi/pgspecial\n\n.. _meta-commands: https://www.postgresql.org/docs/9.6/static/app-psql.html#APP-PSQL-META-COMMANDS\n\n\nOptions\n-------\n\n``-l`` / ``--connections``\n    List all active connections\n\n``-x`` / ``--close <session-name>`` \n    Close named connection \n\n``-c`` / ``--creator <creator-function>``\n    Specify creator function for new connection\n\n``-s`` / ``--section <section-name>``\n    Section of dsn_file to be used for generating a connection string\n\n``-p`` / ``--persist``\n    Create a table name in the database from the named DataFrame\n\n``--append``\n    Like ``--persist``, but appends to the table if it already exists \n\n``-a`` / ``--connection_arguments <""{connection arguments}"">``\n    Specify dictionary of connection arguments to pass to SQL driver\n\n``-f`` / ``--file <path>``\n    Run SQL from file at this path\n\nCaution \n-------\n\nComments\n~~~~~~~~\n\nBecause ipyton-sql accepts ``--``-delimited options like ``--persist``, but ``--`` \nis also the syntax to denote a SQL comment, the parser needs to make some assumptions.\n\n- If you try to pass an unsupported argument, like ``--lutefisk``, it will \n  be interpreted as a SQL comment and will not throw an unsupported argument \n  exception.\n- If the SQL statement begins with a first-line comment that looks like one \n  of the accepted arguments - like ``%sql --persist is great!`` - it will be \n  parsed like an argument, not a comment.  Moving the comment to the second \n  line or later will avoid this.\n\nInstalling\n----------\n\nInstall the latest release with::\n\n    pip install ipython-sql\n\nor download from https://github.com/catherinedevlin/ipython-sql and::\n\n    cd ipython-sql\n    sudo python setup.py install\n\nDevelopment\n-----------\n\nhttps://github.com/catherinedevlin/ipython-sql\n\nCredits\n-------\n\n- Matthias Bussonnier for help with configuration\n- Olivier Le Thanh Duong for ``%config`` fixes and improvements\n- Distribute_\n- Buildout_\n- modern-package-template_\n- Mike Wilson for bind variable code\n- Thomas Kluyver and Steve Holden for debugging help\n- Berton Earnshaw for DSN connection syntax\n- Bruno Harbulot for DSN example \n- Andr\xc3\xa9s Celis for SQL Server bugfix\n- Michael Erasmus for DataFrame truth bugfix\n- Noam Finkelstein for README clarification\n- Xiaochuan Yu for `<<` operator, syntax colorization\n- Amjith Ramanujam for PGSpecial and incorporating it here\n- Alexander Maznev for better arg parsing, connections accepting specified creator\n- Jonathan Larkin for configurable displaycon \n- Jared Moore for ``connection-arguments`` support\n- Gilbert Brault for ``--append`` \n- Lucas Zeer for multi-line bugfixes for var substitution, ``<<`` \n- vkk800 for ``--file``\n- Jens Albrecht for MySQL DatabaseError bugfix\n- meihkv for connection-closing bugfix\n- Abhinav C for SQLAlchemy 2.0 compatibility\n\n.. _Distribute: http://pypi.python.org/pypi/distribute\n.. _Buildout: http://www.buildout.org/\n.. _modern-package-template: http://pypi.python.org/pypi/modern-package-template\n.. _JupySQL: https://github.com/ploomber/jupysql\n'","%%sql magic for IPython, hopefully evolving into full SQL client"
https://github.com/rmalliga/Roja-Malligarjunan,b'# Roja-Malligarjunan\nRandom\n',Random
https://github.com/JuliaPackageMirrors/SpikingNetworks.jl,b'see [SpikingNetworksTutorial.ipynb](http://nbviewer.jupyter.org/gist/YaoLuCNS/6fcedfc9baff9c72f999)',Julia package mirror.
https://github.com/Ramprasad94/Yelp-Data-Analysis,"b'# Yelp-Data-Analysis\nRepository containing the code for performing data analysis on Yelp reviews and tips.\n\n##Task 1 : Predicting categories of a business from review and tip text\n\n-For each business, create a Lucene document containing the reviews and tip text.\n-Proceed to index the Lucene documents.\n-Create a query list consisting of all the categories from dataset, where each category will be a query term.\n-Compute the scores for each query term. (Score function will be tf-idf)\n-Set a threshold value to give out the top 3 category scores as belonging to the business.\n\n### Similarity Used:\n- Classic Similarity\n- BM25 Similarity\n- LMJM Similarity \n- LMD Similarity\n\n### Evaluation measure: \n- Accuracy with penalties for missed and incorrect predictions.\n\n\n## Task 2: User rating prediction from their review\n\n### Data preprocessing:\n- Clean data by removing stop words, punctuations, numbers\n- Convert all text data in lower case\n- Generate tf-idf matrix from the data\n- Normalize\n\n### Data visualization\n- Understand the distribution of words\n- Feature importance\n\n### Predictive models\n- Split train and test data\n\nWe have used following machine learning algorithms for this task:\n- Linear regression\n- Ridge regression\n- Lasso regression\n- Elasticnet regression\n- K-nearest neighbours\n- Decision trees\n- Extra trees\n- Random forest \n- Boosting (Adaboost, gradient boosting)\n\n### Tuning models\n- We used 20% of training data as validation set\n- Use different objective functions like mean absolute error, mean squared error, root mean squared error etc. to optimize models\n \n### Evaluation \n- Root mean squared error\n'",Repository containing source code for performing data analysis on Yelp reviews and tips
https://github.com/cagdasyetkin/raspberryPi3,b'# raspberryPi3\nRaspberry Pi Projects\n\ndata_logging.py collects data from the sensor and creates data_log.csv\n\nand then we can visualize this data using SensorViz.ipynb\n\nblink.py will help you to understand how connections work. It will turn on/off the LED lights\n',Raspberry Pi Projects
https://github.com/Shreyas3108/movielens-analytics-recomendation,"b""# movielens-analytics-recomendation\nAnalytics of Movielens dataset (100k) along with recomendation based on the user preference\nEDA of the dataset along with basic visualization using plot function of pandas has been used. \n\nConsists of analysis of movielens dataset (100k) along with recomendation based on it using python. \nMovielens(100k) dataset consists of 100,000 ratings for movies from 943 users on 1682 movies \n\nThe link to this dataset https://grouplens.org/datasets/movielens/100k/ \n\nThe recomendation takes user's choices and creates a matrix and gives recomendation \n\n\n""",Analytics of Movielens dataset (100k) along with recomendation based on the user preference
https://github.com/daaltces/pydaal-getting-started,"b""\nThis repository consists of various materials introducing PyDAAL (Python API of [Intel Data Analytics Acceleration Library](https://software.intel.com/en-us/intel-daal)) that facilitates Python and Machine Learning practitioners to start off with PyDAAL concepts. \n\nAdditionally, helper functions and classes have been provided to aid frequently performed PyDAAL operations.\n\n# [1-gentle-introductory-series](./1-gentle-introductory-series)\n\nVolume 1, 2 and 3 in PyDAAL Gentle Introduction Series are available as [Jupyter Notebooks](http://jupyter.org/). These volumes are designed to provide a quick introduction to essential features of PyDAAL.\nThese Jupyter Notebooks offer a collection of code examples that can be executed in the interactive command shell, and helper functions to automate common PyDAAL functionalities.\n\n## How to use?\n\nInstall [Intel Distribution for Python](https://software.intel.com/en-us/intel-distribution-for-python) (IDP) through [conda](https://www.continuum.io/downloads). IDP consists of a large set of commonly used mathematical and statistical Python packages that are optimized for Intel architectures. \n\n1. Install the latest version of [Anaconda](https://www.continuum.io/downloads).    \n- Choose the Python 3.5 version2. \n\n2. From the shell prompt (on Windows, use **Anaconda Prompt**), execute these  commands:\n\n```bash    \n    conda create --name idp intelpython3_full python=3 -c intel    \n    source activate idp (on Linux and OS X)      \n    activate idp (on Windows)    \n```\nIDP environment is installed with necessary packages and activated to run these notebooks.  \n  \nMore detailed instructions can be found from [this online article](https://software.intel.com/en-us/articles/using-intel-distribution-for-python-with-anaconda).\n\n# [2-pre-built-helper-classes](./2-pre-built-helper-classes)\n\nVarious stages of machine learning model building process are bundled together to constitute one helper function class. These classes are constructed using PyDAAL\xe2\x80\x99s data management and algorithm libraries to achieve a complete model deployment. \n\n### Stages supported by each helper function classes\n1. Training\n2. Prediction\n3. Model Evaluation and Quality Metrics\n4. Trained Model Storage and Portability\n\nMore details on all these stages are available in [Volume 3](./1-gentle-introductory-series/volume-3-analytics-model-building-deployment.ipynb).\n\n### Currently, helper function classes are provided for\n1. [Linear Regression](./2-pre-built-helper-classes/LinearRegression)\n2. [Ridge Regression](./2-pre-built-helper-classes/RidgeRegression)\n3. [SVM - Binary and Multi-Class classifier](./2-pre-built-helper-classes/SVM)\n4. [Decision Forest(classification and regression)](./2-pre-built-helper-classes/DecisionForest)\n3. [Kmeans](./2-helper-function-classes/Kmeans)\n4. [PCA](./2-pre-built-helper-classes/PCA)\n2. [SVM - Binary and Multi-Class classifier](./2-pre-built-helper-classes/SVM)\n3. [Kmeans](./2-pre-built-helper-classes/Kmeans)\n\n\nFor practice, usage examples with sample datasets are also provided that utilize these helper function classes.\n\n# [3-custom-modules](./3-custom-modules)\n\nPyDAAL API's have been used to tailor Python modules that support common operations on DAAL's Data Management library.\n\nImport the [customUtils](./3-custom-modules/customUtils) module and explore basic utilities provided for data retrieval and manipulation operations on DAAL's Data Management library\n\n1. getArrayFromNT() : Extracts a numpy array from numeric table\n2. getBlockOfNumericTable(): Slices a block of numeric table with specific range of rows and columns\n3. getBlockOfCols(): Extracts a block of numeric table within specific range of columns\n4. getNumericTableFromCSV(): Reads a CSV file into a numeric table\n5. serialize(): Serializes any input data and saves it into a local variable/disk\n6. deserialize(): Deserailizes serialized data from a local variable/disk\n\n# [4-interactive-tutorials](./4-interactive-tutorials)\n\nThese tutorials are spread across a collection of Jupyter notebooks comprising a theoritical explanation on algorithms and interactive command shell to execute using PyDDAL API.  \n\n### Tutorials Notebooks\n\n* [Data management in pyDAAL](./4-interactive-tutorials/NumericTables_example.ipynb)\n\n* [K-Means and PCA](./4-interactive-tutorials/kmeans_example.ipynb)\n\n* [Linear regression](./4-interactive-tutorials/LR_example.ipynb)\n\n* [SVM and multi-class classifier](./4-interactive-tutorials/SVM_example.ipynb)\n\n* [Online Ridgeregression](./4-interactive-tutorials/Regression_online_example.ipynb)\n\n* [Online Multinomial NaiveBayes](./4-interactive-tutorials/NaiveBayes_online_example.ipynb)\n\nData files used in the tutorials are in the [mldata](4-interactive-tutorials/) folder. \nThese data files are downloaded from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets).\n\n\n\n""","Introduction and tutorials for using PyDAAL, i.e. the Python API of Intel Data Analytics Acceleration Library"
https://github.com/kellino/Genetic_Composition,"b""Simple algorithmic composition for Computer Music UCl\n===========\n\nRequirements:\n    * Python3.5\n    * numpy\n    * pydub (available on pip)\n\nWhile the program is written in python3.5, it should be relatively easy to convert to python2.7\nA presentation is also available for the ipython / jupyter notebook. It can be viewed on \nthe [github](https://github.com/kellino/Genetic_Composition) for this project.\n\nThe sample, for which I am not the copyright holder, is taken from a performance by Andreas Scholl of the\ntraditional English Ballad 'I will give my love an apple' \n""",simple attempt at an algorithmic composition using a genetic algorithm
https://github.com/sofroniewn/2pRAM-paper,"b'# Mesoscale two-photon imaging with the 2p-RAM\n\n[![Binder](http://mybinder.org/badge.svg)](http://mybinder.org/repo/sofroniewn/2pRAM-paper)\n\nNotebooks and data acquired with the two-photon random access mesoscope (2p-RAM), accompanying\n\nA large field of view two-photon mesoscope with subcellular resolution for in vivo imaging \n\nSofroniew, N. J. 1, *, Flickinger, D. 1, *, King, J. 2, Svoboda, K. 1\n\n1 Janelia Research Campus, Ashburn VA 20147, USA\n2 Vidrio Technologies, Ashburn VA 20147, USA\n\n*These authors contributed equally to this work\n\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nRaw imaging data is hosted on AWS. Copies of the processed data and meta data are stored in this repository\n\nVideos derived from this data can be found [here](https://www.youtube.com/watch?v=LSYXueH1pzU&list=PLDOd6H-eiYAQ3rmK5HtiMNi-C2s3QsTTK)\n'",Notebooks accompanying the 2-photon random access mesoscope (2p-RAM) paper
https://github.com/fukuta0614/chainer-SeqGAN,"b'# chainer-SeqGAN\n \n- implementation of [SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient](https://arxiv.org/abs/1609.05473)\n- Complete oracle test in this paper\n\n## requirements\n\n- Python > 3.4\n- Chainer > 1.5\n- Tensorflow (CPU-only)\n```\n# Ubuntu/Linux 64-bit Python 3.4\nexport TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0-cp34-cp34m-linux_x86_64.whl\n\n# Ubuntu/Linux 64-bit Python 3.5\nexport TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.11.0-cp35-cp35m-linux_x86_64.whl\n\n# Mac OS X, CPU only, Python 3.4 or 3.5:\nexport TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-0.11.0-py3-none-any.whl\n\npip install $TF_BINARY_URL\n```\n\n## Usage\n\n```\ncd oracle_test && python run_sequence_gan.py \n```\n\n\nAny advice or suggestion is strongly welcomed in issues.\n'",implementation of SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient
https://github.com/RazerM/orbital,"b""## Orbital\n[![Build Status][bsi]][bsl] [![PyPI Version][ppi]][ppl] [![Python Version][pvi]][pvl] [![MIT License][mli]][mll]\n\n  [bsi]: http://img.shields.io/travis/RazerM/orbital.svg?style=flat-square\n  [bsl]: https://travis-ci.org/RazerM/orbital\n  [ppi]: http://img.shields.io/pypi/v/orbitalpy.svg?style=flat-square\n  [ppl]: https://pypi.python.org/pypi/orbitalpy/\n  [pvi]: https://img.shields.io/badge/python-2.7%2C%203-brightgreen.svg?style=flat-square\n  [pvl]: https://www.python.org/downloads/\n  [mli]: http://img.shields.io/badge/license-MIT-blue.svg?style=flat-square\n  [mll]: https://raw.githubusercontent.com/RazerM/orbital/master/LICENSE\n\nOrbital is a high level orbital mechanics package for Python.\n\n### Installation\n\n```bash\n$ pip install orbitalpy\n```\n\n### Example\n\n```python\nfrom orbital import earth, KeplerianElements, Maneuver, plot\n\nfrom scipy.constants import kilo\nimport matplotlib.pyplot as plt\n\norbit = KeplerianElements.with_altitude(1000 * kilo, body=earth)\nman = Maneuver.hohmann_transfer_to_altitude(10000 * kilo)\nplot(orbit, title='Maneuver 1', maneuver=man)\nplt.show()\n```\n\n![Example plot](http://i.fraz.eu/5b84e.png)\n\n### Documentation\n\nFor more information, view the [documentation online][doc].\n\n  [doc]: http://pythonhosted.org/OrbitalPy/""",High level orbital mechanics package.
https://github.com/ekarademir/exercises,"b""# exercises\nExercises from various courses and tutorials\n\n# codewords-issueone-functionalprogramming\n[_An introduction to functional programming_][funcproginpython] by **Mary Rose Cook**\n# fisher-yates-test\nFew timing tests I'm running on the shuffling algorithm. I'm comparing imperative coding to declarative coding.\n\n[funcproginpython]:https://codewords.recurse.com/issues/one/an-introduction-to-functional-programming\n\n# dublin-airport-challenge\nExploratory codes for Dublin Airport Hackathon BUILDchallenge. [GitHub page of the Challenge](https://github.com/rapidanalytics/Dublin-Airport-Challenge). Can't share the dataset.\n\n# genetic_search\nWriting quotes from Shakespeare by bashing on the typewriter. Fittest basher wins.\n""",Exercises from various courses and tutorials
https://github.com/istochkoj/coursera-MIPT-Project,b'# coursera-MIPT-Project\nCourse 6. Final project/\n',Course 6. Final project/
https://github.com/jngaravitoc/Lecture_notes_UofA,b'# Lecture_notes_UofA\nLectures Notes of courses taken at the University of Arizona\n',Lectures Notes of courses taken at the University of Arizona
https://github.com/sofiatti/stat_analysis_radiobiological_data,b'# stat_analysis_radiobiological_data\n',Stat 215A paper refereeing project
https://github.com/angelsalazar95/M-todos-Num-ricos,b'# M-todos-Num-ricos\nC\xc3\xb3digos clases m\xc3\xa9todos num\xc3\xa9ricos\n',Códigos clases métodos numéricos
https://github.com/modee00/DEP-Project,"b'# DEP-Project\nConsulting Project for the Department of Environmental Protection of NYC\n\nThis project has two aims. One allows the water department to look up the consumption for a specific meter,\nand forecast consumption for that specific meter and the building(s) it serve(s).\n\nThe second aim allows the user to compare two building types side by side, study the average consumption for each,\nand examine the forecasted consumption for each building type.\n'",Consulting Project for the Department of Environmental Protection of NYC
https://github.com/suwangcompling/bayesianmodels,"b'# bayesianmodels\n\n* Baseline Models\n  * Model 1: Bimodal Multinomial Topic Model (Collapsed Gibbs, Steyvers & Griffiths 2004; Yao et al. 2009)\n    * A. Toy Data\n    * B. Brown\n    * C. BNC\n'",Bayesian Modeling
https://github.com/djays/DeepLearning_Bike_Rental,b'# DeepLearning_Bike_Rental\n\nBuilding a simple neural network to estimate bike rentals !\n',Building a simple neural network to estimate bike rentals
https://github.com/psygrammer/dprl,"b'# dprl\n* \xec\x9d\x98\xec\x82\xac\xea\xb2\xb0\xec\xa0\x95(DP) + \xea\xb0\x95\xed\x99\x94\xed\x95\x99\xec\x8a\xb5(RL) + \xec\x98\xa8\xeb\x9d\xbc\xec\x9d\xb8\xea\xb4\x91\xea\xb3\xa0(OA) + \xed\x8c\x8c\xec\x9d\xb4\xec\x8d\xac\xec\x9b\xb9(Pyweb) + \xec\x98\xa8\xeb\x9d\xbc\xec\x9d\xb8 \xec\x95\xa1\xec\x85\x98(\xea\xb4\x91\xea\xb3\xa0, \xed\x85\x8c\xec\x8a\xa4\xed\x8c\x85)\n\n# \xec\x8a\xa4\xed\x84\xb0\xeb\x94\x94 \xec\xa0\x95\xeb\xb3\xb4 \n* http://psygrammer.github.io/dprl\n'",의사결정(DP) + 강화학습(RL) + 온라인광고(OA) + 파이썬웹(Pyweb)
https://github.com/rj67/germVar2,"b'germVar2\n=============\n\nSupplementary materials for \n#### Pan-cancer sequencing analysis reveals frequent germline mutations in cancer genes \nRuomu Jiang, William Lee, Nadeem Riaz, Chris Sander, Timothy J Mitchison^*, Debora S Marks^*\n\nInstall\n-----------\n    install.packages(""devtools"")\n    library(devtools)\n    install_github(""rj67/germVar2"")\n\nData objects\n-----------\nDataframes that can be loaded\n\n* list_goi -- candidate gene list\n\n|    |Gene  |Approved.Name                |Ensembl.Gene    |MDG  |CPG  |Class |\n|:---|:-----|:----------------------------|:---------------|:----|:----|:-----|\n|83  |ATM   |ATM serine/threonine kinase  |ENSG00000149311 |TRUE |TRUE |H-TSG |\n|135 |BRCA1 |breast cancer 1, early onset |ENSG00000012048 |TRUE |TRUE |H-TSG |\n\n* all_patients -- all the patient information\n\n|Patient |disease2 | age|       agez|EA   |race2 |gender |\n|:-------|:--------|---:|----------:|:----|:-----|:------|\n|P6-A5OG |ACC      |  45| -0.2248333|TRUE |WHITE |FEMALE |\n|OR-A5JY |ACC      |  68|  1.0679583|TRUE |WHITE |FEMALE |\n\n* LoF_vars -- variant information, each row is a variant and columns are various annotation\n\n|     |Gene  |uid              |EFF                | TAC2|   AN2|rare |AAChange               |Transcript      |\n|:----|:-----|:----------------|:------------------|----:|-----:|:----|:----------------------|:---------------|\n|3444 |BRCA1 |17-41199682-C-T  |stop_gained        |    1| 17630|TRUE |p.Trp711*/c.2133G>A    |ENST00000491747 |\n|3445 |BRCA1 |17-41201208-TG-T |frameshift_variant |    1| 17630|TRUE |p.Gln1732fs/c.5194delC |ENST00000493795 |\n\n* LoF_muts -- variant carrier information, each row corresponds to the carrier of a particular variant.\n\n|    |Gene  |uid              |Patient |disease2 |AAChange               | DP|        AB|N_hom | nA| nB|\n|:---|:-----|:----------------|:-------|:--------|:----------------------|--:|---------:|:-----|--:|--:|\n|8   |BRCA1 |17-41247941-T-G  |04-1336 |OV       |c.453A>C               | 39| 0.9411765|FALSE |  3|  0|\n|205 |BRCA1 |17-41201208-TG-T |09-2045 |OV       |p.Gln1732fs/c.5194delC | 96| 0.8541667|FALSE |  1|  0|\n\n*  nsSNP_vars -- variant information, each row is a variant and columns are various annotation\n\n|                |Gene  |uid             |EFF              | TAC2|   AN2|rare |AAChange               |Transcript      |dele  |RCVaccession              | cosm_scount|\n|:---------------|:-----|:---------------|:----------------|----:|-----:|:----|:----------------------|:---------------|:-----|:-------------------------|-----------:|\n|17-41201181-C-A |BRCA1 |17-41201181-C-A |missense_variant |    1| 17630|TRUE |p.Gly1788Val/c.5363G>T |ENST00000357654 |TRUE  |RCV000048961;RCV000031241 |          NA|\n|17-41215920-G-A |BRCA1 |17-41215920-G-A |missense_variant |    1| 17630|TRUE |p.Ala1708Val/c.5123C>T |ENST00000357654 |FALSE |NA                        |          NA|\n\n* nsSNP_muts -- variant carrier information, each row corresponds to the carrier of a particular variant.\n\n|   |Gene  |uid             |Patient |disease2 |AAChange              |  DP|        AB|N_hom | nA| nB|\n|:--|:-----|:---------------|:-------|:--------|:---------------------|---:|---------:|:-----|--:|--:|\n|3  |BRCA1 |17-41245027-G-A |02-0047 |GBM      |p.Arg841Trp/c.2521C>T | 330| 0.4575758|FALSE |  1|  1|\n|89 |BRCA1 |17-41245027-G-A |05-5425 |LUAD     |p.Arg841Trp/c.2521C>T | 195| 0.5179487|FALSE |  3|  1|\n\n\n\n\nJuptyer notebooks\n-----------\nReproduce most of the analysis/figures in the paper\n\n* project_overview -- Samples, candidate gene, all variants overview.\n* known_cancer_gene_variants -- Summary variants in known cancer genes.\n* loss_of_heterozygousity_analysis -- LOH of all germline variants\n* low_frequency_variants_association -- Assocation test of low frequency missense and truncation variants comparing to 1000G and ESP\n\nConvenience functions\n-----------\n\n* plotMutRNASeq -- Plot mutation RNASeq levels \n* plotDiseaseDistr -- Plot cancer type distribution given a list of mutations\n\nDependency\n-----------\n\n* plyr, dplyr, reshape2, ggplot2, magrittr, RColorbrewer, gridExtra\n'",TCGA germline variants
https://github.com/marcelbernic/visual-teach-and-repeat,b'# visual-teach-and-repeat\nProjet de session (GLO-4001)\n\n# SIFT\n\n# OpenCV\n',Projet de session (GLO-4001)
https://github.com/tperol/am207-NILM-project,"b'# AM207-NILM-project\nThis is the repository for AM 207 final project. It can be found at https://github.com/tperol/am207-NILM-project\n\n# Energy disaggregation from Non-Intrusive Load Monitoring\n\n**Youtube video**: https://www.youtube.com/watch?v=9a8dR9NEe6w\n\nFinal report can be found in the report folder: Report.pdf\n\nThe notebooks on the three implemented methods can be found in this repository: CO, FHMM, ConvNet.\n\nThe presented poster can be found in the poster folder.\n\nThe data can be downloaded freely at http://redd.csail.mit.edu\n\n**Contributors:**\n\n* Karen Yu\n* Nick Vasios\n* Thibaut Perol\n'",Repo for am207 final project
https://github.com/michelleful/ipynb-intro,b'ipynb-intro\n===========\n\niPython notebook intro for PyLadies Boston\n',iPython notebook intro for PyLadies Boston
https://github.com/Himscipy/CODES,"b'# CODES\nCollection of codes, scripts and routines developed by me for research and learning purpose.\n\n* **HS_Chebpy** :  \n  * IPython NoteBook \n  codes in for spectral Method from  \n  Lloyd N. Trefethen, Spectral Methods in MATLAB, SIAM, Philadelphia, 2000\n\n* **HS_CFD_Course Projects**:   \n  * Contains codes developed while learning Numerical analysis and CFD.   \n  * \n\n* **High Performance Computing Example Problem**:\n\n\n* **Gerris Codes**:\n\n\n* **VisIT Scripts**:\n\n\n* **Shell Scripts**:\n\n\n \n'",Collection of Various small CFD Codes 
https://github.com/tayden/titanic-death-decider,"b'#titanic-death-decider\n\nPredict the survival of Titanic passengers using a SVM model.\n\n## About\nThe Titanic dataset (from Kaggle) contains information about the passengers aboard the Titanic, such as age, sex, fare cost, class and wether or not they survived the sinking of the ship.\nThis project implements an SVM classifier model to predict if passengers with unknown fates survived or perished the sinking event.\n\nThe model makes use of 3-fold cross-validation and achieves approximately ~78% accuracy on the test dataset. \n\n## Opening the project\nTo run the project, numpy, sklearn, and ipython notebook python packages must be installed. Refer to their respective documentation to do so.\nOnce these requirements are satisfied, you may run ""ipython notebook"" to start a notebook server where the titanic-death-decider.ipynb file can be opened.\n\n'",Predicts the survival of Titanic passengers using a SVM classifier.
https://github.com/jimmychou0704/Simulation,b'# Simulation\nMonte Carol simulation of stochastic process in Stochastic_process.ipynb.  \nMonte Carol simulation of option pricing in pricing.ipynb.\n',Monte Carol simulation of stochastic process
https://github.com/mike-grayhat/quora_qp,b'# quora_qp\nRepository for quora question pairs competition. This solution is based on an existed NN with decomposable attention model https://github.com/explosion/spaCy/tree/master/examples/keras_parikh_entailment and was adapted for low-mid end hardware.\n',Repository for quora question pairs competition
https://github.com/subimal/class-demos,"b'# class-demos\nSimulations for teaching undergraduates\n\nAuthor : Subimal Deb\n\ne-mail : subimal.deb@gmail.com \n\n### Quantum Mechanics\n\n| Topic |\tDescription |\n| ----- | ----------- |\n| Square well potential |\tIPython notebook. Tested with python 3. |\n\n### Solid State Physics\n| Topic |\tDescription |\n| ----- | ----------- |\n| Unit cells |\tAsymptote codes for unit cells of simple cubic, base-centered cubic, FCC, body-centered cubic, hexagonal close packed structures. |\n'",Simulations for teaching undergraduates
https://github.com/FRESNA/openmod-atlite-de,b'# openmod-atlite-de\n\nDemonstration of using atlite to generate historical German 2012 wind and solar feed-in\n\nRefer to the notebook `openmod-atlite-de.ipynb` and its HTML conversion (with figures) at http://fias.uni-frankfurt.de/~hoersch/openmod-atlite-de.html.\n\n',Demonstration of using atlite to generate historical German 2012 wind and solar feed-in
https://github.com/L4brax/ml_antoine_hess,"b'# Explorer, apprendre\n\nLe document pandas.ipynb sert d\'introduction \xc3\xa0 l\'ensemble de donn\xc3\xa9es\ndans [test.csv](test.csv) et [train.csv](train.csv).  Lisez-le, faites\nles exercices.  Par contre, c\'est pour vous, pas de formalit\xc3\xa9 de\nsoumission.\n\n# \xc3\x80 faire pour le 16 d\xc3\xa9cembre\n\nVous \xc3\xaates nouvel embauche chez CoolCorp, une startup qui sp\xc3\xa9cialise\ndans l\'analyse des d\xc3\xa9sastres maritimes.  Un nouveau client, White Star\nLines, se trouve face \xc3\xa0 un proc\xc3\xa8s \xc3\xa0 travers le temps, affaire d\xc3\xa9licat.\nVotre chef vous demande, en tant que seul data scientist chez\nCoolCorp, d\'analyser les donn\xc3\xa9es `*.csv`.  WSL a d\xc3\xa9j\xc3\xa0 engag\xc3\xa9 un expert\npour annoter une partie des donn\xc3\xa9es (`test.csv`) avec la survie ou pas\ndes passagers.\n\nLe chef comprend que vous \xc3\xaates nouveau, c\'est pourquoi il vous laisse\nune semaine compl\xc3\xa8te pour cette analyse.\n\nCr\xc3\xa9ez un r\xc3\xa9pertoire qui s\'appelle ""P1"" dans _votre_ repository github.\nDans ce repository, \xc3\xa9crivez des analyses des donn\xc3\xa9es.  Cr\xc3\xa9ez des\nipython notebooks afin de pouvoir documenter ce que vous faites,\npourquoi certaines choses sont int\xc3\xa9ressantes et d\'autres moins.\nD\xc3\xa9couvrez de que vous pouvez dans les donn\xc3\xa9es.\n\nEt puis, \xc3\xa0 la fin, \xc3\xa9crivez un ipython notebook avec votre analyse \xc3\xa0\npr\xc3\xa9senter \xc3\xa0 votre chef en pr\xc3\xa9sence du grand chef de White Star Lines.\nAppelez-le ""rapport_final.ipynb"".\n\nN\'oubliez pas de faire des commits et des push au fur et \xc3\xa0 mesure de\nvotre progr\xc3\xa8s.  \xc3\x87a vous prot\xc3\xa8ge en plus, car vous pouvez plus\nfacilement faire marche en arri\xc3\xa8re en cas de b\xc3\xaatise.\n\n# \xc3\x89valuation\n\n## Crit\xc3\xa8res d\'\xc3\xa9valuation\n\nVous serez \xc3\xa9valu\xc3\xa9 sur les axes suivants :\n* profondeur de votre analyse\n* clart\xc3\xa9 (en fran\xc3\xa7ais et technique) de votre pr\xc3\xa9sentation\n* attention au raisonnement\n\n## M\xc3\xa9thodes d\'\xc3\xa9valuation\n\nDeux de vos co-\xc3\xa9tudiants seront choisi au hasard de remarquer sur\nvotre rapport finale en utilisant les crit\xc3\xa8res ci-dessus.\n\n_Une question \xc3\xa0 discuter ensemble : comment communiquer ces analyse de\nrapport._\n\nLe prof \xc3\xa9valuera tout le monde.\n\nBien entendu, vous serez demand\xc3\xa9 \xc3\xa0 remarquer sur les rapports de deux\nautres \xc3\xa9tudiants.  Vous serez \xc3\xa9galement \xc3\xa9valu\xc3\xa9 (par le prof) sur votre\nanalyse de vos coll\xc3\xa8gues.\n'",Machine Learning projects - Ynov
https://github.com/bje-/NEMO,"b'# National Electricity Market Optimiser (NEMO)\n\n![Build status\nbadge](https://github.com/bje-/NEMO/actions/workflows/buildtest.yml/badge.svg)\n[![Coverage\nStatus](https://coveralls.io/repos/github/bje-/NEMO/badge.svg?branch=master)](https://coveralls.io/github/bje-/NEMO?branch=master)\n[![CodeFactor](https://www.codefactor.io/repository/github/bje-/nemo/badge)](https://www.codefactor.io/repository/github/bje-/nemo)\n[![Bandit](https://img.shields.io/badge/security-bandit-yellow.svg)](https://github.com/PyCQA/bandit)\n\nNEMO is a chronological production-cost and capacity expansion model\nfor testing and optimising different portfolios of renewable and\nfossil electricity generation technologies. It has been developed and\nimproved over the past decade and has a growing number of users.\n\n![NEMO dispatch results](http://nemo.ozlabs.org/theworks.png)\n\nIt requires no proprietary software to run, making it particularly\naccessible to the governments of developing countries, academic\nresearchers and students. The model is available for others to inspect\nand, importantly, to validate the results.\n\n## Installation\n\n```bash\npip install nemopt\n```\n\n## Features\n\nFor a set of given (or default) generation or demand traces, users can:\n\n  1. Specify & simulate a custom resource mix, or;\n  2. ""Evolve"" a resource mix using pre-configured scenarios, or\n     configure their own scenario\n\n### Evolution strategy\n\nThe benefit of an evolutionary approach is that while NEMO is\nsearching for the least-cost solution, NEMO can also explore\n""near-optimal"" resource mixes.\n\nNEMO no longer uses genetic algorithms, but has adopted the better\nperforming [CMA-ES](https://en.wikipedia.org/wiki/CMA-ES) method.\n\n### Resource models\n\nNEMO has models for the following resources: wind (including\noffshore), photovoltaics, concentrating solar power (CSP), hydropower,\npumped storage hydro, biomass, black coal, open cycle gas turbines\n(OCGTs), combined cycle gas turbines (CCGTs), diesel generators, coal\nwith carbon capture and storage (CCS), CCGT with CCS, geothermal,\ndemand response, batteries, electrolysers, hydrogen fuelled gas\nturbines, and more.\n\n## Documentation\n\nDocumentation is progressively being added to a [User\'s\nGuide](https://nbviewer.org/urls/nemo.ozlabs.org/guide.ipynb?flush_cache=1)\nin the form of a Jupyter notebook.\n\n[API documentation](http://nemo.ozlabs.org/pdoc/index.html) exists for\nthe `nemo` module. This is useful when building new tools that use the\nsimulation framework.\n\nThe model is described in an Energy Policy paper titled [Least cost\n100% renewable electricity scenarios in the Australian National\nElectricity\nMarket](http://ceem.unsw.edu.au/sites/default/files/documents/LeastCostElectricityScenariosInPress2013.pdf)\nby Elliston, MacGill and Diesendorf (2013).\n\n## System requirements\n\nNEMO should run on any operating system where Python 3 is available\n(eg, Windows, Mac OS X, Linux). It utilises some add-on packages:\n\n- [DEAP](https://deap.readthedocs.io/en/master/),\n- [Gooey](https://pypi.org/project/Gooey/),\n- [Matplotlib](http://matplotlib.org/),\n  [Numpy](http://www.numpy.org/), [Pandas](http://pandas.pydata.org/)\n  and\n- [Pint](https://pint.readthedocs.io).\n\n### Scaling up\n\nFor simple simulations or scripted sensitivity analyses, a laptop or\ndesktop PC will be adequate. However, for optimising larger systems, a\ncluster of compute nodes is desirable. The model is scalable and you\ncan devote as many locally available CPU cores to the model as you\nwish.\n\n> #### Note\n>\n> Due to a lack of active development, support for\n> [SCOOP](https://pypi.python.org/pypi/scoop) has been removed. It\n> will be soon replaced with something like [Ray](https://ray.io/).\n\n## Citation\n\nIf you use NEMO, please cite the following paper:\n\n> Ben Elliston, Mark Diesendorf, Iain MacGill, [Simulations of\n> scenarios with 100% renewable electricity in the Australian National\n> Electricity\n> Market](https://www.sciencedirect.com/science/article/pii/S0301421512002169?via=ihub#s0010),\n> Energy Policy, Volume 45, 2012, Pages 606-613, ISSN 0301-4215,\n> <https://doi.org/10.1016/j.enpol.2012.03.011>\n\n## Community\n\nThe [nemo-devel](https://lists.ozlabs.org/listinfo/nemo-devel) mailing\nlist is where users and developers can correspond.\n\n## Contributing\n\nEnhancements and bug fixes are very welcome. Please report bugs in the\n[issue tracker](https://github.com/bje-/NEMO/issues). Authors retain\ncopyright over their work.\n\n## License\n\nNEMO was first developed by [Dr Ben\nElliston](https://www.ceem.unsw.edu.au/staff/ben-elliston) in 2011 at\nthe [Collaboration for Energy and Environmental Markets, UNSW\nSydney](https://www.ceem.unsw.edu.au/).\n\nNEMO is free software and the source code is licensed under the [GPL version 3 license](COPYING).\n\n## Useful references\n\nAustralian cost data are taken from the [Australian Energy Technology\nAssessments](https://www.industry.gov.au/Office-of-the-Chief-Economist/Publications/Pages/Australian-energy-technology-assessments.aspx)\n(2012, 2013), the [Australian Power Generation Technology\nReport](http://www.co2crc.com.au/publication-category/reports/) (2015)\nand the CSIRO [GenCost\nreports](https://data.csiro.au/collections/collection/CIcsiro:44228)\n(2021, 2022, 2023). The GenCost reports provide the basis of the input\ncost assumptions for the AEMO [Integrated System\nPlan](https://aemo.com.au/en/energy-systems/major-publications/integrated-system-plan-isp).\nCosts for other countries may be added in time.\n\nRenewable energy trace data covering the Australian National\nElectricity Market territory are taken from the AEMO 100% Renewables\nStudy. An accompanying\n[report](http://content.webarchive.nla.gov.au/gov/wayback/20140211194248/http://www.climatechange.gov.au/sites/climatechange/files/files/reducing-carbon/APPENDIX3-ROAM-report-wind-solar-modelling.pdf)\ndescribes the method of generating the traces.\n\n## Acknowledgements\n\nEarly development of NEMO was financially supported by the [Australian\nRenewable Energy Agency](http://www.arena.gov.au/) (ARENA). Thanks to\nundergraduate and postgraduate student users at UNSW who have provided\nvaluable feedback on how to improve (and document!) the model.\n'",National Electricity Market Optimiser
https://github.com/AugustLONG/crawler,"b'# \xe5\xa4\xa7\xe6\x95\xb0\xe6\x8d\xae\xe6\x90\x9c\xe7\xb4\xa2\n\xe5\x9f\xba\xe4\xba\x8escrapy\xe7\x9a\x84\xe9\x87\x87\xe9\x9b\x86\xe7\xb3\xbb\xe7\xbb\x9f\xef\xbc\x8c\xe9\x87\x87\xe7\x94\xa8reids\xe3\x80\x81kafka\xe3\x80\x81celery\xe3\x80\x81rabbitmq\xe3\x80\x81elasticsearch\xe3\x80\x81mysql\xe3\x80\x81django\xe7\xad\x89\xe6\x8a\x80\xe6\x9c\xaf\n\xe6\x89\x93\xe9\x80\xa0\xe6\x96\xb0\xe7\x94\x9f\xe4\xbb\xa3\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe6\x90\x9c\xe7\xb4\xa2\xe5\x92\x8c\xe6\x95\xb0\xe6\x8d\xae\xe5\x9f\xba\xe7\xa1\x80\xe5\xb9\xb3\xe5\x8f\xb0\n\n# Scrapy Cluster\n\nThis Scrapy project uses Redis and Kafka to create a distributed on demand scraping cluster.\n\nThe goal is to distribute seed URLs among many waiting spider instances, whose requests are coordinated via Redis. Any other crawls those trigger, as a result of frontier expansion or depth traversal, will also be distributed among all workers in the cluster.\n\nThe input to the system is a set of Kafka topics and the output is a set of Kafka topics. Raw HTML and assets are crawled interactively, spidered, and output to the log. For easy local development, you can also disable the Kafka portions and work with the spider entirely via Redis, although this is not recommended due to the serialization of the crawl requests.\n\n## Dependencies\n\nPlease see `requirements.txt` for Pip package dependencies across the different sub projects.\n\nOther important components required to run the cluster\n\n- Python 2.7: https://www.python.org/downloads/\n- Redis: http://redis.io\n- Zookeeper: https://zookeeper.apache.org\n- Kafka: http://kafka.apache.org\n\n## Core Concepts\n\nThis project tries to bring together a bunch of new concepts to Scrapy and large scale distributed crawling in general. Some bullet points include:\n\n- The spiders are dynamic and on demand, meaning that they allow the arbitrary collection of any web page that is submitted to the scraping cluster\n- Scale Scrapy instances across a single machine or multiple machines\n- Coordinate and prioritize their scraping effort for desired sites\n- Persist across scraping jobs or have multiple scraping jobs going at the same time\n- Allows for unparalleled access into the information about your scraping job, what is upcoming, and how the sites are ranked\n- Allows you to arbitrarily add/remove/scale your scrapers from the pool without loss of data or downtime\n- Utilizes Apache Kafka as a data bus for any application to interact with the scraping cluster (submit jobs, get info, stop jobs, view results)\n\n## Documentation\n\nPlease check out our official [Scrapy Cluster documentation](http://scrapy-cluster.readthedocs.org/) for more details on how everything works!\n\n\n## Redis Keys\nThe following keys within Redis are used by the Scrapy Cluster:\n\n- timeout:<spiderid>:<appid>:<crawlid> - The timeout value of the crawl in the system, used by the Redis Monitor. The actual value of the key is the date in seconds since epoch that the crawl with that particular spiderid, appid, and crawlid will expire.\n- <spiderid>:queue - The queue that holds all of the url requests for a spider type. Within this sorted set is any other data associated with the request to be crawled, which is stored as a Json object that is Pickle encoded.\n- <spiderid>:dupefilter:<crawlid> - The duplication filter for the spider type and crawlid. This Redis Set stores a scrapy url hash of the urls the crawl request has already seen. This is useful for coordinating the ignoring of urls already seen by the current crawl request.\n- <spiderid>:blacklist - A permanent blacklist of all stopped and expired crawlid\xe2\x80\x98s . This is used by the Scrapy scheduler prevent crawls from continuing once they have been halted via a stop request or an expiring crawl. Any subsequent crawl requests with a crawlid in this list will not be crawled past the initial request url.'",基于django和scrapy的采集系统
https://github.com/donallmc/CarND-Traffic-Sign-Classifier-Project,"b'#**Traffic Sign Recognition** \n\n**Build a Traffic Sign Recognition Project**\n\nThe goals / steps of this project are the following:\n* Load the data set (see below for links to the project data set)\n* Explore, summarize and visualize the data set\n* Design, train and test a model architecture\n* Use the model to make predictions on new images\n* Analyze the softmax probabilities of the new images\n* Summarize the results with a written report\n\n\n[//]: # (Image References)\n\n[test_freqs]: ./images/test_freqs.png ""frequencies""\n[training_freqs]: ./images/training_freqs.png ""frequencies""\n[validation_freqs]: ./images/validation_freqs.png ""frequencies""\n[final_freqs]: ./images/final_freqs.png ""frequencies""\n[no_passing]: ./images/no_passing.png ""no passing""\n[stop]: ./images/stop.png ""stop""\n[speed_80]: ./images/speed_80.png ""speed_80""\n[german1]: ./images/german1.jpg ""german1""\n[german2]: ./images/german2.jpg ""german2""\n[german3]: ./images/german3.jpg ""german3""\n[german4]: ./images/german4.jpg ""german4""\n[german5]: ./images/german5.jpg ""german5""\n[german6]: ./images/german6.jpg ""german6""\n[german7]: ./images/german7.jpg ""german7""\n[german8]: ./images/german8.jpg ""german8""\n\n\n[image2]: ./examples/grayscale.jpg ""Grayscaling""\n[image3]: ./examples/random_noise.jpg ""Random Noise""\n[image4]: ./examples/placeholder.png ""Traffic Sign 1""\n[image5]: ./examples/placeholder.png ""Traffic Sign 2""\n[image6]: ./examples/placeholder.png ""Traffic Sign 3""\n[image7]: ./examples/placeholder.png ""Traffic Sign 4""\n[image8]: ./examples/placeholder.png ""Traffic Sign 5""\n\n## Rubric Points\n###Here I will consider the [rubric points](https://review.udacity.com/#!/rubrics/481/view) individually and describe how I addressed each point in my implementation.  \n\n---\n###Writeup / README\n\n####1. Provide a Writeup / README that includes all the rubric points and how you addressed each one. You can submit your writeup as markdown or pdf. You can use this template as a guide for writing the report. The submission includes the project code.\n\nYou\'re reading it! and here is a link to my [project code](https://github.com/donallmc/CarND-Traffic-Sign-Classifier-Project/blob/master/Traffic_Sign_Classifier.ipynb)\n\n###Data Set Summary & Exploration\n\n####1. Provide a basic summary of the data set and identify where in your code the summary was done. In the code, the analysis should be done using python, numpy and/or pandas methods rather than hardcoding results manually.\n\nThe code for this step is contained in the second code cell of the IPython notebook.  \n\nI used the pandas library to calculate summary statistics of the traffic\nsigns data set:\n\n* Number of training examples = 34,799\n* Number of testing examples = 12,630\n* Number of validation examples = 4,410\n* Image data shape = (32, 32, 3)\n* Number of unique classes/labels = 43\n\n####2. Include an exploratory visualization of the dataset and identify where the code is in your code file.\n\nThe code for this step is contained in the third code cell of the IPython notebook.  \n\nHere is an exploratory visualization of the data set. Here are image frequencies for test, training, and validation sets:\n\n\n![alt text][training_freqs]\n![alt text][validation_freqs]\n![alt text][test_freqs]\n\nHere are some examples of randomly chosen images for 3 classes:\n\n![alt text][speed_80]\n**5: Speed limit (80km/h)**\n\n![alt text][no_passing]\n**9: No passing**\n\n![alt text][stop]\n**14: Stop**\n\nSamples for the full 43 classes can be seen in the python notebook.\n\n###Design and Test a Model Architecture\n\n####1. Describe how, and identify where in your code, you preprocessed the image data. What tecniques were chosen and why did you choose these techniques? Consider including images showing the output of each preprocessing technique. Pre-processing refers to techniques such as converting to grayscale, normalization, etc.\n\nThe code for this step is contained in the fourth and fifth code cells of the IPython notebook.\n\nAs a first step, I decided to convert the images to grayscale to simplify things by ignoring colour and having smaller feature sets to process. I also applied a histogram normalisation to handle the pronounced differences in brightness in the sample dataset. However, after some testing it became apparent that this kind of normalisation was not performing better than the colour dataset so I dropped it.\n\nThe only normalisation that is applied to the colour images is a simple normalisation to constrain all values to ```-1 <= x <= 1``` and I achieved satisfactory results with this approach. \n\nThere is also some data augmentation code in this cell, which I will describe in the next section.\n\n####2. Describe how, and identify where in your code, you set up training, validation and testing data. How much data was in each set? Explain what techniques were used to split the data into these sets. (OPTIONAL: As described in the ""Stand Out Suggestions"" part of the rubric, if you generated additional data for training, describe why you decided to generate additional data, how you generated the data, identify where in your code, and provide example images of the additional data)\n\nThe code for splitting the data into training and validation sets is contained in the first code cell of the IPython notebook along with a snarky comment about how the dataset changed and caused me to lose a lot of time! :)\n\nTo cross validate my model, I randomly split the training data into a training set and validation set. I did this by using the appropriate SKLearn function.\n\nThe fifth code cell of the IPython notebook contains the code for augmenting the data set. I decided to generate additional data because the provided data set is relatively small and this kind of deep learning benefits from much larger sets. In addition, the images provided vary significantly in terms of lighting conditions, image position, obscured components, etc. An augmented dataset would increase the number of examples seen of each permutation of these parameters, leading to a more robust classification. \n\nTo add more data to the the data set, I applied some random transformations to the image including adjusting the brightness, applying a random rotation, cropping, and translating the image. The original version of my code applied some of these transformations (as well as some normalisation) using the TensorFlow library. It was my intention to perform these transformations on-the-fly. However, even running on a GPU instance the time it took to process each image was unreasonably long and it had a detrimental effect on my iteration time. Instead I resolved to pre-process each image and generate additional examples before feeding data into the model. This has the obvious advantage that everything is done only once but it adds a pre-processing dependency that could theoretically lead to bugs. I did some Googling around pre-processing images in python for this dataset and actually came across another student\'s solution. I used the functions he defined on the basis that they looked well put-together and I didn\'t think I could improve on them. I could have easily re-implemented them myself, but I\'d prefer to leave them as is and credit the source.\n\nWhile augmenting the images I also took the opportunity to correct the imbalance in the datasets. As shown in the charts above, some classes are significantly more common than others. To correct this, I implemented a (somewhat hacky) means of selectively generating more images for under-represented classes than for the commonly occurring ones. The final dataset included 450,000 examples. The final class frequency distributions looks like this:\n\n![alt text][final_freqs]\n\n\n####3. Describe, and identify where in your code, what your final model architecture looks like including model type, layers, layer sizes, connectivity, etc.) Consider including a diagram and/or table describing the final model.\n\nThe code for my final model is located in the sixth cell of the ipython notebook. \n\nThe model is based on the LeNet code supplied in the course with the addition of a third convolutional layer and some dropout layers to handle overfitting. I also did a lot of tinkering with the sizes of the convolutions and strides. I was interested in having the model examine smaller chunks of the image to potentially learn components better as the traffic sign dataset contains images that are distinguishable only by small pixel areas. I don\'t think that my final numbers are optimal but they are adequate. My final model consisted of the following layers:\n\n| Layer         \t\t|     Description\t        \t\t\t\t\t| \n|:---------------------:|:---------------------------------------------:| \n| Input         \t\t| 32x32x3 RGB image   \t\t\t\t\t\t\t| \n| Convolution 8x8     \t| 2x2 stride, same padding, outputs 14x14x20 \t|\n| RELU\t\t\t\t\t|\t\t\t\t\t\t\t\t\t\t\t\t|\n| Max pooling\t      \t| 2x2 stride,  outputs 7x7x20 \t\t\t\t|\n| Convolution 5x5     \t| 1x1 stride, same padding, outputs 5x5x400 \t|\n| RELU\t\t\t\t\t|\t\t\t\t\t\t\t\t\t\t\t\t|\n| Max pooling\t      \t| 2x2 stride,  outputs 2x2x400 \t\t\t\t|\n| Convolution 1x1     \t| 1x1 stride, same padding, outputs 4x4x20 \t|\n| RELU\t\t\t\t\t|\t\t\t\t\t\t\t\t\t\t\t\t|\n| Max pooling\t      \t| 1x1 stride,  outputs 4x4x400 \t\t\t\t|\n| Fully connected\t\t| output 120        \t\t\t\t\t\t\t\t\t|\n|\tRELU\t\t\t\t|\t\t\t\t\t\t\t\t\t\t\t\t|\n|\tDropout\t\t\t\t\t|\t50%\t\t\t\t\t\t\t\t\t\t\t|\n| Fully connected\t\t| output 84        \t\t\t\t\t\t\t\t\t|\n|\tRELU\t\t\t\t|\t\t\t\t\t\t\t\t\t\t\t\t|\n|\tDropout\t\t\t\t\t|\t50%\t\t\t\t\t\t\t\t\t\t\t|\n| Fully connected\t\t| output 43        \t\t\t\t\t\t\t\t\t|\n\n\n####4. Describe how, and identify where in your code, you trained your model. The discussion can include the type of optimizer, the batch size, number of epochs and any hyperparameters such as learning rate.\n\nThe code for training the model is located in the seventh cell of the ipython notebook. \n\nTo train the model, I initially used an AdaGrad optimizer as I had read that it performed well on sparse data but as I began augmenting the dataset I noticed that Adam had better performance so I switched to that. In particuar, Adagrad often seemed to hit an accuracy level lesser than the Adam optimizer and then regress to a very low accuracy.\n\nI experimented with the batch size and larger batches seemed to perform better than smaller ones, although the difference wasn\'t that great above a certain threshold. The submitted version of the chose a batch size of 882, chosen because it was a denominator of the validation set and it was useful for debugging reasons!\n\nThe number of epochs in the submitted notebook is 50. After about 30 epochs the rate of improvement decreases noticeably but there\'s still a significant amount of improvement to be had in the final 20 epochs. After 50, however, the improvements just seem to fluctuate around a maximum value.\n\nas for hyperparameters, the learning rate was modified by Adam. I did some experimenting with the initial value but got no improvement so I went with a recommended value of 0.001. I also did some tweaking of the initial data parameters, mu and sigma, but my changes didn\'t result in any desirable effects.\n\n####5. Describe the approach taken for finding a solution. Include in the discussion the results on the training, validation and test sets and where in the code these were calculated. Your approach may have been an iterative process, in which case, outline the steps you took to get to the final solution and why you chose those steps. Perhaps your solution involved an already well known implementation or architecture. In this case, discuss why you think the architecture is suitable for the current problem.\n\nThe code for calculating the accuracy of the model is located in the seventh cell of the Ipython notebook.\n\nMy final model results were:\n* validation set accuracy of 98.4%\n* test set accuracy of 90.8%\n\nWorth noting that I re-ran the test set accuracy a few times (after all model details were finalised!) as part of re-running the ipython notebook after clean-up. Due to the large size of my training set I ran into memory problems which I wasn\'t able to resolve (and which I believe were due to transient conditions on my development machine). As a result, the submitted version included 50% fewer examples than an earlier version which achieved a test set accuracy of 92.5%. This is a good indication that a larger training set would improve the accuracy of the model.\n\n\nAs stated previously, the architecture was initially based on LeNet with some modifications as described above. The iterative design process was heavily impacted by a sudden drop in accuracy which took 3 days to resolve; it turns out I had moved to a new machine and checked out the data from Udacity. The dataset had been substantially modified and I had received any notifications (I have since heard it was mentioned in slack, which I don\'t consider to be an adequate notification). This caused my accuracy to fluctuate from 80%-90%. I tore apart the model and rebuilt everything then started experimenting heavily with adding and removing both fully connected and convolutional layers. Nothing seemed to improve things and after several days I realised that the issue was the modified dataset. Having spent so much time and now concerned that I won\'t get all the projects submitted by the deadline, I ended up tweaking the model that I then had to get it to a reasonable state. While this was an iterative process, I don\'t think it was good development! If Udacity would like me to try again I would request an additional weekend of development time to do it right. I would also politely request that significant changes to the dataset should be communicated more thoroughly and that the course materials should be updated (I followed the LeNet video line by line with my code and that\'s how I discovered that something was fishy about the dataset because David got 96% accuracy and I got < 90% with the exact same code).\n \n\n###Test a Model on New Images\n\n####1. Choose five German traffic signs found on the web and provide them in the report. For each image, discuss what quality or qualities might be difficult to classify.\n\nHere are five German traffic signs that I found on the web:\n\n![alt text][german1] ![alt text][german2] ![alt text][german3] \n![alt text][german4] ![alt text][german5]\n\nHere are three additional German traffic signs that do not map to one of the classes in the training data:\n\n![alt text][german6] ![alt text][german7] ![alt text][german8] \n\nThe first and fifth images might be difficult to classify because they are speeding signs. All speeding signs look reasonably similar and the images are small enough that the numbers are blurred, making it difficult to distinguish them. In addition, the first sign is smaller (in terms of pixels) than the fifth one so the problems are exacerbated. \n\n####2. Discuss the model\'s predictions on these new traffic signs and compare the results to predicting on the test set. Identify where in your code predictions were made. At a minimum, discuss what the predictions were, the accuracy on these new predictions, and compare the accuracy to the accuracy on the test set (OPTIONAL: Discuss the results in more detail as described in the ""Stand Out Suggestions"" part of the rubric).\n\nThe code for making predictions on my final model is located in the ninth cell of the Ipython notebook.\n\nHere are the results of the prediction:\n\n| Image\t\t\t        |     Prediction\t        \t\t\t\t\t|\n|:---------------------:|:---------------------------------------------:| \n| Speed 120      \t\t| Speed 50   \t\t\t\t\t\t\t\t\t| \n| No Vehicles     \t\t\t| No Vehicles \t\t\t\t\t\t\t\t\t\t|\n| Yield\t\t\t\t\t| Yield\t\t\t\t\t\t\t\t\t\t\t|\n| No Passing\t      \t\t| No Passing\t\t\t\t\t \t\t\t\t|\n| Speed 30\t\t\t| Speed 30      \t\t\t\t\t\t\t|\n\n\nThe model was able to correctly guess 4 of the 5 traffic signs, which gives an accuracy of 80%. The pass that I mentioned previously with the larger training dataset correctly classified all 5, supporting the conclusion that a larger training set improves performance.\n\n####3. Describe how certain the model is when predicting on each of the five new images by looking at the softmax probabilities for each prediction and identify where in your code softmax probabilities were outputted. Provide the top 5 softmax probabilities for each image along with the sign type of each probability. (OPTIONAL: as described in the ""Stand Out Suggestions"" part of the rubric, visualizations can also be provided such as bar charts)\n\nThe code for making predictions on my final model is located in the tenth cell of the Ipython notebook.\n\nFor the first image, the model incorrectly classifies it as ""speed limit (50km/h)"". As evidenced by the top 5 scores, the model has difficulties distinguishing between different speed limits signs, as expected. I believe this is solvable with more training data.\n\nImage 0. Correct value: Speed limit (120km/h)\n\n| Predicted Value | Confidence |\n|------------------|-------------------|\n|Speed limit (50km/h)| 0.487|\n|Speed limit (30km/h)| 0.274|\n|Speed limit (100km/h)| 0.071|\n|Speed limit (70km/h)| 0.054|\n|Speed limit (80km/h)| 0.024|\n\nThe other 4 signs are correctly classified. Interestingly, the ""no vehicles"", ""yield"" and ""no passing"" signs have extremely high confidence. This is presumably because they have distinctive features like a different shape, or a horizontal line through the sign, or an empty white circle. The 30 speed limit sign is correctly classified but not quite as confidently as the others, showing that the model does indeed struggle with speed limits.\n\nOverall, the 5 new signs have a classification accuracy of 80%, compared to the test set accuracy of 90.8%. The probabaility distribution for the incorrectly classified sign suggests there might be some overfitting, particularly around the various classes which include speed limits.\n\n\n|Image 1. Correct value: No vehicles\n\n| Predicted Value | Confidence |\n|------------------|-------------------|\n|No vehicles| 1.000|\n|Bumpy road| 0.000|\n|Bicycles crossing| 0.000|\n|Yield| 0.000|\n|Speed limit (60km/h)| 0.000|\n\nImage 2. Correct value: Yield\n\n| Predicted Value | Confidence |\n|------------------|-------------------|\n|Yield| 1.000|\n|Speed limit (20km/h)| 0.000|\n|Speed limit (30km/h)| 0.000|\n|Speed limit (50km/h)| 0.000|\n|Speed limit (60km/h)| 0.000|\n\nImage 3. Correct value: No passing\n\n| Predicted Value | Confidence |\n|------------------|-------------------|\n|No passing| 1.000|\n|Speed limit (20km/h)| 0.000|\n|Speed limit (30km/h)| 0.000|\n|Speed limit (50km/h)| 0.000|\n|Speed limit (60km/h)| 0.000|\n\nImage 4. Correct value: Speed limit (30km/h)\n\n| Predicted Value | Confidence |\n|------------------|-------------------|\n|Speed limit (30km/h)| 0.968|\n|Speed limit (50km/h)| 0.024|\n|Speed limit (70km/h)| 0.007|\n|Speed limit (100km/h)| 0.000|\n|No vehicles| 0.000|\n\nThe three ""unseen"" additions are interesting. The first one is a ""no parking"" with an arrow in the sign. 4 of the 5 suggested results have arrows somewhere on the sign. The third unseend image is also a ""no parking"" sign, but with no arrow. In this case, the model is very confused and guesses a stop sign. If I had to hazard a guess I would say it\'s the angular straight lines that have caused the confusion. \n\nImage 5. Correct value: unseen\n\n| Predicted Value | Confidence |\n|------------------|-------------------|\n|Keep right| 0.567|\n|Turn left ahead| 0.145|\n|Ahead only| 0.085|\n|Right-of-way at the next intersection| 0.044|\n|Road work| 0.030|\n\nImage 7. Correct value: unseen\n\n| Predicted Value | Confidence |\n|------------------|-------------------|\n|Stop| 0.998|\n|Speed limit (30km/h)| 0.001|\n|No vehicles| 0.001|\n|Speed limit (50km/h)| 0.000|\n|No passing| 0.000|\n\nThe second ""unseen"" image is of an end of speed limit (100 km/h). The model is extremely confident that this is an end of speed limit but gets the number wrong. This is clearly caused by the lack of 100km/h ends of speed limits in the training set. It is gratifying to see the model performing so well, though!\n\nImage 6. Correct value: unseen\n\n| Predicted Value | Confidence |\n|------------------|-------------------|\n|End of speed limit (80km/h)| 1.000|\n|Dangerous curve to the right| 0.000|\n|Slippery road| 0.000|\n|Children crossing| 0.000|\n|Speed limit (60km/h)| 0.000|\n\n'",Udacity Self-driving car course project 2
https://github.com/rkraig/cabi-predict,"b'# cabi-predict\n\n## Capital Bikeshare: Predictions of Near-Term Supply and Demand\n\nThis project is a demand and outage predictor for Capital Bikeshare. I take dock status and trip history, weather and calendar data, then fit a random forest regression model to estimate the customer demand for bikes and docks at each bikeshare station as a function of ten predictors:\n- Time of Day\n- Day of Week\n- Day of Year\n- Holiday (Y/N)\n- Year\n- Air Temperature\n- Relative Humidity\n- Wind Speed\n- Precipitation within the past hour\n- Snow depth\n\nI demonstrate this model with a customer-facing app that predicts CaBi demand and station outages in the immediate future. This app scrapes real-time dock status and weather data to form a feature vector, estimates customer demand for bikes and docks using the random forest, then computes outage probabilities with a Poisson model. Predictions are visualized on a map using leaflet.js. Result is published to a web app using Flask, hosted on heroku.\n\n## App\n\nhttps://cabi-predict.herokuapp.com/\n\n(Update: I see that the leaflet.js map of prediction output data has broken on heroku... I have not investigated or fixed that yet.)\n\n'",Predict near-term Capital Bikeshare availability using a random forest and Poisson regression. Display current status and predictions with leaflet.js map visualization.
https://github.com/woobe/h2o_sandbox,b'# h2o_sandbox\nH2O prototyping and experimentation\n',H2O prototyping and experimentation
https://github.com/pfctdayelise/leafvis,"b'Leaflet based visualization framework\n=====================================\n\nThis package makes it possible to easily visualize gridded meteorological data, represented as numpy arrays, using the *excellent* javascript visualization library leaflet.\n\nReferences:\n-----------\n\n     Vladimir Agafonkin, http://leafletjs.com/index.html. \n\n\nMaintainers\n-----------\n\n   - Nathan Faggian\n\nTesting\n-------\n\n[![Build Status](https://travis-ci.org/nfaggian/leafvis.png?branch=master)](https://travis-ci.org/nfaggian/leafvis)\n\nDependencies\n------------\n\nThe required dependencies to build the software are:\n\n  - python\n  - numpy\n  - scipy\n  - flask\n  - pyproj\n  - matplotlib\n  - pyresample\n  - py.test\n  - Cython\n  - Jinja2\n  - MarkupSafe\n  - Werkzeug\n  - configobj\n  - itsdangerous\n  - joblib\n  - numexpr\n  - requests\n  - tables\n  - wsgiref\n  - tornado\n  - pyzmq\n  - ipython!\n\nInstall\n-------\n\nThis packages uses distutils, which is the default way of installing python modules. To install in your home directory, use:\n\n    python setup.py install --home\n\nTo install for all users on Unix/Linux:\n\n    python setup.py build\n    sudo python setup.py install\n\nExamples\n--------\n\nRefer to ``/notebooks/example.ipynb``.\n\nDevelopment\n-----------\n\nFollow: Fork + Pull Model:\n\n    http://help.github.com/send-pull-requests/'",Python framework for visualising layer data using leaflet.
https://github.com/johnkorn/speaker_recognition,b'Speaker recognition and verification from arbitrary audio using deep learning.\n',Speaker recognition and verification with deep learning
https://github.com/gavinmh/GADS12-NYC,"b""# General Assembly Data Science 12\n\n## Contact Information\n\n* Gavin Hackeling: [gmh283@nyu.edu](mailto:gmh283@nyu.edu)\n* James Beveridge: [jamesbev@gmail.com](jamesbev@gmail.com)\n* Shawn Oakley: [shawnoakley@gmail.com](mailto:shawnoakley@gmail.com)\n\n\n## Questions and Discussions:\n\n* [Closed and Open Issue List (via pulse)](https://github.com/gavinmh/GADS12-NYC/pulse#closed-issues)\n* [Create a new issue](https://github.com/gavinmh/GADS12-NYC/issues/new)\n\n\n## Classes\n\n### Lecture 1: Introduction to Data Science\n_Thursday, 2014/08/07_\n\n#### Class Materials\n\n* [Lecture](https://docs.google.com/presentation/d/1r2pWzZ3-ZvA4OiRVqTzDCSMj33RSI7_UrLn6kt9BZzQ/edit?usp=sharing)\n* [lab_01a.md](https://github.com/gavinmh/GADS12-NYC/blob/master/lecture-1/lab-1a.md)\n* [lab-1a-submission-example.md](https://github.com/gavinmh/GADS12-NYC/blob/master/lecture-1/lab-1a-submission-example.md)\n* [lab_01b.md](https://github.com/gavinmh/GADS12-NYC/blob/master/lecture-1/lab-1b.md)\n\n\n### Lecture 2: Simple Linear Regression\n_Tuesday, 2014/08/12_\n\n#### Class Materials\n\n* [Lecture](https://docs.google.com/presentation/d/1b9bYZ9MIBqEmsJ3x5dBVlwxbSW-v0n-SazICmxh_iwU/edit?usp=sharing)\n* [simple-linear-regression-lab.pdf](https://github.com/gavinmh/GADS12-NYC/blob/master/lecture-2/simple-linear-regression-lab.pdf)\n* [python-lab.ipynb](https://github.com/gavinmh/GADS12-NYC/blob/master/lecture-2/python-lab.ipynb)\n* [numpy-lab.ipynb](https://github.com/gavinmh/GADS12-NYC/blob/master/lecture-2/numpy-lab.ipynb)\n\n\n### Lecture 3: From Simple Linear Regression to Multiple Linear Regression & an Introduction to scikit-learn\n_Thursday, 2014/08/14_\n\n#### Class Materials\n\n* [Lecture](https://docs.google.com/presentation/d/1nzGQyOeE-kkv5SUXrBfSgBD_s4FaFeY1GekmqjqgQbE/edit?usp=sharing)\n* [lecture_3_numpy_review.ipynb](https://github.com/gavinmh/GADS12-NYC/tree/master/lecture-3)\n* [lecture_3_intro_to_sklearn.ipynb](https://github.com/gavinmh/GADS12-NYC/tree/master/lecture-3)\n* [lab_3_multiple_linear_regression.ipynb](https://github.com/gavinmh/GADS12-NYC/tree/master/lecture-3)\n\n\n### Lecture 4: Introduction to pandas & matplotlib, Multiple Linear Regression Review\n_Tuesday, 2014/08/19_\n\n#### Class Materials\n\n* [Lecture](https://docs.google.com/presentation/d/11QwBL5HggOPv5VBLLyCjVGu1C41nLxTEEJXNrFWWwb8/edit?usp=drive_web)\n* [multiple_linear_regression_review.ipynb](https://github.com/gavinmh/GADS12-NYC/tree/master/lecture-4)\n* [object_oriented_programming_lab.ipynb](https://github.com/gavinmh/GADS12-NYC/tree/master/lecture-4)\n* [matplotlib_intro_lecture.ipynb](https://github.com/gavinmh/GADS12-NYC/tree/master/lecture-4)\n* [pandas_intro_lecture.ipynb](https://github.com/gavinmh/GADS12-NYC/tree/master/lecture-4)\n* [pandas_lab.ipynb](https://github.com/gavinmh/GADS12-NYC/tree/master/lecture-4)\n\n\n#### Project 1\n\n* [Project 1: Regression](https://github.com/gavinmh/GADS12-NYC/blob/master/lecture-4/project-1-regression.md)\n\n\n### Lecture 5: From Multiple Linear Regression to Polynomial Regression & Regression Project Workshop\n_Tuesday, 2014/08/21_\n\n#### Class Materials\n\n* [Lecture](https://docs.google.com/presentation/d/1NZJv9sgmbFivNMt7X-faPrnGTNri_g4C6soWKzvsUvQ/edit?usp=sharing)\n* [PolynomialRegressionAndRegularization.ipynb](https://github.com/gavinmh/GADS12-NYC/tree/master/lecture-5)\n* [intro_to_matplotlib.ipynb](https://github.com/gavinmh/GADS12-NYC/tree/master/lecture-5)\n\n#### Extra Materials\n\n* [matplotlib_examples.ipynb](https://github.com/gavinmh/GADS12-NYC/tree/master/lecture-5)\n\n\n### Lecture 6: From Multiple Linear Regression to Logistic Regression & Text Feature Extraction\n_Tuesday, 2014/08/26_\n\n#### Class Materials\n\n* [Lecture](https://docs.google.com/presentation/d/1evOrh5fxjUlr6F5zErzO_-Fc1Fo-s-T-0XxNeS8ZJiU/edit?usp=sharing)\n* [document_classification.ipynb](https://github.com/gavinmh/GADS12-NYC/tree/master/lecture-6)\n\n\n### Lecture 7: Classification with Logistic Regression and K-Nearest Neighbors\n_Thursday, 2014/08/28_\n\n#### Class Materials\n* [Lecture](https://docs.google.com/presentation/d/1ibefIZ1XHUnlI7BIAQdoqUxlU-Ayxi1p7k2tPtZyVVM/edit?usp=sharing)\n* [k_nearest_neighbors_classification.ipynb](https://github.com/gavinmh/GADS12-NYC/tree/master/lecture-7)\n\n\n### Lecture 8: (optional class) Kaggle Competitions\n_Tuesday, 2014/09/02_\n\n#### Class Materials\n\n\n### Lecture 9: Non-linear Classification and Regression with Decision Trees and Ensemble Learning\n_Thursday, 2014/09/04_\n\n#### Class Materials\n\n* [Lecture](https://docs.google.com/presentation/d/1TF6iAGHDrIKs3kN-smIWGdxIFUzwXRVK5Zj9GQvVz0E/edit?usp=sharing)\n* [decision_trees.ipynb](https://github.com/gavinmh/GADS12-NYC/tree/master/lecture-9)\n\n\n### Lecture 10: Cluster Analysis with K-Means\n_Tuesday, 2014/09/09_\n\n#### Class Materials\n\n* [Lecture](https://docs.google.com/presentation/d/1-URWRvKe9ecwQiH3U639NzDORjnXE27MA4fVb6APbIY/edit)\n* [elbow-method-solutions.ipynb](https://github.com/gavinmh/GADS12-NYC/tree/master/lecture-10)\n* [semi-supervised-classification.ipynb](https://github.com/gavinmh/GADS12-NYC/tree/master/lecture-10)\n\n\n### Lecture 11: Dimensionality Reduction with Principal Component Analysis\n_Thursday, 2014/09/11_\n\n#### Class Materials\n\n* [Lecture](https://docs.google.com/presentation/d/1xcR9NX7spBurDak-Vzy3tlv08B_9LiHOBhAOr6xKK_Y/edit#slide=id.g3aa599f3c_3483)\n* [eigenfaces.ipynb](https://github.com/gavinmh/GADS12-NYC/tree/master/lecture-11)\n* [iris-reduction.ipynb](https://github.com/gavinmh/GADS12-NYC/tree/master/lecture-11)\n\n\n### Lecture 12: Machine Learning Review\n_Thursday, 2014/09/16_\n\n#### Class Materials\n\n* [Lecture](https://docs.google.com/presentation/d/1WwoIrnmfq5Zd7BFg7kIdU4hvTJfy3gUKMYdfTaB0E8s/edit?usp=sharing)\n\n\n### Lecture 13: Project Workday\n_Thursday, 2014/09/18_\n\n#### Class Materials\n\n* [Lecture](https://docs.google.com/presentation/d/1WwoIrnmfq5Zd7BFg7kIdU4hvTJfy3gUKMYdfTaB0E8s/edit?usp=sharing)\n\n\n### Lecture 14: The Perceptron \n_Thursday, 2014/09/23_\n\n#### Class Materials\n\n* [Lecture](https://docs.google.com/presentation/d/1YpLHCrr_vkXvq-2Pa5Snd1YDfBylexP22vJuDKf5wsU/edit?usp=sharing)\n* [document_classification_2.ipynb](https://github.com/gavinmh/GADS12-NYC/tree/master/lecture-14)\n\n### No class\n_Thursday, 2014/09/25_\n\n\n### Lecture 15: From the Perceptron to Support Vector Machines\n_Thursday, 2014/09/29_\n\n#### Class Materials\n\n* [Lecture](https://docs.google.com/presentation/d/1U8roNE3xY-35u0uT9Nm_0snrNKK-LrHlYzL0RYPWzhI/edit?usp=sharing)\n* [natural_image_character_recognition.ipynb](https://github.com/gavinmh/GADS12-NYC/tree/master/lecture-15)\n\n\n### Lecture 16: From the Perceptron to Artificial Neural Networks\n_Thursday, 2014/10/02_\n\n#### Class Materials\n\n* [Lecture](https://docs.google.com/presentation/d/1KCo4Y5v89M1XJDyrOEnxtgTnROBE5F5vQoLZZo9USpU/edit?usp=sharing)\n* [mlp_xor.ipynb](https://github.com/gavinmh/GADS12-NYC/tree/master/lecture-16)\n* [mlp_mnist.ipynb](https://github.com/gavinmh/GADS12-NYC/tree/master/lecture-16)\n\n\n### Lecture 17: Web Apps with scikit-learn\n_Thursday, 2014/10/07_\n\n#### Class Materials\n\n* [Lecture](https://docs.google.com/presentation/d/11SdNjR-f7DxTii6h9-oHNWILiFPy0CCFCxOLCT24my8/edit?usp=sharing)\n* [Lab](https://github.com/gavinmh/GADS12-NYC/tree/master/lecture-17)\n\n\n### Lecture 18: Recommendation Engines\n_Thursday, 2014/10/02_\n\n#### Class Materials\n\n* [Lecture](https://docs.google.com/presentation/d/1gAKD9-_qRg4vNppzleAc7mxoUOe2aPPPEgDkIUOXqug/edit?usp=sharing)\n* [Lab](https://github.com/gavinmh/GADS12-NYC/tree/master/lecture-18)\n\n\n### Lecture 19: Guest Lecture 1\n_Thursday, 2014/10/09_\n\n#### Class Materials\n\n\n### Lecture 20: Guest Lecture 2: Visualization with D3.js\n_Thursday, 2014/10/14_\n\n#### Class Materials\n\n\n### Lecture 21: Final Project Workday\n_Thursday, 2014/10/16_\n\n#### Class Materials\n\n\n### Lecture 22: Final Project Presentations\n_Thursday, 2014/10/21_\n\n#### Class Materials\n\n\n### Lecture 23: Final Project Presentations\n_Thursday, 2014/10/23_\n\n#### Class Materials\n\n\n## Git Workflow and Command Line Tips:\n\n* [Tips](https://github.com/gavinmh/GADS12-NYC/tree/master/tips)\n\nUsing a virtual machine\n----\n\nFor further help/troubleshooting, feel free to come by the office hours or contact us for further help.\n\nIn case you're running into issues setting the environment up on your local environment, you can download a machine image from the following link: \n\n https://www.dropbox.com/s/7nt0rt54m7jtxj5/GADS-InstalledEnv_1%28import%29.ova\n\nUse the following user info to login: \n\nUser: GADS\n\nPassword: gadspassword\n\n**NOTE**: VMWare Player appears to occassionaly throw errors when dealing with this image.  Therefore, it would probably be easier to use [VirtualBox]\n\nTo install the virtual machine on VirtualBox\n\n* Install VirtualBox\n* Select File>Import Appliance\n* When prompted, select 'GADS-InstalledEnv_1(import).ova' (or whatever you decided to name the downloaded image)\n* Click through the rest of the import process\n* In the main menu of VirtualBox, highlight the name of the machine (it should be whatever you named the ova file), and press 'Start'.\n\nThe image has Ubuntu 14.04 installed.  I found that the default RAM allocation (512 MB) was running a bit slow, so I adjusted the default allocation to 1024 MB.  If you're running into performance issues, you may need to adjust the memory allocation settings.\n\nThe image has the following libraries installed on it:\n\n* [scikit-learn] (v 0.15.1)\n* [numpy] (v 1.8.1)\n* [pandas] (v 0.13.1)\n* [scipy] (v 0.13.3)\n* [pip] (v 1.5.6)\n* [matplotlib] (v 1.3.1)\n* [git]\n* [nltk] (v 2.0.4)\n\n\n#####  Git Repository in VM image\n\nThe class's git repo has been cloned as the following directory:\n\n~/Desktop/courseGit/GADS12-NYC\n\nThere is a small bash script (~/Desktop/courseGit/GADS12-NYC/update) that allows you to type 'update-GADS' in order to clone the latest version from the repo.  The command is only soft-linked to the script, meaning that if 'update' is moved, the command won't work anymore.  Even if that happens though, you can just type 'git pull' in the repo's directory to update your local repo.\n\n\n[scikit-learn]:http://scikit-learn.org/stable/\n[numpy]:http://www.numpy.org/\n[pandas]:http://pandas.pydata.org/\n[scipy]:http://www.scipy.org/\n[pip]:https://pypi.python.org/pypi/pip\n[matplotlib]:http://matplotlib.org/\n[git]:http://git-scm.com/\n[nltk]:http://www.nltk.org/\n[VirtualBox]:https://www.virtualbox.org/\n""",General Assembly Data Science 12
https://github.com/htwangtw/DimensionsOfExperience,b'# DimensionsOfExperience\nScripts for paper Dimensions of Experience: Exploring the Ontology of the Wandering Mind\n',Scripts for paper Dimensions of Experience: Exploring the Ontology of the Wandering Mind
https://github.com/learntextvis/code-samples,"b'# code-samples repo and project notes\ndraft code to communicate ideas.\n\nPlease read the proposal doc for audience and intent: https://docs.google.com/document/d/1aLM0y56zCDUqUd6NRyiL0Nl9HPbKognBvK0aczcq3_4/edit?usp=sharing\n(this is a copy that can be commented on)\n\nWould recommend playing with Lexos if you haven\'t.\n\n## Notes & ToDo List\n\n**Goal: single doc analysis/vis but also comparative multi-document in as many cases as possible.**\n\n### ""Analysis"" to offer\n\n* Tokenization: word, sentence\n* lemmatize and bigrams\n* Parts of Speech\n* Tf-idf\n* Hclust (hierarchical clustering)\n* (PCA? Need an experiment that doesn\'t suck and is interpretable, the R sample one isn\'t)\n* Word-list based sentiment analysis (https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon)\n\n### Counting Things\n\n* characters (maybe - for punctuation in particular)\n* words / lemmas\n* sentences\n* POS\n* bigrams -- needed as part of the tokenization step\n\n**StopWord Handling!**\n\n* We should always display the stop words used in the UI -- because they are a choice and carry consequences.\n* Start simple with an inline variable, then file, then tool in browser to update them?\n\n### Vis using the analysis/counts above\n\n* structure of page/doc? -- is this interesting? might be a good newbie view.\n    * (does this make sense in multi-document cases?)\n    * option to highlight items ""inline""\n    * Examples:\n        * Ben Fry\'s Darwin thing\n        * Art things on my pinterest\n        * My example in html_js/book_shape.html - which doesn\'t work for line preceding spaces yet (e.g., for tabs, indents, poetry, etc)\n\n* count totals, count as percentage of whole\n    * bars\n    * word clouds and variants\n    * tf-idf for multi-document cases\n        * see examples like html_js/shiffman_tfidf.html, wordclouds_with_shiffman_tfidf.html\n\n* Word networks\n\n* Word Clouds: **Let\'s investigate ways to make them less sucky/more tolerable.**\n    * Get rid of random coloring or words - color only as indicator\n    * Idea: little bar charts beside wordclouds to show distribution of counts?\n    * Idea: ordered words with counts (mocked up)\n    * Extremely useful in small-multiple/multi-doc situations, but design issues.\n        * Tf-idf sizing is most interesting in that case. See:\n        * Show dynamic side-by-side and merged, with difference. Examples:\n        * merged: maybe a network style? also, that NYT bubble thing with circles.\n        * need good design for the combination operation for overlapping words when counts differ between the overlaps\n    * Single doc cases can become small-multiple if we allow word-clouds of POS, chars, etc.\n    * Ideally perform bigram analysis first!\n    * stop words are iterative process with word cloud displays\n\n* Timeseries\n    * show location of word in doc over time (concordance view\n    * use windowsize (user-settable) and count, show over time\n        * examples are wordclouds per chapter in a book, in order\n        * who talks when in a debate / play\n        * the Tarantino obscenity chart in 538. We should be able to make that.\n        * Simple example in timeseries.html shows just words per chapter in order in Emma.\n    * Vis types over time - bar, line, even word cloud??\n\n* Clustering docs\n    * see hclust code - actually, this is pretty bad in python in that it requires a bunch of libs. R\'s is easier. (See my notebook code in python dir using Pattern.  It\'s easier to use scikit-learn but harder to explain to newbies. However, outputting the data from Pattern is awkward. Maybe NLTK is the best approach, I think you can save out numpy arrays easily?)\n    * R examples: https://eight2late.wordpress.com/2015/07/22/a-gentle-introduction-to-cluster-analysis-using-r/, mine in: R/tm_clustering_example.R\n    * tree view for first version? see output from R networkD3 dendrogram in html_js/networkD3_hclust_output_from_R.html\n    * Ideally the tree clustering allows collapsible nodes for large trees, and customizable labels on the edges.\n\n## How about a Structure Like This for the Site...\n\n1) Getting Setup:\n    * Options for R, Python, Pure JS\n    * (Explain cons of just in-browser JS)\n2) Shapes of texts\n3) Concordance-views (simple search) -- keywords in context\n4) Tokenization, Simple Counts, stop words\n    [We explain fancy tokenization happens in the python/r scripts, simple space sep can be done in js.]\n    Word networks come here - and bigrams/n-grams.\n5) Parts of speech, lemmatization - show how counts change, what POS gets you.\n6) Word clouds -- this basically sets us up for doing tf-idf because simple counts are bad for comparative documents, but tf-idf is better\n    * includes a variety of word cloud types -- bubble/networks, regular, maybe my ordered count css version\n7) Time Series - breaking a text into sections, using multiple texts that have time ordering\n    * Maybe simple sentiment via polarity word lists I added to the repo too.\n8) Clustering (if we get to it, or I can add it later, if you help with the cluster-output-to-tree structure for js)\n\n## Languages for Discussion\n\n**Discuss: Scripts for pre-processing (python/r) and/or in-page analysis with js.**\n\n###JS\n\n* RiTa for POS: http://www.ghostweather.com/files/image_replacement/; I haven\'t experimented with all of it\'s capabilities yet. Not sure I believe it can be as good as nltk/SpaCy and R\'s tm/NLP/SnowballC etc.\n* shiffman\'s tf-idf in js (used in some of my examples)\n* wordcount.js\n* that Natural node/js lib had issues last time I used it, but Shiffman was issueing merge requests against it...\n* I included some dude\'s textAnalysisSuite in the html_js/js dir, but I don\'t understand it\'s tfidf. The bigram thing looks interesting/useful.\n* I don\'t really think the js tools are as good yet or as complete; and for sizable projects that would be slow in browser, would we give node instructions?  (Argh)\n\n###Python\n\n* Note that python stuff generally needs downloads. SpaCy and NLTK both do.\n* I can make command-line scripts that prep data as json if we want that... My example script in preprocess_files.py was for another class, and i used it here to get POS for the word cloud experiments\n* Need to well-document and test the install&run instructions for someone newbie\n* Requires command line expertise\n* SpaCy might be a good lib to use, I have used it for word2vec related things but haven\'t checked carefully the contrast with NLTK/pattern.  It won\'t do tf-idf for us.\n\n###R\n\n* Is this simplest actually? scripts that can be run from RStudio or command line?\n* Still requires certain packages\n* My R is rusty but Jim\'s is good I bet\n\n\n**Discuss: Should we have some kind of consistent format like JSON for output from the scripts?  Or csv, which is easier for newbies.  Configuration settings could be in a simple JSON file separate from the data files.**\n\n**ToDo: Compare the accuracy and quality of the results in the 3 languagues and multiple libraries in Python/R. There are a lot.  I can do that.**)\n\n###Data Sets ToDo\'s\n\n* Need to act a play and/or script\n* Poetry examples\n* Maybe recipes?  some very different genre...\n\n'",draft code to communicate ideas 
https://github.com/athirara/GBM,b'# GBM\nGeometrical Brownian Motion\n',Geometrical Brownian Motion
https://github.com/alexisperrier/upem-topic-modeling,"b""# upem-topic-modeling\nCours sur le topic modeling - UPEM - Master M\xc3\xa9thode computationnelle et analyse de contenu\n\n## I: Topic Modeling\n\n* Nature et applications\n* Approche Deterministe: LSA\n* Approche Probabiliste: LDA\n* Quelques librairies en R et python\n\n## II: Le package STM en R\n\n* Parametres\n* M\xc3\xa9triques: exclusivit\xc3\xa9 et coh\xc3\xa9rence s\xc3\xa9mantique\n* Appliqu\xc3\xa9 a un corpus propre\n\n## LAB - R STM\n\n* Le corpus: r\xc3\xa9sum\xc3\xa9s d'articles tech, IEEE et Arstechnica\n* Le package STM en R\n* Comment determiner le nombre optimal de topics?\n* Comment interpreter les r\xc3\xa9sultats?\n* Jupyter Notebook et Script R\n\n## III: forum Alt-right sur Facebook\n\n* 500.000 commentaires provenant du forum alt-right God Trump Emperor\n* De la n\xc3\xa9cessit\xc3\xa9 de travailler le contenu\n* Filtrer le bruit avec\n    * Lemmatization, tokenization\n    * Part of Speech tagging\n    * Named entity recognition\n* Jupyter Notebook et Script R\n\n## IV: Application au Francais\n\n* Quelles sont les librairies pour:\n    * Part of Speech\n    * Tokenization\n    * Lemmatization\n\n## V: Resources\n\n* Articles et blogs\n\n""",Cours sur le topic modeling - UPEM - Master Méthode computationnelle et analyse de contenu
https://github.com/bryancshepherd/RunStatus,b'# RunStatus\n',"Build a script running status tracker with R, Python and RPi"
https://github.com/jagman88/HACTproject_Julia,"b'# HACTproject_Julia\n\nDescribes and solves some simple HACT models in Julia. The notes and codes are modified and translated from Ben Moll\'s notes and matlab codes at http://www.princeton.edu/~moll/notes.htm and http://www.princeton.edu/~moll/HACTproject.htm \n\nThis project was carried out as part of the assessment for John Stachurski\'s Computational Economics course at NYU, 2016. \n\nPlease note that the Jupyter notebook files (with the "".ipynb"" extension) may not easily compile into PDF\'s due to the presence of Gadfly plots. To compile from the Jupyter Notebook Viewer, you may need to install the Inkscape and Pandocs packages (and even then, the Gadfly plots may not show up properly...). It may be easier to compile to PDF via Latex. For this reason, I have included Python\'s Matplotlib plots in the PDF writeup file, ""HACT_HuggettEconomy_Julia.pdf"". \n\n'",Describes and solves some simple HACT models in Julia.  The notes and code is modified and translated from Benjamin Moll's notes and codes: http://www.princeton.edu/~moll/notes.htm and http://www.princeton.edu/~moll/HACTproject.htm).
https://github.com/timothydmorton/gaia-explore,b'# gaia-explore\nfirst look into GAIA data\n',first look into GAIA data
https://github.com/doorleyr/SFCrime,"b'# SFCrime\nA brief analysis of crime in San Franscico during summer, 2014, completed as an assignment for the Coursera module ""Communicating Data Science Results"". \n\nThe ipython notebook file was created as a Jupyter notebook using the R Kernel.\n'",Visualising crime in San Fransisco duing summer 2014
https://github.com/RunZGit/Magicmail,b'# Magicmail\nUsing Word2Vec techniques and K-Mean method to classifies Rose-Hulman sharepoint emails based on its content\n',Using Word2Vec techniques and K-Mean method to classifies Rose-Hulman sharepoint emails based on its content
https://github.com/ramon-oliveira/deepstats,"b'# Known Unknowns: Uncertainty Quality in Bayesian Neural Networks\n\nThis repository holds the code for the paper ""Known Unknowns: Uncertainty Quality in Bayesian Neural Networks"" accepted to the Bayesian Deep Learning Workshop at NIPS 2016.\n\nArxiv link: https://arxiv.org/abs/1612.01251\n\nWe evaluate the uncertainty quality in neural networks using anomaly detection. We extract uncertainty measures (e.g. entropy) from the predictions of candidate models, use those measures as features for an anomaly detector, and gauge how well the detector differentiates known from unknown classes. We assign higher uncertainty quality to candidate models that lead to better detectors. We also propose a novel method for sampling a variational approximation of a Bayesian neural network, called One-Sample Bayesian Approximation (OSBA). We experiment on two datasets, MNIST and CIFAR10. We compare the following candidate neural network models: Maximum Likelihood, Bayesian Dropout, OSBA, and --- for MNIST --- the standard variational approximation. We show that Bayesian Dropout and OSBA provide better uncertainty information than Maximum Likelihood, and are essentially equivalent to the standard variational approximation, but much faster.\n\n## Reproducing results (Python 3.5)\n\n### Installing dependences\n```bash\npip install -r requirements.txt\n```\n\n### Running an experiment\n\n```bash\npython run_experiment.py --dataset=mnist --model=mlp-dropout\n```\n\nAvailable dataset options:\n* mnist\n* cifar10\n* svhn (experimental)\n\nAvailable model options:\n* mlp\n* mlp-dropout\n* mlp-poor-bayesian\n* mlp-bayesian\n* convolutional\n* convolutional-dropout\n* convolutional-poor-bayesian\n\n\n### Plotting results\n\n```bash\npython plots_anova.py --dataset=mnist\n```\n\n## ANOVA Results\n\n### MNIST\n\n<img src=""mnist_results/images/diff_iou_io.png"" alt=""Calibrated - Blind"" width=""24%""/>\n<img src=""mnist_results/images/diff_drop_ml.png"" alt=""Dropout - ML"" width=""24%""/>\n<img src=""mnist_results/images/diff_os_ml.png"" alt=""OneSample - ML"" width=""24%""/>\n<img src=""mnist_results/images/diff_os_drop.png"" alt=""OneSample - Dropout"" width=""24%""/>\n\n### CIFAR10 (Updated Results)\n\n<img src=""cifar10_results/images/diff_iou_io.png"" alt=""Calibrated - Blind"" width=""24%""/>\n<img src=""cifar10_results/images/diff_drop_ml.png"" alt=""Dropout - ML"" width=""24%""/>\n<img src=""cifar10_results/images/diff_os_ml.png"" alt=""OneSample - ML"" width=""24%""/>\n<img src=""cifar10_results/images/diff_os_drop.png"" alt=""OneSample - Dropout"" width=""24%""/>\n'","Repository for ""Known Unknowns: Uncertainty Quality in Bayesian Neural Networks"" paper."
https://github.com/Aniruddha-Tapas/Android_OpenCV_Scan2Excel,"b'<hr>\n\n# Android OpenCV demo with TensorFlow\n\n* Scans tabular images using OpenCV for Android\n* Recognizes digits using TensorFlow\n* Writes to Excel file [To be done!]\n\n\n<hr>\n\nMNIST For ML Beginners\nhttps://www.tensorflow.org/versions/r0.10/tutorials/mnist/beginners/index.html\n\nDeep MNIST for Experts\nhttps://www.tensorflow.org/versions/r0.10/tutorials/mnist/pros/index.html\n\n## How to train model.\nTraining scripts for neural network model are located at\n\nhttps://github.com/miyosuda/TensorFlowAndroidMNIST/tree/master/trainer-script\n\nTo create model by yourself, install Tensorflow and run python scripts like\n\n    $ python beginner.py\n\nor\n\n    $ python expert.py\n\nand locate exported .pb file to assets dir.\n\nTo export training model, I added some modification to original tutorial scripts.\n\nNow Tensorflow cannot export network graph and trained network weight Variable at the same time,\nso we need to create another graph to export and convert Variable into constants.\n\nAfter training is finished, converted trained Variable to numpy ndarray.\n\n    _W = W.eval(sess)\n    _b = b.eval(sess)\n\nand then convert them into constant and re-create graph for exporting.\n\n    W_2 = tf.constant(_W, name=""constant_W"")\n    b_2 = tf.constant(_b, name=""constant_b"")\n\nAnd then use tf.train.write_graph to export graph with trained weights.\n\n\n## How to build JNI codes\n\nNative .so files are already built in this project, but if you would like to\nbuild it by yourself, please install and setup NDK.\n\nFirst download, extract and place Android NDK.\n\nhttp://developer.android.com/intl/ja/ndk/downloads/index.html\n\nAnd then update your PATH environment variable. For example,\n\n    export NDK_HOME=""/Users/[your-username]/Development/android/android-ndk-r11b""\n    export PATH=$PATH:$NDK_HOME\n\nAnd build .so file in jni-build dir.\n\n    $ cd jni-build\n    $ make\n    \nand copy .so file into app/src/main/jniLibs/armeabi-v7a/ with\n\n    $ make install\n\n(Unlike original Android demo in Tensorflow, you don\'t need to install bazel to build this demo.\n\nTensorflow library files (.a files) and header files are extracted from original Tensorflow Android demo r0.10.\n\n<hr>'",Android app to convert scanned tabular digit data to Excel using Tensorflow. [Incomplete]
https://github.com/koaning/python_data_intro,b'# python_data_intro\n\nA beginner notebook for people who want to get started with python and data. Joy ensues! Feel free to use this notebook to teach people some basic pydata! \n',A beginner notebook for people who want to get started with python and data. Joy ensues! 
https://github.com/profhuster/profhuster.github.io,b'# profhuster.github.io\nPersonal Web Site for Michael Huster (profhuster)\n',Personal Web Site for Michael Huster (profhuster)
https://github.com/bz866/Exploration-on-New-York-Crime-Open-Data-Based-on-PolyGamy-Thoughts,"b""# BigData2017Project\nBigData2017Project\n\n### File Explaination\nThe repo consists of three file:\n\n1. ```spark_job_script.py``` This is used for generating validation check and output 24 files for each column.\n2. ```spark_validity_statistics.py``` spark script for summarizing how many Valid/Invalid/Null in each category.\n3. ```sparksql_script``` code for using pysparkSQL to check how many distinct values in each category.\n4. ```all_area_code.py``` This is used for generating all precincts code.\n5. ```area_desc_rate.py``` This is used for generating proportions of all kinds of crime in all precincts. \n6. ```average_compliant_duration_each_borough.py``` This is used for generating the average time used to solve a case in different boroughs. This script must be run after ```duration.py```.\n7.  ```duration.py``` This is used for generating required data to compute the average time used to solve a case in different boroughs. This script must be run before ```average_compliant_duration_each_borough.py```.\n8. ```pattern.py``` This is used for generating some of patterns we used in this project.\n9. ```plot.ipynb``` This is the jupyter notebook used for generating all plots in this project.\n\n### Steps for running jobs\nTo get the result required, i.e. check for base_type, semantic_type, validity, simply run the spark_job_script.py on your dumbo as follows,\n\n1. Upload ```spark_job_script.py``` to your hpc storage with command:\n```\nscp  dir/spark_job_script.py  NetID@dumbo.es.its.nyu.edu:/home/NetID\n```\n\n2. Upload ```NYPD_Complaint_Data_Historic.csv``` to hpc like above then upload it to hdfs with command:\n```\nhadoop fs -copyFromLocal NYPD_Complaint_Data_Historic.csv\n```\n\n3. Since we need to use some python packages, first set up your python environment with below sentences\n\n```\nmodule load python/gnu/3.4.4\nexport PYSPARK_PYTHON=/share/apps/python/3.4.4/bin/python\nexport PYTHONHASHSEED=0\nexport SPARK_YARN_USER_ENV=PYTHONHASHSEED=0\n```\n\n4. Submit spark job\n```\nspark-submit spark_job_script.py\n```\n\n5. Get output \n```\nhadoop fs -getmerge name.out name.out\n```\n\n### Please note:\n1. We assume user won't change the name of the CSV file and keep it as NYPD_Complaint_Data_Historic.csv;\n2. Please make sure the previously output on your HPC is removed so it won't influence running the script.\n\n\n""",NYU Big Data CourseTerm Project
https://github.com/nikitakit/nbtools,b'nbtools - Tools for IPython notebooks\n',Tools for IPython notebooks
https://github.com/fperez/gh-activity,b'# gh-activity\nTools to measure activity on github for a set of orgs and repos\n',Tools to measure activity on github for a set of orgs and repos
https://github.com/Anima-OS/Anima_Terminal,"b'.. _README:\n\nREADME\n==================================================================\n \n.. contents::\n\nIntroduction\n----------------------------------------------------------------------------------------------\n\n``GraphTerm`` is a browser-based graphical terminal interface, that\naims to seamlessly blend the command line and graphical user\ninterfaces. You can use it just like a regular terminal,\nbackwards-compatible with ``xterm``, and access the additional\ngraphical features as needed. These features can help impove your\nterminal workflow by integrating graphical operations with the\ncommand line and letting you view images and HTML output inline.\n\nGraphTerm has several funky features, but two of the most useful\npractical applications are:\n\n - an **inline data visualization tool** for plotting with Python or R\n   that can work seamlessly across SSH login\n   boundaries, with an optional notebook interface. (For remote\n   access, it also serves as a detachable terminal, like\n   ``tmux`` or ``screen``.)\n\n - a `virtual computer lab <http://code.mitotic.org/graphterm/virtual-setup.html>`_ for teaching and demonstrations. The\n   GraphTerm server can be set up in the cloud and accessed by\n   multiple users using their laptop/mobile browsers, with Google\n   Authentication. The lab instructor can\n   `monitor all the users\'  terminals <http://code.mitotic.org/graphterm/screenshots.html#dashboard-for-a-virtual-computer-lab-viewing-user-terminals>`_\n   via a ""dashboard"", and users can collaborate with each other by\n   sharing terminals and notebooks.\n\n\n **Screenshot 1: Inline plotting on a remote machine (via SSH)**\n\n.. figure:: https://github.com/mitotic/graphterm/raw/master/doc-images/gt-ssh-plot.png\n   :align: center\n   :width: 90%\n   :figwidth: 85%\n\n.. raw:: html\n\n   <hr style=""margin-bottom: 3em;"">\n..\n\n **Screenshot 2: Monitoring multiple user terminals in a ""virtual computer lab""**\n\n.. figure:: https://github.com/mitotic/graphterm/raw/master/doc-images/gt-screen-gadmin-terminals.png\n   :align: center\n   :width: 90%\n   :figwidth: 85%\n\n\nGraphTerm builds upon two earlier projects, \n`XMLTerm <http://www.xml.com/pub/a/2000/06/07/xmlterm/index.html>`_\nwhich implemented a terminal using the Mozilla framework and\n`AjaxTerm <https://github.com/antonylesuisse/qweb/tree/master/ajaxterm>`_\nwhich is an AJAX/Python terminal implementation. (Other recent\nprojects along these lines include  `TermKit <http://acko.net/blog/on-termkit/>`_\nand `Terminology <http://www.enlightenment.org/p.php?p=about/terminology>`_.)\n\nA GraphTerm terminal window is just a web page served from the\nGraphTerm server program. Multiple users can connect\nsimultaneously to the web server to share terminal sessions.\nMultiple hosts can also connect to the server (on a different port),\nallowing a single user to access all of them via the browser.\nThe GraphTerm server acts as a *router*, sending input from browser\nwindows for different users to the appropriate terminal (pseudo-tty)\nsessions running on different hosts, and transmitting the\nterminal output back to the browser windows.\n\nThe interface is designed to be touch-friendly for use with\ntablets, with tappable links and command re-use to minimize the need for\na keyboard. It preserves history for all commands,\nwhether entered by typing, clicking, or tapping.\nIt is also themable using CSS.\n\nYou can use the GraphTerm API to build ""mashups"" of web applications\nthat work seamlessly within the terminal.  Sample mashups include:\n\n - ``greveal``: Inline version of ``reveal.js`` to display Markdown files as slideshows\n - ``gtutor``: Inline version of `pythontutor.com <http://pythontutor.com>`_ for visual tracing of python programs\n - ``yweather``: Using Yahoo weather API to display weather\n\nImages of GraphTerm in action can be found in `screenshots <https://github.com/mitotic/graphterm/blob/master/docs/screenshots.rst>`_ \nand in this `YouTube Video <http://youtu.be/TvO1SnEpwfE>`_.\nHere is a sample screenshot showing the output of the\n`metro.sh <https://github.com/mitotic/graphterm/blob/master/graphterm/bin/metro.sh>`_\ncommand, which embeds six smaller terminals within the main terminal, running\nsix different commands from the GraphTerm toolchain: (i) live twitter stream output using\n``gtweet``, (ii) weather info using ``yweather``,\n(ii) slideshow from markdown file using ``greveal`` and *reveal.js*,\n(iv) word cloud using ``d3cloud`` and *d3.js*, (v) inline graphics using ``gmatplot.py``,\nand (vi) notebook mode using the standard python interpreter.\n\n\n **Screenshot 3: Embedding terminals within GraphTerm**\n\n.. figure:: https://github.com/mitotic/graphterm/raw/master/doc-images/gt-metro.jpg\n   :align: center\n   :width: 90%\n   :figwidth: 100%\n\n.. _installation:\n\nInstallation\n----------------------------------------------------------------------------------------------\n\nTo install ``GraphTerm``, you need to have Python 2.6+ and the Bash\nshell on your Mac/Linux/Unix computer. For a quick install, use one of\nthe following two options::\n\n   sudo pip install graphterm\n        OR\n   sudo easy_install graphterm; sudo gterm_setup\n\nIf you wish to install GraphTerm as a non-root user within an Anaconda\nor Enthought Python environment, you can omit the ``sudo`` prefix.\n\nFor a manual install procedure, download the release tarball from the\n`Python Package Index <http://pypi.python.org/pypi/graphterm>`_, untar,\nand execute the following command in the ``graphterm-<version>`` directory::\n\n   python setup.py install\n\nFor the manual install, you will also need to install the ``tornado``\nweb server, which can be downloaded from\n`http://www.tornadoweb.org <http://www.tornadoweb.org>`_\n\nYou can also try out GraphTerm without installing it, by untarring the\nsource tarball (or checking out the source from ``github``). You can\nrun the server as ``./gtermserver.py`` within the ``graphterm``\nsubdirectory of the distribution, after you have installed the\n``tornado`` package on your system (or within the ``graphterm``\nsubdirectory of the source distribution). In this case, certain\ncommands in the ``graphterm/bin`` subdirectory, such as ``gterm`` and\n``gauth``, would need to be accessed as ``gterm.py`` and ``gauth.py`` respectively.\n\nYou can browse the ``GraphTerm`` source code, and download the development\nversion, at `Github <https://github.com/mitotic/graphterm>`_.\n\nQuick Start\n----------------------------------------------------------------------------------------------\n\nTo start the ``GraphTerm`` server, use the command::\n\n    gtermserver --terminal --auth_type=none\n\nThis will run the  server and open a GraphTerm terminal window\nusing the default browser. For multi-user computers,\nomit the ``--auth_type=none`` option\nwhen starting the server, and enter the authentication code stored in\nthe file ``~/.graphterm/_gterm_auth.txt`` as needed. (The ``gterm``\ncommand can automatically enter this code for you.)\n\nYou can access the GraphTerm server using any browser that supports\nwebsockets. Google Chrome works best, but Firefox, Safari, or IE10\nare also supported. Start by entering the following URL::\n\n    http://localhost:8900\n\nIn the ``graphterm`` browser page, select the GraphTerm host you\nwish to connect to and create a new terminal session. (Note: The GraphTerm\nhost is different from the network hostname for the server.)\nWithin a GraphTerm window, you can use *terminal/new* menu option, or\ntype the command ``gmenu new``, to create a new GraphTerm session \n\nYou can also open additional GraphTerm terminal windows using\nthe ``gterm`` command::\n\n    gterm --noauth [session_name]\n\nwhere the terminal session name argument is optional.\n\nOnce you have a terminal, try out the following commands::\n\n    gls <directory>\n    gvi <text-filename>\n\nThese are commands in the GraphTerm toolchain that imitate\nbasic features of the standard ``ls`` and ``vi`` commands.\n(*Note:* You need to execute the ``sudo gterm_setup`` command\nto be able to use the GraphTerm toolchain. Otherwise, you will\nencounter a ``Permission denied`` error.)\nSee `Getting Started with GraphTerm <http://code.mitotic.org/graphterm/start.html>`_\nfor more info on using GraphTerm. You can also\n`set up a virtual computer lab\n<http://code.mitotic.org/graphterm/virtual-setup.html>`_\nusing GraphTerm.\n\nDocumentation and Support\n----------------------------------------------------------------------------------------------\n\nUsage info and other documentation can be found on the project home page,\n`code.mitotic.org/graphterm <http://code.mitotic.org/graphterm>`_.\nSee the `Contents <http://code.mitotic.org/graphterm/contents.html>`_\npage for an overview of the documentation and the\n`Talks and Tutorials <http://code.mitotic.org/graphterm/talks.html>`_\npage for more advanced usage examples.\n\nYou can also use the following command::\n\n  greveal $GTERM_DIR/bin/landslide/graphterm-talk1.md | gframe -f\n\nto view a slideshow about GraphTerm within GraphTerm.\nClick on the red X in the top right corner to exit the slideshow.\n\nThere is a `Google Groups mailing list <https://groups.google.com/group/graphterm>`_\nfor announcements of new releases, posting questions related to\nGraphTerm etc. You can also follow `@graphterm <https://twitter.com/intent/user?screen_name=graphterm>`_ on Twitter for updates.\n\nTo report bugs and other issues, use the Github `Issue Tracker <https://github.com/mitotic/graphterm/issues>`_.\n\nCaveats and Limitations\n----------------------------------------------------------------------------------------------\n\n - *Reliability:*  This software has not been subject to extensive testing. Use at your own risk.\n\n - *Platforms:* The ``GraphTerm`` client should work on most recent\n    browsers that support Websockets, such as Google Chrome, Firefox,\n    and Safari. (Google Chrome usually works best.) The ``GraphTerm``\n    server is pure-python, but with some OS-specific calls for file,\n    shell, and terminal-related operations. It has been tested only on\n    Linux and Mac OS X so far.\n\n - *Current limitations:*\n          * Support for ``xterm`` escape sequences is incomplete.\n          * Most features of GraphTerm only work with the bash shell, not with C-shell, due the need for PROMPT_COMMAND to keep track of the current working directory.\n          * At the moment, you cannot customize the shell prompt. (You\n            should be able to so in the future.)\n\nCredits\n----------------------------------------------------------------------------------------------\n\n``GraphTerm`` is inspired by two earlier projects that implement the\nterminal interface within the browser,\n`XMLTerm <http://www.xml.com/pub/a/2000/06/07/xmlterm/index.html>`_ and\n`AjaxTerm <https://github.com/antonylesuisse/qweb/tree/master/ajaxterm>`_. \nIt borrows many of the ideas from *XMLTerm* and re-uses chunks of code from\n*AjaxTerm*. The server uses the asynchronous `Tornado web framework\n<http://tornadoweb.org>`_ and the client uses `jQuery <http://jquery.com>`_.\n\nThe ``gls`` command uses icons from the `Tango Icon Library\n<http://tango.freedesktop.org>`_, and graphical editing uses the\n`Ajax.org Cloud9 Editor <http://ace.ajax.org>`_ as well as\n`CKEditor <http://ckeditor.com>`_\n\nThe 3D perspective mode was inspired by Sean Slinsky\'s `Star Wars\nOpening Crawl with CSS3 <http://www.seanslinsky.com/star-wars-crawl-with-css3>`_.\n\nOther packaged open source components include:\n\n - `d3.js <http://d3js.org/>`_  Data driven documents\n\n - `Landslide <https://github.com/adamzap/landslide>`_ presentation\n   program\n\n - Online Python Tutorial from `pythontutor.com <http://pythontutor.com>`_\n\n - `Pagedown <http://code.google.com/p/pagedown/>`_ Javascript\n   Markdown converter\n\n - `Superfish <http://users.tpg.com.au/j_birch/plugins/superfish/>`_\n   menu plugin\n\n - `underscore.js <http://underscorejs.org/>`_ utility library\n\n\n\nLicense\n----------------------------------------------------------------------------------------------\n\n``GraphTerm`` is distributed as open source under the `BSD-license <http://www.opensource.org/licenses/bsd-license.php>`_.\n\n'",The default terminal. Based on gterm
https://github.com/fanshi118/NLP_NMT_Project,b'# NLP_NMT_Project\nNeural Machine Translation project for NLP Fall 2016\n\n## Steps to run\n`python translate.py source_language target_language`\n(i.e. `python translate.py fr en` for fr->en translation)\n\nTraining results are shown in `plot_training_process.ipynb`.\n\nTesting results and findings are discussed in the paper.',Neural Machine Translation project for NLP Fall 2016
https://github.com/yannche/pyCommon,b'# pyCommon\n',ensemble de fichiers a mettre dans l'image virtualbox
https://github.com/grantrosario/advanced_lane_finding,"b'\n#**Advanced Lane Finding Project**\n---\nThe goals / steps of this project are the following:\n\n* Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n* Apply a distortion correction to raw images.\n* Use color transforms, gradients, etc., to create a thresholded binary image.\n* Apply a perspective transform to rectify binary image (""birds-eye view"").\n* Detect lane pixels and fit to find the lane boundary.\n* Determine the curvature of the lane and vehicle position with respect to center.\n* Warp the detected lane boundaries back onto the original image.\n* Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.\n\n[//]: # (Image References)\n\n[image1]: output_images/calibration_img.png ""Undistorted""\n[image2]: output_images/undistort_street.png ""Road Transformed""\n[image3]: output_images/binary.png ""Road Binary""\n[image4]: output_images/warped.jpg ""Warped Lanes""\n[image5]: output_images/warped_binary.png ""Warped Binary Lanes""\n[image6]: output_images/histogram.png ""Histogram""\n[image7]: output_images/find_lanes.png ""Polynomial Lanes""\n[image8]: output_images/find_lanes_margin.png ""Margin Lanes""\n[image9]: output_images/final.png ""Output Image""\n[video1]: final_attempt.mp4 ""Video""\n\n\n### Camera Calibration\n\n#### 1. Description\n\nThe code for this step is contained in the second code cell of the IPython notebook located in ""advanced\\_lane\\_finding.ipynb"" (or in lines 44 through 71 of the file called `alf.py`).  \n\nI start by preparing ""object points"", which will be the (x, y, z) coordinates of the chessboard corners in the world. Here I am assuming the chessboard is fixed on the (x, y) plane at z=0, such that the object points are the same for each calibration image.  Thus, `objp` is just a replicated array of coordinates, and `objpoints` will be appended with a copy of it every time I successfully detect all chessboard corners in a test image.  `imgpoints` will be appended with the (x, y) pixel position of each of the corners in the image plane with each successful chessboard detection.  \n\nI then used the output `objpoints` and `imgpoints` to compute the camera calibration and distortion coefficients using the `cv2.calibrateCamera()` function.  I applied this distortion correction to the test image using the `cv2.undistort()` function and obtained this result: \n\n![alt text][image1]\n\n### Pipeline (single images)\n\n#### 1. Undistort\n\nThe first step in the pipeline is to undistort the test image as seen below: \n\n![alt text][image2]\n\n#### 2. Create thresholded binary image\n\nI decided to avoid using gradients since they do not handle shadows well. Instead, I used a combination of color thresholds to generate a binary image (thresholding steps at lines 73 through 145 in `alf.py`). Specifically, I utilized the rgb color set to focus on yellow lines and the L channel in the HLS color set to focus on white lines. Here\'s an example of my output for this step.\n\n![alt text][image3]\n\n#### 3. Perspective Transform\n\nThe code for my perspective transform includes a function called `transform()`, which appears in lines 150 through 159 in the file `alf.py` (or in the 4th code cell of the IPython notebook).  The `transform()` function takes as inputs an image (`img`), as well as source (`src`) and destination (`dst`) points.  I chose the hardcode the source and destination points in the following manner:\n\n```python\nw,h = 1280,720\nx,y = 0.5*w, 0.8*h\n\nsrc = np.float32([[200./1280*w,720./720*h],\n                  [453./1280*w,547./720*h],\n                  [835./1280*w,547./720*h],\n                  [1100./1280*w,720./720*h]])\n                  \ndst = np.float32([[(w-x)/2.,h],\n                  [(w-x)/2.,0.82*h],\n                  [(w+x)/2.,0.82*h],\n                  [(w+x)/2.,h]])\n```\n\nThis resulted in the following source and destination points:\n\n| Source        | Destination   | \n|:-------------:|:-------------:| \n| 200, 720      | 320, 720        | \n| 453, 547      | 320, 590.40002441      |\n| 835, 547     | 960, 590.40002441      |\n| 1100, 720      | 960, 720       |\n\nI verified that my perspective transform was working as expected by drawing the `src` and `dst` points onto a test image and its warped counterpart to verify that the lines appear parallel in the warped image. I then converted it to binary as well.\n\n![alt text][image4]\n![alt text][image5]\n\n#### 4. Lane-line Identification\n\nThis step can be seen in lines 152 through 320 of the code in `alf.py`. I calculated the following histogram for the image which shows active pixels along all columns on the lower half of the image.\n\n![alt text][image6] \n\nThe two tallest peaks on the left half and right half show where in the image the lane lines appear. \n \n  \nNext, I created 9 individual windows starting from the bottom and generating one by one and scanning their area for pixels. Using this, I was able to generate a fit for the left and right lane lines as seen below.\n\n![alt text][image7] \n\nOnce I was able to estimate this from scratch, I could use this estimation for future frames of the video instead of having to estimate the lines from scratch each time. I instead scanned around future areas using a margin of 100 starting from the point of the previous frame line. Below is a visualization of this margin mathod. \n\n![alt text][image8]\n\n#### 5. Radius of Curvature and Distance Calculation\n\nI did this in lines 323 through 371 in my code in `alf.py`\n\nThe radius of curvature was calculated based on the previously calculated second order polynomial.\n\nI first converted the pixels to meters in accordance with U.S. regulations that lane width be 3.7 meters and our images have a lane _length_ of about 30 meters.\n\nOnce I do this I just fit the left and right lane to a polynomial and use the following formula to calculate the radius. \n \n```python\nleft_curverad = ((1 + (2*left_fit_cr[0]*y_eval*ym_per_pix + left_fit_cr[1])**2)**1.5) / np.absolute(2*left_fit_cr[0])\n\nright_curverad = ((1 + (2*right_fit_cr[0]*y_eval*ym_per_pix + right_fit_cr[1])**2)**1.5) / np.absolute(2*right_fit_cr[0])\n```\nIn order to calculate the distance from the center I just calculated the car position as the center of the image, subtracted the mean of the left and right intercepts from it and mutiplied that by the image.\n\n#### 6. Provide an example image of your result plotted back down onto the road such that the lane area is identified clearly.\n\nI implemented this step in lines 373 through 419 in my code in `alf.py` in the functions `drawLines()` and `drawData`.  Here is an example of my result on a test image:\n\n![alt text][image9]\n\n---\n\n### Pipeline (video)\n\n#### 1. Provide a link to your final video output.  Your pipeline should perform reasonably well on the entire project video (wobbly lines are ok but no catastrophic failures that would cause the car to drive off the road!).\n\nHere\'s a [link to my video result](final_attempt.mp4)\n\n---\n\n### Discussion\n\n#### 1. Briefly discuss any problems / issues you faced in your implementation of this project.  Where will your pipeline likely fail?  What could you do to make it more robust?\n\nHere I\'ll talk about the approach I took, what techniques I used, what worked and why, where the pipeline might fail and how I might improve it if I were going to pursue this project further.\n\nThe biggest factors of this project were thresholding, skipping the sliding windows functionality if need be, and smoothing the fits. These were the 3 areas that took me the most time, had the largest effect, and I believe could be greatly improved upon.  \n  \nAs stated, I used the sliding window functionality in order to read the lane lines and skipped this funcationaly when I could see the line based on the previous frame. I would like to see how using convolution would compare to this for possible improvement.\n\nI also only used the RGB and L-channel of HLS for thresholding. This is the biggest weakness as the program does not fair well in shadows and noisy environemnts. I would definitely like to explore this in more detail and build a more robust threshold. One possibility would be to create a dynamic threshold by scanning the brightness of the image.\n  \nLastly, smoothing the fits took a lot of trial and error. I ended up using an `add_fit()` method in the Line() class to average the fits and find the best one. I think this may be able to be improved upon by avoiding the hard-coded values I used.\n'",An advance implementation of detecting lane lines 
https://github.com/tuanavu/udacity-course,b'Udacity\n---\n\nThis repo contains my lectures and assignments of all [Udacity](https://www.udacity.com/) courses.\n\n\n## License\n\nAll Solutions licensed under MIT License. See LICENSE for further details.\n',Udacity courses
https://github.com/mu529/airport_modes,"b'<iframe width=""100%"" height=""520"" frameborder=""0"" src=""https://mu529.cartodb.com/viz/e25cc8e4-316d-11e6-8921-0ef7f98ade21/embed_map"" allowfullscreen webkitallowfullscreen mozallowfullscreen oallowfullscreen msallowfullscreen></iframe>'",Modal choice study for NYC airports
https://github.com/rllabmcgill/rlcourse-march-10-hugobb,"b'# Automatic Discovery of Subgoals Using Diverse Density\n\nThis repository contains an implementation of [[1](#1)]\n\nThe code is tested on a two-room gridworld as in the paper. Only the subgoal discovery is implemented, the policy learning over the option hasn\'t been implemented.\n\nOpen notebook for explanation, and results.\n\n<a name=\'1\'></a> [1] [McGovern, Amy, and Andrew G. Barto. ""Automatic discovery of subgoals in reinforcement learning using diverse density."" (2001).](https://pdfs.semanticscholar.org/7eca/3acd1a4239d8a299478885c7c0548f3900a8.pdf)\n'",rlcourse-march-10-hugobb created by GitHub Classroom
https://github.com/data-8/datascience,"b'# datascience\n\nA Berkeley library for introductory data science.\n\n_written by Professor [John DeNero](http://denero.org), Professor\n[David Culler](http://www.cs.berkeley.edu/~culler),\n[Sam Lau](https://github.com/samlau95), and [Alvin Wan](http://alvinwan.com)_\n\nFor an example of usage, see the [Berkeley Data 8 class](http://data8.org/).\n\n[![Documentation Status](https://readthedocs.org/projects/datascience/badge/?version=master)](http://datascience.readthedocs.org/en/master/?badge=master)\n[![Build Status](https://github.com/data-8/datascience/actions/workflows/run_tests.yml/badge.svg?branch=master)](https://github.com/data-8/datascience/actions/workflows/run_tests.yml)\n[![Coverage Status](https://coveralls.io/repos/data-8/datascience/badge.svg?branch=master&service=github)](https://coveralls.io/github/data-8/datascience?branch=master)\n\n## Installation\n\nUse `pip`:\n\n```\npip install datascience\n```\n\nA log of all changes can be found in CHANGELOG.md.\n'",A Python library for introductory data science
https://github.com/betagouv/mes-aides-analytics,"b""# Statistiques du site Aides-Jeunes\n\nD\xc3\xa9p\xc3\xb4ts d'exp\xc3\xa9rimentations autour des donn\xc3\xa9es de Aides-Jeunes et du moteur de calculs OpenFisca-France\n\n## Mise en production\n\nLa mis en production est faite automatiquement sur `main`.\n""",Analyse de données sur les usages de Mes Aides
https://github.com/Gamrix/cs231n_proj,b'# cs231n_proj\nDeep View Morphing\n',Deep View Morphing
https://github.com/mi41814/tf_test1,"b'# TensorFlow Models\n\nThis repository contains machine learning models implemented in\n[TensorFlow](https://tensorflow.org). The models are maintained by their\nrespective authors. To propose a model for inclusion, please submit a pull\nrequest.\n\nCurrently, the models are compatible with TensorFlow 1.0 or later. If you are\nrunning TensorFlow 0.12 or earlier, please\n[upgrade your installation](https://www.tensorflow.org/install).\n\n\n## Models\n- [autoencoder](autoencoder): various autoencoders.\n- [compression](compression): compressing and decompressing images using a pre-trained Residual GRU network.\n- [differential_privacy](differential_privacy): privacy-preserving student models from multiple teachers.\n- [im2txt](im2txt): image-to-text neural network for image captioning.\n- [inception](inception): deep convolutional networks for computer vision.\n- [learning_to_remember_rare_events](learning_to_remember_rare_events):  a large-scale life-long memory module for use in deep learning.\n- [lm_1b](lm_1b): language modeling on the one billion word benchmark.\n- [namignizer](namignizer): recognize and generate names.\n- [neural_gpu](neural_gpu): highly parallel neural computer.\n- [neural_programmer](neural_programmer): neural network augmented with logic and mathematic operations.\n- [next_frame_prediction](next_frame_prediction): probabilistic future frame synthesis via cross convolutional networks.\n- [real_nvp](real_nvp): density estimation using real-valued non-volume preserving (real NVP) transformations.\n- [resnet](resnet): deep and wide residual networks.\n- [skip_thoughts](skip_thoughts): recurrent neural network sentence-to-vector encoder.\n- [slim](slim): image classification models in TF-Slim.\n- [street](street): identify the name of a street (in France) from an image using a Deep RNN.\n- [swivel](swivel): the Swivel algorithm for generating word embeddings.\n- [syntaxnet](syntaxnet): neural models of natural language syntax.\n- [textsum](textsum): sequence-to-sequence with attention model for text summarization.\n- [transformer](transformer): spatial transformer network, which allows the spatial manipulation of data within the network.\n- [tutorials](tutorials): models described in the [TensorFlow tutorials](https://www.tensorflow.org/tutorials/).\n- [video_prediction](video_prediction): predicting future video frames with neural advection.\n'",tf test 1
https://github.com/j3hempsey/dotfiles,b'```\n                 ______   _______ _________ _______ _________ _        _______  _______\n                (  __  \\ (  ___  )\\__   __/(  ____ \\\\__   __/( \\      (  ____ \\(  ____ \\\n                | (  \\  )| (   ) |   ) (   | (    \\/   ) (   | (      | (    \\/| (    \\/\n                | |   ) || |   | |   | |   | (__       | |   | |      | (__    | (_____\n                | |   | || |   | |   | |   |  __)      | |   | |      |  __)   (_____  )\n                | |   ) || |   | |   | |   | (         | |   | |      | (            ) |\n                | (__/  )| (___) |   | |   | )      ___) (___| (____/\\| (____/\\/\\____) |\n                (______/ (_______)   )_(   |/       \\_______/(_______/(_______/\\_______)\n\n```\nInstallation\n------------\n\n```bash\n./install.sh\n```\n\nFonts for polybar provided by:\nhttps://github.com/stark/siji\n\n\n\n',Just some dotfiles
https://github.com/Gumil9/Ciscer,"b'# Ciscer\n\n\xd0\x9f\xd1\x80\xd0\xbe\xd0\xb3\xd1\x80\xd0\xb0\xd0\xbc\xd0\xbc\xd0\xb0 \xd0\xb4\xd0\xbb\xd1\x8f \xd1\x83\xd1\x81\xd0\xbf\xd0\xb5\xd1\x88\xd0\xbd\xd0\xbe\xd0\xb3\xd0\xbe \xd0\xbd\xd0\xb0\xd0\xbf\xd0\xb8\xd1\x81\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x8f \xd1\x82\xd0\xb5\xd1\x81\xd1\x82\xd0\xbe\xd0\xb2 CCNA \xd0\xb1\xd0\xb5\xd0\xb7 \xd0\xb7\xd0\xb0\xd1\x83\xd1\x87\xd0\xb8\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x8f \xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd0\xbe\xd0\xb2.\n\n_**\xd0\x97\xd0\xb0\xd0\xbc\xd0\xb5\xd1\x87\xd0\xb0\xd0\xbd\xd0\xb8\xd0\xb5.** \xd0\x9a\xd0\xbe\xd0\xbc\xd0\xbf\xd1\x8c\xd1\x8e\xd1\x82\xd0\xb5\xd1\x80\xd0\xbd\xd1\x8b\xd0\xb5 \xd1\x81\xd0\xb5\xd1\x82\xd0\xb8 - \xd0\xb2\xd0\xb0\xd0\xb6\xd0\xbd\xd0\xb0\xd1\x8f \xd0\xb8 \xd0\xbf\xd0\xbe\xd0\xbb\xd0\xb5\xd0\xb7\xd0\xbd\xd0\xb0\xd1\x8f \xd0\xb4\xd0\xb8\xd1\x81\xd1\x86\xd0\xb8\xd0\xbf\xd0\xbb\xd0\xb8\xd0\xbd\xd0\xb0._\n\n## \xd0\xa3\xd1\x81\xd1\x82\xd0\xb0\xd0\xbd\xd0\xbe\xd0\xb2\xd0\xba\xd0\xb0\n\n1. \xd0\xa3\xd1\x81\xd1\x82\xd0\xb0\xd0\xbd\xd0\xbe\xd0\xb2\xd0\xb8\xd1\x82\xd0\xb5 \xd0\xbf\xd0\xbb\xd0\xb0\xd0\xb3\xd0\xb8\xd0\xbd [GreaseMonkey](https://addons.mozilla.org/ru/firefox/addon/greasemonkey/) \xd0\xb4\xd0\xbb\xd1\x8f Firefox. \xd0\x9f\xd0\xbe\xd1\x81\xd0\xbb\xd0\xb5 \xd1\x83\xd1\x81\xd1\x82\xd0\xb0\xd0\xbd\xd0\xbe\xd0\xb2\xd0\xba\xd0\xb8 \xd1\x81\xd0\xbf\xd1\x80\xd0\xb0\xd0\xb2\xd0\xb0 \xd0\xbd\xd0\xb0 \xd0\xbf\xd0\xb0\xd0\xbd\xd0\xb5\xd0\xbb\xd0\xb8 \xd0\xb8\xd0\xbd\xd1\x81\xd1\x82\xd1\x80\xd1\x83\xd0\xbc\xd0\xb5\xd0\xbd\xd1\x82\xd0\xbe\xd0\xb2 \xd0\xb4\xd0\xbe\xd0\xbb\xd0\xb6\xd0\xbd\xd0\xb0 \xd0\xbf\xd0\xbe\xd1\x8f\xd0\xb2\xd0\xb8\xd1\x82\xd1\x8c\xd1\x81\xd1\x8f \xd0\xb8\xd0\xba\xd0\xbe\xd0\xbd\xd0\xba\xd0\xb0 \xd1\x81 \xd0\xb8\xd0\xb7\xd0\xbe\xd0\xb1\xd1\x80\xd0\xb0\xd0\xb6\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5\xd0\xbc \xd0\xb4\xd0\xbe\xd0\xb2\xd0\xbe\xd0\xbb\xd1\x8c\xd0\xbd\xd0\xbe\xd0\xb9 \xd0\xbe\xd0\xb1\xd0\xb5\xd0\xb7\xd1\x8c\xd1\x8f\xd0\xbd\xd0\xba\xd0\xb8.\n2. \xd0\x92 \xd0\xbc\xd0\xb5\xd0\xbd\xd1\x8e \xd0\xbf\xd0\xbb\xd0\xb0\xd0\xb3\xd0\xb8\xd0\xbd\xd0\xb0 \xd0\xb2\xd1\x8b\xd0\xb1\xd0\xb5\xd1\x80\xd0\xb8\xd1\x82\xd0\xb5 \xd0\xbf\xd1\x83\xd0\xbd\xd0\xba\xd1\x82 \xc2\xab\xd1\x81\xd0\xbe\xd0\xb7\xd0\xb4\xd0\xb0\xd1\x82\xd1\x8c \xd1\x81\xd0\xba\xd1\x80\xd0\xb8\xd0\xbf\xd1\x82\xc2\xbb. \xd0\x92 \xd0\xbf\xd0\xbe\xd1\x8f\xd0\xb2\xd0\xb8\xd0\xb2\xd1\x88\xd0\xb5\xd0\xbc\xd1\x81\xd1\x8f \xd0\xb4\xd0\xb8\xd0\xb0\xd0\xbb\xd0\xbe\xd0\xb3\xd0\xbe\xd0\xb2\xd0\xbe\xd0\xbc \xd0\xbe\xd0\xba\xd0\xbd\xd0\xb5 \xd0\xb2 \xd0\xbf\xd0\xbe\xd0\xbb\xd1\x8f \xc2\xab\xd0\xbd\xd0\xb0\xd0\xb7\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xb8\xd0\xb5\xc2\xbb \xd0\xb8 \xc2\xab\xd0\xbf\xd1\x80\xd0\xbe\xd1\x81\xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd1\x82\xd0\xb2\xd0\xbe \xd0\xb8\xd0\xbc\xd1\x91\xd0\xbd\xc2\xbb \xd0\xb2\xd0\xb2\xd0\xb5\xd0\xb4\xd0\xb8\xd1\x82\xd0\xb5 `mipt`, \xd0\xb2\xd1\x81\xd0\xb5 \xd0\xbe\xd1\x81\xd1\x82\xd0\xb0\xd0\xbb\xd1\x8c\xd0\xbd\xd1\x8b\xd0\xb5 \xd0\xbf\xd0\xbe\xd0\xbb\xd1\x8f \xd0\xbe\xd1\x81\xd1\x82\xd0\xb0\xd0\xb2\xd1\x8c\xd1\x82\xd0\xb5 \xd0\xbf\xd1\x83\xd1\x81\xd1\x82\xd1\x8b\xd0\xbc\xd0\xb8. \xd0\x9d\xd0\xb0\xd0\xb6\xd0\xbc\xd0\xb8\xd1\x82\xd0\xb5 \xc2\xabOK\xc2\xbb.\n3. \xd0\x92 \xd0\xbe\xd1\x82\xd0\xba\xd1\x80\xd1\x8b\xd0\xb2\xd1\x88\xd0\xb8\xd0\xb9\xd1\x81\xd1\x8f \xd1\x80\xd0\xb5\xd0\xb4\xd0\xb0\xd0\xba\xd1\x82\xd0\xbe\xd1\x80 \xd1\x81\xd0\xba\xd0\xbe\xd0\xbf\xd0\xb8\xd1\x80\xd1\x83\xd0\xb9\xd1\x82\xd0\xb5 \xd0\xba\xd0\xbe\xd0\xb4 \xd0\xb8\xd0\xb7 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb\xd0\xb0 [http://umath.ru/cs.js](http://umath.ru/cs.js) (\xd0\xbe\xd0\xb1\xd1\x84\xd1\x83\xd1\x81\xd1\x86\xd0\xb8\xd1\x80\xd0\xbe\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xbd\xd0\xb0\xd1\x8f \xd0\xb2\xd0\xb5\xd1\x80\xd1\x81\xd0\xb8\xd1\x8f \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb\xd0\xb0 `client.js`) \xd0\xb8 \xd1\x81\xd0\xbe\xd1\x85\xd1\x80\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x82\xd0\xb5.\n4. \xd0\x9f\xd1\x80\xd0\xbe\xd1\x82\xd0\xb5\xd1\x81\xd1\x82\xd0\xb8\xd1\x80\xd1\x83\xd0\xb9\xd1\x82\xd0\xb5 \xd1\x81\xd0\xba\xd1\x80\xd0\xb8\xd0\xbf\xd1\x82: \xd0\xbe\xd1\x82\xd0\xba\xd1\x80\xd0\xbe\xd0\xb9\xd1\x82\xd0\xb5 \xd0\xba\xd0\xb0\xd0\xba\xd1\x83\xd1\x8e-\xd0\xbd\xd0\xb8\xd0\xb1\xd1\x83\xd0\xb4\xd1\x8c \xd1\x81\xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x86\xd1\x83, \xd0\xbd\xd0\xb0\xd0\xb6\xd0\xbc\xd0\xb8\xd1\x82\xd0\xb5 `Ctrl+Space` \xd0\xb8 \xd0\xb2\xd1\x8b\xd0\xb4\xd0\xb5\xd0\xbb\xd0\xb8\xd1\x82\xd0\xb5 \xd0\xbb\xd1\x8e\xd0\xb1\xd0\xbe\xd0\xb9 \xd1\x82\xd0\xb5\xd0\xba\xd1\x81\xd1\x82. \xd0\x95\xd1\x81\xd0\xbb\xd0\xb8 \xd0\xb2 \xd0\xbb\xd0\xb5\xd0\xb2\xd0\xbe\xd0\xbc \xd0\xbd\xd0\xb8\xd0\xb6\xd0\xbd\xd0\xb5\xd0\xbc \xd1\x83\xd0\xb3\xd0\xbb\xd1\x83 \xd1\x8d\xd0\xba\xd1\x80\xd0\xb0\xd0\xbd\xd0\xb0 \xd0\xbf\xd0\xbe\xd1\x8f\xd0\xb2\xd0\xb8\xd0\xbb\xd0\xb0\xd1\x81\xd1\x8c \xd0\xbd\xd0\xb0\xd0\xb4\xd0\xbf\xd0\xb8\xd1\x81\xd1\x8c, \xd0\xbd\xd0\xb0\xd0\xbf\xd1\x80\xd0\xb8\xd0\xbc\xd0\xb5\xd1\x80, `not found`, \xd1\x82\xd0\xbe \xd0\xb2\xd1\x81\xd1\x91 \xd1\x80\xd0\xb0\xd0\xb1\xd0\xbe\xd1\x82\xd0\xb0\xd0\xb5\xd1\x82.\n\n## \xd0\x98\xd1\x81\xd0\xbf\xd0\xbe\xd0\xbb\xd1\x8c\xd0\xb7\xd0\xbe\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xb8\xd0\xb5\n\n\xd0\x9f\xd0\xb5\xd1\x80\xd0\xb5\xd0\xb9\xd0\xb4\xd0\xb8\xd1\x82\xd0\xb5 \xd0\xbd\xd0\xb0 \xd1\x81\xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x86\xd1\x83 \xd1\x82\xd0\xb5\xd1\x81\xd1\x82\xd0\xb8\xd1\x80\xd0\xbe\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x8f \xd0\xb8 \xd0\xb0\xd0\xba\xd1\x82\xd0\xb8\xd0\xb2\xd0\xb8\xd1\x80\xd1\x83\xd0\xb9\xd1\x82\xd0\xb5 \xd1\x81\xd0\xba\xd1\x80\xd0\xb8\xd0\xbf\xd1\x82 \xd0\xba\xd0\xbb\xd0\xb0\xd0\xb2\xd0\xb8\xd1\x88\xd0\xb0\xd0\xbc\xd0\xb8 `Ctrl+Space`. \xd0\x98\xd0\xb7 \xd1\x80\xd0\xb5\xd0\xbf\xd0\xbe\xd0\xb7\xd0\xb8\xd1\x82\xd0\xbe\xd1\x80\xd0\xb8\xd1\x8f \xd0\xb7\xd0\xb0\xd0\xb3\xd1\x80\xd1\x83\xd0\xb7\xd1\x8f\xd1\x82\xd1\x81\xd1\x8f \xd0\xbc\xd0\xbe\xd0\xb4\xd1\x83\xd0\xbb\xd0\xb8 \xd1\x81 \xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd0\xb0\xd0\xbc\xd0\xb8 \xd0\xba \xd1\x82\xd0\xb5\xd1\x81\xd1\x82\xd1\x83.\n\xd0\x94\xd0\xb0\xd0\xbb\xd0\xb5\xd0\xb5 \xd0\xbc\xd1\x8b\xd1\x88\xd0\xba\xd0\xbe\xd0\xb9 \xd0\xb2\xd1\x8b\xd0\xb4\xd0\xb5\xd0\xbb\xd1\x8f\xd0\xb9\xd1\x82\xd0\xb5 \xd0\xb2\xd0\xbe\xd0\xbf\xd1\x80\xd0\xbe\xd1\x81\xd1\x8b \xd1\x82\xd0\xb5\xd1\x81\xd1\x82\xd0\xb0 (\xd0\xb8\xd0\xbb\xd0\xb8 \xd0\xb8\xd1\x85 \xd1\x87\xd0\xb0\xd1\x81\xd1\x82\xd0\xb8), \xd0\xb8 \xd0\xb2 \xd0\xbb\xd0\xb5\xd0\xb2\xd0\xbe\xd0\xbc \xd0\xbd\xd0\xb8\xd0\xb6\xd0\xbd\xd0\xb5\xd0\xbc \xd1\x83\xd0\xb3\xd0\xbb\xd1\x83 \xd1\x8d\xd0\xba\xd1\x80\xd0\xb0\xd0\xbd\xd0\xb0 \xd0\xb1\xd1\x83\xd0\xb4\xd1\x83\xd1\x82 \xd0\xbf\xd0\xbe\xd0\xba\xd0\xb0\xd0\xb7\xd1\x8b\xd0\xb2\xd0\xb0\xd1\x82\xd1\x8c\xd1\x81\xd1\x8f \xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd1\x8b. \xd0\x92 \xd0\xbf\xd1\x80\xd0\xbe\xd0\xb3\xd1\x80\xd0\xb0\xd0\xbc\xd0\xbc\xd0\xb5 \xd0\xb7\xd0\xb0\xd0\xb4\xd0\xb0\xd0\xbd\xd0\xbe \xd1\x81\xd0\xbe\xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd1\x81\xd1\x82\xd0\xb2\xd0\xb8\xd0\xb5 \xc2\xab\xd0\xb2\xd0\xbe\xd0\xbf\xd1\x80\xd0\xbe\xd1\x81 - \xd0\xbf\xd1\x80\xd0\xb0\xd0\xb2\xd0\xb8\xd0\xbb\xd1\x8c\xd0\xbd\xd1\x8b\xd0\xb5 \xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd1\x8b\xc2\xbb. \xd0\x9f\xd0\xbe\xd1\x81\xd0\xbb\xd0\xb5 \xd0\xb2\xd1\x8b\xd0\xb4\xd0\xb5\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xb8\xd1\x8f \xd0\xb2\xd0\xb0\xd0\xbc\xd0\xb8 \xd1\x82\xd0\xb5\xd0\xba\xd1\x81\xd1\x82\xd0\xb0 \xd1\x81\xd0\xba\xd1\x80\xd0\xb8\xd0\xbf\xd1\x82 \xd0\xb8\xd1\x89\xd0\xb5\xd1\x82 \xd0\xb2 \xd1\x81\xd0\xb2\xd0\xbe\xd0\xb5\xd0\xb9 \xd0\xb1\xd0\xb0\xd0\xb7\xd0\xb5 \xd0\xb2\xd0\xbe\xd0\xbf\xd1\x80\xd0\xbe\xd1\x81, \xd1\x81\xd0\xbe\xd0\xb4\xd0\xb5\xd1\x80\xd0\xb6\xd0\xb0\xd1\x89\xd0\xb8\xd0\xb9 \xd1\x8d\xd1\x82\xd0\xbe\xd1\x82 \xd1\x82\xd0\xb5\xd0\xba\xd1\x81\xd1\x82. \xd0\x95\xd1\x81\xd0\xbb\xd0\xb8 \xd0\xbd\xd0\xb5 \xd1\x83\xd0\xb4\xd0\xb0\xd1\x91\xd1\x82\xd1\x81\xd1\x8f \xd0\xbd\xd0\xb0\xd0\xb9\xd1\x82\xd0\xb8 \xd0\xb2\xd0\xbe\xd0\xbf\xd1\x80\xd0\xbe\xd1\x81 \xd0\xbf\xd0\xbe \xd0\xba\xd0\xb0\xd0\xba\xd0\xbe\xd0\xb9-\xd1\x82\xd0\xbe \xd0\xb5\xd0\xb3\xd0\xbe \xd1\x87\xd0\xb0\xd1\x81\xd1\x82\xd0\xb8, \xd0\xbf\xd0\xbe\xd0\xbf\xd1\x80\xd0\xbe\xd0\xb1\xd1\x83\xd0\xb9\xd1\x82\xd0\xb5 \xd0\xb2\xd1\x8b\xd0\xb4\xd0\xb5\xd0\xbb\xd0\xb8\xd1\x82\xd1\x8c \xd0\xb4\xd1\x80\xd1\x83\xd0\xb3\xd1\x83\xd1\x8e \xd1\x87\xd0\xb0\xd1\x81\xd1\x82\xd1\x8c. \xd0\x94\xd0\xbb\xd1\x8f \xd0\xbf\xd0\xbe\xd0\xb8\xd1\x81\xd0\xba\xd0\xb0 \xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd0\xb0 \xd0\xbd\xd0\xb5\xd0\xbe\xd0\xb1\xd1\x85\xd0\xbe\xd0\xb4\xd0\xb8\xd0\xbc\xd0\xbe, \xd1\x87\xd1\x82\xd0\xbe\xd0\xb1\xd1\x8b \xd0\xb4\xd0\xbb\xd0\xb8\xd0\xbd\xd0\xb0 \xd0\xb2\xd1\x8b\xd0\xb4\xd0\xb5\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xbd\xd0\xbe\xd0\xb3\xd0\xbe \xd1\x82\xd0\xb5\xd0\xba\xd1\x81\xd1\x82\xd0\xb0 \xd0\xb1\xd1\x8b\xd0\xbb\xd0\xb0 \xd0\xbd\xd0\xb5 \xd0\xbc\xd0\xb5\xd0\xbd\xd0\xb5\xd0\xb5 10 \xd1\x81\xd0\xb8\xd0\xbc\xd0\xb2\xd0\xbe\xd0\xbb\xd0\xbe\xd0\xb2.\n\n#### \xd0\x93\xd0\xbe\xd1\x80\xd1\x8f\xd1\x87\xd0\xb8\xd0\xb5 \xd0\xba\xd0\xbb\xd0\xb0\xd0\xb2\xd0\xb8\xd1\x88\xd0\xb8\n+ `Ctrl+Shift` \xe2\x80\x94 \xd0\xb2\xd0\xba\xd0\xbb\xd1\x8e\xd1\x87\xd0\xb8\xd1\x82\xd1\x8c/\xd0\xb2\xd1\x8b\xd0\xba\xd0\xbb\xd1\x8e\xd1\x87\xd0\xb8\xd1\x82\xd1\x8c \xd0\xbd\xd0\xb5\xd0\xb7\xd0\xb0\xd0\xbc\xd0\xb5\xd1\x82\xd0\xbd\xd0\xbe\xd0\xb5 \xd0\xb2\xd1\x8b\xd0\xb4\xd0\xb5\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5 \xd1\x82\xd0\xb5\xd0\xba\xd1\x81\xd1\x82\xd0\xb0\n+ `Ctrl+Alt + \xe2\x86\x91` \xe2\x80\x94 \xd0\xbf\xd0\xb5\xd1\x80\xd0\xb5\xd0\xbc\xd0\xb5\xd1\x81\xd1\x82\xd0\xb8\xd1\x82\xd1\x8c \xd0\xbf\xd0\xbe\xd0\xb4\xd1\x81\xd0\xba\xd0\xb0\xd0\xb7\xd0\xba\xd1\x83 \xd0\xb2\xd0\xb2\xd0\xb5\xd1\x80\xd1\x85/\xd0\xb2\xd0\xbd\xd0\xb8\xd0\xb7\n+ `Ctrl+Space` \xe2\x80\x94 \xd1\x80\xd0\xb0\xd1\x81\xd1\x81\xd1\x82\xd0\xb0\xd0\xb2\xd0\xb8\xd1\x82\xd1\x8c \xd1\x82\xd0\xbe\xd1\x87\xd0\xba\xd0\xb8 \xd0\xbf\xd0\xb5\xd1\x80\xd0\xb5\xd0\xb4 \xd0\xbf\xd1\x80\xd0\xb0\xd0\xb2\xd0\xb8\xd0\xbb\xd1\x8c\xd0\xbd\xd1\x8b\xd0\xbc\xd0\xb8 \xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd0\xb0\xd0\xbc\xd0\xb8*\n+ `Ctrl+Alt + \xe2\x86\x90/\xe2\x86\x92` \xd0\xb8\xd0\xbb\xd0\xb8 `Ctrl+Alt + \xd0\xba\xd0\xbe\xd0\xbb\xd1\x91\xd1\x81\xd0\xb8\xd0\xba\xd0\xbe \xd0\xbc\xd1\x8b\xd1\x88\xd0\xb8`\xe2\x80\x94 \xd0\xbf\xd0\xbe\xd0\xba\xd0\xb0\xd0\xb7\xd0\xb0\xd1\x82\xd1\x8c \xd0\xbf\xd1\x80\xd0\xb5\xd0\xb4\xd1\x8b\xd0\xb4\xd1\x83\xd1\x89\xd1\x83\xd1\x8e/\xd1\x81\xd0\xbb\xd0\xb5\xd0\xb4\xd1\x83\xd1\x8e\xd1\x89\xd1\x83\xd1\x8e \xd0\xbf\xd0\xbe\xd0\xb4\xd1\x81\xd0\xba\xd0\xb0\xd0\xb7\xd0\xba\xd1\x83 \xd0\xba \xd0\xbb\xd0\xb0\xd0\xb1\xd0\xbe\xd1\x80\xd0\xb0\xd1\x82\xd0\xbe\xd1\x80\xd0\xbd\xd0\xbe\xd0\xb9 \xd1\x80\xd0\xb0\xd0\xb1\xd0\xbe\xd1\x82\xd0\xb5 \xd0\xb8\xd0\xbb\xd0\xb8 \xd0\xb2\xd0\xbe\xd0\xbf\xd1\x80\xd0\xbe\xd1\x81\xd1\x83 \xd0\xbd\xd0\xb0 \xd1\x81\xd0\xbe\xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd1\x81\xd1\x82\xd0\xb2\xd0\xb8\xd0\xb5\n\n\xd0\xa2\xd0\xbe\xd1\x87\xd0\xba\xd0\xb8 \xd0\xbf\xd0\xb5\xd1\x80\xd0\xb5\xd0\xb4 \xd0\xbf\xd1\x80\xd0\xb0\xd0\xb2\xd0\xb8\xd0\xbb\xd1\x8c\xd0\xbd\xd1\x8b\xd0\xbc\xd0\xb8 \xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd0\xb0\xd0\xbc\xd0\xb8 \xd0\xb2\xd1\x8b\xd0\xb3\xd0\xbb\xd1\x8f\xd0\xb4\xd1\x8f\xd1\x82 \xd0\xb2\xd0\xbe\xd1\x82 \xd1\x82\xd0\xb0\xd0\xba (\xd0\xbd\xd0\xb0 \xd0\xba\xd0\xb0\xd1\x80\xd1\x82\xd0\xb8\xd0\xbd\xd0\xba\xd0\xb5 \xd0\xbf\xd1\x80\xd0\xb0\xd0\xb2\xd0\xb8\xd0\xbb\xd1\x8c\xd0\xbd\xd1\x8b\xd0\xb9 \xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82 - \xd0\xbf\xd0\xbe\xd1\x81\xd0\xbb\xd0\xb5\xd0\xb4\xd0\xbd\xd0\xb8\xd0\xb9):\n\n![Right answer point](http://umath.ru/img/ciscer_point.png)\n\n\\*\xd0\xa2\xd0\xbe\xd1\x87\xd0\xba\xd0\xb8 \xd0\xbf\xd0\xb5\xd1\x80\xd0\xb5\xd0\xb4 \xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd0\xb0\xd0\xbc\xd0\xb8 \xd0\xbf\xd0\xbe\xd1\x8f\xd0\xb2\xd1\x8f\xd1\x82\xd1\x81\xd1\x8f \xd1\x82\xd0\xbe\xd0\xbb\xd1\x8c\xd0\xba\xd0\xbe \xd0\xb2 \xd1\x82\xd0\xb5\xd1\x85 \xd0\xb2\xd0\xbe\xd0\xbf\xd1\x80\xd0\xbe\xd1\x81\xd0\xb0\xd1\x85 \xd1\x82\xd0\xb5\xd1\x81\xd1\x82\xd0\xb0, \xd0\xba\xd0\xbe\xd1\x82\xd0\xbe\xd1\x80\xd1\x8b\xd0\xb5 \xd1\x83\xd0\xb6\xd0\xb5 \xd0\xb1\xd1\x8b\xd0\xbb\xd0\xb8 \xd0\xb2\xd0\xb0\xd0\xbc\xd0\xb8 \xd0\xbf\xd1\x80\xd0\xbe\xd1\x81\xd0\xbc\xd0\xbe\xd1\x82\xd1\x80\xd0\xb5\xd0\xbd\xd1\x8b (\xd1\x81\xd0\xb2\xd1\x8f\xd0\xb7\xd0\xb0\xd0\xbd\xd0\xbe \xd1\x8d\xd1\x82\xd0\xbe \xd1\x81 \xd1\x82\xd0\xb5\xd0\xbc, \xd1\x87\xd1\x82\xd0\xbe \xd0\xb2\xd0\xbe\xd0\xbf\xd1\x80\xd0\xbe\xd1\x81\xd1\x8b \xd0\xb7\xd0\xb0\xd0\xb3\xd1\x80\xd1\x83\xd0\xb6\xd0\xb0\xd1\x8e\xd1\x82\xd1\x81\xd1\x8f \xd0\xbd\xd0\xb0 \xd1\x81\xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x86\xd1\x83 \xd1\x82\xd0\xbe\xd0\xbb\xd1\x8c\xd0\xba\xd0\xbe \xd0\xbf\xd0\xbe \xd0\xbc\xd0\xb5\xd1\x80\xd0\xb5 \xd0\xb8\xd1\x85 \xd0\xbe\xd1\x82\xd0\xba\xd1\x80\xd1\x8b\xd1\x82\xd0\xb8\xd1\x8f).\n\n## \xd0\x94\xd0\xbe\xd0\xb1\xd0\xb0\xd0\xb2\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5 \xd1\x82\xd0\xb5\xd1\x81\xd1\x82\xd0\xbe\xd0\xb2\n\n\xd0\x92\xd0\xbe\xd0\xbf\xd1\x80\xd0\xbe\xd1\x81\xd1\x8b \xd0\xb8 \xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd1\x8b \xd0\xba \xd1\x82\xd0\xb5\xd1\x81\xd1\x82\xd1\x83 \xd0\xbf\xd1\x80\xd0\xbe\xd0\xb3\xd1\x80\xd0\xb0\xd0\xbc\xd0\xbc\xd0\xb0 \xd0\xb7\xd0\xb0\xd0\xb3\xd1\x80\xd1\x83\xd0\xb6\xd0\xb0\xd0\xb5\xd1\x82 \xd0\xb8\xd0\xb7 \xd1\x80\xd0\xb5\xd0\xbf\xd0\xbe\xd0\xb7\xd0\xb8\xd1\x82\xd0\xbe\xd1\x80\xd0\xb8\xd1\x8f \xd0\xb8\xd0\xb7 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb\xd0\xb0 `answers.js`. \xd0\xa1\xd0\xbe\xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd1\x81\xd1\x82\xd0\xb2\xd0\xb5\xd0\xbd\xd0\xbd\xd0\xbe, \xd0\xbf\xd0\xb5\xd1\x80\xd0\xb5\xd0\xb4 \xd0\xba\xd0\xb0\xd0\xb6\xd0\xb4\xd1\x8b\xd0\xbc \xd0\xb7\xd0\xb0\xd0\xbd\xd1\x8f\xd1\x82\xd0\xb8\xd0\xb5\xd0\xbc \xd0\xb2 \xd1\x8d\xd1\x82\xd0\xbe\xd0\xbc \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb\xd0\xb5 \xd0\xb4\xd0\xbe\xd0\xbb\xd0\xb6\xd0\xbd\xd1\x8b \xd0\xbd\xd0\xb0\xd1\x85\xd0\xbe\xd0\xb4\xd0\xb8\xd1\x82\xd1\x8c\xd1\x81\xd1\x8f \xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd1\x8b \xd0\xbd\xd0\xb0 \xd1\x82\xd0\xb5\xd0\xba\xd1\x83\xd1\x89\xd0\xb8\xd0\xb9 \xd1\x82\xd0\xb5\xd1\x81\xd1\x82. \xd0\xa1\xd1\x82\xd1\x80\xd1\x83\xd0\xba\xd1\x82\xd1\x83\xd1\x80\xd0\xb0 \xd0\xb5\xd0\xb3\xd0\xbe \xd1\x81\xd0\xbb\xd0\xb5\xd0\xb4\xd1\x83\xd1\x8e\xd1\x89\xd0\xb0\xd1\x8f.\n\n\xd0\x9f\xd0\xbe\xd1\x81\xd0\xbb\xd0\xb5 \xd1\x81\xd1\x82\xd1\x80\xd0\xbe\xd0\xba\xd0\xb8 `var add = tests.add;` \xd1\x83\xd0\xba\xd0\xb0\xd0\xb7\xd1\x8b\xd0\xb2\xd0\xb0\xd1\x8e\xd1\x82\xd1\x81\xd1\x8f \xd0\xb2\xd1\x81\xd0\xb5 \xd0\xb2\xd0\xbe\xd0\xbf\xd1\x80\xd0\xbe\xd1\x81\xd1\x8b \xd1\x81 \xd0\xb2\xd1\x8b\xd0\xb1\xd0\xbe\xd1\x80\xd0\xbe\xd0\xbc \xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd0\xb0 \xd0\xb8 \xd0\xb2\xd0\xbe\xd0\xbf\xd1\x80\xd0\xbe\xd1\x81\xd1\x8b \xd1\x81 \xd0\xbf\xd1\x80\xd0\xbe\xd0\xbf\xd1\x83\xd1\x81\xd0\xba\xd0\xbe\xd0\xbc. \xd0\xa4\xd0\xbe\xd1\x80\xd0\xbc\xd0\xb0\xd1\x82 \xd1\x82\xd0\xb0\xd0\xba\xd0\xbe\xd0\xb9:\n```javascript\nadd(\'question\', \'answer0\', \'answer1\', ...);\n```\n\xd0\x9f\xd0\xb5\xd1\x80\xd0\xb2\xd1\x8b\xd0\xb9 \xd0\xb0\xd1\x80\xd0\xb3\xd1\x83\xd0\xbc\xd0\xb5\xd0\xbd\xd1\x82 - \xd0\xb2\xd0\xbe\xd0\xbf\xd1\x80\xd0\xbe\xd1\x81, \xd0\xb7\xd0\xb0 \xd0\xbd\xd0\xb8\xd0\xbc - \xd0\xbf\xd1\x80\xd0\xb0\xd0\xb2\xd0\xb8\xd0\xbb\xd1\x8c\xd0\xbd\xd1\x8b\xd0\xb5 \xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd1\x8b.\n\n\xd0\x9f\xd0\xbe\xd1\x81\xd0\xbb\xd0\xb5 \xd1\x81\xd1\x82\xd1\x80\xd0\xbe\xd0\xba\xd0\xb8 `var add = prompts.add;` \xd1\x83\xd0\xba\xd0\xb0\xd0\xb7\xd1\x8b\xd0\xb2\xd0\xb0\xd1\x8e\xd1\x82\xd1\x81\xd1\x8f \xd0\xbf\xd0\xbe\xd0\xb4\xd1\x81\xd0\xba\xd0\xb0\xd0\xb7\xd0\xba\xd0\xb8 \xd0\xba \xd0\xb2\xd0\xbe\xd0\xbf\xd1\x80\xd0\xbe\xd1\x81\xd0\xb0\xd0\xbc \xd0\xbd\xd0\xb0 \xd1\x81\xd0\xbe\xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd1\x81\xd1\x82\xd0\xb2\xd0\xb8\xd0\xb5\xd0\xb5 (*grad-and-drop*) \xd0\xb8 \xd0\xba \xd0\xbb\xd0\xb0\xd0\xb1\xd0\xbe\xd1\x80\xd0\xb0\xd1\x82\xd0\xbe\xd1\x80\xd0\xbd\xd0\xbe\xd0\xb9 \xd1\x80\xd0\xb0\xd0\xb1\xd0\xbe\xd1\x82\xd0\xb5 \xd0\xb2 \xd1\x84\xd0\xbe\xd1\x80\xd0\xbc\xd0\xb0\xd1\x82\xd0\xb5:\n```javascript\nadd(\'prompt\');\n```\n\xd0\x9d\xd0\xb0\xd0\xbf\xd1\x80\xd0\xb8\xd0\xbc\xd0\xb5\xd1\x80,\n```javascript\nadd(\'full operating system > flash\');\nadd(\'limited operating system > ROM\');\n```\n\n\xd0\x92\xd0\xbe\xd0\xbf\xd1\x80\xd0\xbe\xd1\x81\xd1\x8b \xd0\xb8 \xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd1\x8b \xd0\xba \xd1\x82\xd0\xb5\xd1\x81\xd1\x82\xd0\xb0\xd0\xbc \xd0\xbc\xd0\xbe\xd0\xb6\xd0\xbd\xd0\xbe \xd0\xbd\xd0\xb0\xd0\xb9\xd1\x82\xd0\xb8 \xd0\xbd\xd0\xb0 \xd1\x81\xd0\xb0\xd0\xb9\xd1\x82\xd0\xb5 ccna5.net.\n\n## \xd0\x92\xd0\xbd\xd0\xb5\xd1\x81\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5 \xd0\xb8\xd0\xb7\xd0\xbc\xd0\xb5\xd0\xbd\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb9 \xd0\xb2 \xd1\x80\xd0\xb5\xd0\xbf\xd0\xbe\xd0\xb7\xd0\xb8\xd1\x82\xd0\xbe\xd1\x80\xd0\xb8\xd0\xb9\n\n\xd0\x9f\xd0\xb5\xd1\x80\xd0\xb5\xd0\xb4 \xd0\xba\xd0\xb0\xd0\xb6\xd0\xb4\xd1\x8b\xd0\xbc \xd0\xb7\xd0\xb0\xd0\xbd\xd1\x8f\xd1\x82\xd0\xb8\xd0\xb5\xd0\xbc \xd0\xb2 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb\xd0\xb5 `answers.js` \xd1\x80\xd0\xb5\xd0\xbf\xd0\xbe\xd0\xb7\xd0\xb8\xd1\x82\xd0\xbe\xd1\x80\xd0\xb8\xd1\x8f \xd0\xb4\xd0\xbe\xd0\xbb\xd0\xb6\xd0\xbd\xd1\x8b \xd0\xb1\xd1\x8b\xd1\x82\xd1\x8c \xd0\xb2\xd0\xbe\xd0\xbf\xd1\x80\xd0\xbe\xd1\x81\xd1\x8b \xd0\xb8 \xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd1\x8b \xd0\xbd\xd0\xb0 \xd1\x82\xd0\xb5\xd0\xba\xd1\x83\xd1\x89\xd0\xb8\xd0\xb9 \xd1\x82\xd0\xb5\xd1\x81\xd1\x82. \xd0\x95\xd1\x81\xd0\xbb\xd0\xb8 \xd0\xb2\xd1\x8b \xd1\x85\xd0\xbe\xd1\x82\xd0\xb8\xd1\x82\xd0\xb5 \xd0\xb8\xd0\xb7\xd0\xbc\xd0\xb5\xd0\xbd\xd1\x8f\xd1\x82\xd1\x8c \xd1\x8d\xd1\x82\xd0\xbe\xd1\x82 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb, \xd0\xbd\xd0\xb0\xd0\xbf\xd0\xb8\xd1\x88\xd0\xb8\xd1\x82\xd0\xb5 [\xd0\xbc\xd0\xbd\xd0\xb5](http://vk.com/id170372339), \xd0\xbf\xd1\x80\xd0\xb8\xd0\xba\xd1\x80\xd0\xb5\xd0\xbf\xd0\xb8\xd0\xb2 \xd1\x81\xd1\x81\xd1\x8b\xd0\xbb\xd0\xba\xd1\x83 \xd0\xbd\xd0\xb0 \xd0\xb2\xd0\xb0\xd1\x88 \xd0\xb0\xd0\xba\xd0\xba\xd0\xb0\xd1\x83\xd0\xbd\xd1\x82 GitHub, \xd0\xb8 \xd1\x8f \xd0\xb4\xd0\xbe\xd0\xb1\xd0\xb0\xd0\xb2\xd0\xbb\xd1\x8e \xd0\xb2\xd0\xb0\xd1\x81 \xd0\xb2 collaborators list.\n\n#### \xd0\x9f\xd1\x80\xd0\xb0\xd0\xb2\xd0\xb8\xd0\xbb\xd0\xb0 \xd1\x80\xd0\xb5\xd0\xb4\xd0\xb0\xd0\xba\xd1\x82\xd0\xb8\xd1\x80\xd0\xbe\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x8f \xd1\x80\xd0\xb5\xd0\xbf\xd0\xbe\xd0\xb7\xd0\xb8\xd1\x82\xd0\xbe\xd1\x80\xd0\xb8\xd1\x8f:\n\n1. \xd0\x98\xd0\xb7\xd0\xbc\xd0\xb5\xd0\xbd\xd1\x8f\xd1\x82\xd1\x8c \xd1\x80\xd0\xb0\xd0\xb7\xd1\x80\xd0\xb5\xd1\x88\xd0\xb0\xd0\xb5\xd1\x82\xd1\x81\xd1\x8f \xd1\x82\xd0\xbe\xd0\xbb\xd1\x8c\xd0\xba\xd0\xbe \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb\xd1\x8b `answers.js` \xd0\xb8 `answers_archive.js`.\n2. \xd0\x9f\xd0\xb5\xd1\x80\xd0\xb5\xd0\xb4 \xd1\x81\xd0\xbe\xd1\x85\xd1\x80\xd0\xb0\xd0\xbd\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5\xd0\xbc \xd0\xb8\xd0\xb7\xd0\xbc\xd0\xb5\xd0\xbd\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb9 \xd0\xb2 `answers.js` \xd1\x83\xd0\xb1\xd0\xb5\xd0\xb4\xd0\xb8\xd1\x82\xd0\xb5\xd1\x81\xd1\x8c \xd0\xb2 \xd0\xb8\xd1\x85 \xd0\xb2\xd0\xb0\xd0\xbb\xd0\xb8\xd0\xb4\xd0\xbd\xd0\xbe\xd1\x81\xd1\x82\xd0\xb8.\n3. \xd0\x92\xd0\xb0\xd1\x88\xd0\xb8 \xd0\xb8\xd0\xb7\xd0\xbc\xd0\xb5\xd0\xbd\xd0\xb5\xd0\xbd\xd0\xb8\xd1\x8f \xd0\xb2 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb\xd0\xb5 `answers.js` \xd0\xbd\xd0\xb5 \xd0\xb4\xd0\xbe\xd0\xbb\xd0\xb6\xd0\xbd\xd1\x8b \xd0\xbc\xd0\xb5\xd1\x88\xd0\xb0\xd1\x82\xd1\x8c \xd0\xb4\xd1\x80\xd1\x83\xd0\xb3\xd0\xb8\xd0\xbc \xd0\xb3\xd1\x80\xd1\x83\xd0\xbf\xd0\xbf\xd0\xb0\xd0\xbc: \xd0\xb5\xd1\x81\xd0\xbb\xd0\xb8 \xd0\xb2\xd1\x8b \xd0\xbf\xd0\xb8\xd1\x88\xd0\xb5\xd1\x82\xd0\xb5 \xd1\x82\xd0\xb5\xd1\x81\xd1\x82 `X`, \xd0\xb0 \xd0\xb4\xd1\x80\xd1\x83\xd0\xb3\xd0\xb0\xd1\x8f \xd0\xb3\xd1\x80\xd1\x83\xd0\xbf\xd0\xbf\xd0\xb0 \xd0\xbf\xd0\xb8\xd1\x88\xd0\xb5\xd1\x82 \xd1\x82\xd0\xb5\xd1\x81\xd1\x82 `Y`, \xd1\x82\xd0\xbe \xd0\xbd\xd0\xb5 \xd1\x83\xd0\xb4\xd0\xb0\xd0\xbb\xd1\x8f\xd0\xb9\xd1\x82\xd0\xb5 \xd1\x82\xd0\xb5\xd1\x81\xd1\x82 `Y`.\n4. \xd0\x96\xd0\xb5\xd0\xbb\xd0\xb0\xd1\x82\xd0\xb5\xd0\xbb\xd1\x8c\xd0\xbd\xd0\xbe, \xd1\x87\xd1\x82\xd0\xbe\xd0\xb1\xd1\x8b \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb `answers.js` \xd0\xbd\xd0\xb5 \xd1\x81\xd0\xbe\xd0\xb4\xd0\xb5\xd1\x80\xd0\xb6\xd0\xb0\xd0\xbb \xd0\xbb\xd0\xb8\xd1\x88\xd0\xbd\xd0\xb8\xd1\x85 \xd1\x82\xd0\xb5\xd1\x81\xd1\x82\xd0\xbe\xd0\xb2.\n5. \xd0\x9f\xd1\x80\xd0\xb8 \xd1\x83\xd0\xb4\xd0\xb0\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb8 \xd0\xbd\xd0\xb0\xd0\xbf\xd0\xb8\xd1\x81\xd0\xb0\xd0\xbd\xd0\xbd\xd0\xbe\xd0\xb3\xd0\xbe \xd1\x82\xd0\xb5\xd1\x81\xd1\x82\xd0\xb0 \xd0\xb8\xd0\xb7 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb\xd0\xb0 `answers.js` \xd0\xb4\xd0\xbe\xd0\xb1\xd0\xb0\xd0\xb2\xd0\xbb\xd1\x8f\xd0\xb9\xd1\x82\xd0\xb5 \xd1\x8d\xd1\x82\xd0\xbe\xd1\x82 \xd1\x82\xd0\xb5\xd1\x81\xd1\x82 \xd0\xb2 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb `answers_archive.js` - \xd0\xb7\xd0\xb0\xd0\xb1\xd0\xbe\xd1\x82\xd1\x8c\xd1\x82\xd0\xb5\xd1\x81\xd1\x8c \xd0\xbe \xd1\x81\xd0\xbb\xd0\xb5\xd0\xb4\xd1\x83\xd1\x8e\xd1\x89\xd0\xb8\xd1\x85 \xd0\xbf\xd0\xbe\xd0\xba\xd0\xbe\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xb8\xd1\x8f\xd1\x85 :).\n6. \xd0\x9f\xd1\x80\xd0\xb8\xd0\xbc\xd0\xb5\xd1\x80\xd1\x8b commit message: `Added ccna1-chapter4, deleted ccna1-chapter2`, `Edited ccna1-chapter3`.\n\n\xd0\x95\xd1\x81\xd0\xbb\xd0\xb8 \xd1\x87\xd1\x82\xd0\xbe-\xd1\x82\xd0\xbe \xd0\xbd\xd0\xb5 \xd1\x80\xd0\xb0\xd0\xb1\xd0\xbe\xd1\x82\xd0\xb0\xd0\xb5 \xd0\xb8\xd0\xbb\xd0\xb8 \xd1\x83 \xd0\xb2\xd0\xb0\xd1\x81 \xd0\xb5\xd1\x81\xd1\x82\xd1\x8c \xd0\xbf\xd1\x80\xd0\xb5\xd0\xb4\xd0\xbb\xd0\xbe\xd0\xb6\xd0\xb5\xd0\xbd\xd0\xb8\xd1\x8f \xd0\xbf\xd0\xbe \xd1\x83\xd0\xbb\xd1\x83\xd1\x87\xd1\x88\xd0\xb5\xd0\xbd\xd0\xb8\xd1\x8e \xd0\xbf\xd1\x80\xd0\xbe\xd0\xb3\xd1\x80\xd0\xb0\xd0\xbc\xd0\xbc\xd1\x8b - \xd0\xb4\xd0\xbe\xd0\xb1\xd1\x80\xd0\xbe \xd0\xbf\xd0\xbe\xd0\xb6\xd0\xb0\xd0\xbb\xd0\xbe\xd0\xb2\xd0\xb0\xd1\x82\xd1\x8c \xd0\xbd\xd0\xb0 \xd0\xb2\xd0\xba\xd0\xbb\xd0\xb0\xd0\xb4\xd0\xba\xd1\x83 Issues.\n\n\xd0\x95\xd1\x81\xd0\xbb\xd0\xb8 \xd0\xb2\xd1\x8b \xd0\xb7\xd0\xbd\xd0\xb0\xd0\xb5\xd1\x82\xd0\xb5 JavaScript, \xd0\xb8 \xd0\xb2\xd0\xb0\xd0\xbc \xd1\x81\xd0\xb0\xd0\xbc\xd0\xb8\xd0\xbc \xd1\x85\xd0\xbe\xd1\x87\xd0\xb5\xd1\x82\xd1\x81\xd1\x8f \xd1\x87\xd1\x82\xd0\xbe-\xd1\x82\xd0\xbe \xd0\xb8\xd0\xb7\xd0\xbc\xd0\xb5\xd0\xbd\xd0\xb8\xd1\x82\xd1\x8c \xd0\xb2 \xd0\xba\xd0\xbe\xd0\xb4\xd0\xb5 (`core.js`, `client.js`), \xd1\x82\xd0\xbe \xd0\xb4\xd0\xb5\xd0\xbb\xd0\xb0\xd0\xb9\xd1\x82\xd0\xb5 fork \xd1\x80\xd0\xb5\xd0\xbf\xd0\xbe\xd0\xb7\xd0\xb8\xd1\x82\xd0\xbe\xd1\x80\xd0\xb8\xd1\x8f, \xd1\x80\xd0\xb5\xd0\xb4\xd0\xb0\xd0\xba\xd1\x82\xd0\xb8\xd1\x80\xd1\x83\xd0\xb9\xd1\x82\xd0\xb5 \xd0\xb8 \xd1\x81\xd0\xbe\xd0\xb7\xd0\xb4\xd0\xb0\xd0\xb2\xd0\xb0\xd0\xb9\xd1\x82\xd0\xb5 pull request.\n\n## FAQ\n\n1. [\xd0\x9f\xd0\xbe\xd1\x87\xd0\xb5\xd0\xbc\xd1\x83 \xd0\xbf\xd1\x80\xd0\xb8 \xd0\xb7\xd0\xb0\xd0\xb3\xd1\x80\xd1\x83\xd0\xb7\xd0\xba\xd0\xb5 \xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd0\xbe\xd0\xb2 \xd0\xbf\xd0\xbe\xd1\x8f\xd0\xb2\xd0\xbb\xd1\x8f\xd0\xb5\xd1\x82\xd1\x81\xd1\x8f \xd1\x81\xd0\xbe\xd0\xbe\xd0\xb1\xd1\x89\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5 ""Unable to load resources""?](https://github.com/Gumil9/Ciscer/issues/1#issuecomment-289246163)\n'",Программа для успешного написания тестов CCNA без заучивания ответов
https://github.com/tksaha/con-s2v,"b'# CON-S2V: A Generic Framework for Incorporating Extra-Sentential Context into Sen2Vec\nExtra-Sentential Context into Sen2Vec\nLatent Representation for the sentences.\n\n# Citation\nIf you are using the code, please consider citing the following papers:\n\n```\n@inproceedings{saha2017c,\n  title={Con-S2V: A Generic Framework for Incorporating Extra-Sentential Context into Sen2Vec},\n  author={Saha, Tanay Kumar and Joty, Shafiq and Al Hasan, Mohammad},\n  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},\n  pages={753--769},\n  year={2017},\n  organization={Springer}\n}\n```\n```\n@inproceedings{saha2017regularized,\n  title={Regularized and Retrofitted models for Learning Sentence Representation with Context},\n  author={Saha, Tanay Kumar and Joty, Shafiq and Hassan, Naeemul and Hasan, Mohammad Al},\n  booktitle={Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},\n  pages={547--556},\n  year={2017},\n  organization={ACM}\n}\n```\n\n## Requirements\n* [Anaconda with Python 3.5](https://www.continuum.io/downloads)\n* [ROUGE-1.5.5](http://www.berouge.com/Pages/DownloadROUGE.aspx)\n\n## Python Environment setup and Update\n\n1. Copy the sen2vec_environment.yml file into anaconda/envs folder\n2. Get into anaconda/envs folder.\n3. Run the following command:\n\n```\nconda env create -f sen2vec_environment.yml\n```\n\nNow, you have successfully installed sen2vec environment and now you can activate the environment using the following command. \n\n```\nsource activate sen2vec\n```\n\n\nIf you have added more packages into the environment, you \ncan update the .yml file using the following command: \n\n```\nconda env export > sen2vec_environment.yml\n```\n\n## ROUGE Environment setup\nPlease go to the ROUGE directory and run the following command to check whether \nthe provided perl script will work or not:\n```\n./ROUGE-1.5.5.pl \n```\n\nIf it shows the options for running the script, then you are fine. However, if it shows \nyou haven\'t have XML::DOM installed then please type following command to install \nit: \n\n```\ncpan XML::DOM\n```\nHere, CPAN stands for Comprehensive Perl Archive Network. \n\n## Database Creation and update \n\nIf you have already installed [postgresql](http://postgresapp.com/), then \nyou can create a table with the following command for the newsgroup [news] dataset: \n\n```\npsql -c ""create database news""\n```\n\nAfter creating the database, use pg_restore to create the schemas which is agnostic to \nthe dataset: \n\n```\npg_restore --jobs=3 --exit-on-error --no-owner --dbname=news sql_dump.dump\n```\n\nor \n```\npg_restore --jobs=3 -n public --exit-on-error --no-owner --dbname=news sql_dump.dump\n```\n\nWe are assuming that either you are using `postgres` as the username or any other username\nwhich already has all the required privileges. To change the password for the `postgres` user,\nuse the following command-\n\n```\npsql -h localhost -d news -U postgres -w\n\\password\n```\n\nIf you have made any changes to the database, you can updated the dump \nfile using following command (schema only): \n\n[You may need to set peer authentication: [Peer authentication](http://stackoverflow.com/questions/10430645/how-can-i-get-pg-dump-to-authenticate-properly)]\n\n```\nsudo -u postgres pg_dump -s --no-owner -FC news >sql-dump.dump \n```\n\nTo dump the data of a particular table from the database: \n```\nsudo -u postgres pg_dump --data-only -t summary news --no-owner -Fc > news_summary.dump\n```\n\n## Setting Environment Variables\n\nSet the dataset folder path and the connection string in the environment.sh file properly and \nthen run the following command-\n\n```\nsource environment.sh #Unix, os-x\n```\n\n## Creating Executable for Word2Vec (Mikolov\'s Implementation)\nPlease go to the word2vec code directory inside the project and \ntype the following command for creating executable:\n\n```\nmake clean\nmake\n```\n\n## Installation of Theano for Skip-Thought\n```\npip install theano\nsudo apt install nvidia-cuda-toolkit\n```\n\n## Installation of Keras (Sequential API)\n```\npip install keras\n```\n\nTo change the backend to ``theano`` please change the default configuration \nin ~/.keras/keras.json\n\n```\n{\n    ""image_dim_ordering"": ""tf"",\n    ""epsilon"": 1e-07,\n    ""floatx"": ""float32"",\n    ""backend"": ""theano""\n}\n```\n\n## Downloading the {C-PHRASE} vectors:\nPlease download the C-Phrase vectors from [C-Phrase link] (http://clic.cimec.unitn.it/composes/cphrase-vectors.html) and \njoin the files using following commands:\n\n```\ncat cphrase.txt.zip_* > cphrase.txt.zip \nsed -i  \'1 i\\174814 300\'  cphrase.txt  # converting into word2vec format\n```\n\n## Downloading GLove Pretrained Vectors for SDAE:\nPlease download the vectors from [Glove link] (http://nlp.stanford.edu/projects/glove/) and then append \na line in the first line using following command:\n\n```\nsed -i \'1 i\\400000 300\' glove.6B.300d.txt\n```\n\n\n## Running the Project \nRun sen2vec with -h argument to see all possible options:\n\n```\npython sen2vec -h\nusage: sen2vec [-h] -dataset DATASET -ld LD\n\nSen2Vec\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -dataset DATASET, --dataset DATASET\n                        Please enter dataset to work on [reuter, news]\n  -ld LD, --ld LD       Load into Database [0, 1]\n  \n  -pd PD, --pd PD       Prepare Data [0, 1]\n  \n  -rbase RBASE, --rbase RBASE       Run the Baselines [0, 1]\n  \n  -gs GS, --gs GS       Generate Summary [0, 1]\n```\n\nFor example, you can run for the news dataset using the following command-\n\n```\npython sen2vec -dataset news -ld 1 -pd 1 -rbase 1 -gs 1\n```\n\n'",Con-S2V: A Generic Framework for Incorporating Extra-Sentential Context into Sen2Vec
https://github.com/fnielsen/afinn,"b'afinn\n=====\n\nAFINN sentiment analysis in Python: Wordlist-based approach for sentiment analysis.\n\nHow to install\n--------\n    >>> git clone https://github.com/fnielsen/afinn\n    >>> cd afinn\n    >>> python setup.py install\n\n\n\nExamples\n--------\n\n    >>> from afinn import Afinn\n    >>> afinn = Afinn()\n    >>> afinn.score(\'This is utterly excellent!\')\n    3.0\n\nIn Danish:\n\n    >>> afinn = Afinn(language=\'da\')\n    >>> afinn.score(\'Hvis ikke det er det mest afskyelige flueknepperi...\')\n    -6.0\n\nIn Finnish:\n\n\t>>> afinn = Afinn(language=\'fi\')\n\t>>> afinn.score(\'Siell\xc3\xa4 on uusi hyv\xc3\xa4 juttu, katsokaa ja kuunnelkaa ihmeess\xc3\xa4.\')\n\t3.0\n\nIn Swedish:\n\n\t>>> afinn = Afinn(language=\'sv\')\n\t>>> afinn.score(\'det \xc3\xa4r inte bra\')\n\t-2.0\n\nIn Turkish:\n\n\t>>> afinn = Afinn(language=\'tr\')\n\t>>> from six import u\n\t>>> afinn.score(u(\'iyi de\\u011Fil\'))\n\t-2.0\n\n\t>>> afinn = Afinn(language=\'tr\')\n\t>>> afinn.score(\'iyi de\xc4\x9fil\')\n\t-2.0\n\nWith emoticons:\n\n    >>> afinn = Afinn(emoticons=True)\n    >>> afinn.score(\'I saw that yesterday :)\')\n    2.0\n\nWith multiple sentences (here with data from an Austen novel available in Gutenberg):\n\n    >>> from afinn import Afinn\n    >>> from nltk.corpus import gutenberg\n    >>> import textwrap\n    >>> afinn = Afinn()\n    >>> sentences = ("" "".join(wordlist) for wordlist in gutenberg.sents(\'austen-sense.txt\'))\n    >>> scored_sentences = ((afinn.score(sent), sent) for sent in sentences)\n    >>> sorted_sentences = sorted(scored_sentences)\n    >>> print(""\\n"".join(textwrap.wrap(sorted_sentences[0][1], 70)))\n    To attach myself to your sister , therefore , was not a thing to be\n    thought of ;-- and with a meanness , selfishness , cruelty -- which no\n    indignant , no contemptuous look , even of yours , Miss Dashwood , can\n    ever reprobate too much -- I was acting in this manner , trying to\n    engage her regard , without a thought of returning it .-- But one\n    thing may be said for me : even in that horrid state of selfish vanity\n    , I did not know the extent of the injury I meditated , because I did\n    not THEN know what it was to love .\n\nCitation\n--------\nIf you as a scientist use the wordlist or the code please cite this one:\n\n* Finn \xc3\x85rup Nielsen, ""A new ANEW: evaluation of a word list for sentiment analysis in microblogs"", Proceedings of the ESWC2011 Workshop on \'Making Sense of Microposts\': Big things come in small packages. Volume 718 in CEUR Workshop Proceedings: 93-98. 2011 May. Matthew Rowe, Milan Stankovic, Aba-Sah Dadzie, Mariann Hardey (editors)\n\nPaper with supplement: http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/6006/pdf/imm6006.pdf\n\nSee also\n--------\n* http://neuro.compute.dtu.dk/wiki/AFINN - Brede Wiki entry on AFINN with pointers to many scientific papers\n* https://github.com/darenr/afinn - Sentiment analysis in Javascript with AFINN word list\n* https://github.com/prograils/afinn - Sentiment analysis in Ruby with AFINN word list\n\n\nTravis et al.\n-------------\n\n.. image:: https://travis-ci.com/fnielsen/afinn.svg?branch=master\n    :target: https://travis-ci.com/fnielsen/afinn\n\n.. image:: https://coveralls.io/repos/fnielsen/afinn/badge.svg?branch=master :target: https://coveralls.io/github/fnielsen/afinn?branch=master\n\n.. image:: https://img.shields.io/pypi/dm/afinn.svg?style=flat\n   :target: https://pypi.python.org/pypi/afinn\n   :alt: Downloads\n\n.. image:: https://www.openhub.net/p/afinn/widgets/project_thin_badge.gif\n   :target: https://www.openhub.net/p/afinn\n   :alt: Open Hub\n'",AFINN sentiment analysis in Python
https://github.com/vck/geophy,b'# geophy\ngeophysics homework\n',geophysics homework
https://github.com/blairhudson/learningml,"b'\n[<img src=""learningml.png"" alt=""Learning ML"" width=""400px"" />](http://learning.ml)\n\n\n# Learning ML\n\nA practical guide to understanding and applying machine learning algorithms in the quest to become a \xf0\x9f\xa6\x84.\n\nby **[@blairhudson](http://twitter.com/blairhudson)**\n\n*This book is a work in progress. Follow [@Learning_ML](http://twitter.com/Learning_ML) for the latest updates! For detailed changes, see [commits](https://github.com/blairhudson/learningml/commits/master) on GitHub.*\n\n\n## Table of Contents\n---\n\n### [1. Introduction](01.00-Introduction.ipynb)\n* [1.01 Help](01.01-Help.ipynb)\n* [1.02 Getting Started](01.02-Getting-Started.ipynb)\n\n### [2. Classification](02.00-Classification.ipynb)\n* [2.01 Dummy Classifiers](02.01-Dummy-Classifiers.ipynb)\n* [2.02 Naive Bayes](02.02-Naive-Bayes.ipynb)\n* [2.03 k-Nearest Neighbours](02.03-k-Nearest-Neighbours.ipynb)\n* [2.04 Decision Trees](02.04-Decision-Trees.ipynb)\n* Logistic Regression\n* Support Vector Machines\n* Elastic Net\n* Stochastic Gradient Descent\n* RuleFit\n* Ensembles\n* Neural Networks\n\n### [3. Regression](03.00-Regression.ipynb)\n* Linear Regression\n* Support Vector Regression\n\n### [4. Unsupervised](04.00-Unsupervised.ipynb)\n* k-means\n* t-SNE\n* Apriori\n* PCA\n* LDA\n\n### [5. Deep Learning](05.00-Deep-Learning.ipynb)\n\n* Keras and TensorFlow\n* Deep Neural Networks: Classification and Regression\n* DNN Problems and Architectures\n\n### [6. Big Data](06.00-Big-Data.ipynb)\n\n* Spark MLLib\n\n### [Appendix](99.00-Appendix.ipynb)\n* [a. Glossary](99.01-Glossary.ipynb)\n* [b. Acknowledgements](99.02-Acknowledgements.ipynb)\n* [c. Resources](99.03-Resources.ipynb)\n\n---\n\n[learning.ml](http://learning.ml) / This work is licensed under [CC BY-NC-ND 3.0 AU](https://creativecommons.org/licenses/by-nc-nd/3.0/au/).\n\n<script>\n  (function(i,s,o,g,r,a,m){i[\'GoogleAnalyticsObject\']=r;i[r]=i[r]||function(){\n  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n  })(window,document,\'script\',\'https://www.google-analytics.com/analytics.js\',\'ga\');\n\n  ga(\'create\', \'UA-96446446-1\', \'auto\');\n  ga(\'send\', \'pageview\');\n\n</script>\n'",Learning ML ~ A practical guide to understanding and applying machine learning algorithms in the quest to become a 🦄 by Blair Hudson
https://github.com/johnnygreco/hugs-graveyard,b'## A graveyard for HUGs \n',old hugs project repository
https://github.com/jam1245/Visualizations,b'# Visualizations\nCollection of various visualizations\n',Collection of various visualizations
https://github.com/ericwayman/nflData,"b'# nflData\n#Script to implement a Logistic regression model of the form\n#Y^*_i = b_0+ b_1*HWP_i + b_2*AWP_i + b_3*HL4_i + b_4*AL4_i b_5FAV_i + e_i\n#i is the index of the game number\n#Y^*_i is our prediction for Y_i the indicator of the home team beating the spread in game i\n#{b_0,b_1,b_2,b_3,b_4,b_5} the parameters to train\n#HWP_i: overall win percentage (relative to the spread) of home team\n#AWP_i: overall win percentage (relative to the spread) of away team\n#HL4_i: number of times home team has beaten the spread in the last 4 games played (this season)\n#HL4_i: number of times away team has beaten the spread in the last 4 games played (this season)\n#FAV_i: indicator the home team is the favorite \n'",Some models and analysis to investigate efficiency of the NFL gambling market
https://github.com/rgdn-info-community/piramidy,"b'\xd0\x94\xd0\xb0\xd0\xbd\xd0\xbd\xd1\x8b\xd0\xb9 \xd1\x80\xd0\xb5\xd0\xbf\xd0\xbe\xd0\xb7\xd0\xb8\xd1\x82\xd0\xbe\xd1\x80\xd0\xb8\xd0\xb9 \xd1\x81\xd0\xbe\xd0\xb7\xd0\xb4\xd0\xb0\xd0\xbd \xd0\xb4\xd0\xbb\xd1\x8f \xd0\xb1\xd0\xbe\xd0\xbb\xd0\xb5\xd0\xb5 \xd1\x83\xd0\xb4\xd0\xbe\xd0\xb1\xd0\xbd\xd0\xbe\xd0\xb9 \xd1\x80\xd0\xb0\xd0\xb1\xd0\xbe\xd1\x82\xd1\x8b \xd0\xbf\xd0\xbe \xd1\x81\xd0\xbe\xd0\xb7\xd0\xb4\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x8e \xd0\xbe\xd0\xb1\xd1\x89\xd0\xb5\xd0\xb9 \xd0\xb1\xd0\xb0\xd0\xb7\xd1\x8b \xd0\xbf\xd0\xb8\xd1\x80\xd0\xb0\xd0\xbc\xd0\xb8\xd0\xb4 (\xd0\xb8 \xd0\xbf\xd0\xbe\xd0\xba\xd0\xb0 \xd0\xbd\xd0\xb0\xd1\x85\xd0\xbe\xd0\xb4\xd0\xb8\xd1\x82\xd1\x81\xd1\x8f \xd0\xb2 \xd0\xbf\xd1\x80\xd0\xbe\xd1\x86\xd0\xb5\xd1\x81\xd1\x81\xd0\xb5 \xd0\xb4\xd0\xbe\xd1\x80\xd0\xb0\xd0\xb1\xd0\xbe\xd1\x82\xd0\xba\xd0\xb8).\n\n# \xd0\x9d\xd0\xbe\xd0\xb2\xd0\xbe\xd1\x81\xd1\x82\xd0\xb8\n\n## \xd0\xa7\xd1\x82\xd0\xbe \xd1\x83\xd0\xb6\xd0\xb5 \xd0\xb4\xd0\xbe\xd0\xb1\xd0\xb0\xd0\xb2\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xbe\n- \xd0\x94\xd0\xbe\xd0\xb1\xd0\xb0\xd0\xb2\xd0\xbb\xd0\xb5\xd0\xbd\xd1\x8b \xd0\xbc\xd0\xb5\xd1\x82\xd0\xba\xd0\xb8 \xd0\xbe\xd1\x82 Lada\n- \xd0\x9f\xd0\xb8\xd1\x80\xd0\xb0\xd0\xbc\xd0\xb8\xd0\xb4\xd1\x8b \xd0\xb2 \xd0\x90\xd0\xbd\xd1\x82\xd0\xb0\xd1\x80\xd0\xba\xd1\x82\xd0\xb8\xd0\xb4\xd0\xb5\n  - 2 \xd1\x88\xd1\x82\xd1\x83\xd0\xba\xd0\xb8 \xd1\x80\xd1\x8f\xd0\xb4\xd0\xbe\xd0\xbc \xd1\x81 [\xd0\xbd\xd0\xbe\xd0\xb2\xd0\xbe\xd0\xb9 (2004) \xd0\xbc\xd0\xb0\xd0\xb3\xd0\xbd\xd0\xb8\xd1\x82\xd0\xbd\xd0\xbe\xd0\xb9 \xd0\xbe\xd0\xb1\xd1\x81\xd0\xb5\xd1\x80\xd0\xb2\xd0\xb0\xd1\x82\xd0\xbe\xd1\x80\xd0\xb8\xd0\xb5\xd0\xb9\xe2\x80\x93\xd1\x81\xd1\x82\xd0\xb0\xd0\xbd\xd1\x86\xd0\xb8\xd0\xb5\xd0\xb9 \xd0\xbf\xd1\x80\xd0\xb8\xd0\xbd\xd1\x86\xd0\xb5\xd1\x81\xd1\x81\xd1\x8b \xd0\x95\xd0\xbb\xd0\xb8\xd0\xb7\xd0\xb0\xd0\xb2\xd0\xb5\xd1\x82\xd1\x8b (Princess Elisabeth Station)](http://www.antarcticstation.org/science)\n  ![\xd0\xa4\xd0\xbe\xd1\x82\xd0\xbe \xd0\xbf\xd0\xb8\xd1\x80\xd0\xb0\xd0\xbc\xd0\xb8\xd0\xb4\xd1\x8b](http://static.panoramio.com/photos/large/47977512.jpg)\n    - \xd0\xbf\xd0\xbb\xd0\xbe\xd1\x85\xd0\xbe \xd0\xb2\xd0\xb8\xd0\xb4\xd0\xbd\xd1\x8b \xd0\xbd\xd0\xb0 \xd1\x81\xd0\xbd\xd0\xb8\xd0\xbc\xd0\xba\xd0\xbe\xd0\xb2 \xd1\x81 \xd0\xba\xd0\xbe\xd1\x81\xd0\xbc\xd0\xbe\xd1\x81\xd0\xb0, \xd0\xbd\xd0\xb0 \xd0\xbd\xd0\xb0 \xd1\x84\xd0\xbe\xd1\x82\xd0\xbe \xd1\x81\xd0\xbe \xd1\x81\xd1\x82\xd0\xb0\xd0\xbd\xd1\x86\xd0\xb8\xd0\xb8 \xd0\xb2\xd0\xb8\xd0\xb4\xd0\xbd\xd1\x8b \xd1\x85\xd0\xbe\xd1\x80\xd0\xbe\xd1\x88\xd0\xbe\n    - [\xd1\x81\xd1\x81\xd1\x8b\xd0\xbb\xd0\xba\xd0\xb0 \xd0\xbd\xd0\xb0 \xd1\x81\xd1\x82\xd0\xb0\xd1\x82\xd1\x8c\xd1\x8e \xd1\x81 \xd0\xb1\xd0\xbe\xd0\xbb\xd1\x8c\xd1\x88\xd0\xb8\xd0\xbc \xd0\xba\xd0\xbe\xd0\xbb\xd0\xb8\xd1\x87\xd0\xb5\xd1\x81\xd1\x82\xd0\xb2\xd0\xbe\xd0\xbc \xd1\x84\xd0\xbe\xd1\x82\xd0\xbe](http://sibved.livejournal.com/152355.html)\n      - [\xd1\x81\xd1\x81\xd1\x8b\xd0\xbb\xd0\xba\xd0\xb0 \xd0\xbd\xd0\xb0 \xd0\xb2\xd0\xb8\xd0\xb4\xd0\xb5\xd0\xbe \xd1\x82\xd0\xb0\xd0\xba\xd0\xbe\xd0\xb3\xd0\xbe \xd0\xb6\xd0\xb5 \xd0\xb8\xd1\x89\xd1\x83\xd1\x89\xd0\xb5\xd0\xb3\xd0\xbe \xd0\xbf\xd0\xb8\xd1\x80\xd0\xb0\xd0\xbc\xd0\xb8\xd0\xb4\xd1\x8b](http://wordscience.org/v-antarktide-uchenye-obnaruzhili-tri-zagadochnye-piramidy.html). \xd0\x9d\xd0\xb0\xd0\xb4\xd0\xbe \xd0\xb1\xd1\x8b \xd0\xbd\xd0\xb0\xd0\xb9\xd1\x82\xd0\xb8 \xd0\xb5\xd0\xb3\xd0\xbe \xd0\xbc\xd0\xb5\xd1\x82\xd0\xba\xd0\xb8...\n- \xd0\x9c\xd0\xb5\xd1\x82\xd0\xba\xd0\xb8 \xd0\xbe\xd1\x82 saha \xd0\xb8 austria\n- Geolines.ru\n  - \xd0\xb2\xd1\x81\xd0\xb5 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb\xd1\x8b \xd0\xbe\xd0\xb1\xd1\x8a\xd0\xb5\xd0\xb4\xd0\xb8\xd0\xbd\xd0\xb5\xd0\xbd\xd1\x8b \xd0\xb2 \xd0\xbe\xd0\xb4\xd0\xb8\xd0\xbd \xd0\xb8 \xd0\xbd\xd0\xb5\xd0\xbc\xd0\xbd\xd0\xbe\xd0\xb3\xd0\xbe \xd0\xbe\xd0\xb1\xd1\x80\xd0\xb0\xd0\xb1\xd0\xbe\xd1\x82\xd0\xb0\xd0\xbd\xd1\x8b (\xd1\x83\xd0\xb1\xd1\x80\xd0\xb0\xd0\xbd\xd1\x8b \xd0\xbb\xd0\xb8\xd1\x88\xd0\xbd\xd0\xb8\xd0\xb5 \xd0\xbf\xd0\xb0\xd0\xbf\xd0\xba\xd0\xb8, \xd0\xbd\xd0\xb0\xd0\xb7\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x8f \xd0\xbf\xd0\xb5\xd1\x80\xd0\xb5\xd0\xb2\xd0\xb5\xd0\xb4\xd0\xb5\xd0\xbd\xd1\x8b \xd0\xbc\xd0\xb5\xd1\x81\xd1\x82\xd0\xb0\xd0\xbc\xd0\xb8 \xd0\xb8 \xd1\x82.\xd0\xb4.)\n  - \xd1\x81\xd0\xbe\xd0\xb7\xd0\xb4\xd0\xb0\xd0\xbd \xd0\xbe\xd1\x82\xd0\xb4\xd0\xb5\xd0\xbb\xd1\x8c\xd0\xbd\xd1\x8b\xd0\xb9 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb \xd1\x81 \xd0\xbf\xd0\xb8\xd1\x80\xd0\xb0\xd0\xbc\xd0\xb8\xd0\xb4\xd0\xb0\xd0\xbc\xd0\xb8: [`\xd0\xb1\xd0\xb0\xd0\xb7\xd0\xb0-\xd0\xbf\xd0\xb8\xd1\x80\xd0\xb0\xd0\xbc\xd0\xb8\xd0\xb4.kml`](\xd1\x80\xd0\xb0\xd0\xb1\xd0\xbe\xd1\x87\xd0\xb8\xd0\xb5-\xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb\xd1\x8b/\xd0\xb1\xd0\xb0\xd0\xb7\xd0\xb0-\xd0\xbf\xd0\xb8\xd1\x80\xd0\xb0\xd0\xbc\xd0\xb8\xd0\xb4.kml) (\xd0\xbc\xd0\xbe\xd0\xb6\xd0\xbd\xd0\xbe \xd0\xba\xd0\xbb\xd0\xb8\xd0\xba\xd0\xbd\xd1\x83\xd1\x82\xd1\x8c \xe2\x80\x93 \xd1\x81\xd0\xbc. \xd0\xba\xd0\xb0\xd0\xba \xd1\x81\xd0\xba\xd0\xb0\xd1\x87\xd0\xb0\xd1\x82\xd1\x8c \xd0\xbd\xd0\xb8\xd0\xb6\xd0\xb5)\n  - \xd0\xbd\xd0\xb0\xd0\xb4\xd0\xbe \xd0\xb1\xd1\x83\xd0\xb4\xd0\xb5\xd1\x82 \xd0\xb5\xd1\x89\xd1\x91 \xd1\x80\xd0\xb0\xd0\xb7 \xd0\xbf\xd0\xb5\xd1\x80\xd0\xb5\xd1\x81\xd0\xbc\xd0\xbe\xd1\x82\xd1\x80\xd0\xb5\xd1\x82\xd1\x8c \xd0\xbf\xd0\xbe\xd0\xbb\xd0\xbd\xd1\x8b\xd0\xb9 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb \xe2\x80\x93 [`geolines-ru-\xd0\xbf\xd0\xbe\xd0\xbb\xd0\xbd\xd1\x8b\xd0\xb9.kml`](\xd1\x80\xd0\xb0\xd0\xb1\xd0\xbe\xd1\x87\xd0\xb8\xd0\xb5-\xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb\xd1\x8b/geolines-ru-\xd0\xbf\xd0\xbe\xd0\xbb\xd0\xbd\xd1\x8b\xd0\xb9.kml), \xd1\x82.\xd0\xba. \xd1\x8f \xd0\xb5\xd0\xb3\xd0\xbe \xd0\xbb\xd0\xb8\xd1\x88\xd1\x8c \xd0\xbf\xd0\xbe\xd0\xb2\xd0\xb5\xd1\x80\xd1\x85\xd0\xbd\xd0\xbe\xd1\x81\xd1\x82\xd0\xbd\xd0\xbe \xd0\xbf\xd1\x80\xd0\xbe\xd1\x81\xd0\xbc\xd0\xbe\xd1\x82\xd1\x80\xd0\xb5\xd0\xbb, \xd0\xbc\xd0\xbe\xd0\xb6\xd0\xb5\xd1\x82 \xd1\x87\xd1\x82\xd0\xbe-\xd1\x82\xd0\xbe \xd1\x83\xd0\xbf\xd1\x83\xd1\x81\xd1\x82\xd0\xb8\xd0\xbb, \xd0\xbd\xd0\xb0\xd0\xb4\xd0\xbe \xd0\xbf\xd0\xb5\xd1\x80\xd0\xb5\xd1\x81\xd0\xbc\xd0\xbe\xd1\x82\xd1\x80\xd0\xb5\xd1\x82\xd1\x8c \xd0\xb8 \xd1\x83\xd0\xb4\xd0\xb0\xd0\xbb\xd0\xb8\xd1\x82\xd1\x8c \xd0\xbd\xd0\xb5 \xd0\xbf\xd0\xb8\xd1\x80\xd0\xb0\xd0\xbc\xd0\xb8\xd0\xb4\xd1\x8b\n  - \xd1\x82\xd0\xb0\xd0\xba\xd0\xb6\xd0\xb5, \xd0\xbd\xd0\xb5 \xd0\xb7\xd0\xbd\xd0\xb0\xd1\x8e \xd1\x87\xd1\x82\xd0\xbe \xd0\xb4\xd0\xb5\xd0\xbb\xd0\xb0\xd1\x82\xd1\x8c \xd1\x81 \xd0\xbe\xd0\xb1\xd1\x8a\xd0\xb5\xd0\xba\xd1\x82\xd0\xb0\xd0\xbc\xd0\xb8 \xd0\x9c\xd0\xb0\xd0\xb9\xd1\x8f (\xd0\xb8 \xd0\xb4\xd1\x80\xd1\x83\xd0\xb3\xd0\xb8\xd1\x85 \xd0\xba\xd0\xbe\xd1\x80\xd0\xb5\xd0\xbd\xd0\xbd\xd1\x8b\xd1\x85 \xd0\xb6\xd0\xb8\xd1\x82\xd0\xb5\xd0\xbb\xd0\xb5\xd0\xb9 \xd0\xae\xd0\xb6\xd0\xbd\xd0\xbe\xd0\xb9/\xd0\xa1\xd0\xb5\xd0\xb2\xd0\xb5\xd1\x80\xd0\xbd\xd0\xbe\xd0\xb9 \xd0\x90\xd0\xbc\xd0\xb5\xd1\x80\xd0\xb8\xd0\xba\xd0\xb8) \xe2\x80\x93 \xd1\x82\xd0\xbe\xd0\xb6\xd0\xb5 \xd0\xbd\xd0\xb0\xd0\xb4\xd0\xbe \xd0\xb3\xd0\xbb\xd1\x8f\xd0\xbd\xd1\x83\xd1\x82\xd1\x8c \xd0\xb8 \xd0\xb4\xd0\xbe\xd0\xb1\xd0\xb0\xd0\xb2\xd0\xb8\xd1\x82\xd1\x8c \xd1\x82\xd0\xb5, \xd0\xba\xd0\xbe\xd1\x82\xd0\xbe\xd1\x80\xd1\x8b\xd0\xb5 \xd0\xbf\xd0\xbe\xd1\x85\xd0\xbe\xd0\xb6\xd0\xb8 \xd0\xbd\xd0\xb0 \xd0\xbf\xd0\xb8\xd1\x80\xd0\xb0\xd0\xbc\xd0\xb8\xd0\xb4\xd1\x8b\n\n## \xd0\x92 \xd0\xbf\xd0\xbb\xd0\xb0\xd0\xbd\xd0\xb0\xd1\x85\n- \xd0\xa1\xd1\x82\xd0\xb0\xd1\x82\xd1\x8c\xd1\x8f \xd0\xbf\xd1\x80\xd0\xbe \xd0\xbf\xd0\xb8\xd1\x80\xd0\xb0\xd0\xbc\xd0\xb8\xd0\xb4\xd1\x8b \xd0\x95\xd0\xb3\xd0\xb8\xd0\xbf\xd1\x82\xd0\xb0 \xe2\x80\x93 \xd0\xb2\xd1\x81\xd1\x91 \xd1\x81\xd0\xbe\xd1\x85\xd1\x80\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x82\xd1\x8c \xd0\xb2 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb\n  - \xd0\x98 \xd1\x83\xd0\xb4\xd0\xb0\xd0\xbb\xd0\xb8\xd1\x82\xd1\x8c \xd0\xbf\xd0\xbe\xd0\xb2\xd1\x82\xd0\xbe\xd1\x80\xd1\x8b\n- \xd0\x9f\xd1\x80\xd0\xbe\xd0\xb2\xd0\xb5\xd1\x80\xd0\xb8\xd1\x82\xd1\x8c \xd1\x81\xd0\xb0\xd0\xb9\xd1\x82\xd1\x8b\n  - [\xd0\x9a\xd0\xb0\xd1\x80\xd1\x82\xd0\xb0 \xd1\x80\xd0\xb0\xd1\x81\xd0\xbf\xd0\xbe\xd0\xbb\xd0\xbe\xd0\xb6\xd0\xb5\xd0\xbd\xd0\xb8\xd1\x8f \xd0\xb2\xd1\x81\xd0\xb5\xd1\x85 \xd0\xbf\xd0\xb8\xd1\x80\xd0\xb0\xd0\xbc\xd0\xb8\xd0\xb4 \xd0\xbd\xd0\xb0 \xd0\x97\xd0\xb5\xd0\xbc\xd0\xbb\xd0\xb5](http://lah.flybb.ru/topic1785.html)\n  - [\xd0\x9c\xd0\xb8\xd1\x80\xd0\xbe\xd0\xb2\xd0\xb0\xd1\x8f \xd1\x81\xd0\xb8\xd1\x81\xd1\x82\xd0\xb5\xd0\xbc\xd0\xb0 \xd0\xbf\xd0\xb8\xd1\x80\xd0\xb0\xd0\xbc\xd0\xb8\xd0\xb4.](http://www.kosmm.ru/arl6.html)\n    - [\xd1\x83\xd0\xb6\xd0\xb5 \xd0\xb2\xd1\x8b\xd0\xbb\xd0\xbe\xd0\xb6\xd0\xb5\xd0\xbd\xd0\xbe](http://home.hiwaay.net/~jalison/index.html) \xd0\xb8 \xd0\xb4\xd0\xb0\xd0\xbb\xd0\xb5\xd0\xb5 \xd0\xbf\xd0\xbe \xd1\x81\xd1\x81\xd1\x8b\xd0\xbb\xd0\xba\xd0\xb0\xd0\xbc\n  - http://sibved.livejournal.com/tag/\xd0\x9f\xd0\xb8\xd1\x80\xd0\xb0\xd0\xbc\xd0\xb8\xd0\xb4\xd1\x8b\n  - http://www.tart-aria.info/frantsiya-piramida-le-pertus-pod-sloem-gliny/\n  - http://www.tart-aria.info/piramidalnye-holmy-terrikony/\n  - https://ru.wikipedia.org/wiki/%D0%A1%D0%BF%D0%B8%D1%81%D0%BE%D0%BA_%D0%B5%D0%B3%D0%B8%D0%BF%D0%B5%D1%82%D1%81%D0%BA%D0%B8%D1%85_%D0%BF%D0%B8%D1%80%D0%B0%D0%BC%D0%B8%D0%B4\n  - http://xn--e1adcaacuhnujm.xn--p1ai/drevnie-piramidy-mira-sistema-ili-sovpadenie.html\n  - http://sirderya.blogspot.com/2013/12/ilim-cinde-de-olsa-gidip-alnz.html\n    - http://www.onemforum.com/dunya-sehirleri-resim/375784-beyaz-piramitlerin-google-earth-goruntusu.html\n  - http://www.kazan-newage.ru/sila/81-egi769petskie-pirami769d.html\n  - http://www.exomapia.ru/places/281713\n  - http://piramidainfo.net/map.php\n  - http://www.world-pyramids.com/\n  - http://hghltd.yandex.net/yandbtm?fmode=inject&url=http%3A%2F%2Fwww.mir.h19.ru%2Fxronologiya%2F16-WhiteGods%2Fimg%2Ffoto.php%3Fpgnum%3D49&tld=ru&lang=ru&la=1492602112&tm=1493745767&text=url%3Amir.h19.ru%2Fxronologiya%2F16-WhiteGods%2Fimg%2Ffoto.php%3Fpgnum%3D49%20%7C%20url%3Awww.mir.h19.ru%2Fxronologiya%2F16-WhiteGods%2Fimg%2Ffoto.php%3Fpgnum%3D49&l10n=ru&mime=html&type=touch&sign=629dd6c200aafda637770bd5c163dbca&keyno=0\n  - http://lah.flybb.ru/topic455.html\n  - http://lah.flybb.ru/topic1785.html\n  - https://www.google.com/maps/d/viewer?mid=19FJI3bP6wwTP5Hcvk5DKkFCbOyk\n  - http://lifeglobe.net/entry/1582\n  - http://turbina.ru/q/advice/69771/\n  - https://slavmaps.ru/spisok\n  - http://doublepyramid.org/zagadka-drevnih-piramid-na-kolskom-poluostrove/\n  - http://taboo.su/istoriya/zapretnaya-arkheologiya/74-kompleksy/233-teotiuakan-kompleks-piramid-v-meksike.html\n  - http://taboo.su/istoriya/zapretnaya-arkheologiya/74-kompleksy/193-ninchurt.html\n  - http://www.kosmm.ru/arl4.html\n\n# \xd0\xa7\xd0\xb0\xd0\x92\xd0\xbe? (\xd0\xa7\xd0\x90\xd1\x81\xd1\x82\xd0\xbe \xd0\xb7\xd0\xb0\xd0\xb4\xd0\xb0\xd0\xb2\xd0\xb0\xd0\xb5\xd0\xbc\xd1\x8b\xd0\xb5 \xd0\x92\xd0\x9e\xd0\xbf\xd1\x80\xd0\xbe\xd1\x81\xd1\x8b)\n## \xd0\x9a\xd0\xb0\xd0\xba \xd1\x81\xd0\xba\xd0\xb0\xd1\x87\xd0\xb0\xd1\x82\xd1\x8c...\n### \xd0\xb2\xd1\x81\xd0\xb5 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb\xd1\x8b?\n\xd0\xa7\xd0\xb5\xd1\x80\xd0\xb5\xd0\xb7 \xd0\xba\xd0\xbd\xd0\xbe\xd0\xbf\xd0\xba\xd1\x83 [`Download .zip`](https://github.com/rgdn-info-community/piramidy/zipball/master)\n- \xd0\xb2\xd0\xb0\xd0\xbc \xd0\xbd\xd1\x83\xd0\xb6\xd0\xbd\xd0\xb0 \xd0\xbf\xd0\xb0\xd0\xbf\xd0\xba\xd0\xb0 [`\xd0\xb1\xd0\xb0\xd0\xb7\xd0\xb0-\xd0\xbf\xd0\xb8\xd1\x80\xd0\xb0\xd0\xbc\xd0\xb8\xd0\xb4`](\xd0\xb1\xd0\xb0\xd0\xb7\xd0\xb0-\xd0\xbf\xd0\xb8\xd1\x80\xd0\xb0\xd0\xbc\xd0\xb8\xd0\xb4), \xd0\xb3\xd0\xb4\xd0\xb5 \xd0\xbb\xd0\xb5\xd0\xb6\xd0\xb0\xd1\x82 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb\xd1\x8b \xd1\x81 \xd1\x80\xd0\xb0\xd1\x81\xd1\x88\xd0\xb8\xd1\x80\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5\xd0\xbc `.kml`\n- \xd0\xbe\xd1\x81\xd1\x82\xd0\xb0\xd0\xbb\xd1\x8c\xd0\xbd\xd1\x8b\xd0\xb5 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb\xd1\x8b/\xd0\xbf\xd0\xb0\xd0\xbf\xd0\xba\xd0\xb8 \xd0\xbd\xd1\x83\xd0\xb6\xd0\xbd\xd1\x8b \xd0\xb4\xd0\xbb\xd1\x8f \xd1\x81\xd0\xb0\xd0\xb9\xd1\x82\xd0\xb0 \xd0\xb8 \xd0\xb4\xd0\xbb\xd1\x8f \xd1\x81\xd0\xb0\xd0\xbc\xd0\xbe\xd0\xb3\xd0\xbe \xd1\x80\xd0\xb5\xd0\xbf\xd0\xbe\xd0\xb7\xd0\xb8\xd1\x82\xd0\xbe\xd1\x80\xd0\xb8\xd1\x8f \xd0\xb3\xd0\xb8\xd1\x82\xd1\x85\xd0\xb0\xd0\xb1\xd0\xb0\n\n### \xd0\xbe\xd0\xb4\xd0\xb8\xd0\xbd \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb?\n- \xd0\xbf\xd0\xb5\xd1\x80\xd0\xb5\xd0\xb9\xd0\xb4\xd0\xb8\xd1\x82\xd0\xb5 \xd0\xb2 \xd0\xbf\xd0\xb0\xd0\xbf\xd0\xba\xd1\x83 [`\xd0\xb1\xd0\xb0\xd0\xb7\xd0\xb0-\xd0\xbf\xd0\xb8\xd1\x80\xd0\xb0\xd0\xbc\xd0\xb8\xd0\xb4`](\xd0\xb1\xd0\xb0\xd0\xb7\xd0\xb0-\xd0\xbf\xd0\xb8\xd1\x80\xd0\xb0\xd0\xbc\xd0\xb8\xd0\xb4)\n- \xd0\xbd\xd0\xb0\xd0\xb9\xd0\xb4\xd0\xb8\xd1\x82\xd0\xb5 \xd0\xbd\xd1\x83\xd0\xb6\xd0\xbd\xd1\x8b\xd0\xb9 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb\n- \xd0\xbd\xd0\xb0 \xd1\x81\xd0\xbb\xd0\xb5\xd0\xb4\xd1\x83\xd1\x8e\xd1\x89\xd0\xb5\xd0\xb9 \xd1\x81\xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x86\xd0\xb5 \xd0\xbe\xd1\x82\xd0\xba\xd1\x80\xd0\xbe\xd0\xb9\xd1\x82\xd0\xb5 \xd0\xba\xd0\xbe\xd0\xbd\xd1\x82\xd0\xb5\xd0\xba\xd1\x81\xd1\x82\xd0\xbd\xd0\xbe\xd0\xb5 \xd0\xbc\xd0\xb5\xd0\xbd\xd1\x8e (\xd0\xbb\xd0\xb5\xd0\xb2\xd0\xb0\xd1\x8f \xd0\xba\xd0\xbd\xd0\xbe\xd0\xbf\xd0\xba\xd0\xb0 \xd0\xbc\xd1\x8b\xd1\x88\xd0\xb8/\xd1\x82\xd1\x80\xd1\x8d\xd0\xba\xd0\xbf\xd0\xb0\xd0\xb4\xd0\xb0/\xd0\xbf\xd1\x80.) \xd1\x83 \xd0\xba\xd0\xbd\xd0\xbe\xd0\xbf\xd0\xba\xd0\xb8 ![`Raw`](img/raw.png) \xd0\xb8 \xd0\xb2\xd1\x8b\xd0\xb1\xd0\xb5\xd1\x80\xd0\xb5\xd1\x82\xd0\xb5 \xd0\xbf\xd1\x83\xd0\xbd\xd0\xba\xd1\x82 `\xd0\xa1\xd0\xbe\xd1\x85\xd1\x80\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x82\xd1\x8c \xd1\x81\xd0\xbe\xd0\xb4\xd0\xb5\xd1\x80\xd0\xb6\xd0\xb8\xd0\xbc\xd0\xbe\xd0\xb5 \xd0\xba\xd0\xb0\xd0\xba...` (\xd0\xbd\xd0\xb0\xd0\xb7\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xb8\xd0\xb5 \xd0\xb7\xd0\xb0\xd0\xb2\xd0\xb8\xd1\x81\xd0\xb8\xd1\x82 \xd0\xbe\xd1\x82 \xd0\xb1\xd1\x80\xd0\xb0\xd1\x83\xd0\xb7\xd0\xb5\xd1\x80\xd0\xb0/\xd1\x8f\xd0\xb7\xd1\x8b\xd0\xba\xd0\xb0)\n\n<div style=""text-align:center""><img src =""img/raw-save-as.png"" alt=""\xd0\xa1\xd0\xbe\xd1\x85\xd1\x80\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x82\xd1\x8c \xd0\xba\xd0\xb0\xd0\xba...""/></div>\n- \xd1\x83\xd0\xba\xd0\xb0\xd0\xb6\xd0\xb8\xd1\x82\xd0\xb5 \xd0\xba\xd1\x83\xd0\xb4\xd0\xb0 \xd1\x81\xd0\xbe\xd1\x85\xd1\x80\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x82\xd1\x8c\n\n## \xd0\x9a\xd0\xb0\xd0\xba \xd0\xb7\xd0\xb0\xd1\x80\xd0\xb5\xd0\xb3\xd0\xb8\xd1\x81\xd1\x82\xd1\x80\xd0\xb8\xd1\x80\xd0\xbe\xd0\xb2\xd0\xb0\xd1\x82\xd1\x8c\xd1\x81\xd1\x8f \xd0\xb8 \xd0\xbd\xd0\xb0\xd1\x87\xd0\xb0\xd1\x82\xd1\x8c \xd1\x80\xd0\xb0\xd0\xb1\xd0\xbe\xd1\x82\xd1\x83?\n1\\. \xd1\x81\xd0\xbe\xd0\xb7\xd0\xb4\xd0\xb0\xd1\x91\xd1\x82\xd0\xb5 \xd0\xb0\xd0\xba\xd0\xba\xd0\xb0\xd1\x83\xd0\xbd\xd1\x82 (sign up) \xd0\xbd\xd0\xb0 \xd1\x81\xd0\xb0\xd0\xb9\xd1\x82\xd0\xb5 [\xd0\xb3\xd0\xb8\xd1\x82\xd1\x85\xd0\xb0\xd0\xb1\xd0\xb0](https://github.com)<br>\n  <div style=""text-align:center""><img src =""img/sign-up.png"" alt=""\xd0\x9e\xd0\xba\xd0\xbe\xd1\x88\xd0\xba\xd0\xbe \xd1\x80\xd0\xb5\xd0\xb3\xd0\xb8\xd1\x81\xd1\x82\xd1\x80\xd0\xb0\xd1\x86\xd0\xb8\xd0\xb8""/></div>\n\n  - \xd0\xb2\xd1\x8b\xd0\xb1\xd0\xb8\xd1\x80\xd0\xb0\xd0\xb5\xd1\x82\xd0\xb5 \xd0\xbd\xd0\xb0\xd0\xb7\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xb8\xd0\xb5 \xd0\xb0\xd0\xba\xd0\xba\xd0\xb0\xd1\x83\xd0\xbd\xd1\x82\xd0\xb0 (`username`)\n  - \xd0\xb2\xd0\xb2\xd0\xbe\xd0\xb4\xd0\xb8\xd1\x82\xd0\xb5 \xd1\x81\xd0\xb2\xd0\xbe\xd1\x8e \xd0\xbf\xd0\xbe\xd1\x87\xd1\x82\xd1\x83 \xd0\xb8 \xd0\xbf\xd0\xb0\xd1\x80\xd0\xbe\xd0\xbb\xd1\x8c\n\n2\\. \xd0\xbf\xd0\xbe\xd1\x81\xd0\xbb\xd0\xb5 \xd1\x80\xd0\xb5\xd0\xb3\xd0\xb8\xd1\x81\xd1\x82\xd1\x80\xd0\xb0\xd1\x86\xd0\xb8\xd0\xb8:\n\n  - \xd0\xbf\xd0\xb5\xd1\x80\xd0\xb5\xd1\x85\xd0\xbe\xd0\xb4\xd0\xb8\xd1\x82\xd0\xb5 \xd0\xbd\xd0\xb0 \xd1\x81\xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x86\xd1\x83 [`rgdn-info-community/pyramidy`](https://github.com/rgdn-info-community/piramidy)\n  - \xd0\xb8 \xd0\xbd\xd0\xb0\xd0\xb6\xd0\xb8\xd0\xbc\xd0\xb0\xd0\xb5\xd1\x82\xd0\xb5 \xd0\xbd\xd0\xb0 \xd0\xba\xd0\xbd\xd0\xbe\xd0\xbf\xd0\xba\xd1\x83 `Watch (\xd0\xa1\xd0\xbb\xd0\xb5\xd0\xb4\xd0\xb8\xd1\x82\xd1\x8c)` ![Watch](img/watch-button.png)  \n  - \xd1\x82\xd0\xb5\xd0\xbc \xd1\x81\xd0\xb0\xd0\xbc\xd1\x8b\xd0\xbc \xd0\xb2\xd1\x8b \xd0\xb5\xd1\x89\xd1\x91 \xd0\xb8 \xd0\xb1\xd1\x83\xd0\xb4\xd0\xb5\xd1\x82\xd0\xb5 \xd0\xbf\xd0\xbe\xd0\xbb\xd1\x83\xd1\x87\xd0\xb0\xd1\x82\xd1\x8c \xd1\x83\xd0\xb2\xd0\xb5\xd0\xb4\xd0\xbe\xd0\xbc\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xb8\xd1\x8f \xd0\xbe \xd1\x80\xd0\xb0\xd0\xb1\xd0\xbe\xd1\x82\xd0\xb5 \xd0\xbd\xd0\xb0\xd0\xb4 \xd1\x80\xd0\xb5\xd0\xbf\xd0\xbe\xd0\xb7\xd0\xb8\xd1\x82\xd0\xbe\xd1\x80\xd0\xb8\xd0\xb5\xd0\xbc \xd0\xbd\xd0\xb0 \xd0\xb3\xd0\xbb\xd0\xb0\xd0\xb2\xd0\xbd\xd0\xbe\xd0\xb9 \xd1\x81\xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x86\xd0\xb5 \xd1\x81\xd0\xb2\xd0\xbe\xd0\xb5\xd0\xb3\xd0\xbe \xd0\xb3\xd0\xb8\xd1\x82\xd1\x85\xd0\xb0\xd0\xb1-\xd0\xb0\xd0\xba\xd0\xba\xd0\xb0\xd1\x83\xd0\xbd\xd1\x82\xd0\xb0\n\n3\\. \xd1\x80\xd0\xb0\xd0\xb1\xd0\xbe\xd1\x82\xd0\xb0 \xd1\x81 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb\xd0\xb0\xd0\xbc\xd0\xb8\n\n  - \xd0\xbf\xd0\xbe\xd0\xba\xd0\xb0 \xd1\x8f \xd0\xb2\xd0\xb0\xd1\x81 \xd0\xb5\xd1\x89\xd1\x91 \xd0\xbd\xd0\xb5 \xd0\xb4\xd0\xbe\xd0\xb1\xd0\xb0\xd0\xb2\xd0\xb8\xd0\xbb (\xd0\xb2 \xd0\xbe\xd1\x80\xd0\xb3\xd0\xb0\xd0\xbd\xd0\xb8\xd0\xb7\xd0\xb0\xd1\x86\xd0\xb8\xd1\x8e [`rgdn-info-community`](https://github.com/rgdn-info-community/)), \xd0\xb2\xd1\x8b \xd0\xbd\xd0\xb5 \xd0\xbc\xd0\xbe\xd0\xb6\xd0\xb5\xd1\x82\xd0\xb5 \xd0\xb4\xd0\xbe\xd0\xb1\xd0\xb0\xd0\xb2\xd0\xbb\xd1\x8f\xd1\x82\xd1\x8c/\xd1\x80\xd0\xb5\xd0\xb4\xd0\xb0\xd0\xba\xd1\x82\xd0\xb8\xd1\x80\xd0\xbe\xd0\xb2\xd0\xb0\xd1\x82\xd1\x8c \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb\xd1\x8b, \xd0\xbd\xd0\xbe \xd0\xb2\xd1\x8b \xd0\xbc\xd0\xbe\xd0\xb6\xd0\xb5\xd1\x82\xd0\xb5 \xd1\x81\xd0\xba\xd0\xb0\xd1\x87\xd0\xb0\xd1\x82\xd1\x8c \xd0\xb8\xd1\x85 \xd0\xb8 \xd0\xbd\xd0\xb0\xd1\x87\xd0\xb0\xd1\x82\xd1\x8c \xd1\x80\xd0\xb0\xd0\xb1\xd0\xbe\xd1\x82\xd0\xb0\xd1\x82\xd1\x8c \xd1\x81 \xd0\xbd\xd0\xb8\xd0\xbc\xd0\xb8\n  - \xd0\xba\xd0\xbe\xd0\xb3\xd0\xb4\xd0\xb0 \xd1\x8f \xd0\xb2\xd0\xb0\xd1\x81 \xd1\x83\xd0\xb6\xd0\xb5 \xd0\xb4\xd0\xbe\xd0\xb1\xd0\xb0\xd0\xb2\xd0\xb8\xd0\xbb, \xd0\xbf\xd0\xb5\xd1\x80\xd0\xb5\xd1\x85\xd0\xbe\xd0\xb4\xd0\xb8\xd1\x82\xd0\xb5 \xd0\xba \xd1\x81\xd0\xbb\xd0\xb5\xd0\xb4\xd1\x83\xd1\x8e\xd1\x89\xd0\xb8\xd0\xbc \xd0\xbf\xd1\x83\xd0\xbd\xd0\xba\xd1\x82\xd0\xb0\xd0\xbc\n\n*\xd0\x9f\xd1\x80\xd0\xb8\xd0\xbc\xd0\xb5\xd1\x87\xd0\xb0\xd0\xbd\xd0\xb8\xd0\xb5: \xd0\xb5\xd1\x81\xd0\xbb\xd0\xb8 \xd0\xb2\xd1\x8b \xd0\xb7\xd0\xbd\xd0\xb0\xd0\xba\xd0\xbe\xd0\xbc\xd1\x8b \xd1\x81 git, \xd0\xbc\xd0\xbe\xd0\xb6\xd0\xb5\xd1\x82\xd0\xb5 \xd1\x84\xd0\xbe\xd1\x80\xd0\xba\xd0\xbd\xd1\x83\xd1\x82\xd1\x8c \xd1\x80\xd0\xb5\xd0\xbf\xd0\xbe\xd0\xb7\xd0\xb8\xd1\x82\xd0\xbe\xd1\x80\xd0\xb8\xd0\xb9 \xd0\xb8 \xd0\xb2\xd1\x80\xd0\xb5\xd0\xbc\xd1\x8f \xd0\xbe\xd1\x82 \xd0\xb2\xd1\x80\xd0\xb5\xd0\xbc\xd0\xb5\xd0\xbd\xd0\xb8 \xd0\xbf\xd1\x80\xd0\xb8\xd1\x81\xd1\x8b\xd0\xbb\xd0\xb0\xd1\x82\xd1\x8c \xd0\xb7\xd0\xb0\xd0\xbf\xd1\x80\xd0\xbe\xd1\x81\xd1\x8b \xd0\xbd\xd0\xb0 \xd1\x81\xd0\xbb\xd0\xb8\xd1\x8f\xd0\xbd\xd0\xb8\xd0\xb5. \xd0\x90 \xd0\xbc\xd0\xbe\xd0\xb6\xd0\xb5\xd1\x82 \xd0\xbb\xd1\x83\xd1\x87\xd1\x88\xd0\xb5 \xd0\xbf\xd1\x80\xd0\xbe\xd1\x81\xd1\x82\xd0\xbe \xd0\xb2\xd0\xb5\xd1\x82\xd0\xba\xd1\x83 \xd1\x81\xd0\xb4\xd0\xb5\xd0\xbb\xd0\xb0\xd1\x82\xd1\x8c? \xd0\x98\xd0\xbb\xd0\xb8 \xd0\xb2\xd0\xbe\xd0\xbe\xd0\xb1\xd1\x89\xd0\xb5 \xd1\x82\xd0\xbe\xd0\xbb\xd1\x8c\xd0\xba\xd0\xbe \xd0\xbd\xd0\xb0 \xd0\xbc\xd0\xb0\xd1\x81\xd1\x82\xd0\xb5\xd1\x80\xd0\xb5 \xd0\xb2\xd1\x81\xd0\xb5\xd0\xbc \xd1\x80\xd0\xb0\xd0\xb1\xd0\xbe\xd1\x82\xd0\xb0\xd1\x82\xd1\x8c? \xd0\x95\xd1\x81\xd0\xbb\xd0\xb8 \xd0\xba\xd1\x82\xd0\xbe \xd0\xb7\xd0\xbd\xd0\xb0\xd0\xb5\xd1\x82 \xe2\x80\x93 \xd0\xbf\xd0\xbe\xd0\xb4\xd1\x81\xd0\xba\xd0\xb0\xd0\xb6\xd0\xb8\xd1\x82\xd0\xb5, \xd0\xbf\xd0\xbe\xd0\xb6\xd0\xb0\xd0\xbb\xd1\x83\xd0\xb9\xd1\x81\xd1\x82\xd0\xb0, \xd0\xba\xd0\xb0\xd0\xba \xd0\xbb\xd1\x83\xd1\x87\xd1\x88\xd0\xb5.*\n\n## \xd0\x9a\xd0\xb0\xd0\xba \xd0\xb4\xd0\xbe\xd0\xb1\xd0\xb0\xd0\xb2\xd0\xbb\xd1\x8f\xd1\x82\xd1\x8c \xd1\x81\xd0\xb2\xd0\xbe\xd0\xb8 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb\xd1\x8b?\n\xd0\x97\xd0\xb0\xd1\x85\xd0\xbe\xd0\xb4\xd0\xb8\xd1\x82\xd0\xb5 \xd0\xb2 \xd0\xbf\xd0\xb0\xd0\xbf\xd0\xba\xd1\x83 [`\xd1\x80\xd0\xb0\xd0\xb1\xd0\xbe\xd1\x87\xd0\xb8\xd0\xb5 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb\xd1\x8b`](\xd1\x80\xd0\xb0\xd0\xb1\xd0\xbe\xd1\x87\xd0\xb8\xd0\xb5-\xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb\xd1\x8b) \xd0\xb8 \xd0\xbf\xd1\x80\xd0\xbe\xd1\x81\xd1\x82\xd0\xbe \xd0\xbf\xd0\xb5\xd1\x80\xd0\xb5\xd1\x82\xd0\xb0\xd1\x81\xd0\xba\xd0\xb8\xd0\xb2\xd0\xb0\xd0\xb5\xd1\x82\xd0\xb5 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb \xd0\xb2 \xd0\xbe\xd0\xba\xd0\xbd\xd0\xbe \xd0\xb1\xd1\x80\xd0\xb0\xd1\x83\xd0\xb7\xd0\xb5\xd1\x80\xd0\xb0, \xd0\xbf\xd1\x80\xd0\xb8 \xd1\x8d\xd1\x82\xd0\xbe\xd0\xbc \xd0\xbe\xd0\xba\xd0\xbe\xd1\x88\xd0\xba\xd0\xbe \xd0\xb4\xd0\xbe\xd0\xbb\xd0\xb6\xd0\xbd\xd0\xbe \xd0\xb8\xd0\xb7\xd0\xbc\xd0\xb5\xd0\xbd\xd0\xb8\xd1\x82\xd1\x8c\xd1\x81\xd1\x8f. \xd0\x95\xd1\x81\xd0\xbb\xd0\xb8 \xd0\xbd\xd0\xb5\xd1\x82, \xd1\x82\xd0\xbe \xd1\x83 \xd0\xb2\xd1\x81\xd0\xb5 \xd0\xb2\xd1\x81\xd1\x91 \xd0\xb5\xd1\x89\xd1\x91 \xd0\xbd\xd0\xb5\xd1\x82 \xd0\xb4\xd0\xbe\xd1\x81\xd1\x82\xd1\x83\xd0\xbf\xd0\xb0 \xd0\xba \xd1\x80\xd0\xb5\xd0\xb4\xd0\xb0\xd0\xba\xd1\x82\xd0\xb8\xd1\x80\xd0\xbe\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x8e.  \n\n**\xd0\x96\xd0\xb5\xd0\xbb\xd0\xb0\xd1\x82\xd0\xb5\xd0\xbb\xd1\x8c\xd0\xbd\xd0\xbe**, \xd1\x87\xd1\x82\xd0\xbe\xd0\xb1\xd1\x8b \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb \xd0\xb1\xd1\x8b\xd0\xbb \xd1\x81 \xd1\x80\xd0\xb0\xd1\x81\xd1\x88\xd0\xb8\xd1\x80\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5\xd0\xbc `.kml` \xe2\x80\x93 \xd1\x8d\xd1\x82\xd0\xbe \xd0\xbd\xd0\xb5\xd0\xb7\xd0\xb0\xd0\xb0\xd1\x80\xd1\x85\xd0\xb8\xd0\xb2\xd0\xb8\xd1\x80\xd0\xbe\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xbd\xd1\x8b\xd0\xb9 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb \xd1\x81 \xd0\xb2\xd0\xb0\xd1\x88\xd0\xb8\xd0\xbc\xd0\xb8 \xd0\xbe\xd1\x82\xd0\xbc\xd0\xb5\xd1\x82\xd0\xba\xd0\xb0\xd0\xbc\xd0\xb8 \xd0\xbf\xd0\xb8\xd1\x80\xd0\xb0\xd0\xbc\xd0\xb8\xd0\xb4 (`.kmz` \xe2\x80\x93 \xd1\x8d\xd1\x82\xd0\xbe\xd1\x82 \xd1\x82\xd0\xbe\xd1\x82 \xd0\xb6\xd0\xb5 `.kml` \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb, \xd1\x82\xd0\xbe\xd0\xbb\xd1\x8c\xd0\xba\xd0\xbe \xd0\xb2 \xd0\xb0\xd1\x80\xd1\x85\xd0\xb8\xd0\xb2\xd0\xb5).\n\n## \xd0\x9a\xd0\xb0\xd0\xba \xd0\xbe\xd0\xb1\xd0\xbd\xd0\xbe\xd0\xb2\xd0\xbb\xd1\x8f\xd1\x82\xd1\x8c \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb \xd1\x81 \xd1\x82\xd0\xb5\xd0\xba\xd1\x83\xd1\x89\xd0\xb5\xd0\xb9 \xd0\xb1\xd0\xb0\xd0\xb7\xd0\xbe\xd0\xb9?\n\xd0\x94\xd0\xbe \xd1\x82\xd0\xbe\xd0\xb3\xd0\xbe, \xd0\xba\xd0\xb0\xd0\xba \xd1\x81\xd0\xbe\xd0\xb7\xd0\xb4\xd0\xb0\xd0\xb2\xd0\xb0\xd1\x82\xd1\x8c \xd0\xb5\xd0\xb4\xd0\xb8\xd0\xbd\xd1\x83\xd1\x8e \xd0\xb1\xd0\xb0\xd0\xb7\xd1\x83, \xd0\xbd\xd0\xb0\xd0\xb4\xd0\xbe \xd0\xbf\xd1\x80\xd0\xbe\xd0\xb4\xd1\x83\xd0\xbc\xd0\xb0\xd1\x82\xd1\x8c, \xd0\xba\xd0\xb0\xd0\xba\xd0\xb8\xd0\xbc \xd0\xb4\xd0\xbe\xd0\xbb\xd0\xb6\xd0\xb5\xd0\xbd \xd0\xb1\xd1\x8b\xd1\x82\xd1\x8c \xd0\xbd\xd0\xb0\xd0\xb7\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xb8\xd0\xb5, \xd1\x82.\xd0\xb5. \xd0\xb8\xd0\xb4\xd0\xb5\xd0\xbd\xd1\x82\xd0\xb8\xd1\x84\xd0\xb8\xd0\xba\xd0\xb0\xd1\x82\xd0\xbe\xd1\x80 \xd0\xbf\xd0\xb8\xd1\x80\xd0\xb0\xd0\xbc\xd0\xb8\xd0\xb4. \xd0\xad\xd1\x82\xd0\xbe \xd0\xbf\xd0\xbe\xd0\xbc\xd0\xbe\xd0\xb6\xd0\xb5\xd1\x82 \xd0\xb8\xd0\xb7\xd0\xb1\xd0\xb5\xd0\xb6\xd0\xb0\xd1\x82\xd1\x8c \xd0\xbf\xd0\xbe\xd0\xb2\xd1\x82\xd0\xbe\xd1\x80\xd0\xbe\xd0\xb2 \xd0\xb8 \xd1\x81\xd0\xb4\xd0\xb5\xd0\xbb\xd0\xb0\xd0\xb5\xd1\x82 \xd0\xb1\xd0\xb0\xd0\xb7\xd1\x83 \xd0\xb1\xd0\xbe\xd0\xbb\xd0\xb5\xd0\xb5 \xd0\xbe\xd1\x80\xd0\xb3\xd0\xb0\xd0\xbd\xd0\xb8\xd0\xb7\xd0\xbe\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xbd\xd0\xbe\xd0\xb9. \xd0\x9f\xd1\x80\xd0\xb5\xd0\xb4\xd0\xbb\xd0\xb0\xd0\xb3\xd0\xb0\xd1\x8e:\n\n- \xd0\xb2 \xd0\xba\xd0\xb0\xd1\x87\xd0\xb5\xd1\x81\xd1\x82\xd0\xb2\xd0\xb5 \xd0\xbd\xd0\xb0\xd0\xb7\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x8f \xd0\xb8\xd1\x81\xd0\xbf\xd0\xbe\xd0\xbb\xd1\x8c\xd0\xb7\xd0\xbe\xd0\xb2\xd0\xb0\xd1\x82\xd1\x8c \xd0\xbf\xd0\xb5\xd1\x80\xd0\xb2\xd1\x8b\xd0\xb5 \xd1\x82\xd1\x80\xd0\xb8 \xd1\x86\xd0\xb8\xd1\x84\xd1\x80\xd1\x8b \xd0\xba\xd0\xbe\xd0\xbe\xd1\x80\xd0\xb4\xd0\xb8\xd0\xbd\xd0\xb0\xd1\x82 (\xd0\xbd\xd0\xb0\xd0\xb4\xd0\xbe \xd0\xb2\xd1\x8b\xd0\xb1\xd1\x80\xd0\xb0\xd1\x82\xd1\x8c \xd0\xbc\xd0\xb5\xd0\xb6\xd0\xb4\xd1\x83 \xd0\xb2\xd0\xb0\xd1\x80\xd0\xb8\xd0\xb0\xd0\xbd\xd1\x82\xd0\xbe\xd0\xbc \xd1\x81 \xd0\xb3\xd1\x80\xd0\xb0\xd0\xb4\xd1\x83\xd1\x81\xd0\xb0\xd0\xbc\xd0\xb8 \xd0\xb8\xd0\xbb\xd0\xb8 \xd0\xb1\xd0\xb5\xd0\xb7)\n- \xd0\xb5\xd1\x81\xd0\xbb\xd0\xb8 \xd1\x83 \xd0\xbf\xd0\xb8\xd1\x80\xd0\xb0\xd0\xbc\xd0\xb8\xd0\xb4\xd1\x8b \xd0\xb5\xd1\x81\xd1\x82\xd1\x8c \xd0\xbd\xd0\xb0\xd0\xb7\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xb8\xd0\xb5, \xd0\xb5\xd0\xb3\xd0\xbe \xd0\xbc\xd0\xbe\xd0\xb6\xd0\xbd\xd0\xbe \xd1\x83\xd0\xba\xd0\xb0\xd0\xb7\xd0\xb0\xd1\x82\xd1\x8c \xd0\xbf\xd0\xbe\xd1\x81\xd0\xbb\xd0\xb5 \xd0\xb8\xd0\xb4\xd0\xb5\xd0\xbd\xd1\x82\xd0\xb8\xd1\x84\xd0\xb8\xd0\xba\xd0\xb0\xd1\x82\xd0\xbe\xd1\x80\xd0\xb0\n\n\xd0\x9b\xd0\xb5\xd0\xb3\xd1\x87\xd0\xb5 \xd0\xb2\xd1\x81\xd0\xb5\xd0\xb3\xd0\xbe \xd1\x8d\xd1\x82\xd0\xbe \xd1\x81\xd0\xb4\xd0\xb5\xd0\xbb\xd0\xb0\xd1\x82\xd1\x8c \xd0\xb2 \xd0\xbf\xd1\x80\xd0\xbe\xd0\xb3\xd1\x80\xd0\xb0\xd0\xbc\xd0\xbc\xd0\xb5 Google Earth \xe2\x80\x93 \xd1\x82\xd0\xb0\xd0\xbc \xd0\xbc\xd0\xbe\xd0\xb6\xd0\xbd\xd0\xbe \xd1\x81\xd0\xbe\xd0\xb7\xd0\xb4\xd0\xb0\xd0\xb2\xd0\xb0\xd1\x82\xd1\x8c \xd0\xbf\xd0\xb0\xd0\xbf\xd0\xba\xd0\xb8 \xd1\x81 \xd0\xb2\xd0\xb0\xd1\x88\xd0\xb8\xd0\xbc\xd0\xb8 \xd0\xb7\xd0\xb0\xd0\xbc\xd0\xb5\xd1\x82\xd0\xba\xd0\xb0\xd0\xbc\xd0\xb8, \xd0\xba\xd0\xbe\xd0\xbf\xd0\xb8\xd1\x80\xd0\xbe\xd0\xb2\xd0\xb0\xd1\x82\xd1\x8c, \xd0\xb2\xd1\x8b\xd1\x80\xd0\xb5\xd0\xb7\xd0\xb0\xd1\x82\xd1\x8c \xd0\xb8 \xd0\xb2\xd1\x81\xd1\x82\xd0\xb0\xd0\xb2\xd0\xbb\xd1\x8f\xd1\x82\xd1\x8c \xd0\xb8\xd1\x85. \xd0\x9e\xd1\x82\xd0\xba\xd1\x80\xd1\x8b\xd0\xb2\xd0\xb0\xd0\xb5\xd1\x82\xd0\xb5 \xd0\xb2\xd0\xb0\xd1\x88 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb \xd0\xb8 \xd1\x82\xd0\xb5\xd0\xba\xd1\x83\xd1\x89\xd1\x83\xd1\x8e \xd0\xb1\xd0\xb0\xd0\xb7\xd1\x83 \xd0\xbf\xd0\xb8\xd1\x80\xd0\xb0\xd0\xbc\xd0\xb8\xd0\xb4, \xd0\xb8 `Ctrl+C`, `Ctrl+V` (`Cmd` \xd0\xb4\xd0\xbb\xd1\x8f \xd0\xbc\xd0\xb0\xd0\xba\xd0\xb8\xd0\xbd\xd1\x82\xd0\xbe\xd1\x88).\n\n\xd0\x9f\xd0\xbe\xd0\xba\xd0\xb0 \xd1\x8d\xd1\x82\xd0\xbe \xd0\xb1\xd1\x83\xd0\xb4\xd1\x83 \xd0\xb4\xd0\xb5\xd0\xbb\xd0\xb0\xd1\x82\xd1\x8c \xd1\x8f, \xd0\xb0 \xd0\xb5\xd1\x81\xd0\xbb\xd0\xb8 \xd0\xbb\xd1\x8e\xd0\xb4\xd0\xb5\xd0\xb9 \xd0\xb1\xd1\x83\xd0\xb4\xd0\xb5\xd1\x82 \xd0\xbc\xd0\xbd\xd0\xbe\xd0\xb3\xd0\xbe, \xd1\x82\xd0\xbe \xd1\x8f \xd0\xbe\xd0\xbf\xd0\xb8\xd1\x88\xd1\x83 \xd0\xba\xd0\xb0\xd0\xba \xd0\xb4\xd0\xb5\xd0\xbb\xd0\xb0\xd1\x82\xd1\x8c \xd1\x81 \xd0\xbf\xd0\xbe\xd0\xbc\xd0\xbe\xd1\x89\xd1\x8c \xd0\xbc\xd0\xb5\xd1\x85\xd0\xb0\xd0\xbd\xd0\xb8\xd0\xb7\xd0\xbc\xd0\xbe\xd0\xb2 \xd0\xb3\xd0\xb8\xd1\x82\xd0\xb0 (\xd1\x81 \xd0\xbf\xd0\xbe\xd0\xbc\xd0\xbe\xd1\x89\xd1\x8c\xd1\x8e \xd0\xb2\xd0\xb5\xd1\x82\xd0\xbe\xd0\xba (branches) \xd0\xb8\xd0\xbb\xd0\xb8 \xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd0\xb2\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb9 (forks)).\n\n*\xd0\x9f\xd1\x80\xd0\xb8\xd0\xbc\xd0\xb5\xd1\x87\xd0\xb0\xd0\xbd\xd0\xb8\xd0\xb5: \xd0\x95\xd1\x81\xd0\xbb\xd0\xb8 \xd0\xb2\xd1\x8b \xd0\xb7\xd0\xbd\xd0\xb0\xd0\xb5\xd1\x82\xd0\xb5, \xd1\x87\xd1\x82\xd0\xbe `.kml` \xd1\x8d\xd1\x82\xd0\xbe \xd0\xbe\xd0\xb1\xd1\x8b\xd1\x87\xd0\xbd\xd1\x8b\xd0\xb9 `XML`-\xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb, \xd1\x82\xd0\xbe \xd0\xbc\xd0\xbe\xd0\xb6\xd0\xb5\xd1\x82\xd0\xb5 \xd0\xb5\xd0\xb3\xd0\xbe \xd1\x80\xd0\xb5\xd0\xb4\xd0\xb0\xd0\xba\xd1\x82\xd0\xb8\xd1\x80\xd0\xbe\xd0\xb2\xd0\xb0\xd1\x82\xd1\x8c \xd0\xb2 \xd0\xbb\xd1\x8e\xd0\xb1\xd0\xbe\xd0\xbc \xd1\x82\xd0\xb5\xd0\xba\xd1\x81\xd1\x82\xd0\xbe\xd0\xb2\xd0\xbe\xd0\xbc \xd1\x80\xd0\xb5\xd0\xb4\xd0\xb0\xd0\xba\xd1\x82\xd0\xbe\xd1\x80\xd0\xb5.*\n\n## \xd0\x95\xd1\x81\xd1\x82\xd1\x8c \xd0\xbf\xd1\x80\xd0\xb5\xd0\xb4\xd0\xbb\xd0\xbe\xd0\xb6\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5? \xd0\xa7\xd1\x82\xd0\xbe-\xd1\x82\xd0\xbe \xd0\xbd\xd0\xb5 \xd1\x80\xd0\xb0\xd0\xb1\xd0\xbe\xd1\x82\xd0\xb0\xd0\xb5\xd1\x82?\n\xd0\x94\xd0\xbb\xd1\x8f \xd1\x82\xd0\xbe\xd0\xb3\xd0\xbe, \xd1\x87\xd1\x82\xd0\xbe\xd0\xb1\xd1\x8b \xd0\xb7\xd0\xb0\xd0\xb4\xd0\xb0\xd1\x82\xd1\x8c \xd0\xb2\xd0\xbe\xd0\xbf\xd1\x80\xd0\xbe\xd1\x81, \xd0\xbd\xd0\xb0\xd0\xb4\xd0\xbe \xd1\x81\xd0\xb4\xd0\xb5\xd0\xbb\xd0\xb0\xd1\x82\xd1\x8c \xd1\x81\xd0\xbb\xd0\xb5\xd0\xb4\xd1\x83\xd1\x8e\xd1\x89\xd0\xb5\xd0\xb5:\n- \xd0\xbd\xd0\xb0\xd0\xb6\xd0\xb0\xd1\x82\xd1\x8c \xd0\xbd\xd0\xb0 \xd0\xb7\xd0\xb0\xd0\xba\xd0\xbb\xd0\xb0\xd0\xb4\xd0\xba\xd1\x83 ![\xd0\x9f\xd1\x80\xd0\xbe\xd0\xb1\xd0\xbb\xd0\xb5\xd0\xbc\xd1\x8b](img/issues.png), \xd0\xb0 \xd0\xb7\xd0\xb0\xd1\x82\xd0\xb5\xd0\xbc \xd0\xbd\xd0\xb0 \xd0\xba\xd0\xbd\xd0\xbe\xd0\xbf\xd0\xba\xd1\x83 ![\xd0\x9d\xd0\xbe\xd0\xb2\xd0\xb0\xd1\x8f ""\xd0\xbf\xd1\x80\xd0\xbe\xd0\xb1\xd0\xbb\xd0\xb5\xd0\xbc\xd0\xb0""](/img/issues-new.png)\n- \xd0\xbd\xd0\xb0\xd0\xb1\xd1\x80\xd0\xb0\xd1\x82\xd1\x8c \xd0\xbd\xd0\xb0\xd0\xb7\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xb8\xd0\xb5 \xd0\xb8 \xd1\x82\xd0\xb5\xd0\xba\xd1\x81\xd1\x82 \xd1\x81 \xd0\xbe\xd0\xbf\xd0\xb8\xd1\x81\xd0\xb0\xd0\xbd\xd0\xb8\xd0\xb5\xd0\xbc \xd0\xbf\xd1\x80\xd0\xbe\xd0\xb1\xd0\xbb\xd0\xb5\xd0\xbc\xd1\x8b\n- \xd0\xbd\xd0\xb0\xd0\xb6\xd0\xb0\xd1\x82\xd1\x8c \xd0\xbd\xd0\xb0 \xd0\xba\xd0\xbd\xd0\xbe\xd0\xbf\xd0\xba\xd1\x83 ![\xd0\x94\xd0\xbe\xd0\xb1\xd0\xb0\xd0\xb2\xd0\xb8\xd1\x82\xd1\x8c \xd0\xbd\xd0\xbe\xd0\xb2\xd1\x83\xd1\x8e ""\xd0\xbf\xd1\x80\xd0\xbe\xd0\xb1\xd0\xbb\xd0\xb5\xd0\xbc\xd1\x83""](img/issues-new-submit.png)\n\n# \xd0\x90 \xd0\xb2\xd1\x8b \xd0\xb7\xd0\xbd\xd0\xb0\xd0\xb5\xd1\x82\xd0\xb5, \xd1\x87\xd1\x82\xd0\xbe...\n- \xd0\x92 \xd0\xbf\xd1\x80\xd0\xb8\xd0\xbb\xd0\xbe\xd0\xb6\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb8 Google Earth \xd0\xbc\xd0\xbe\xd0\xb6\xd0\xbd\xd0\xbe \xd1\x83\xd0\xb2\xd0\xb8\xd0\xb4\xd0\xb5\xd1\x82\xd1\x8c \xd0\xba\xd0\xb0\xd1\x80\xd1\x82\xd1\x8b \xd0\xb7\xd0\xb0 \xd1\x80\xd0\xb0\xd0\xb7\xd0\xbd\xd1\x8b\xd0\xb5 \xd0\xbc\xd0\xbe\xd0\xbc\xd0\xb5\xd0\xbd\xd1\x82\xd1\x8b \xd0\xb2\xd1\x80\xd0\xb5\xd0\xbc\xd0\xb5\xd0\xbd\xd0\xb8? \xd0\x9d\xd0\xb0\xd0\xb4\xd0\xbe \xd0\xbb\xd0\xb8\xd1\x88\xd1\x8c \xd0\xbd\xd0\xb0\xd0\xb6\xd0\xb0\xd1\x82\xd1\x8c \xd0\xbd\xd0\xb0 \xd0\xba\xd0\xbd\xd0\xbe\xd0\xbf\xd0\xba\xd1\x83 ![\xd0\x9a\xd0\xbd\xd0\xbe\xd0\xbf\xd0\xba\xd0\xb0 \xd1\x81\xd1\x82\xd0\xb0\xd1\x80\xd1\x8b\xd1\x85 \xd0\xba\xd0\xb0\xd1\x80\xd1\x82](img/timeback-button.png)\n- \xd0\x92 Google Earth \xd0\xbc\xd0\xbe\xd0\xb6\xd0\xbd\xd0\xbe \xd0\xb8 \xd0\xbd\xd0\xb0 \xd0\xb7\xd0\xb2\xd1\x91\xd0\xb7\xd0\xb4\xd1\x8b \xd0\xbf\xd0\xbe\xd1\x81\xd0\xbc\xd0\xbe\xd1\x82\xd1\x80\xd0\xb5\xd1\x82\xd1\x8c, \xd0\xbf\xd1\x80\xd0\xb8\xd1\x87\xd1\x91\xd0\xbc \xd1\x81 \xd0\xba\xd0\xb0\xd1\x80\xd1\x82\xd0\xb8\xd0\xbd\xd0\xba\xd0\xb0\xd0\xbc\xd0\xb8 \xd0\xbe\xd1\x82 \xd0\x9d\xd0\x90\xd0\xa1\xd0\x90 \xd0\xb8 \xd0\xb4\xd1\x80. \xd1\x82\xd0\xbe\xd0\xb2\xd0\xb0\xd1\x80\xd0\xb8\xd1\x89\xd0\xb5\xd0\xb9 \xd0\xb2\xd0\xbc\xd0\xb5\xd1\x81\xd1\x82\xd0\xb5 \xd1\x81 \xd0\xb7\xd0\xb0\xd0\xbc\xd0\xb5\xd1\x82\xd0\xba\xd0\xb0\xd0\xbc\xd0\xb8. \xd0\x9d\xd0\xb0\xd0\xb4\xd0\xbe \xd0\xbb\xd0\xb8\xd1\x88\xd1\x8c \xd0\xbd\xd0\xb0\xd0\xb6\xd0\xb0\xd1\x82\xd1\x8c \xd0\xbd\xd0\xb0 \xd1\x81\xd0\xb8\xd0\xbc\xd0\xb2\xd0\xbe\xd0\xbb \xd0\xbf\xd0\xbb\xd0\xb0\xd0\xbd\xd0\xb5\xd1\x82\xd1\x8b ![\xd0\xbf\xd0\xbb\xd0\xb0\xd0\xbd\xd0\xb5\xd1\x82\xd0\xb0](img/planet.png) \xd0\xb8 \xd0\xbf\xd0\xbe\xd1\x8f\xd0\xb2\xd0\xb8\xd1\x82\xd1\x81\xd1\x8f \xd1\x81\xd0\xbf\xd0\xb8\xd1\x81\xd0\xbe\xd0\xba:  \n\n<div style=""text-align:center""><img src =""img/list-of-resources.png"" alt=""\xd0\xa1\xd0\xbf\xd0\xb8\xd1\x81\xd0\xbe\xd0\xba \xd1\x80\xd0\xb5\xd1\x81\xd1\x83\xd1\x80\xd1\x81\xd0\xbe\xd0\xb2""/></div>\n\n# \xd0\x9f\xd0\xbb\xd0\xb0\xd0\xbd \xd1\x80\xd0\xb0\xd0\xb1\xd0\xbe\xd1\x82\xd1\x8b \xd0\xbf\xd0\xbe \xd1\x81\xd0\xb0\xd0\xb9\xd1\x82\xd1\x83\n- \xd0\xb3\xd0\xbe\xd1\x82\xd0\xbe\xd0\xb2 \xd0\xba \xd0\xb2\xd0\xb0\xd1\x88\xd0\xb8\xd0\xbc \xd0\xbf\xd1\x80\xd0\xb5\xd0\xb4\xd0\xbb\xd0\xbe\xd0\xb6\xd0\xb5\xd0\xbd\xd0\xb8\xd1\x8f\xd0\xbc, \xd0\xb4\xd0\xbe\xd1\x80\xd0\xbe\xd0\xb3\xd0\xb8\xd0\xb5 \xd0\xb4\xd1\x80\xd1\x83\xd0\xb7\xd1\x8c\xd1\x8f\n- \xd0\xb4\xd0\xbe\xd0\xb1\xd0\xb0\xd0\xb2\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5 \xd0\xba\xd0\xb0\xd1\x80\xd1\x82\xd1\x8b \xd1\x81 Google.Maps \xd0\xb4\xd0\xbb\xd1\x8f \xd0\xb2\xd0\xb8\xd0\xb7\xd1\x83\xd0\xb0\xd0\xbb\xd0\xb8\xd0\xb7\xd0\xb0\xd1\x86\xd0\xb8\xd0\xb8 \xd0\xbf\xd1\x80\xd0\xbe\xd0\xb4\xd0\xb5\xd0\xbb\xd0\xb0\xd0\xbd\xd0\xbd\xd0\xbe\xd0\xb9 \xd1\x80\xd0\xb0\xd0\xb1\xd0\xbe\xd1\x82\xd1\x8b \xd1\x8f \xd0\xbf\xd0\xbe\xd0\xba\xd0\xb0 \xd0\xbe\xd1\x82\xd0\xbb\xd0\xbe\xd0\xb6\xd0\xb8\xd0\xbb \xd0\xb2 \xd0\xb4\xd0\xbe\xd0\xbb\xd0\xb3\xd0\xb8\xd0\xb9 \xd1\x8f\xd1\x89\xd0\xb8\xd0\xba \xe2\x80\x93 \xd0\xb5\xd1\x81\xd0\xbb\xd0\xb8 \xd0\xba\xd1\x82\xd0\xbe-\xd1\x82\xd0\xbe \xd1\x81\xd0\xbc\xd0\xbe\xd0\xb6\xd0\xb5\xd1\x82 \xd0\xbf\xd0\xbe\xd0\xbc\xd0\xbe\xd1\x87\xd1\x8c \xd1\x81 \xd0\xbe\xd1\x80\xd0\xb3\xd0\xb0\xd0\xbd\xd0\xb8\xd0\xb7\xd0\xb0\xd1\x86\xd0\xb8\xd0\xb5\xd0\xb9 \xd1\x81\xd0\xb5\xd0\xb3\xd0\xbe \xe2\x80\x93 \xd0\xbd\xd0\xb0\xd0\xbf\xd0\xb8\xd1\x88\xd0\xb8\xd1\x82\xd0\xb5 \xd0\xbc\xd0\xbd\xd0\xb5, \xd0\xbf\xd0\xbe\xd0\xb6\xd0\xb0\xd0\xbb\xd1\x83\xd0\xb9\xd1\x81\xd1\x82\xd0\xb0\n'",База пирамид
https://github.com/mikeengland/Twitter-MongoDB-Python-Integration,"b""# Twitter-MongoDB-Python-Integration\nExample adapted from Python for Data Analysis by Wes McKinney (the example in the book used the old Twitter Search API)\n\nThis example shows how to connect to MongoDB using Python, query and store tweets in MongoDB relating to the keyword of 'python', load these JSON results into a data frame and display the data frame.\n""",Example adapted from Python for Data Analysis
https://github.com/TueVJ/Python-Introduction,"b'# Introduction to Python (and Gurobi)\n\nAn introduction to basic Python concepts, and most of what is needed to have the full work flow in OR research in Python.\nThe course is not intended to give an in-depth treatment of all aspects of Python, but should provide a solid starting point for self-learning.\nThe course level assumes that the user has experience in some other programming language (e.g. MATLAB, Java), and is looking to learn Python.\n\nThis workshop is intended for 2 sessions of 2 hours each, with the first 2 hours (notebooks 1 & 2) focusing on python basics, and the latter 2 on using standard python modules.\nNotebook 4 is directly intended for OR researchers; if your group has a different focus, e.g. machine learning, this session can be adapted to that purpose.\n\n## Needed tools\n\n**To run these tutorials** on **Windows or Macintosh**, I recommend an installation of [Anaconda](https://www.continuum.io/downloads). \nOn **Linux**, use the version of iPython/Jupyter from your package manager.\n\nThe tutorial is written for Python 3.X, and is organized as ipython notebooks. These will work in [Jupyter Notebooks](http://jupyter.org/) as well.\n\nTo run **session 4**, you will need an installation of [Gurobi](http://www.gurobi.com/downloads/gurobi-optimizer), along with a license. [Free Academic Licenses](https://user.gurobi.com/download/licenses/free-academic) are available for academic users.\n\n## Authors\n\n- *Tue Vissing Jensen*: Initial version, maintainer\n- *Esteban Morales Bondy*: Update of sessions 1-3 for Python 3, css prettiness\n'",Introduces basic Python concepts for those used to other programming languages. Ends with writing optimization problems in Gurobi for Python.
https://github.com/PierreGe/RL-movie-recommender,"b'# RL-movie-recommender\n\n\n### Abstract\n\nThe purpose of our research is to study reinforcement learning\napproaches to building a movie recommender system.\nWe formulate the problem of interactive recommendation as\na contextual multi-armed bandit, learning user preferences\nrecommending new movies and receiving their ratings. We\nshow that using reinforcement learning solves the problem of\nexploitation-exploration trade-off and the cold-start problem.\nWe integrate the novelty of movies to the model. We explore\na content based approach as well as a collaborative filtering\napproach and both yield viable recommendation results.\n'",The purpose of our research is to study reinforcement learning approaches to building a movie recommender system. We formulate the problem of interactive recommendation as a contextual multi-armed bandit.
https://github.com/QuantEcon/NUS_workshop_2016,"b'#  Computational Economics with Python and Julia\n\n##  National University of Singapore, October 2016\n\n\nLecturer: [John Stachurski](http://johnstachurski.net/)\n\nSupport: [Matthew McKay](https://github.com/mmcky)\n\nEconomists are increasingly adopting modern open source computing environments\nsuch as [Python](https://www.python.org/) and [Julia](http://julialang.org/)\nfor research and policy analysis.\n\nThis workshop will cover the basics of these two languages and their use in\ncomputational economics.  We assume that participants \nhave basic programming skills but do not know either Python or Julia.  We\nwill provide a comparison and overview of these languanges, including set up,\ninstallation and some useful libraries.\n\n\n### Instructions\n\n\n*  Before coming, please install [Anaconda Python](https://www.continuum.io/downloads) on your laptop\n\n    * Python + the main scientific libraries\n    * Free from http://continuum.io/downloads\n    * Choose the Python 3.5 version\n    * Make it your default Python distribution\n\n* Install [Julia](http://julialang.org/downloads)\n\n    * Follow instructions for your OS\n    * Some extra info [here](http://julialang.org/downloads/platform.html) if you need it\n\nPlease be sure to bring your laptop to the workshop.  We will have a trouble shooting session at the start.\n\nPlease make sure that you have wireless access.\n\n### Notebooks\n\n* [AR1 simulation in Julia](http://nbviewer.jupyter.org/github/QuantEcon/NUS_workshop_2016/blob/master/ar1_plots_julia.ipynb)\n* [AR1 simulation in Python](http://nbviewer.jupyter.org/github/QuantEcon/NUS_workshop_2016/blob/master/ar1_plots_python.ipynb)\n\n### Resources\n\n* [QuantEcon lectures](http://lectures.quantecon.org/)\n* [Cheat sheet](http://cheatsheets.quantecon.org/)\n* [Forum](http://discourse.quantecon.org/)\n\n\n\n### Get the Files\n\nTo access the files in this repositiory, either \n\n* [use git](https://git-scm.com), if you know what that is, or\n\n* download the zip file (see top right of page)\n\n\n### Other\n\nAlso consider installing \n\n* a modern browser (Chrome / Firefox / Vivaldi) \n\n* a good text editor such as [Atom](https://atom.io/)\n\n* [Git](https://git-scm.com/downloads)\n\n'",Computational Economics with Python and Julia
https://github.com/bcongdon/sgdq-2017-schedule-analysis,b'# sgdq-2017-schedule-analysis\n\xf0\x9f\x93\x8a Visualizations of the SGDQ 2017 Marathon Schedule\n',📊 Visualizations of the SGDQ 2017 Marathon Schedule
https://github.com/mnagaku/naumanni-exercises,"b'{\n ""cells"": [\n  {\n   ""cell_type"": ""markdown"",\n   ""metadata"": {},\n   ""source"": [\n    ""# \xe3\x83\x8a\xe3\x82\xa6\xe3\x83\x9e\xe3\x83\xb3\xe3\x82\x92\xe6\x94\xb9\xe8\xb1\xa1\\n"",\n    ""\\n"",\n    ""Windows\xe3\x81\xa7\xe3\x82\x82Mac\xe3\x81\xa7\xe3\x82\x82Linux\xe3\x81\xa7\xe3\x82\x82\xe3\x80\x81Docker\xe3\x82\x92\xe3\x82\xa4\xe3\x83\xb3\xe3\x82\xb9\xe3\x83\x88\xe3\x83\xbc\xe3\x83\xab\xe3\x81\x99\xe3\x82\x8c\xe3\x81\xb0\xe3\x80\x81\xe4\xbb\xa5\xe4\xb8\x8b\xe3\x81\xab\xe8\xaa\xac\xe6\x98\x8e\xe3\x81\x99\xe3\x82\x8b\xe6\x89\x8b\xe9\xa0\x86\xe3\x81\xa7\xe3\x80\x81\xe4\xb8\xbb\xe3\x81\xabweb\xe3\x83\x96\xe3\x83\xa9\xe3\x82\xa6\xe3\x82\xb6\xe7\xb5\x8c\xe7\x94\xb1\xe3\x81\xa7\xe3\x80\x81naumanni\xe3\x82\x92\xe3\x83\x93\xe3\x83\xab\xe3\x83\x89\xe3\x81\x97\xe3\x81\x9f\xe3\x82\x8a\xe3\x80\x81\xe5\xae\x9f\xe8\xa1\x8c\xe3\x81\x97\xe3\x81\x9f\xe3\x82\x8a\xe3\x80\x81\xe6\x94\xb9\xe9\x80\xa0\xe3\x81\x97\xe3\x81\x9f\xe3\x82\x8a\xe3\x81\xa7\xe3\x81\x8d\xe3\x82\x8b\xe3\x80\x82\\n"",\n    ""\\n"",\n    ""# \xe4\xbd\xbf\xe3\x81\x84\xe6\x96\xb9\\n"",\n    ""\\n"",\n    ""## JupyterLab\xe8\xb5\xb7\xe5\x8b\x95\\n"",\n    ""\\n"",\n    ""docker\xe3\x82\xb3\xe3\x83\x9e\xe3\x83\xb3\xe3\x83\x89\xe3\x81\x8c\xe4\xbd\xbf\xe3\x81\x88\xe3\x82\x8b\xe3\x81\xa8\xe3\x81\x93\xe3\x81\xa7\xe3\x80\x81\\n"",\n    ""\\n"",\n    ""```\\n"",\n    ""# docker run -d -v /var/run/docker.sock:/var/run/docker.sock -p 8888:8888 -e GRANT_SUDO=yes --user root --name minimal-lab mnagaku/minimal-lab start-notebook.sh --NotebookApp.token=\'\'\\n"",\n    ""```\\n"",\n    ""\\n"",\n    ""\xe3\x82\x92\xe5\xae\x9f\xe8\xa1\x8c\xe3\x81\x99\xe3\x82\x8b\xe3\x81\xa8JupyterLab\xe3\x81\x8c8888\xe7\x95\xaa\xe3\x83\x9d\xe3\x83\xbc\xe3\x83\x88\xe3\x81\xa7\xe3\x82\xa2\xe3\x82\xaf\xe3\x82\xbb\xe3\x82\xb9\xe3\x81\xa7\xe3\x81\x8d\xe3\x82\x8b\xe7\x8a\xb6\xe6\x85\x8b\xe3\x81\xa7\xe8\xb5\xb7\xe5\x8b\x95\xe3\x81\x99\xe3\x82\x8b\xe3\x80\x82\\n"",\n    ""\\n"",\n    ""## JupyterLab\xe6\x8e\xa5\xe7\xb6\x9a\\n"",\n    ""\\n"",\n    ""web\xe3\x83\x96\xe3\x83\xa9\xe3\x82\xa6\xe3\x82\xb6\xe3\x81\x8b\xe3\x82\x89\xe3\x80\x81\xe8\xb5\xb7\xe5\x8b\x95\xe3\x81\x97\xe3\x81\x9fJupyterLab\xe3\x81\xab\xe3\x82\xa2\xe3\x82\xaf\xe3\x82\xbb\xe3\x82\xb9\xe3\x81\x99\xe3\x82\x8b\xe3\x80\x82\xe4\xbe\x8b\xe3\x81\x88\xe3\x81\xb0\xe3\x80\x81\xe6\x93\x8d\xe4\xbd\x9c\xe3\x81\x97\xe3\x81\xa6\xe3\x81\x84\xe3\x82\x8bPC\xe4\xb8\x8a\xe3\x81\xab\xe8\xb5\xb7\xe5\x8b\x95\xe3\x81\x97\xe3\x81\x9f\xe5\xa0\xb4\xe5\x90\x88\xe3\x81\xaf\xe3\x80\x81localhost\xe3\x81\xae8888\xe7\x95\xaa\xe3\x83\x9d\xe3\x83\xbc\xe3\x83\x88\xe3\x81\xab\xe3\x82\xa2\xe3\x82\xaf\xe3\x82\xbb\xe3\x82\xb9\xe3\x81\x99\xe3\x82\x8c\xe3\x81\xb0\xe3\x82\x88\xe3\x81\x84\xe3\x81\xae\xe3\x81\xa7\xe3\x80\x81\\n"",\n    ""\\n"",\n    ""```\\n"",\n    ""http://localhost:8888/lab\\n"",\n    ""```\\n"",\n    ""\\n"",\n    ""\xe3\x81\xa7\xe3\x82\xa2\xe3\x82\xaf\xe3\x82\xbb\xe3\x82\xb9\xe3\x81\xa7\xe3\x81\x8d\xe3\x82\x8b\xe3\x80\x82\\n"",\n    ""\\n"",\n    ""## readme.ipynb\xe3\x81\xaeUpload\\n"",\n    ""\\n"",\n    ""web\xe3\x83\x96\xe3\x83\xa9\xe3\x82\xa6\xe3\x82\xb6\xe3\x81\xa7\xe3\x80\x81JupyterLab\xe3\x81\xaeHome\xe7\x94\xbb\xe9\x9d\xa2\xe3\x81\x8c\xe5\x87\xba\xe3\x81\x9f\xe3\x82\x89\xe3\x80\x81\xe5\xb7\xa6\xe4\xb8\x8a\xe3\x81\xab\xe3\x81\x82\xe3\x82\x8b\xe3\x80\x8c\xe4\xb8\x8b\xe7\xb7\x9a\xe3\x81\xab\xe4\xb8\x8a\xe5\x90\x91\xe3\x81\x8d\xe7\x9f\xa2\xe5\x8d\xb0\xe3\x80\x8d\xe3\x83\x9c\xe3\x82\xbf\xe3\x83\xb3\xe3\x81\x8b\xe3\x82\x89\xe3\x80\x81\xe3\x81\x93\xe3\x81\xae\xe6\x96\x87\xe6\x9b\xb8\xe3\x80\x8creadme.ipynb\xe3\x80\x8d\xe3\x82\x92\xe3\x82\xa2\xe3\x83\x83\xe3\x83\x97\xe3\x83\xad\xe3\x83\xbc\xe3\x83\x89\xe3\x81\x99\xe3\x82\x8b\xe3\x80\x82\xe3\x82\xa2\xe3\x83\x83\xe3\x83\x97\xe3\x83\xad\xe3\x83\xbc\xe3\x83\x89\xe3\x81\x8c\xe5\xae\x8c\xe4\xba\x86\xe3\x81\x97\xe3\x81\xa6\xe3\x80\x81Home\xe7\x94\xbb\xe9\x9d\xa2\xe3\x81\xae\xe4\xb8\x80\xe8\xa6\xa7\xe3\x81\xab\xe3\x80\x8creadme.ipynb\xe3\x80\x8d\xe3\x81\x8c\xe8\xa1\xa8\xe7\xa4\xba\xe3\x81\x95\xe3\x82\x8c\xe3\x81\x9f\xe3\x82\x89\xe3\x80\x81\xe3\x82\xaf\xe3\x83\xaa\xe3\x83\x83\xe3\x82\xaf\xe3\x81\x97\xe3\x81\xa6\xe3\x80\x8creadme.ipynb\xe3\x80\x8d\xe3\x81\xa7\xe3\x81\xae\xe6\x93\x8d\xe4\xbd\x9c\xe3\x81\xab\xe7\xa7\xbb\xe3\x82\x8b\xe3\x80\x82\\n"",\n    ""\\n"",\n    ""## JupyterLab\xe3\x82\xb3\xe3\x83\xb3\xe3\x83\x86\xe3\x83\x8a\xe5\x86\x85\xe3\x81\x8b\xe3\x82\x89docker\xe3\x82\xb3\xe3\x83\x9e\xe3\x83\xb3\xe3\x83\x89\xe3\x82\x92\xe4\xbd\xbf\xe3\x81\x88\xe3\x82\x8b\xe3\x82\x88\xe3\x81\x86\xe3\x81\xab\xe3\x81\x99\xe3\x82\x8b\\n"",\n    ""\\n"",\n    ""\xe5\xbf\xb5\xe3\x81\xae\xe3\x81\x9f\xe3\x82\x81\xe3\x80\x81\xe3\x82\xb3\xe3\x83\xb3\xe3\x83\x86\xe3\x83\x8a\xe5\x86\x85\xe3\x81\xae\xe3\x83\xa2\xe3\x82\xb8\xe3\x83\xa5\xe3\x83\xbc\xe3\x83\xab\xe3\x82\x92update\xe3\x81\x97\xe3\x81\xa6\xe3\x81\x8b\xe3\x82\x89\xe3\x80\x81docker\xe3\x82\xb3\xe3\x83\x9e\xe3\x83\xb3\xe3\x83\x89\xe3\x82\x92\xe3\x82\xa4\xe3\x83\xb3\xe3\x82\xb9\xe3\x83\x88\xe3\x83\xbc\xe3\x83\xab\xe3\x81\x99\xe3\x82\x8b\xe3\x80\x82\\n""\n   ]\n  },\n  {\n   ""cell_type"": ""code"",\n   ""execution_count"": null,\n   ""metadata"": {},\n   ""outputs"": [],\n   ""source"": [\n    ""%%bash\\n"",\n    ""sudo -i\\n"",\n    ""apt-get update\\n"",\n    ""apt-get upgrade -y\\n"",\n    ""apt-get install -y curl\\n"",\n    ""curl -fsSL https://get.docker.com/builds/Linux/x86_64/docker-17.05.0-ce.tgz | tar -xzC /usr/local/bin --strip=1 docker/docker\\n"",\n    ""docker --version""\n   ]\n  },\n  {\n   ""cell_type"": ""markdown"",\n   ""metadata"": {},\n   ""source"": [\n    ""## naumanni\xe3\x81\xae\xe3\x83\x93\xe3\x83\xab\xe3\x83\x89\xe7\x92\xb0\xe5\xa2\x83\xe3\x82\x92\xe6\x95\xb4\xe3\x81\x88\xe3\x82\x8b\\n"",\n    ""\\n"",\n    ""\xe3\x81\xbe\xe3\x81\x9a\xe3\x81\xafnode.js\xe3\x81\x8b\xe3\x82\x89\xe3\x80\x82\\n"",\n    ""node >= v7.5.0 \xe3\x81\xa7\xe3\x81\x82\xe3\x82\x8b\xe3\x81\x93\xe3\x81\xa8\xe3\x82\x92\xe7\xa2\xba\xe8\xaa\x8d\xe3\x81\x99\xe3\x82\x8b\xe3\x80\x82\\n""\n   ]\n  },\n  {\n   ""cell_type"": ""code"",\n   ""execution_count"": null,\n   ""metadata"": {},\n   ""outputs"": [],\n   ""source"": [\n    ""%%bash\\n"",\n    ""sudo -i\\n"",\n    ""curl -sL https://deb.nodesource.com/setup_7.x | sudo -E bash -\\n"",\n    ""apt-get install -y nodejs\\n"",\n    ""nodejs -v""\n   ]\n  },\n  {\n   ""cell_type"": ""markdown"",\n   ""metadata"": {},\n   ""source"": [\n    ""\xe6\xac\xa1\xe3\x81\xabyarn\xe3\x80\x82\\n"",\n    ""yarn >= 0.23.4 \xe3\x81\xa7\xe3\x81\x82\xe3\x82\x8b\xe3\x81\x93\xe3\x81\xa8\xe3\x82\x92\xe7\xa2\xba\xe8\xaa\x8d\xe3\x81\x99\xe3\x82\x8b\xe3\x80\x82\\n""\n   ]\n  },\n  {\n   ""cell_type"": ""code"",\n   ""execution_count"": null,\n   ""metadata"": {},\n   ""outputs"": [],\n   ""source"": [\n    ""%%bash\\n"",\n    ""sudo -i\\n"",\n    ""curl -sS https://dl.yarnpkg.com/debian/pubkey.gpg | apt-key add -\\n"",\n    ""echo \\""deb https://dl.yarnpkg.com/debian/ stable main\\"" | tee /etc/apt/sources.list.d/yarn.list\\n"",\n    ""apt-get update\\n"",\n    ""apt-get install -y yarn\\n"",\n    ""yarn""\n   ]\n  },\n  {\n   ""cell_type"": ""markdown"",\n   ""metadata"": {},\n   ""source"": [\n    ""## naumanni\xe3\x81\xae\xe3\x82\xbd\xe3\x83\xbc\xe3\x82\xb9\xe3\x82\x92\xe3\x82\xaf\xe3\x83\xad\xe3\x83\xbc\xe3\x83\xb3\xe3\x81\x99\xe3\x82\x8b\\n""\n   ]\n  },\n  {\n   ""cell_type"": ""code"",\n   ""execution_count"": null,\n   ""metadata"": {},\n   ""outputs"": [],\n   ""source"": [\n    ""%%bash\\n"",\n    ""rm -rf naumanni\\n"",\n    ""git clone --depth 1 https://github.com/naumanni/naumanni.git""\n   ]\n  },\n  {\n   ""cell_type"": ""markdown"",\n   ""metadata"": {},\n   ""source"": [\n    ""## naumanni\xe3\x82\x92\xe3\x83\x93\xe3\x83\xab\xe3\x83\x89\xe3\x81\x99\xe3\x82\x8b\\n""\n   ]\n  },\n  {\n   ""cell_type"": ""code"",\n   ""execution_count"": null,\n   ""metadata"": {},\n   ""outputs"": [],\n   ""source"": [\n    ""%%bash\\n"",\n    ""cd naumanni\\n"",\n    ""yarn\\n"",\n    ""yarn run build""\n   ]\n  },\n  {\n   ""cell_type"": ""markdown"",\n   ""metadata"": {},\n   ""source"": [\n    ""## naumanni\xe3\x81\xae\xe3\x82\xb3\xe3\x83\xb3\xe3\x83\x86\xe3\x83\x8a\xe3\x82\x92\xe3\x83\x93\xe3\x83\xab\xe3\x83\x89\xe3\x81\x99\xe3\x82\x8b\\n""\n   ]\n  },\n  {\n   ""cell_type"": ""code"",\n   ""execution_count"": null,\n   ""metadata"": {},\n   ""outputs"": [],\n   ""source"": [\n    ""%%bash\\n"",\n    ""sudo docker rm -f nora\\n"",\n    ""sudo docker rmi nora/naumanni\\n"",\n    ""cd naumanni\\n"",\n    ""sudo docker build -t nora/naumanni .\\n"",\n    ""sudo docker images""\n   ]\n  },\n  {\n   ""cell_type"": ""markdown"",\n   ""metadata"": {},\n   ""source"": [\n    ""## naumanni\xe3\x82\x92\xe5\xae\x9f\xe8\xa1\x8c\xe3\x81\x99\xe3\x82\x8b\\n"",\n    ""\\n"",\n    ""```\\n"",\n    ""http://localhost/\\n"",\n    ""```\\n"",\n    ""\\n"",\n    ""\xe3\x81\xa7\xe3\x82\xa2\xe3\x82\xaf\xe3\x82\xbb\xe3\x82\xb9\xe3\x81\xa7\xe3\x81\x8d\xe3\x82\x8b\xe3\x80\x82\\n""\n   ]\n  },\n  {\n   ""cell_type"": ""code"",\n   ""execution_count"": null,\n   ""metadata"": {},\n   ""outputs"": [],\n   ""source"": [\n    ""%%bash\\n"",\n    ""sudo docker run -d -p 80:80 --name nora nora/naumanni""\n   ]\n  },\n  {\n   ""cell_type"": ""markdown"",\n   ""metadata"": {},\n   ""source"": [\n    ""## \xe6\x94\xb9\xe8\xb1\xa1\\n"",\n    ""\\n"",\n    ""\xe5\xb7\xa6\xe3\x81\xab\xe8\xa1\xa8\xe7\xa4\xba\xe3\x81\x95\xe3\x82\x8c\xe3\x81\xa6\xe3\x81\x84\xe3\x82\x8b\xe3\x80\x8cFiles\xe3\x80\x8d\xe3\x81\x8b\xe3\x82\x89\xe3\x80\x81\\n"",\n    ""\\n"",\n    ""naumanni > src > js > utils > html.es6\\n"",\n    ""\\n"",\n    ""\xe3\x81\xa8\xe8\xbe\xbf\xe3\x81\xa3\xe3\x81\xa6\xe3\x80\x81html.es6\xe3\x82\x92\xe3\x83\x80\xe3\x83\x96\xe3\x83\xab\xe3\x82\xaf\xe3\x83\xaa\xe3\x83\x83\xe3\x82\xaf\xe3\x81\x99\xe3\x82\x8b\xe3\x81\xa8\xe3\x80\x81html.es6\xe3\x82\x92\xe7\xb7\xa8\xe9\x9b\x86\xe3\x81\x99\xe3\x82\x8b\xe3\x82\xbf\xe3\x83\x96\xe3\x81\x8c\xe9\x96\x8b\xe3\x81\x8f\xe3\x80\x82\\n"",\n    ""\\n"",\n    ""_expandMastodonStatus()\xe3\x81\xa8\xe3\x82\x86\xe3\x83\xbc\xe3\x80\x81\xe3\x82\xb5\xe3\x83\xbc\xe3\x83\x90\xe3\x81\x8b\xe3\x82\x89\xe5\x8f\x96\xe3\x81\xa3\xe3\x81\xa6\xe3\x81\x8d\xe3\x81\x9f\xe5\x91\x9f\xe3\x81\x8d\xe3\x82\x92\xe6\xad\xa3\xe8\xa6\x8f\xe5\x8c\x96\xe3\x81\x99\xe3\x82\x8b\xe5\x87\xa6\xe7\x90\x86\xe3\x81\x8c\xe3\x81\x82\xe3\x82\x8b\xe3\x81\xae\xe3\x81\xa7\xe3\x80\x81\\n"",\n    ""\\n"",\n    ""```\\n"",\n    ""function _expandMastodonStatus(content) {\\n"",\n    ""  content = \'\xe3\x81\x8a\xe3\x81\xa3\xe3\x81\xb1\xe3\x81\x84\';\\n"",\n    ""```\\n"",\n    ""\\n"",\n    ""\xe3\x81\xa81\xe8\xa1\x8c\xe8\xb6\xb3\xe3\x81\x99\xe3\x81\xa8\xe3\x80\x81\xe3\x81\xbf\xe3\x82\x93\xe3\x81\xaa\xe3\x81\xae\xe5\x91\x9f\xe3\x81\x8d\xe3\x82\x92\xe5\xbc\xb7\xe5\x88\xb6\xe7\x9a\x84\xe3\x81\xab\xe6\x9b\xb8\xe3\x81\x8d\xe6\x8f\x9b\xe3\x81\x88\xe3\x82\x8b\xe3\x81\x93\xe3\x81\xa8\xe3\x81\x8c\xe3\x81\xa7\xe3\x81\x8d\xe3\x82\x8b\xe3\x80\x82\\n"",\n    ""\\n"",\n    ""\xe5\xa4\x89\xe6\x9b\xb4\xe3\x82\x92\xe4\xbf\x9d\xe5\xad\x98\xe3\x81\x97\xe3\x81\x9f\xe5\xbe\x8c\xe3\x80\x81\xe3\x83\x93\xe3\x83\xab\xe3\x83\x89\xe3\x83\xbb\xe3\x82\xb3\xe3\x83\xb3\xe3\x83\x86\xe3\x83\x8a\xe3\x81\xae\xe3\x83\x93\xe3\x83\xab\xe3\x83\x89\xe3\x83\xbb\xe5\xae\x9f\xe8\xa1\x8c\xe3\x81\xa7\xe3\x80\x81\xe5\x8b\x95\xe4\xbd\x9c\xe3\x82\x92\xe7\xa2\xba\xe8\xaa\x8d\xe3\x81\xa7\xe3\x81\x8d\xe3\x82\x8b\xe3\x80\x82\\n""\n   ]\n  }\n ],\n ""metadata"": {\n  ""kernelspec"": {\n   ""display_name"": ""Python 3"",\n   ""language"": ""python"",\n   ""name"": ""python3""\n  },\n  ""language_info"": {\n   ""codemirror_mode"": {\n    ""name"": ""ipython"",\n    ""version"": 3\n   },\n   ""file_extension"": "".py"",\n   ""mimetype"": ""text/x-python"",\n   ""name"": ""python"",\n   ""nbconvert_exporter"": ""python"",\n   ""pygments_lexer"": ""ipython3"",\n   ""version"": ""3.5.2""\n  }\n },\n ""nbformat"": 4,\n ""nbformat_minor"": 2\n}\n'",ナウマンを改象
https://github.com/solvcon/solvcon,"b'SOLVCON implements conservation-law solvers that use the space-time\n`Conservation Element and Solution Element (CESE) method\n<http://www.grc.nasa.gov/WWW/microbus/>`__.\n\n|rtd_status|\n\n.. |rtd_status| image:: https://readthedocs.org/projects/solvcon/badge/?version=latest\n  :target: http://doc.solvcon.net/en/latest/\n  :alt: Documentation Status\n\nInstall\n=======\n\nClone from https://github.com/solvcon/solvcon::\n\n  $ git clone https://github.com/solvcon/solvcon\n\nSOLVCON needs the following packages: A C/C++ compiler supporting C++11, `cmake\n<https://cmake.org>`_ 3.7+, `pybind11 <https://github.com/pybind/pybind11>`_\nGit master, `Python <http://www.python.org/>`_ 3.6+, `Cython\n<http://www.cython.org/>`_ 0.16+, `Numpy <http://www.numpy.org/>`_ 1.5+,\n`LAPACK <http://www.netlib.org/lapack/>`_, `NetCDF\n<http://www.unidata.ucar.edu/software/netcdf/index.html>`_ 4+, `SCOTCH\n<http://www.labri.fr/perso/pelegrin/scotch/>`_ 6.0+, `Nose\n<https://nose.readthedocs.org/en/latest/>`_ 1.0+, `Paramiko\n<https://github.com/paramiko/paramiko>`_ 1.14+, `boto\n<http://boto.readthedocs.org/>`_ 2.29.1+, and `gmsh <http://geuz.org/gmsh/>`_\n3+.  Support for `VTK <http://vtk.org/>`_ is to be enabled for conda\nenvironment.\n\nTo install the dependency, run the scripts ``contrib/conda.sh`` and\n``contrib/build-pybind11-in-conda.sh`` (they use `Anaconda\n<https://www.anaconda.com/download/>`__).\n\nThe development version of SOLVCON only supports local build::\n\n  $ make; python setup.py build_ext --inplace\n\nTo build SOLVCON from source code and install it to your system::\n\n  $ make; make install\n\nTest the build::\n\n  $ nosetests --with-doctest\n  $ nosetests ftests/gasplus/*\n\nBuilding document requires `Sphinx <http://sphinx.pocoo.org/>`_ 1.3.1+, `pstake\n<http://pstake.readthedocs.org/>`_ 0.3.4+, and `graphviz\n<http://www.graphviz.org/>`_ 2.28+.  Use the following command::\n\n  $ make -C doc html\n\nThe document will be available at ``doc/build/html/``.\n'",A software framework of conservation-law solvers that use the space-time Conservation Element and Solution Element (CESE) method.
https://github.com/pdfliberation/pdf_table_extraction,"b'experiment_table-extraction\n===========================\n\nusing poppler, pdf-table-extract and numpy to extract tabular data\n\n# visuals\n\nThe ""visuals"" folder contains the output of `pdftotext -bbox`  with additional css and js code to visualize the locations of all detected text.\n\n    pdftoppm -r 150 -png -f 1 -l 1 pdf_documents/DHS.TSA.2013.pdf visuals/DHS.TSA.2013.png\n\n    pdftotext -r 150 -bbox -f 1 -l 1 pdf_documents/DHS.TSA.2013.pdf visuals/DHS.TSA.2013.html\n\n# table-extract\n\nThe ""table-extract"" folder looks at the performance of the pdf-table-extract library on various source documents. Also some rough code for generating pandas dataframes from the output.\n'",experimenting with pdf2text and python pdf-table-extract
https://github.com/nvasilius/PyBootCamp,b'# PyBootCamp\nFiles for the uDemy Complete Python Bootcamp\n',Files for the uDemy Complete Python Bootcamp
https://github.com/vishal8266/Tensorflow-Project---Bank-Note-Classification,"b'# Tensorflow-Project---Bank-Note-Classification\nTensorflow Project - Bank Note Classification\n\nUCI - banknote authentication Data Set\n\nData Set Information:\n\nData were extracted from images that were taken from genuine and forged banknote-like specimens. For digitization, \nan industrial camera usually used for print inspection was used. The final images have 400x 400 pixels. \nDue to the object lens and distance to the investigated object gray-scale pictures with a resolution of about 660 dpi were gained. \nWavelet Transform tool were used to extract features from images. \n'",Tensorflow Project - Bank Note Classification
https://github.com/brakmic/Julia-Articles,"b'# Julia-Articles\n\nCode examples from my <a href=""http://blog.brakmic.com/first-steps-with-julia/"" target=""_blank"">articles</a> on the Julia programming language\n\n# License\n\nMIT\n'",Code examples from my articles on the Julia programming language
https://github.com/qmcs/qmcs.github.io,"b""EECS society at Queen Mary\n==========================\n\n\nThis is the content of the `EECS Society website <http://qmcs.io/>`_. You are\nwelcome to contribute articles on any more or less technical topic.\n\nSharing your knowledge is cool. You can always mention in your CV that you\ncontribute to a blog, know git, familiar with peer reviews, and able to read\ndocumentation.\n\nYou can also directly point to your work. Your next employer will definitely\nlike it.\n\nWriting an article\n------------------\n\nThe content of the blog is hosted on `github <http://github.com>`__, a project\nhosting service. There are two ways of adding the content to the website. The\nsimplest one is to use web interface, the most powerful is to use the ``git``\nprogram.\n\nTo add an article using the web based interface follow these steps:\n\n1. Get an account on `github <http://github.com>`__ and login.\n\n2. `Create a file`_ in the ``content/articles/`` folder.\n\n3. Name the file as ``nnn-slug.rst``, where\n\n   * ``nnn`` is a number of the article. Articles are numbered sequentially, pick\n     the next integer from the the `articles`_ directory.\n\n   * ``slug`` is an informative but short identifier of the article.\n\n   * ``.rst`` is the file format we are using. It allows to define basic\n     formatting, such as headers, links or references. Refer to the\n     `reStructuredText quick reference`_ for examples.\n\n4. Apart form the article, the file should contain some meta information, such\n   as the name of the author, the date the article was written, its tags and\n   category. Here is an example, copy, paste and modify it:\n\n   .. code-block:: rst\n\n       ==========================================\n       An example of an article in the rst format\n       ==========================================\n\n       .. Some metadata\n\n       :date: 2014-04-02 15:00\n       :tags: technology, example\n       :category: blog\n       :author: Dmitrijs Milajevs\n\n       The first paragraph should introduce and possibly summarize the article.\n       It should be relatively short: 2 - 3 sentences.\n\n       .. Explicitly mark the end of the summary/introduction\n\n       -- PELICAN_END_SUMMARY --\n\n       .. Here goes the rest of the article.\n\n       This is a header\n       ================\n\n       This is the second paragraph. You can mark some text as **bold** or `italic`.\n\n5. Write the article!\n\n6. Save the file by clicking a light green button in the bottom right corner of the page.\n\n7. Now you are ready to create a `pull request`_!\n\n   a) Go to ``https://github.com/YOUR_USERNAME/qmcs.github.io/``.\n\n   b) Click on *create pull request*.\n\n   c) Fill in all the relevant information.\n\n   d) Send a pull request.\n\nYou article will be reviewed by other contributers. You might be given\nsuggestions on how the article can be improved. Once the suggestions are\nimplemented the article is be published!\n\n.. _Create a file: https://github.com/qmcs/qmcs.github.io/new/pelican/content/articles\n.. _articles: https://github.com/qmcs/qmcs.github.io/tree/pelican/content/articles\n.. _reStructuredText quick reference: http://docutils.sourceforge.net/docs/user/rst/quickref.html\n.. _pull request: https://help.github.com/articles/creating-a-pull-request\n\nUsing git\n---------\n\nFirst of all, `set up an ssh key <https://help.github.com/articles/generating-ssh-keys>`_,\nand `fork <https://help.github.com/articles/fork-a-repo>`_ the `original repo <https://github.com/qmcs/qmcs.github.io/>`_.\nYou also need to `configure git <https://help.github.com/articles/set- up-git>`_.\nIf you didn't use git before, check out `Github tutorial <http://try.github.io>`_,\na `tutorial provided by Software Carpentry <http://apawlik.github.io/2014-02-03-TGAC/lessons/tgac/version-control/tutorial.html>`_\nor `Github guides <https://guides.github.com>`_ to get a general idea.\n\n\n1. Before writing an article, clone the repo:\n\n.. code-block:: bash\n\n    git clone git@github.com:username/qmcs.github.io\n    cd qmcs.github.io\n\n2. Write an article!\n\n3. Commit and push your changes:\n\n.. code-block:: bash\n\n    git st  # see what files have been changed\n    git diff  # see the actual changes\n    git add RELATED_FILES  # probably, somethig like content/articles/001-intro.rst\n    git ci -m'An article describing the enterprise (R) power of Java.'\n    git push  # send you changes to github\n\nCreate a `pull request <https://help.github.com/articles/creating-a-pull-request>`_.\n\nPersonal page\n~~~~~~~~~~~~~\n\nYou can add information about yourself, such as a brief description of who you\nare, your interests, your homepage and contact information, and, most\nimportantly, a picture.\n\nAuthor bibliographies are stored in ``content/authors``. Here is an example of\n``dmitrijs-milajevs.rst``:\n\n.. code-block:: rst\n\n    =================\n    Dmitrijs Milajevs\n    =================\n\n    :slug: dmitrijs-milajevs\n    :cover_image: static/author_images/dmilajevs.jpg\n    :homepage: http://www.eecs.qmul.ac.uk/~dm303/\n    :service__github: https://github.com/dimazest/\n    :service__bitbucket: https://bitbucket.org/dimazest/\n    :service__twitter: https://twitter.com/dimazest\n    :service__linkedin-square: https://www.linkedin.com/in/dmitrijsmilajevs\n    :cv: dmilajevs_cv.pdf\n\n\n    `Dima <http://www.eecs.qmul.ac.uk/~dm303/>`_ enjoys programming since he was a\n    teenager. He is interested in natural language processing.\n\n    -- PELICAN_END_SUMMARY --\n\n    He spends working days in his office surrounded by monitors and pile of\n    scientific papers. On a weekend, he escapes the office and spends most of the\n    day in a coffee shop somewhere in Central London. To compensate time spent\n    sitting, he does Modern Pentathlon.\n\n    You can find him (re)tweeting as `@dimazest <https://twitter.com/dimazest>`__\n    and showing off his `professional achievements`__ on Linkedin.\n\n\n    __ https://www.linkedin.com/in/dmitrijsmilajevs\n\nThe first paragraph should be short and clear. Note usage of\n``-- PELICAN_END_SUMMARY --`` to mark the end of the summary.\n\nMetadata field prefixed with ``service__`` will appear as icons to the listed\nwebsites. Use names of the services that are available in `Font Awesome\n<http://fortawesome.github.io/Font-Awesome/icons/>`__.\n\nThe cover image is a 461x461 picture of you or an avatar and should be located\nin ``content/static/author_images``.\n\nPut you CV to ``content/static/cv`` and add the ``:cv:`` metadata field.\n\nArticle with Cover Photo (alternative)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nYou can, instead of using the standard article template, use a template which\nincludes a cover photo \xe2\x80\x93 much inspired from medium.com. Here is how to use\nit. Beware, the procedure looks different in Markdown and RST.\n\nMeta header in Markdown:\n\n.. code-block:: md\n\n    Title: Space Kittens\n    Date: 2014-04-15\n    Tags: markdown, markup, languages\n    Category: languages\n    Author: Henrik O. Skogmo\n    Template: article_cover\n    Cover: space-cat.png\n\nMeta header in RST:\n\n.. code-block:: rst\n\n    =============\n    Space Kittens\n    =============\n\n    :date: 2014-04-15\n    :tags: markdown, markup, languages\n    :category: languages\n    :author: Henrik O. Skogmo\n    :template: article_cover\n    :cover: space-cat.png\n\n1. Write your article as you would otherwise, following the ``Writing an\narticle`` guide in this readme.\n\n2. Change the meta ``template`` line to look like this\n``Template: article_cover`` instead of ``Template: article``. This will trigger\nthe system to use the alternative template.\n\n3. Add the line ``Cover: space-cat.png`` underneath. Here I use the photo named\n``space-cat.png`` which I added in the folder ``content/static/article_covers``.\nThis is how we tell the system which photo to use as a cover photo.\n\nNow all you have to do is commit and push your article, together with your \ndesired photo.\n\nPeer review\n-----------\n\nEvery article should be reviewed by two people. You are welcome to go trough any\nopen pull request and comment on the things you like or dislike. If you find the\nchanges to be merged, write a comment::\n\n :+1:\n\nIt's completely fine to comment about anything, but it's important to be polite,\nprecise and constructive.\n\nTo speed up the process assign someone from the team to do peer review. If your\narticle got comments from someone else, please fix them in a timely manner. The\nsooner you fix all the issues, the sooner the article appears on the website.\n\nGenerating the blog locally\n---------------------------\n\nWe use `buildout <https://pypi.python.org/pypi/zc.buildout/2.2.1>`_ to deploy\nneeded software. A typical biuldout deployment consists of two steps:\nbootstrapping and building out.\n\nBootstraping is simple::\n\n    python bootstrap.py\n\nIn case you get an error about setuptools, you can install them:\n\n.. code-block:: bash\n\n    # Only if you get an error in the previus step!\n    python ez_setup.py --user\n    python bootstrap.py\n\nNow you are ready to ``buildout``::\n\n    bin/buildout\n\nAn easy way to see rendered article files\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nYou can use `restview <https://pypi.python.org/pypi/restview>`_ to see rendered\n``.rst``  or `meow <https://pypi.python.org/pypi/meow/>`_ for ``.md`` files in\nyour browser. For example:\n\n.. code-block:: bash\n\n    bin/restview content/articles/001-intro.rst  # to see the intro article\n    bin/meow content/articles/009-markdown.md  # to see the Markdown article\n\nThere are rumors, that you can feed a directory to restview and then select\nfiles in the browser::\n\n    bin/restview content\n\nGenerating the HTML version of a blog locally\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nNow, you can get a local version of the blog:\n\n.. code-block:: bash\n\n    make devserver\n    open http://localhost:8000  # gnome-open on Linux\n    # make stopserver is a logical way to stop the server\n\n\nDeveloping the theme and plugins\n--------------------------------\n\nOur blog uses a custom theme and plugins. The theme and the plugins are external\nprojects and don't belong to this git repository! However, during the\n``buildout`` step they are cloned to the ``src/`` folder, thanks to `Mr.\nDeveloper <https://pypi.python.org/pypi/mr.developer>`_. Here are the external\nprojects we depend on:\n\n.. code-block:: bash\n\n    tree -L 1 src/\n    src/\n    \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 pelican-plugins  # Extenal plugins. Don't bother about it.\n    \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 pelican_extended_authors # Our plugin that provided authors' metadata.\n    \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 pelicanium  # The theme we use.\n\nNote that, by default ``pelicanium`` and ``pelican_extended_authors`` are clones\nof https://github.com/pyclub, but if you want to make changes to these projects\nyou need to use your own fork! These are the steps you need to perform, to be\nable to push to your own fork:\n\n1. Fork ``pelicanium`` and ``pelican_extended_authors`` in github web interface.\n   Refer to the ``[sources]`` section of ``buildout.cfg`` to see what projects\n   you should fork. Such projects are located under ``https://github.com/qmcs/``.\n\n2. Create ``.mr.developer-options.cfg`` with the following content:\n\n.. code-block:: ini\n\n    [rewrites]\n    # Use your own forks instead of the upstream repos for the ``qmcs``projects.\n    qmcs =\n        url ~ ^https://github.com/qmcs/\n        git@github.com:YOUR_USERNAME/\n        kind = git\n\n    # Use ssh instead of https for all github repos.\n    github =\n        url ~ ^https://github.com/\n        git@github.com:\n        kind = git\n\n3. Remove the ``src`` folder. Be sure that there are no any changes you want to\n   keep. If there are, `change remote urls in git repo`_.\n\n.. code-block:: bash\n\n    rm src/ -rf  # Again, be vary careful!\n\n4. Run ``bin/buildout``.\n\nChange remote urls in git repo\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIn case you want to push to github after you run ``buildout``, you need to\nchange remote urls by yourself, for example:\n\n.. code-block:: bash\n\n    cd src/pelicanium\n    git remote set-url origin git@github.com:YOUR_USERNAME/qmcs.github.io\n\nUpdate dependencies\n~~~~~~~~~~~~~~~~~~~\n\nIf you want to update the dependencies, run::\n\n    bin/develop up\n\nAdd a remote\n~~~~~~~~~~~~\n\nIn case you want to refer not only to your repo, but to others, you need to add\nanother remote:\n\n.. code-block:: bash\n\n    git remote add upstream git@github.com:qmcs/qmcs.github.io\n\nNow you can merge with the recent ``pelican`` branch:\n\n.. code-block:: bash\n\n    git checkout pelican\n    git fetch upstream\n    git merge upstream/pelican\n\nYou can also checkout feature branches:\n\n.. code-block:: bash\n\n    git checkout -b theme upstream/theme  # Get the theme branch from upstream\n    git push -u theme origin/theme  # Push it to your fork and set it as the default push destination\n\nUpdating the web site\n---------------------\n\nIn case you are lucky and have write access to the main repo you can upload the\ngenerated HTML version of the site, however you need to clone\n``git@github.com:qmcs/qmcs.github.io``.\n\nTo upload the HTML just run::\n\n    make github\n\nLicense\n-------\n\n.. image:: http://i.creativecommons.org/l/by/4.0/80x15.png\n\nThis work is licensed under a `Creative Commons Attribution 4.0 International\nLicense <http://creativecommons.org/licenses/by/4.0/deed.en_US>`_.\n""",EECS Society. Queen Mary University of London
https://github.com/brunoperry/Udacity,b'# Udacity DeepLearning Foundation\n',Deep Learning Nanodegree Foundation
https://github.com/hkaushalya/CMSPubWordFreqAnalysis,"b""CMSPubWordFreqAnalysis\n======================\n\nCDF/CMS Publications' WORD analysis.\n""",CDF/CMS Publications' WORD analysis.
https://github.com/anubhav-reddy/Fun-Projects,b'This repository contains some fun little code and projects that I made for learning.\n',Practice and Fun Project Code
https://github.com/natalia2q/Misc_Python,"b""# Misc_Python\na mix of Python code that I've been working.\n""",a mix of Python code that I've been working.
https://github.com/davebiagioni/pylighter,"b""# pylighter\nSimple, score-based text highlighting for Python\n\nExample usage:\n\n```\nimport pylighter as pl\nfrom numpy import linspace\n\ntokens = 'Life is like riding a bicycle. To keep your balance, you must keep moving.'.split()\nscores = linspace(0, 1, len(tokens))\n\nfig, ax = pl.render(tokens, scores)\n```\n\nRenders:\n\n![Rendered Image](_img/example.png)\n\nSee [pylighter/examples.ipynb](pylighter/examples.ipynb) for additional examples.\n""","Simple, score-based text highlighting for Python"
https://github.com/llathrop/AIND-Recognizer,"b'# Artificial Intelligence Engineer Nanodegree\n## Probabilistic Models\n## Project: Sign Language Recognition System\n\n### Install\n\nThis project requires **Python 3** and the following Python libraries installed:\n\n- [NumPy](http://www.numpy.org/)\n- [SciPy](https://www.scipy.org/)\n- [scikit-learn](http://scikit-learn.org/0.17/install.html)\n- [pandas](http://pandas.pydata.org/)\n- [matplotlib](http://matplotlib.org/)\n- [jupyter](http://ipython.org/notebook.html)\n- [hmmlearn](http://hmmlearn.readthedocs.io/en/latest/)\n\nNotes: \n1. It is highly recommended that you install the [Anaconda](http://continuum.io/downloads) distribution of Python and load the environment included in the ""Your conda env for AI ND"" lesson.\n2. The most recent development version of hmmlearn, 0.2.1, contains a bugfix related to the log function, which is used in this project.  In order to install this version of hmmearn, install it directly from its repo with the following command from within your activated Anaconda environment:\n```sh\npip install git+https://github.com/hmmlearn/hmmlearn.git\n```\n\n### Code\n\nA template notebook is provided as `asl_recognizer.ipynb`. The notebook is a combination tutorial and submission document.  Some of the codebase and some of your implementation will be external to the notebook. For submission, complete the **Submission** sections of each part.  This will include running your implementations in code notebook cells, answering analysis questions, and passing provided unit tests provided in the codebase and called out in the notebook. \n\n### Run\n\nIn a terminal or command window, navigate to the top-level project directory `AIND_recognizer/` (that contains this README) and run one of the following command:\n\n`jupyter notebook asl_recognizer.ipynb`\n\nThis will open the Jupyter Notebook software and notebook in your browser which is where you will directly edit and run your code. Follow the instructions in the notebook for completing the project.\n\n\n### Additional Information\n##### Provided Raw Data\n\nThe data in the `asl_recognizer/data/` directory was derived from \nthe [RWTH-BOSTON-104 Database](http://www-i6.informatik.rwth-aachen.de/~dreuw/database-rwth-boston-104.php). \nThe handpositions (`hand_condensed.csv`) are pulled directly from \nthe database [boston104.handpositions.rybach-forster-dreuw-2009-09-25.full.xml](boston104.handpositions.rybach-forster-dreuw-2009-09-25.full.xml). The three markers are:\n\n*   0  speaker\'s left hand\n*   1  speaker\'s right hand\n*   2  speaker\'s nose\n*   X and Y values of the video frame increase left to right and top to bottom.\n\nTake a look at the sample [ASL recognizer video](http://www-i6.informatik.rwth-aachen.de/~dreuw/download/021.avi)\nto see how the hand locations are tracked.\n\nThe videos are sentences with translations provided in the database.  \nFor purposes of this project, the sentences have been pre-segmented into words \nbased on slow motion examination of the files.  \nThese segments are provided in the `train_words.csv` and `test_words.csv` files\nin the form of start and end frames (inclusive).\n\nThe videos in the corpus include recordings from three different ASL speakers.\nThe mappings for the three speakers to video are included in the `speaker.csv` \nfile.\n'",Udacity AIND Recognizer project
https://github.com/LiXiling/ml_insurancePred,"b'# ml_insurancePred\nInsurance Claim Prediction using Machine Learning \n------------\nThis is my ""Capstone Project"" for the Udacity Machine Learning Engineer Nanodegree (https://www.udacity.com/course/machine-learning-engineer-nanodegree--nd009)\n\nIt uses the data provided by a competition hosted on kaggle.com by Allstate (https://www.kaggle.com/c/allstate-claims-severity).\n\n## Contents:\n* /docu/Capstone_proposal.pdf - the proposal file for this Capstone Project\n* /docu/Capstone_report.pdf - the final report for this Capstone Project\n* /insurance_prediction.ipynb - the developed code, as a IPython Notebook\n* /docu/insurance_prediction.html - the IPython Notebook exported as HTML\n* /data/train.csv - the dataset used in this project\n\n## Requirements\n* Python 2.7\n* scikit-learn 0.17\n'",Insurance Claim Prediction using Machine Learning - Udacity Nanodegree Capstone Project
https://github.com/eisproject/levin,b'# levin\nLancaster Energy Visualization Interface\n',Lancaster Energy Visualization Interface
https://github.com/shead-custom-design/pipecat,"b'# Welcome!\n\n<img src=""artwork/pipecat.png"" width=""300"" style=""float:right""/>\n\nWelcome to Pipecat ... elegant, flexible data logging in Python for\nconnected sensors and instruments.  Use Pipecat to log data from\nbattery chargers, GPS, automobiles, gyros, weather, and more!\n\nHere are some devices supported by Pipecat and examples of how to log their data:\n\n* [Battery chargers](http://pipecat.readthedocs.io/en/latest/battery-chargers.html).\n* [GPS receivers](http://pipecat.readthedocs.io/en/latest/gps-receivers.html) that generate NMEA data.\n* Vehicles that generate OBD-II data.\n* Motion (accelerometer) data from iOS devices.\n* METAR (aviation weather) data from the National Weather Service.\n* Any device that communicate over a serial port.\n* Any device that can handle HTTP GET requests.\n* Any device that can write to a socket using UDP.\n* Any device that can generate XML data.\n\nYou can see the full Pipecat documentation with tutorials at\nhttps://pipecat.readthedocs.io ... for questions, comments, or suggestions, get\nin touch with our team at https://gitter.im/shead-custom-design/pipecat.\n\nLicense\n=======\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n'","Elegant, flexible data logging in Python for connected sensors and instruments."
https://github.com/bdhammel/machine-learning-models,"b'# Basic machine learning models for reference\n\nAlgorithms benchmarked to the MNIST dataset\n\n## Principle Component Analysis (PCA)\n\n![VGG16](./media/pca.png)  \nNaive Bayes accuracy: **56%**\n\n## Artificial Neural Network (ANN)\n\nSimple fully-connected network\n\nMNIST accuracy: **96%**\n\n## Convolutional Neural Network (CNN)\n\nSimple convolutional neural network\n\nMNIST accuracy: **98%**\n\nNotes: Slow, only ran for 2 epoch. \n\n## Convolutional Neural Network (VGG16)\n\nImplementation of the VGG16 architecture with pretrained MNIST data\n\n![VGG16](./media/vgg16.png)  \n<small> cite: https://www.cs.toronto.edu/~frossard/post/vgg16/ </small>\n\n*Note:* Because MNIST is 28x28, first 10 layers omitted and layer depth made a factor of 3 smaller (as MNIST has only 1 color layer)\n\nMNIST accuracy: \n\n## Stacked Autoencoder (X-wing AE)\n\nA symmetric autoencoder for unsupervised learning \n\nNotes: Success is largely dependent on layer sizes, seems to have a small window of convergence\n\n![VGG16](./media/xwing.png)\n\n## Convolutional Autoencoder (CAE)\n\nA symmetric convolutional autoencoder\n\n![Convolutional Auto Encoder](./media/cae.png)\n\n## Variational Autoencoder (VAE)\n\n![Variational Auto Encoder](./media/vae.png)\n\n## Generative Adversarial Network (DCGAN)\n\nA Deep Convolutional GAN\n\n - [""Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks\xe2\x80\x9d](https://arxiv.org/pdf/1511.06434v2.pdf)\n\n![](./media/dcgan.png)\n\n## Recurrent Neural Network (RNN)\n\nTODO: Add in additional hidden layers\n\n### Basic RNN cell\nMNIST accuracy: **98%**   \nepochs: 100\n\n### LSTM cell\nMNIST accuracy: **95%**   \nepochs: 10\n'",Tensorflow scripts of different machine learning algorithms 
https://github.com/rjleveque/amath584a2016,"b'# amath584a2016\n\nUW Applied Math 584, Autumn 2016\n\nSee [class webpages](http://faculty.washington.edu/rjl/classes/am584a2016/)\nfor a rendered version of the files in the `sphinx` directory.\n\nSee\n[github](https://github.com/rjleveque/amath584a2016/tree/master/index.ipynb)\nfor rendered versions of the Jupyter notebooks.\n\nTo run the notebooks on the cloud you can use [binder](http://mybinder.org)\nby clicking on the lauch icon below:\n\n[![Binder](http://mybinder.org/badge.svg)](http://mybinder.org:/repo/rjleveque/amath584a2016)\n\n'","UW Applied Math 584, Autumn 2016"
https://github.com/arve0/microscopestitching,"b""# microscopestitching\n\n[![build-status-image]][travis]\n[![pypi-version]][pypi]\n[![wheel]][pypi]\n\n## Overview\n\nThis software aims to be a reliable way to stitch your microscope images. To\nget good results, a couple of assumptions should be true about your dataset:\n\n- images are regular spaced\n- images are of same size\n- side by side images have translation only in one dimension\n  - (if not, check your scanning mirror rotation)\n- scale in edge of images are constant\n\n## Installation\n\nInstall using `pip`...\n\n```bash\npip install microscopestitching\n```\n\n## Example\n```python\nfrom microscopestitching import stitch\nfrom skimage.io import imsave\n\nimages = []\nfor i in range(50):\n    row = i // 10\n    col = i % 10\n    images.append(('%d.png' % i, row, col))\n\nmerged = stitch(images)\nimsave('merged.png', merged)\n```\n\nSee also [notebook examples](http://nbviewer.ipython.org/github/arve0/microscopestitching/blob/master/notebooks/).\n\n## API reference\n\nAPI reference is at http://microscopestitching.rtfd.org.\n\n## Development\nInstall dependencies and link development version of microscopestitching to pip:\n```bash\ngit clone https://github.com/arve0/microscopestitching\ncd microscopestitching\npip install -r requirements.txt # install dependencies and microscopestitching-package\n```\n\n### Testing\n```bash\ntox\n```\n\n### Build documentation locally\nTo build the documentation:\n```bash\npip install -r docs/requirements.txt\nmake docs\n```\n\n\n\n[build-status-image]: https://secure.travis-ci.org/arve0/microscopestitching.png?branch=master\n[travis]: http://travis-ci.org/arve0/microscopestitching?branch=master\n[pypi-version]: https://img.shields.io/pypi/v/microscopestitching.svg\n[pypi]: https://pypi.python.org/pypi/microscopestitching\n[wheel]: https://img.shields.io/pypi/wheel/microscopestitching.svg\n""",Automatic merge/stitching of regular spaced images
https://github.com/marisarivera/Arrivals,b'# Arrivals\nAn\xc3\xa1lisis Preliminar\n',Análisis Preliminar
https://github.com/ericinlinux/BDiSN_Assignment_6_old,"b'# Glascow research on smoking habits in teenagers\n\nAll the data was taken from [Description \'Teenage Friends and Lifestyle Study\' data](https://www.stats.ox.ac.uk/~snijders/siena/Glasgow_data.htm).\n\nThis folder contains the data and the Python code for the data set provided at http://tinyurl.com/hunndyg\n\n## The matrix of friendships\n\nWe have three matrices with the question about the relations in 3 different years.\n\nThe data are valued; code 1 stands for ""best friend"", code 2 for ""just a friend"", and code 0 for ""no friend"".\nCode 10 indicates structural absence of the tie, i.e., at least one of the involved students was not yet part of the\nschool cohort, or had already left the school cohort at the given time point.\n\nFor this work, we will make the following changes:\n\n* code 1 = 0.9\n* code 2 = 0.5\n* code 10 = 0\n* code 0 = 0.1\n\n## The information about smoking habits\n\nTobacco use has the scores 1 (non), 2 (occasional) and 3 (regular, i.e. more than once per week). So for this pattern\nthe values are initialized with values 0.1, 0.5 and 0.9 respectively.\n\nThe idea of this code is to simulate the opinion change over time according to the initial values and the network\nprovided by the data set.\n\n## The assignment\n\nThe document for the assignment is [here](Assignment06v2.pdf).\n'",Deprecated.
https://github.com/kho226/nueralNetwork,"b'# NeuralNetwork\n## A neural network built with the purpose of recognizing handwritten characters.\n###### A brief overview...\nThe concepts that underpin neural Networks are both **mathematical and cognitive in nature.** Mathematical concepts include **elements of linear algebra such as matrix multiplication and the transpose of a matrix, elements of calculus such as the derivative and gradient descent, elements of statistics such as the normal distribution and the standard deviation, and elements of algebra such as slope-intercept form** and **cognitive concepts such as a basic understanding of the structure of a neuron.**\n\nComputers are effectively extremely fast calculators, which makes them perfect for carrying out calculations.\nHumans brains are technically slower than calculators, yet we are capable of doing things that computers cannot; such as recognizing handwritten characters or even faces.\n\nNeuralNetworks attempt to bridge the gap between the human brain and computers; allowing us to imbue computers with almost human-like cognitive abilities. Such as the ability to effectively teach itself.\n\nThe neuron is the most basic building block of intelligent life on earth. Neuron\'s consist of **dendrites, axons and terminals.** A dendrite of one neuron will connect to the terminal of another neuron, and within each neuron axons connect dendrites and terminals. Neurons are effectively the circuitry of our brains. They take an electrical signal as input and output another electrical signal. The output of a neuron does not share a linear relationship with it\'s input. Instead, a neuron will suppress it\'s output until the input crosses a threshold. An **activation function** is a function that takes an input signal and generates and output signal, but takes into account some threshold. I will be using the **Sigmoid function** or **logistic function** but there are many other candidates to use for the **activation function**.\n\nWe can model the neurons of a human brain with a matrix of nodes, where each node in the matrix represents a neuron. Take for example a 2 x 2 matrix of nodes. There are a total of 4 nodes. Each node in the first column has a link to every node in the second column. Each of these **links have an associated weight / probability** as well as an **activation function**. The activation functions and link weights determine how information travels through the matrix.\n\nInformation can **propagate forward** and **propagate backward** through the matrix of nodes.\n\nPropagating information forward is carried out by applying the **activation function** to the **dot product** of the **input matrix** and the matrix representing the link weights between the **input layer** and the **hidden layer**. Repeating this algorithm for every matrix of link weights in the network. The resulting matrix we will refer to as the **output Matrix**\n\nPropagating information backwards is carried out in a similar fashion as propagating information forward. Take the **dot product** of the **transpose of the matrix of respective link weights** and the **error matrix**. The **error matrix** is the difference between the **output Matrix** and the **matrix of expected values**.\n\n***A neural network can learn.***\n\nA neural network learns by **adjusting the weights associated with the links**. Link weights are adjusted using **gradient descent**.\n\n#TODO...\n- [x] write skeleton of class\n- [x] wrote display function for testing purposes\n- [x] add numpy to package\n- [x] updated initializer to create matrices of link weights\n- [x] updated initializer to randomly select link weights based off of the normal distribution with mean = 0 and standard deviation = (# of incoming links to a node)^(-1/2)\n- [x] add scipy to the package\n- [x] write query method\n- [x] write train method\n- [x] updated GUI\n- [x] added main function\n- [x] clean up code in main()\n- [x] format README.md\n- []  build a basic website around the network\n\n![Alt text](/screenshots/usage.png?raw=true ""Usage"")\n\n# [Jupyter / IPython quickstart](http://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/install.html)\n> moved development to Jupyter / IPython since the Python IDLE does not support matplotlib interactive updates\n'",A nueral network built with the purpose of recognizing handwritten letters
https://github.com/paulovn/docker-dl-gpu,b'# docker-dl-gpu\nBuild a Docker image for Deep Learning processes running on GPUs\n',Build a Docker image for Deep Learning processes running on GPUs
https://github.com/priyaranjan1202/Machine-Learning,"b'## Machine-Learning\nThis repository contains  Machine learning projects I have been working on.Currently it contains projects from Udacity Nanodegree.\n\n## Requirments and Installation\n* All the code is written using python 2.\n* Each project cotains Related Data, IPYTHON notebook solution and supporting files.\n* Anaconda distribution server is recommened.\n* Open **COMMAND PROMPT** or **TERMINAL** and type `jupyter notebook` to start the server.\n* Navigate to required project and open corresponding .ipynb notebook. \n* Run each indivual cell to understand the project.\n\n\n\n   \n   \n'",This repository contains  Machine learning projects I have been working on.Currently it contains projects from Udacity Nanodegree.
https://github.com/selvakumar-sss/FirstGitProject,b'# FirstGitProject\nLoading working jupyter notebooks\n',Loading working jupyter notebooks
https://github.com/miguel-faria/robot-serving,b'# robot-serving\r\nThis is the repo for my master thesis project on social robotics.\r\n',Master Thesis on Human-Robot Collaboration
https://github.com/RaoOfPhysics/phd-notebooks,"b'# My PhD notebooks\n\n[![Achintya\'s PhD repos](https://img.shields.io/badge/collection-Achintya\'s%20PhD%20repos-yellowgreen.svg)](https://github.com/RaoOfPhysics/phd)\n![language: R](https://img.shields.io/badge/language-R-blue.svg)\n![status: WIP](https://img.shields.io/badge/status-WIP-red.svg)\n\nA collection of my PhD-related Jupyter notebooks, containing mostly WIP material, which serves as a scratchpad for my work.\n\n## Setup information\n\nI work mostly with [R](https://www.r-project.org/), and that\'s what I\'ve used in these notebooks.\n\nPlease note that efforts to run the code in the `.ipynb` files involving the `dat` data frame **will fail**, since I cannot make the underlying research data public yet.\nIn particular, when the following line is called in the code, the file it refers to (`final_dataset.csv`) can only be found in my private storage space on CERN\'s internal servers:\n\n```{r}\ndat <- read.csv(file = ""../quant_analysis/final_dataset.csv"", header = TRUE)\n```\n\nHowever, if you want to download the notebooks and poke around, you will need to install:\n\n- Jupyter, for opening the notebooks themselves: [http://jupyter.readthedocs.io/en/latest/install.html](http://jupyter.readthedocs.io/en/latest/install.html)\n- IRkernel, for running R code in the notebooks: [https://irkernel.github.io/installation/](https://irkernel.github.io/installation/)\n\n## Licence\n\n<a rel=""license"" href=""http://creativecommons.org/licenses/by-sa/4.0/""><img alt=""Creative Commons License"" style=""border-width:0"" src=""https://i.creativecommons.org/l/by-sa/4.0/80x15.png"" /></a><br /><span xmlns:dct=""http://purl.org/dc/terms/"" href=""http://purl.org/dc/dcmitype/InteractiveResource"" property=""dct:title"" rel=""dct:type"">My PhD notebooks</span> by <a xmlns:cc=""http://creativecommons.org/ns#"" href=""https://github.com/RaoOfPhysics/201608_ICHEP"" property=""cc:attributionName"" rel=""cc:attributionURL"">Achintya Rao</a> is licensed under a <a rel=""license"" href=""http://creativecommons.org/licenses/by-sa/4.0/"">Creative Commons Attribution-ShareAlike 4.0 International License</a>.\n'",R Notebooks for my PhD research
https://github.com/chrisjsewell/pandas3js,"b'**NOTE: this is deprecated, in favour of https://github.com/chrisjsewell/ase-notebook**\n\n[![Build Status](https://travis-ci.org/chrisjsewell/pandas3js.svg?branch=master)](https://travis-ci.org/chrisjsewell/pandas3js)\n[![Coverage Status](https://coveralls.io/repos/github/chrisjsewell/pandas3js/badge.svg?branch=master)](https://coveralls.io/github/chrisjsewell/pandas3js?branch=master)\n[![PyPI](https://img.shields.io/pypi/v/pandas3js.svg)](https://pypi.python.org/pypi/pandas3js/)\n\n# pandas3js: 3D Graphics UIs in the Jupyter Notebook\n\nAn extension for [traitlets](https://traitlets.readthedocs.io/en/stable/index.html) and [pythreejs](https://github.com/jovyan/pythreejs) that:\n\n1. Provides a 2-way [pandas](http://pandas.pydata.org/) dataframe interface for trait objects.\n2. Provides simple, high level (renderer agnostic) geometries, with default json specified mappings to pythreejs primitives.\n3. Creates bespoke 3D Graphics GUIs in the Jupyter Notebook with only a few lines of code.\n    \n## Examples\n\n![pandas3js_example.ipynb](/pandas3js_example.ipynb)\n\n![IPYNB Example](/pandas3js_example.gif)\n\nFor more information, all functions contain docstrings with tested examples.\n\n## Installation\n\n    $ pip install pandas3js\n    $ jupyter nbextension enable --py --sys-prefix pythreejs\n\t\n`pandas3js` is integration tested against python versions 2.7, 3.4, 3.5 and 3.6\n\n## Technical Details\n\nEmploying a meta Model/View design; Unique geometry objects are stored in a `GeometryCollection` **model** object, \nwhich can be viewed as (and modified by) a `pandas.DataFrame`, containing objects (by row) and traits/object_type (by column). \nThe `GeometryCollection` (and its objects) can then be directionally synced to a `pythreejs.Scene` (and `pythreejs.3DObject`s) **view**, *via* a json mapping specification.\n'",a pandas dataframe interface for traitlets and pythreejs
https://github.com/pprzetacznik/IElixir,"b'IElixir\n=======\n\nJupyter\'s kernel for Elixir\n\n[![Build Status](https://travis-ci.org/pprzetacznik/IElixir.svg?branch=master)](https://travis-ci.org/pprzetacznik/IElixir)\n[![IElixir CI](https://github.com/pprzetacznik/IElixir/workflows/IElixir%20CI/badge.svg)](https://github.com/pprzetacznik/IElixir/actions?query=workflow%3A""IElixir+CI"")\n[![IElixir Docker](https://github.com/pprzetacznik/IElixir/workflows/IElixir%20Docker/badge.svg)](https://github.com/pprzetacznik/IElixir/actions?query=workflow%3A""IElixir+Docker"")\n[![Inline docs](http://inch-ci.org/github/pprzetacznik/IElixir.svg?branch=master)](http://inch-ci.org/github/pprzetacznik/IElixir)\n[![Coverage Status](https://coveralls.io/repos/github/pprzetacznik/IElixir/badge.svg?branch=master)](https://coveralls.io/github/pprzetacznik/IElixir?branch=master)\n[![Join the chat at https://gitter.im/pprzetacznik/IElixir](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/pprzetacznik/IElixir?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\nHex: https://hex.pm/packages/ielixir.\n\nPlease see generated documentation for implementation details: http://hexdocs.pm/ielixir/.\n\n## Getting Started\n\n### Table of contents\n\n* [Configure Jupyter](#configure-jupyter)\n* [Configure IElixir](#configure-ielixir)\n* [Install Kernel](#install-kernel)\n* [Use IElixir](#use-ielixir)\n* [Package management with Boyle](#package-management-with-boyle)\n* [Developement mode](#developement-mode)\n* [Generate documentation](#generate-documentation)\n* [Docker](#docker)\n* [Some issues](#some-issues)\n* [Contribution](#contribution)\n* [References](#references)\n* [License](#license)\n\n### Configure Jupyter\n\nI recommend you to use `virtualenv` and `virtualenvwrapper` for this project to isolate dependencies between this and other projects however you may also work without this if you don\'t like this.\n```Bash\n$ pip install virtualenv virtualenvwrapper\n```\nNow you need to load `virtualenvwrapper.sh` script into your current environment. I recommend you to add this like as well to the `~/.bash_profile.sh` script to have this script loaded every time you open fresh bash.\n```Bash\n$ source /usr/local/bin/virtualenvwrapper.sh\n```\n\nNow using our new tools we can easily create isolated virtual environment for jupyter installation.\n```Bash\n$ mkvirtualenv jupyter-env\n$ workon jupyter-env\n(jupyter-env) $ pip install jupyter\n```\n\n### Configure IElixir\n\nClone IElixir repository and prepare the project\n```Bash\n$ git clone https://github.com/pprzetacznik/IElixir.git\n$ cd IElixir\n$ mix deps.get\n$ mix test\n$ MIX_ENV=prod mix compile\n```\n\nIf you\'re meeting problem with missing zeromq header files then you should install it. [More](#zeromq-header-files-missing).\n\nRunning all tests, including longer ones that requires more time for evaluation:\n```Bash\n$ mix test --include skip\n```\n\nThere may be also need to install rebar before IElixir installation, you can do this with command:\n```Bash\nmix local.rebar --force\n```\nAfter this you may need to add `~/.mix/` to your `$PATH` variable if you don\'t have `rebar` visible yet outside `~/.mix/` directory.\n\n### Install Kernel\n\nSimply run installation script to create file `kernel.json` file in `./resouces` directory and bind it to the jupyter:\n```Bash\n$ ./install_script.sh\n```\n\n### Use IElixir\n\nRun Jupyter console with following line:\n```Bash\n(jupyter-env) $ jupyter console --kernel ielixir\n```\n\nTo quit IElixir type `Ctrl-D`.\n\nRun Jupyter Notebook with following line:\n```Bash\n(jupyter-env) $ jupyter notebook resources/example.ipynb\n```\n\nGo to [http://localhost:8888/](http://localhost:8888/) site (by default) in your browser and pick IElixir kernel:\n\n![Pick IElixir](/resources/jupyter_pick_kernel.png?raw=true)\n\nEvaluate some commands in your new notebook:\n\n![IElixir basics](/resources/jupyter_ielixir_matrex_demo.png?raw=true)\n\n### Magic commands\n\nYou can also use `ans` variable to access output of previous cell. Moreover, if you want to access any cell which you can do it using it\'s number by calling `out` map, eg. `out[1]`.\n\n### Package management with Boyle\n\nYou can manage your packages in runtime with Boyle. Name of the package honours remarkable chemist, Robert Boyle. This package allows you to manage your Elixir virtual enviromnent without need of restarting erlang virtual machine. Boyle installs environment into `./envs/you_new_environment` directory and creates new mix project there with requested dependencies. It keeps takes care of fetching, compiling and loading/unloading modules from dependencies list of that environment.\n\nYou can also use this environment as a separate mix project and run it interactively with `iex -S mix` from the environment directory.\n\n\nCreating new Elixir virtual environment\n```Elixir\niex> Boyle.mk(""my_new_environment"")\n{:ok, [""my_new_environment""]}\n```\n\nList available virtual environments\n```Elixir\niex> Boyle.list()\n{:ok, [""my_new_environment""]}\n```\n\nActivate virtual environment\n```Elixir\niex> Boyle.activate(""my_new_environment"")\n:ok\n```\n\nInstall new package in virtual environment and use new package\n```Elixir\niex> Boyle.install({:number, ""~> 0.5.7""})\n:ok\niex> Number.Currency.number_to_currency(2034.46)\n""$2,034.46""\n```\n\nDeactivate virtual environment and unload packages installed within that virtual environment\n```Elixir\niex> Boyle.deactivate()\n:ok\niex> Number.Currency.number_to_currency(2034.46)\n** %UndefinedFunctionError{arity: 1, exports: nil, function: :number_to_currency, module: Number.Currency, reason: nil}\n```\n\nAdditional resources:\n* [Notebook with Boyle examples](https://github.com/pprzetacznik/IElixir/blob/master/resources/boyle%20example.ipynb)\n* [Notebook with Boyle examples with usage of Matrex library](https://github.com/pprzetacznik/IElixir/blob/master/resources/boyle%20example%20-%20matrex%20installation%20and%20usage.ipynb)\n* [Notebook with inline image examples with usage of Gnuplot library](https://github.com/pprzetacznik/IElixir/blob/master/resources/inlineplot%20example%20-%20gnuplot.ipynb)\n\n### Developement mode\n\nIf you want to see requests passing logs please use `dev` environment to see what is happening in the background.\n\n```Bash\n(jupyter-env) $ MIX_ENV=dev jupyter console --kernel ielixir\n```\n\n### Generate documentation\n\nRun following command and see `doc` directory for generated documentation in HTML:\n```Bash\n$ MIX_ENV=docs mix docs\n```\n\n### Docker\n\nYou can find docker image at [pprzetacznik/ielixir](https://hub.docker.com/r/pprzetacznik/ielixir).\n\nRunning jupyter notebook:\n```\n$ docker run -p 8888:8888 --hostname localhost -v /my/workspace:/home/jovyan/work pprzetacznik/ielixir\n```\n\nDocker image is based on following images:\n* [jupyter/base-notebook image](https://hub.docker.com/r/jupyter/base-notebook/) - this is image use as a base for ielixir image,\n* [elixir image](https://hub.docker.com/_/elixir/) - some installation parts were taken from dockerfile used for this image,\n* [pprzetacznik/ielixir-requirements image](https://hub.docker.com/r/pprzetacznik/ielixir-requirements/) - this image resolves all dependencies for jupyter and elixir so only IElixir installation is left.\n\nIf you would like to make some changes to the images you can find dockerfiles in:\n* docker/ielixir - for dockerfile source of pprzetacznik/ielixir image,\n* docker/ielixir-requirements - for dockerfile source of pprzetacznik/ielixir-requirements image.\n\n#### Other docker images worth seeing\n\n* [Dockerfile for smaller image based on alpine](https://github.com/cprieto/docker-jupyter/blob/master/elixir/Dockerfile)\n\n### Some issues\n\n#### ZeroMQ header files missing\n\n```\n===> /home/travis/build/bryanhuntesl/IElixir/deps/erlzmq/c_src/erlzmq_nif.c:24:17: fatal error: zmq.h: No such file or directory\n #include ""zmq.h""\n                 ^\ncompilation terminated.\n```\n\nInstall ZeroMQ development package for you operating system.\n\n* On RHEL/Centos/Fedora `dnf install zeromq-devel`\n* On OSX `brew install zmq`\n* On Ubuntu `apt-get install libzmq3-dev`\n* On Alpine Linux `apk add zeromq-dev`\n\n#### Erlang configuration\n\nThere may be need to run IElixir kernel with specific erlang attribute which can be turned on by setting variable:\n```Bash\nELIXIR_ERL_OPTIONS=""-smp enable""\n```\nThis option has been included inside `install_script.sh` and `start_script.sh` scripts.\n\n### Contribution\n\n* Try to write some description of the feature or bug fix you\'re working in pull request\'s description and concise description of new modules or functions in moduledoc annotations,\n* Please follow Elixir style guides to keep style clear, consider Elixir and Phoenix source code as the style ground truth,\n* Keep as little comments as you can, comments tend to expire so try to use doctests instead to show how your code works,\n* Write some unit tests for your code but don\'t try to test private functions, class tests are bad and units tests are good - https://blog.arkency.com/2014/09/unit-tests-vs-class-tests/\n\n### References\n\nSome useful articles:\n\n* [IElixir Notebook in Docker](https://mattvonrocketstein.github.io/heredoc/ielixir-notebook-in-docker.html)\n* [Hydrogen plugin for Atom](https://atom.io/packages/hydrogen)\n* [Installation guide](http://blog.jonharrington.org/elixir-and-jupyter/)\n* [Jupyter Notebooks with Elixir and RDF](https://medium.com/@tonyhammond/jupyter-notebooks-with-elixir-and-rdf-598689c2dad3)\n* [Making Graphs using Elixir inside Jupyter Notebook snapshot from 2019/04 on Internet Archive](https://web.archive.org/web/20191228085409/http://aipotato.com/2019/04/making-graphs-using-elixir-inside-jupyter-notebook/)\n\nI was inspired by following codes and articles:\n\n* [https://github.com/pminten/ielixir](https://github.com/pminten/ielixir)\n* [https://github.com/robbielynch/ierlang](https://github.com/robbielynch/ierlang)\n* [https://github.com/dsblank/simple_kernel](https://github.com/dsblank/simple_kernel)\n* [http://andrew.gibiansky.com/blog/ipython/ipython-kernels/](http://andrew.gibiansky.com/blog/ipython/ipython-kernels/)\n* [https://ipython.org/ipython-doc/dev/development/messaging.html](https://ipython.org/ipython-doc/dev/development/messaging.html)\n\n### License\n\nCopyright 2015 Piotr Przetacznik.\nIElixir source code is released under Apache 2 License.\nCheck [NOTICE](NOTICE) and [LICENSE](LICENSE) files for more information.\n'",Jupyter's kernel for Elixir programming language
https://github.com/Pavle992/Cash-me-outside,b'# Data Mining and Text Mining Course Project\n\n### Team: Cash-me-outside\n\n### Description\nThe repo contains [Python notebook][2] created as a result of [Data Mining and Text mining Course Project][1] at Politecnico di Milano in academic year 2016/2017.\n\n### Documentation\n+ [Project Presentation][4]\n+ [Description of our work][3]\n\n[1]: ./CourseProjectDescription.pdf \n[2]: ./CashMeOutside%20-%20Data%20Mining%20Project/Data%20Mining%20and%20Text%20Mining%20Course%20Project.ipynb\n[3]: ./CashMeOutside%20-%20Data%20Mining%20Project/DataMiningandTextMiningCourseProject.pdf\n[4]: ./CashMeOutside%20-%20Data%20Mining%20Project/Project%20Presentation.pptx.pptx',How about that!?
https://github.com/mahkuss/inflammation,b'# Inflammation Project\n\nThis is the inflammation project\n',inflammation project
https://github.com/ilevcovitz/econ_tutoring_sign_in,"b""##Read Me\n\nThis is a repository for the web-based tutoring sign-in system for the Economics Tutoring center. The software provides a framework for students to sign in when visiting the tutoring center. \n\nAccurate student attendance information is necessary in order to research the effectiveness of the tutoring center and learn how to improve it. \n\nThe software addionally allows instructors to view which of their course's students visited the tutoring center. This real-time attendance information gives instructors the possibility to provide incentives for students to visit tutoring. \n\nAnnonymouse attendance information is also displayed in an online ipython notebook which auto-updates daily.\n\nThe software runs under a Python Flask framework and mySQL database.\n\n###Guides\n\nInformation on how to setup and use this software is provided in this repo in the documents: \n\nINITIAL_SETUP_GUIDE.md (for initial setup)\n\nDATABASE_SETUP.md (for a guide on the mysql database structure)\n""",Sign In for the Econ Tutoring Center
https://github.com/brent-lemieux/p2-baseball,"b""# Baseball: Salaries and Wins\nAn investigation into whether or not the same factors that are correlated with players contributions to winning games are also correlated with salaries.  The goal is to identify any player types (or individuals) that are not valued properly by the MLB.\n\nI began by trimming the data to exclude years before 1980 because I am primarily concerned with the modern game.  I also created new metrics that are popular among stats minded teams and fans.  Read more here: https://en.wikipedia.org/wiki/Sabermetrics.\n\n### What factors are important to winning games?\n\nFirst I looked at the distribution of team wins for the data from 1980 on.  I expected the plot to be normal, but it is fairly negatively skewed.  It turns out this was mainly due to the player strike in 1994 that led to the season being shortened.\n\n![Wins Histogram](/images/winshisto.png)\n\n\nHere is what the distribution looks like with 1994 removed:\n\n![Wins Histogram2](/images/winshisto2.png)\n\nThe difference is subtle, but you can easily tell that there are far fewer teams with less than 60 wins.\n\nFor each team metric, I wanted to see what variables were most related to winning percentage.  I ran a simple linear regression on each metric with win percentage as the dependent variable.  It's worth noting that each variable is standardized.  As you can see, Runs and Runs Against have the highest and lowest coefficients, respectively.  This makes intuitive, because the more runs you score and the less runs you allow on average, the higher you'd expect your win percentage to be.  It's also worth noting that On Base Percentage (OBP), Slugging Percentage (SP), and OPS (On Base Plus Slugging) are all near the top.  These are the metrics I created using other metrics that already existed in the data.  It makes sense why these are so popular among stats gurus across MLB front offices.\n\n![Linear Regression Coefficients](/images/coefficients.png)\n\nI also ran a Random Forest Regressor on all of the metrics at once and charted the feature importance or the information gain from each metric.  Once again, Runs and Runs Against are the most important features, with Earned Run Average (ERA) coming in third.  ERA is essentially the rate at which a pitcher allows runs that are explicitly that pitchers fault, so runs allowed due to fielding errors are not factored in.  Since most runs allowed are indeed the pitchers fault, it makes sense that this comes in third for feature importance.\n\n![Random Forest Feature Importances](/images/feature_importances.png)\n\n\n### What factors are important to scoring runs?\nBecause scoring runs is such an important part of winning games, we will now look at what factors are important to scoring runs at the team level.  After we have a good idea of what metrics correlate with scoring, we will be able to look at individual player data to see how those important factors lineup with salaries.\n\n# Work in progress...\n""",An investigation into whether or not the same factors that are correlated with scoring runs are also correlated with player salaries
https://github.com/grigi/pyza14-docs,b'# How Python helps writing documentation less painful\nPyConZA 2014 Documentation presentation.\n\n## Slides:\n[View Slides Online](http://rawgit.com/grigi/pyza14-docs/master/Documentation.slides.html) \n\n',PyConZA 2014 Documentation presentation.
https://github.com/kamidox/pandas_tutor,b'## Pandas \xe6\x95\x99\xe7\xa8\x8b\xe7\xa4\xba\xe4\xbe\x8b\xe4\xbb\xa3\xe7\xa0\x81\n\nThis is the sample code for Tutorial for Pandas hosted in http://www.maiziedu.com/\n\n## \xe8\xaf\xbe\xe7\xa8\x8b\xe6\x80\x9d\xe7\xbb\xb4\xe5\xaf\xbc\xe5\x9b\xbe\n\nhttp://naotu.baidu.com/file/5eba96c2d922e30b7a4bf6b74c638dd0?token=440f2f0ff8c8b88f\n\n\xe6\x8f\x90\xe5\x8f\x96\xe5\xaf\x86\xe7\xa0\x81\xef\xbc\x9a706V\n\n\n',Tutorial for Pandas
https://github.com/youralien/sign_follower,"b'# Sign Follower\n\nBy the end of the Traffic Sign Follower project, you will have a robot that will look like this!\n\n**Video Demo:** [NEATO ROBOT OBEYS TRAFFIC SIGNS](https://youtu.be/poReVhj1lSA)\n\n## System\n![alt text][system-overview]\n\n[system-overview]: images/vision-nav-system-overview.png ""Three stages of the vision and navigation system: 1) waypoint navigation 2) sign recognition, and 3) sign obeyance via changing the next waypoint""\n\nNavigation and mapping is handled by the built-in ROS package ```neato_2dnav``` .  Mapping the location of the robot in the environment was handled by [```gmapping```](http://wiki.ros.org/gmapping), a package that provides laser-based SLAM (simultaneous localization and mapping).  Navigation was handled by the [```move_base```](http://wiki.ros.org/move_base) package;   our program published waypoints to the ```/move_base_simple/goal``` topic while the internals of path planning and obstacle avoidance were abstracted away.\n\nYou will put your comprobo-chops to the test by developing a sign detection node which publishes to a topic, ```/predicted_sign```, once it is confident about recognizing the traffic sign in front of it.\n\n### Running and Developing the street_sign_recognizer node\n\nWe\'ve provided some rosbags that will get you going.\n\n[uturn.bag](https://drive.google.com/open?id=0B85lERk460TUYjFGLVg1RXRWams)\n[rightturn.bag](https://drive.google.com/open?id=0B85lERk460TUN3ZmUk15dmtPTFk)\n[leftturn.bag](https://drive.google.com/open?id=0B85lERk460TUTkdTQW5yQ0FwSEE)\n\nStart by replaying these rosbags on loop:\n\n```\nrosbag play uturn.bag -l\n```\n\nThey have `/camera/image_raw/compressed` channel recorded. In order to republish the compressed image as a raw image,\n\n```\nrosrun image_transport republish compressed in:=/camera/image_raw _image_transport:=compressed raw out:=/camera/image_raw\n```\n\nTo run the node,\n\n```\nrosrun sign_follower street_sign_recognizer.py\n```\n\nIf you ran on the steps above correctly, a video window should appear visualizing the Neato moving towards a traffic sign.\n\n### Detecting Signs in Scene Images\nReliable detection of traffic signs and creating accurate bounding box crops is an important preprocessing step for further steps in the data pipeline.\n\nYou will implement code that will find the bounding box around the traffic sign in a scene. We\'ve outlined a suggested data processing pipeline for this task.\n\n1. colorspace conversion to hue, saturation, value (```hsv_image``` seen in the top left window).\n2. a filter is applied that selects only for objects in the yellow color spectrum. The range of this spectrum can be found using hand tuning (```binarized_image``` seen in the bottom window).\n3. a bounding box is drawn around the most dense, yellow regions of the image.\n\n![][yellow_sign_detector]\n[yellow_sign_detector]: images/yellow-sign-detector.gif ""Bounding box generated around the yellow parts of the image.  The video is converted to HSV colorspace, an inRange operation is performed to filter out any non yellow objects, and finally a bounding box is generated.""\n\nYou will be writing all your image processing pipeline within the `process_image` callback function. Here is what the starter code looks like so far.\n\n```python\n    def process_image(self, msg):\n        """""" Process image messages from ROS and stash them in an attribute\n            called cv_image for subsequent processing """"""\n        self.cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding=""bgr8"")\n\n        left_top, right_bottom = self.sign_bounding_box()\n        left, top = left_top\n        right, bottom = right_bottom\n\n        # crop bounding box region of interest\n        cropped_sign = self.cv_image[top:bottom, left:right]\n\n        # draw bounding box rectangle\n        cv2.rectangle(self.cv_image, left_top, right_bottom, color=(0, 0, 255), thickness=5)\n```\n\nThe goal of localizing the signs in the scene is to determine `left_top = (x1,y1)` and `right_bottom = (x2,y2)` points that define the upper lefthand corner and lower righthand corner of a bounding box around the sign. You can do most of your work in the instance method `sign_bounding_box`.\n\n```python\n    def sign_bounding_box(self):\n        """"""\n        Returns\n        -------\n        (left_top, right_bottom) where left_top and right_bottom are tuples of (x_pixel, y_pixel)\n            defining topleft and bottomright corners of the bounding box\n        """"""\n        # TODO: YOUR SOLUTION HERE\n        left_top = (200, 200)\n        right_bottom = (400, 400)\n        return left_top, right_bottom""\n```\n\nWhether you follow along with the suggested steps for creating a sign recognizer or have ideas of your own, revisit these questions often when designing your image processing pipeline:\n\n* What are some distinguishing visual features about the sign?  Is there similarities in color and/or geometry?\n* Since we are interested in generating a bounding box to be used in cropping out the sign from the original frame, what are different methods of generating candidate boxes?\n* What defines a good bounding box crop?  It depends a lot on how robust the sign recognizer you have designed.\n\nFinally, if you think that working with individual images, outside of the `StreetSignRecognizer` class would be helpful -- I often like to prototype the computer vision algorithms I am developing in a jupyter notebook -- feel free to use some of the image frames in the `images/` folder.  In addition, you can save your own images from the video feed by using OpenCV\'s [`imwrite` method](http://docs.opencv.org/2.4/modules/highgui/doc/reading_and_writing_images_and_video.html?highlight=imwrite#imwrite).\n\n#### Red-Green-Blue to Hue-Saturation-Value Images\n\nThere are different ways to represent the information in an image. A gray-scale image has `(n_rows, n_cols)`. An rgb image has shape `(n_rows, n_cols, 3)` since it has three channels: red, green, and blue (note: as you saw in class, and it is the case with the given starter code, that OpenCV uses the channel ordering blue, green, red instead).\n\nColor images are also represented in different ways too.  Aside from the default RGB colorspace, there exists alot of others. We\'ll be focused on using [HSV/HSL](https://en.wikipedia.org/wiki/HSL_and_HSV): Hue, Saturation, and Value/Luminosity. Like RGB, a HSV image has three channels and is shape `(n_rows, n_cols, 3)`. The hue channel is well suited for color detection tasks, because we can filter by color on a single dimension of measurement, and it is a measure that is invariant to lighting conditions.\n\n[OpenCV provides methods to convert images from one color space to another](http://docs.opencv.org/2.4/modules/imgproc/doc/miscellaneous_transformations.html#cvtcolor).\n\nA good first step would be convert `self.cv_image` into an HSV image and visualize it. Like any good roboticist, visualize everything to make sure it meets your expectations.  Note: if you are using OpenCV 3.1 (which is the case for anyone on Kinetic and Ubuntu 16.04), make sure to never called cv2.imshow from one of your sensor callback threads.  You should only ever call it from the main thread.\n\n#### Filtering the image for only yellow\n\nSince the set of signs we are recognizing are all yellow, by design, we can handtune a filter that will only select the certain shade of yellow in our image.\n\nHere\'s a callback that will help to display the RGB value when hovering over the image window with a mouse (Note: you get this behavior for free with OpenCV 3.1).\n\n```python\n    def process_mouse_event(self, event, x,y,flags,param):\n        """""" Process mouse events so that you can see the color values associated\n            with a particular pixel in the camera images """"""\n        self.image_info_window = 255*np.ones((500,500,3))\n\n        # show hsv values\n        cv2.putText(self.image_info_window,\n                    \'Color (h=%d,s=%d,v=%d)\' % (self.hsv_image[y,x,0], self.hsv_image[y,x,1], self.hsv_image[y,x,2]),\n                    (5,50), # 5 = x, 50 = y\n                    cv2.FONT_HERSHEY_SIMPLEX,\n                    1,\n                    (0,0,0))\n\n        # show bgr values\n        cv2.putText(self.image_info_window,\n                    \'Color (b=%d,g=%d,r=%d)\' % (self.cv_image[y,x,0], self.cv_image[y,x,1], self.cv_image[y,x,2]),\n                    (5,100),\n                    cv2.FONT_HERSHEY_SIMPLEX,\n                    1,\n                    (0,0,0))\n```\n\nIn the `__init__` method, connect this callback by adding the following line:\n\n```python\nself.image_info_window = None\ncv2.setMouseCallback(\'video_window\', self.process_mouse_event)\n```\n\nAnd add the following lines to your run loop:\n\n```python\n            if not self.image_info_window is None:\n                cv2.imshow(\'image_info\', self.image_info_window)\n                cv2.waitKey(5)\n```\n\nNow, if you hover over a certain part of the image, it will tell you what R, G, B value you are hovering over. Once you have created an HSV image, you can edit this function to also display the Hue, Saturation, and Value numbers.\n\nOpenCV windows can be pretty powerful when setting up interactive sliders to change parameters.  As stated in class for Neato soccer, if you want to learn dynamic_reconfigure, you can use that instead of OpenCV\'s trackbars.\n\nIn the `__init__` method, copy the following lines which\n```python\n            cv2.namedWindow(\'threshold_image\')\n            self.hsv_lb = np.array([0, 0, 0]) # hsv lower bound\n            cv2.createTrackbar(\'H lb\', \'threshold_image\', 0, 255, self.set_h_lb)\n            cv2.createTrackbar(\'S lb\', \'threshold_image\', 0, 255, self.set_s_lb)\n            cv2.createTrackbar(\'V lb\', \'threshold_image\', 0, 255, self.set_v_lb)\n            self.hsv_ub = np.array([255, 255, 255]) # hsv upper bound\n            cv2.createTrackbar(\'H ub\', \'threshold_image\', 0, 255, self.set_h_ub)\n            cv2.createTrackbar(\'S ub\', \'threshold_image\', 0, 255, self.set_s_ub)\n            cv2.createTrackbar(\'V ub\', \'threshold_image\', 0, 255, self.set_v_ub)\n```\n\nThen, add the following callback methods to the class definition that respond to changes in the trackbar sliders\n\n```python\n    def set_h_lb(self, val):\n        """""" set hue lower bound """"""\n        self.hsv_lb[0] = val\n\n    def set_s_lb(self, val):\n        """""" set saturation lower bound """"""\n        self.hsv_lb[1] = val\n\n    def set_v_lb(self, val):\n        """""" set value lower bound """"""\n        self.hsv_lb[2] = val\n\n    def set_h_ub(self, val):\n        """""" set hue upper bound """"""\n        self.hsv_ub[0] = val\n\n    def set_s_ub(self, val):\n        """""" set saturation upper bound """"""\n        self.hsv_ub[1] = val\n\n    def set_v_ub(self, val):\n        """""" set value upper bound """"""\n        self.hsv_ub[2] = val\n```\n\nThe sliders will help set the hsv lower and upper bound limits (`self.hsv_lb` and `self.hsv_ub`), which you can then use as limits for filtering certain parts of the HSV spectrum. Check out the OpenCV [inRange method](http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_colorspaces/py_colorspaces.html) for more details on how to threshold an image for a range of a particular color.\n\nBy the end of this step, you should have a binary image mask where all the pixels that are white represent the color range that was specified in the thresholding operation.\n\n#### Generating a bounding box\n\nYou can develop an algorithm that operates on the binary image mask that you developed in the step above.\n\nOne method that could be fruitful would be dividing the image in a grid.  You might want to write a method that divides the image into a binary grid of `grid_size=(M,N)`; if tile in the grid contains a large enough percentage of white pixels, the tile will be turned on.\n\nSince the images are stored as 2D arrays, you can use NumPy-like syntax to slice the images in order to obtain these grid cells. We\'ve provided an example in `grid_image.py` which I\'ll show here:\n\n```python\nimport cv2\nimport os\n\nimgpath = os.path.join(os.path.dirname(os.path.realpath(__file__)),\n                       ""../images/leftturn_scene.jpg"")\nimg = cv2.imread(imgpath)\n\ngrid_cell_w = 64*3\ngrid_cell_h = 48*3\n\ncv2.namedWindow(""my_window"")\n\n# NumPy array slicing!!\ngrid_cell = img[grid_cell_h:2*grid_cell_h,\n                grid_cell_w:2*grid_cell_w] \n\ncv2.imshow(""my_window"", grid_cell)\ncv2.waitKey(0);\n```\n\nThe task now is to decide which grid cells contain the region of interest. You can write another function that takes this binary grid and determines the bounding box that will include all the grid cells that were turned on.\n\n![][grid]\n[grid]: images/grid.png\n\nOpenCV has a method called `boundingRect` which seems promising too.  I found a nice example, albeit in C++, that [finds a bounding rectangle using a threshold mask](http://answers.opencv.org/question/4183/what-is-the-best-way-to-find-bounding-box-for-binary-mask/) like you have.  It seems like it will depend on your thresholding operation to be pretty clean (i.e. no spurious white points, the only object that should be unmasked is the sign of interest).\n\n![][boundingRectStars]\n[boundingRectStars]: images/boundingRectStars.png\n\nThe goal is to produce `left_top = (x1, y1)` and `right_bottom = (x2, y2)` that define the top left and bottom right corners of the bounding box.\n\n## Recognition\n\nRecognizing the signs involves determining how well the cropped image if the sign matches the template image for each type of sign. To do this, we will find keypoints in the template image and in the input image, then see how well we can align the keypoints, and finally see how similar the aligned images are.\n\n### testing\n\nWe have template images as well as static test images for the template matcher in the repository, so we can run the code with the correct template images and try to match them to the static test images we have.\n\nTo do this, first we need to initialize the template matcher with the template images:\n``` python\nif __name__ == \'__main__\':\n    images = {\n        ""left"": \'../images/leftturn_box_small.png\',\n        ""right"": \'../images/rightturn_box_small.png\',\n        ""uturn"": \'../images/uturn_box_small.png\'\n        }\n        \n    tm = TemplateMatcher(images)\n```\nYou can put this at the bottom of the file, and this if statement will mean that this part won\'t run when template_matcher is imported by other files.\n\nNext, we can run `tm.predict` on our test scenes using this:\n``` python\nscenes = [\n    ""../images/uturn_scene.jpg"",\n    ""../images/leftturn_scene.jpg"",\n    ""../images/rightturn_scene.jpg""\n]\n\nfor filename in scenes:\n    scene_img = cv2.imread(filename, 0)\n    pred = tm.predict(scene_img)\n    print filename.split(\'/\')[-1]\n    print pred\n```\nThis reads the test images, runs `tm.predict` on each image, and prints the file name followed by the prediction. Given just the starter code, this should be the output:\n``` bash\nuturn_scene.jpg\n{}\nleftturn_scene.jpg\n{}\nrightturn_scene.jpg\n{}\n```\n\n### finding keypoints\n\nWe are finding keypoints using open cv\'s implementation of the [SIFT algorithm](http://docs.opencv.org/3.1.0/da/df5/tutorial_py_sift_intro.html), then filtering the keypoints ourselves to find the points that match between the input image and each template.\n\nFor the template images, we can calculate the keypoints in the initialization function because the images won\'t chage. To find those keypoints, we can cycle through the input dictionary of template images, read the image files as grayscale images and compute the keypoints using openCV\'s SIFT implementation:\n``` python\nfor k, filename in images.iteritems():\n    # load template sign images as grayscale\n    self.signs[k] = cv2.imread(filename,0)\n\n    # precompute keypoints and descriptors for the template sign \n    self.kps[k], self.descs[k] = self.sift.detectAndCompute(self.signs[k],None)\n```\nThe `predict` method is the ""main"" method of TemplateMatcher. It begins by finding the keypoints in the input image as the first step to matching it with at template. At this point, the template images are initialized and the predict method should run, so you can run the program and it should return predictions for each template image with a zero confidence value. Next, `predict` calls `_compute_prediction` to find how well the input image matches each template image and stores the predictions in a dictionary.\n\n### aligning keypoints\n\nIn `_compute_prediction`, the first step to aligning the images is to find which keypoints from the two images match. The simplest method for that is to say keypoints whcih are close to eachother match. (note, later steps will correct for ""matched"" keypionts from mismatched images)\n\nBased on the matched keypoints, the following lines find how to transform the input image so the keypoints align with the template images keypoints (using a homography matrix), then transorms the input image using that matrix, so it should align with the template.\n``` python\n# Transform input image so that it matches the template image as well as possible\nM, mask = cv2.findHomography(img_pts, template_pts, cv2.RANSAC, self.ransac_thresh)\nimg_T = cv2.warpPerspective(img, M, self.signs[k].shape[::-1])\n```\nOnce you add these lines, you should change `img` in the line `visual_diff = compare_images(img, self.signs[k])` to `img_T`.\n\n### comparing images\n\nAt this point, we have two images which, if they are of the same sign, should be aligned with each other. If they are of different signs, the matched keypoints were likely not well aligned, and the homography matrix probably skewed the image into an unrecognizable blob, but the computer can\'t tell what is a reasonable image and what is an unrecognizable blob, so now we have to determine how similar the two images are.\n\nThe `compare_images` function at the bottom of the file is used to find how similar two images are to each other. This one is left up to you, but here are a few hints:\n\nFirst, there is one thing we have yet to account for while comparing images: lighting. If you have tried to do blob detection and then tried again when the sun went down, you know that lighting wreaks havoc on computer vision. Since we are using grayscale images, and we have cropped them so both images are of the same thing, we can eliminate most of the effects of lighting by normalizing the image. Mathematically this can be done by taking `(each_element - mean)/standard_dev`. Images are stored as numpy arrays, so you can use some nice numpy functions to make this math easier.\n\nFor finding the difference between the images, remember that, in code, an image is just an array of numbers. You can do arithmatic with the images to find how close they are to the same.\n\n### converting to useful output\n\nBack in the `predict` method, the output of `compare_images` passes through `_compute_prediction` and is saved in the dictionary `visual_diff`, which maps the keys associated with the template images to the calulated difference between that template image and the input image.\n\nThe final step for `TemplateMatcher` is to convert these differences into a scaled confidence value representing how well the input image matches each of the templates. This step is really just algebra: you need to make large numbers small and small numbers large, but you also want to scale your output so that a value near 1 always represents a high confidence.\n\n### conclusion\n\nThat\'s all, this class now outputs how well an input image matches each of a given set of templates. One nice part about this approach is that no single step needs to be perfectly tuned: finding slightly too many keypoints initially is quickly corrected when you find matches, and incorrect matches are generally eliminated in the homography matrix transformation, so by the time you get to the numerical comparison of images, you are usually looking at either a reasonable match or two extremely different images. This system is therefore relatively robust, and has a low rate of false positives; however, it is suseptible to false negatives.\n\n## Navigating\n'",base repo for scaffolded computer vision project for comprobo17
https://github.com/ihmeuw/fungi,"b""Elasticsearch for storage of and queries on provenance data.\n============================================================\n\nThis project provides storage and query of provenance\ninformation for data and processes, using the format\nunderlying the `provda package for provenance management\n<https://stash.ihme.washington.edu/users/adolgert/repos/provda/browse>`_.\n\nThis project's motivation is provision of command-line tools\nfor interacting with provenance information. It provides abstractions\nand functionality for storing and querying provenance information for\ndata and processes, leveraging elasticsearch-py and elasticsearch-dsl-py.""","Command-line interface to search provenance data generated by Provda, stored in ElasticSearch"
https://github.com/maxalbert/auto-exec-notebook,"b'Example how to programmatically create and execute an IPython notebook,\nin response to [this question on StackOverflow](http://stackoverflow.com/questions/22328052/ipython-notebook-programatically-read-and-execute-cells).\n\nClick [here](http://nbviewer.ipython.org/github/maxalbert/auto-exec-notebook/blob/master/how-to-programmatically-generate-and-execute-an-ipython-notebook.ipynb) to view the example on `nbviewer`.\n\nClick [here](http://nbviewer.ipython.org/github/maxalbert/auto-exec-notebook/blob/master/executed_notebook_with_freq_5.0.ipynb) to see the result of executing the automatically generated notebook.\n'",Illustrates how to programmatically generate and execute an IPython notebook
https://github.com/ziyanfeng/udacity-deep-learning,"b'Assignments for Udacity Deep Learning class with TensorFlow\n===========================================================\n\nCourse information can be found at https://www.udacity.com/course/deep-learning--ud730\n\nRunning the Docker container from the Google Cloud repository\n-------------------------------------------------------------\n\n    docker run -p 8888:8888 -it b.gcr.io/tensorflow-udacity/assignments:0.5.0\n\nAccessing the Notebooks\n-----------------------\n\nOn linux, go to: http://127.0.0.1:8888\n\nOn mac, find the virtual machine\'s IP using:\n\n    docker-machine ip default\n\nThen go to: http://IP:8888 (likely http://192.168.99.100:8888)\n\nSaving Your Progress\n--------------------\n\nBecause of the `--rm` flag above, stopping the docker container removes it, so any changes you\'ve made will disappear. One way around this is to remove the `--rm` flag, and name the container for easy restarting:\n```sh\n# you only need to ""run"" the container the first time:\ndocker run -p 8888:8888 -it --name tensorflow-udacity b.gcr.io/tensorflow-udacity/assignments:0.5.0\n# \xe2\x80\xa6do various things\xe2\x80\xa6\n# when you\'re done, control-C to kill jupyter and stop the container\n# when you\'re ready to do more things, you can now just ""start"" the container:\ndocker start -ai tensorflow-udacity\n# \xe2\x80\xa6do more things\xe2\x80\xa6\n# \xe2\x80\xa6repeat\xe2\x80\xa6\n```\n\nFAQ\n---\n\n* **I\'m getting a MemoryError when loading data in the first notebook.**\n\nIf you\'re using a Mac, Docker works by running a VM locally (which\nis controlled by `docker-machine`). It\'s quite likely that you\'ll\nneed to bump up the amount of RAM allocated to the VM beyond the\ndefault (which is 1G).\n[This Stack Overflow question](http://stackoverflow.com/questions/32834082/how-to-increase-docker-machine-memory-mac)\nhas two good suggestions; we recommend using 8G.\n\nIn addition, you may need to pass `--memory=8g` as an extra argument to\n`docker run`.\n\n* **I want to create a new virtual machine instead of the default one.**\n\n`docker-machine` is a tool to provision and manage docker hosts, it supports multiple platform (ex. aws, gce, azure, virtualbox, ...). To create a new virtual machine locally with built-in docker engine, you can use\n\n    docker-machine create -d virtualbox --virtualbox-memory 8196 tensorflow\n    \n`-d` means the driver for the cloud platform, supported drivers listed [here](https://docs.docker.com/machine/drivers/). Here we use virtualbox to create a new virtual machine locally. `tensorflow` means the name of the virtual machine, feel free to use whatever you like. You can use\n\n    docker-machine ip tensorflow\n    \nto get the ip of the new virtual machine. To switch from default virtual machine to a new one (here we use tensorflow), type\n\n    eval $(docker-machine env tensorflow)\n    \nNote that `docker-machine env tensorflow` outputs some environment variables such like `DOCKER_HOST`. Then your docker client is now connected to the docker host in virtual machine `tensorflow`\n\n\nNotes for anyone needing to build their own containers (mostly instructors)\n===========================================================================\n\nBuilding a local Docker container\n---------------------------------\n\n    cd tensorflow/examples/udacity\n    docker build --pull -t $USER/assignments .\n\nRunning the local container\n---------------------------\n\nTo run a disposable container:\n\n    docker run -p 8888:8888 -it --rm $USER/assignments\n\nNote the above command will create an ephemeral container and all data stored in the container will be lost when the container stops.\n\nTo avoid losing work between sessions in the container, it is recommended that you mount the `tensorflow/examples/udacity` directory into the container:\n\n    docker run -p 8888:8888 -v </path/to/tensorflow/examples/udacity>:/notebooks -it --rm $USER/assignments\n\nThis will allow you to save work and have access to generated files on the host filesystem.\n\nPushing a Google Cloud release\n------------------------------\n\n    V=0.5.0\n    docker tag $USER/assignments b.gcr.io/tensorflow-udacity/assignments:$V\n    gcloud docker push b.gcr.io/tensorflow-udacity/assignments\n    docker tag -f $USER/assignments b.gcr.io/tensorflow-udacity/assignments:latest\n    gcloud docker push b.gcr.io/tensorflow-udacity/assignments\n\nHistory\n-------\n\n* 0.1.0: Initial release.\n* 0.2.0: Many fixes, including lower memory footprint and support for Python 3.\n* 0.3.0: Use 0.7.1 release.\n* 0.4.0: Move notMMNIST data for Google Cloud.\n* 0.5.0: Actually use 0.7.1 release.\n'",Assignments for Udacity's Deep Learning with TensorFlow course
https://github.com/hplgit/bumpy,"b'# bumpy\n\nQuick appetizer and first tutorial on scientific computing with Python\nusing examples from basic physics.\n\n## Contents\n\n * Part I introduces the fundamental Python syntax for\n   variables, loops, if-tests, arrays, plotting, files, and classes,\n   using a simple physics formula as example.\n * Part II is a real physics application involving analysis of mechanical\n   vibrations. Besides showing how typical scientific Matlab-style scripts\n   look in Python, this example also introduces more advanced concepts\n   like flexible storage of objects in lists and files, downloading\n   data from web sites, user input via the command line, unit testing,\n   symbolic mathematics, and modules.\n\n## Goal\n\nThe goal of the tutorials is to bring the reader quickly up to speed\nwith how Matlab-style programming (with functions of one variable)\ncan be done in Python. With such basic background, it is easier to\nfollow scientific courses and teaching material that apply Python as\nprogramming language. The tutorials are application-driven and brief.\nFor further and more detailed information on scientific computing\nwith Python we have compiled\na [list of much more detailed tutorials and books](http://hplgit.github.io/bumpy/doc/pub/._bumpy010.html#app:resources).\n'",Quick tutorial of scientific computing with Python using a real physics application.
https://github.com/tarlen5/cbb_analytics,"b""cbb_analytics\n=============\n\nCollege Basketball analytics software for analyzing trends in 5 man units and rosters. Originally written to analyze the 2014 - 2015 UCLA basketball season.\n\n* Stage 1: ```LoadAllGames.py```. Load games by webscraping the base url, following the urls for individual game play by play information, then store data frames containing event play by play as an sqlite3 database table, with rosters for both sides for each game. Each raw play by play table and each side's roster gets its own table in the database, tagged with the date in format: yyyy-mm-dd.\n\n* Stage 2: (currently in the notebook ```notebooks/Explore_Play_By_Play_DB.ipynb``` but will be made into its own script when finished). Process each game by loading each game's `play_by_play` and roster table into a `game_table` DataFrame with the time stamps of each player on the court and the running total score for each side. This will be used to compute the strength of each lineup, accounting for factors such as: strength of opponent, leverage of the situation, strength of opponent's lineup, and any other factors deemed important.\n\n### Requirements\n\nInstalling this package requires the following \n\n* [BeautifulSoup](http://www.crummy.com/software/BeautifulSoup/)\n* [tabulate](https://pypi.python.org/pypi/tabulate)\n* [pandas](http://pandas.pydata.org)\n\n""",College Basketball analytics software for analyzing trends in 5 man units and rosters
https://github.com/anoblega/Spectral-Clustering-,b'# Spectral-Clustering-\n',Trabajo de Spectral Clustering para la matéria de Machine Learning
https://github.com/shounakG/2012-US-presidential-Elections-Lobbying-Analysis,"b'# 2012 US presidential Elections: Lobbying Analysis\n\n\nThis project studied the trend of lobbying in the 2012 Presidential elections intending to find out useful statistical results for lobbying analysis.\n\nTechnologies used: IBM Bluemix, Apache Spark data and analytics service, IPython Notebook, [Languages: Python]\n'",Presidential Elections 2012
https://github.com/jklymak/PelicanSite,b'# PelicanSite\nMy Pelican Website\n\nhttp://web.uvic.ca/~jklymak/\n\nOnly trick from all the other versions online is a special version of `themes/pelican-bootstrapJMK/templates/publications.html` that makes the publication list.\n\nTo run:\n```bash\nsource activate pelican\ncd ~/Dropbox/PelicanSite/\nmake bib # to make the bibliography\nmake cp_upload # to install on site\n```\n\nIf we need to use the python environment then run\n```bash\nconda env create -f pelican.env\n```\n\nThe other trick was to use `liquid_tags.img` to include figures.  Much better than raw markdown.  \n',My Peilican Website
https://github.com/lapis-zero09/deep_learning_book,b'# deep_learning_book\n\nDeep Learning Book\n\nChapter 7.1-7.4\n\nslide [https://www.slideshare.net/ShinsakuKono/deep-learningbook-chap7](https://www.slideshare.net/ShinsakuKono/deep-learningbook-chap7)\n',Deep Learning Book勉強会の資料
https://github.com/HanchenXiong/Python_Essential,b'# Python_Essential\n',a notebook for collecting pieces of Python programming at different corners
https://github.com/lschollmeyer/gis_visualizations,"b'# gis_visualizations\n\nVery rudimentary notebooks for a hour-long talk to show some basic GIS data visualizations using matplotlib (extended via Pandas and GeoPandas) and Folium. Obviously, not intended to be anything other than being an introduction to GIS concepts and using Python libraries to perform them. \n'",Introductory Python visualizations for GIS data
https://github.com/bamtak/titanic_survival_exploration,"b'# Machine Learning Engineer Nanodegree\n## Introduction and Foundations\n## Project: Titanic Survival Exploration\n\n### Install\n\nThis project requires **Python 2.7** and the following Python libraries installed:\n\n- [NumPy](http://www.numpy.org/)\n- [Pandas](http://pandas.pydata.org)\n- [matplotlib](http://matplotlib.org/)\n- [scikit-learn](http://scikit-learn.org/stable/)\n\nYou will also need to have software installed to run and execute a [Jupyter Notebook](http://ipython.org/notebook.html)\n\nIf you do not have Python installed yet, it is highly recommended that you install the [Anaconda](http://continuum.io/downloads) distribution of Python, which already has the above packages and more included. Make sure that you select the Python 2.7 installer and not the Python 3.x installer.\n\n### Code\n\nTemplate code is provided in the notebook `titanic_survival_exploration.ipynb` notebook file. Additional supporting code can be found in `visuals.py`. While some code has already been implemented to get you started, you will need to implement additional functionality when requested to successfully complete the project. Note that the code included in `visuals.py` is meant to be used out-of-the-box and not intended for students to manipulate. If you are interested in how the visualizations are created in the notebook, please feel free to explore this Python file.\n\n### Run\n\nIn a terminal or command window, navigate to the top-level project directory `titanic_survival_exploration/` (that contains this README) and run one of the following commands:\n\n```bash\njupyter notebook titanic_survival_exploration.ipynb\n```\nor\n```bash\nipython notebook titanic_survival_exploration.ipynb\n```\n\nThis will open the Jupyter Notebook software and project file in your web browser.\n\n### Data\n\nThe dataset used in this project is included as `titanic_data.csv`. This dataset is provided by Udacity and contains the following attributes:\n\n**Features**\n- `pclass` : Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n- `name` : Name\n- `sex` : Sex\n- `age` : Age\n- `sibsp` : Number of Siblings/Spouses Aboard\n- `parch` : Number of Parents/Children Aboard\n- `ticket` : Ticket Number\n- `fare` : Passenger Fare\n- `cabin` : Cabin\n- `embarked` : Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n\n**Target Variable**\n- `survival` : Survival (0 = No; 1 = Yes)'",Titanic survival exploration Machine Learning project
https://github.com/evizitei/NFIRS-property-loss-prediction,"b'# NFIRS property loss prediction\n\nSee project_report.md for project overview\n\n### Requirements\n*make sure to pip install these all before running anything*\n\nsklearn (http://scikit-learn.org/)\ndbfread (https://dbfread.readthedocs.io)\ndataset (https://dataset.readthedocs.io/en/latest/)\n\n### Exploring Data files\n\nyou can download the data for this project from\n\nhttp://www.fema.gov/media-library-data/20130726-2126-31471-8394/nfirs_2011_120612.zip\n\nAnd then to use any of the following commands\nyou can unzip that file into the ""data"" folder\nin this project directory (which is gitignored)\n\nAll the data files are in .dbf format, so\nyou need dbfread in order to consume them.\n\n```\nfrom dbfread import DBF\nincidents_table = DBF(\'data/basicincident.dbf\')\nlen(incidents_table) # -> 2,311,716\n\n# this will be a great deal of output\n# but it\'s useful for seeing how records are structured\nfor record in table:\n    print(record)\n```\n\nthe basicincident.dbf file is the most interesting one, and it\'s\nrecords look like this:\n\n```\nOrderedDict([\n  (u\'STATE\', u\'AK\'),\n  (u\'FDID\', u\'13425\'),\n  (u\'INC_DATE\', 3292011),\n  (u\'INC_NO\', u\'0000051\'),\n  (u\'EXP_NO\', 0),\n  (u\'VERSION\', u\'5.0\'),\n  (u\'DEPT_STA\', u\'8\'),\n  (u\'INC_TYPE\', u\'461\'),\n  (u\'ADD_WILD\', u\'N\'),\n  (u\'AID\', u\'1\'),\n  (u\'ALARM\', 32920110944),\n  (u\'ARRIVAL\', 32920110950),\n  (u\'INC_CONT\', 32920111004),\n  (u\'LU_CLEAR\', 32920111016),\n  (u\'SHIFT\', u\'\'),\n  (u\'ALARMS\', u\'0\'),\n  (u\'DISTRICT\', u\'\'),\n  (u\'ACT_TAK1\', u\'86\'),\n  (u\'ACT_TAK2\', u\'\'),\n  (u\'ACT_TAK3\', u\'\'),\n  (u\'APP_MOD\', u\'Y\'),\n  (u\'SUP_APP\', 0),\n  (u\'EMS_APP\', 0),\n  (u\'OTH_APP\', 4),\n  (u\'SUP_PER\', 0),\n  (u\'EMS_PER\', 0),\n  (u\'OTH_PER\', 5),\n  (u\'RESOU_AID\', u\'Y\'),\n  (u\'PROP_LOSS\', 18000),\n  (u\'CONT_LOSS\', 0),\n  (u\'PROP_VAL\', 18000),\n  (u\'CONT_VAL\', 0),\n  (u\'FF_DEATH\', 0),\n  (u\'OTH_DEATH\', 0),\n  (u\'FF_INJ\', 0),\n  (u\'OTH_INJ\', 0),\n  (u\'DET_ALERT\', u\'U\'),\n  (u\'HAZ_REL\', u\'N\'),\n  (u\'MIXED_USE\', u\'NN\'),\n  (u\'PROP_USE\', u\'419\'),\n  (u\'CENSUS\', u\'\')])\n```\n\nThey use numerical codes a lot here so the data is going to need\nsome cleaning. Some of the codes can be found in ""codelookup.DBF"".\n\n```\nfrom dbfread import DBF\nlookup_table = DBF(\'data/codelookup.DBF\')\nlen(lookup_table) # -> 6,619\n\nfor record in lookup_table:\n    print(record)\n```\n\nIt\'s useful to have this data in csv format for easy reference, and\nit\'s only 6.6k lines or so. There\'s a script in the bin folder\nthat can do this for you, and it runs in less than a second:\n\n`./bin/build_lookup_csv`\n\n\n\n Additionally, it\'s hard to explore\nin dbf format, so it seems worth shoving the relevant files\n into sqlite tables with indexes for easier querying.\nThere\'s a useful helper script for doing this\n(make sure your sqlite folder exists):\n\n`./bin/data_to_sqlite`\n\nit will take some time to run, since there are a few\nmillion basic incidents and 600k fire incidents,\nand it\'s processing them iteratively.  This takes\non the order of 3 hours, and so the final dataset\nwill be provided as a google drive file here:\n\n*_include file link at some point_*\n\nexploring the data in a sql database is a little easier:\n\n```\nimport dataset\n\ndb = dataset.connect(\'sqlite:///./sqlite/incidents.sqlite\')\nbasic_table = db[\'basic_incidents\']\nfire_table = db[\'fire_incidents\']\n\n# return one row from each table\nbasic_table.find_one()\nfire_table.find_one()\n\n# find by column values:\nbasic_table.find(FDID=u\'11100\', INC_NO=u\'391\')\n```\n\nAlthough there are 2.3 million incidents in the basic incident file,\nmany of them are non-fire incidents, and for the purposes of\nthis process we really only care about the ones that are in the\nfire incidents file.  This next transformation reads in the sqlite\ndatabase that has all the incidents and produces a single joined table\nthat has only entries for fire incidents.\n\n`./bin/join_incidents_to_one_table`\n\nThere are > 670k records to build here, and it can do\nabout 50/second, so this operation can take about 4 hours.\n\nThis data is now in one-record-per-fire, which is easier\nto explore for feature exploration and extraction.\n\n```\nimport dataset\n\ndb = dataset.connect(\'sqlite:///./sqlite/fire_incidents.sqlite\')\ntable = db[\'incidents\']\ntable.find_one()\n```\n\nIn the FeatureExploration notebook you can find some analysis of which features\nwere selected for learning and why.  Then there is another script\nthat reads in the joined fire records and produces a ""useful"" dataset\nthat has only records with property losses > 0 and only the features\nI believe to be useful.  There are about 215k records to scan through that\nmeet this criteria (though some are skipped for other reasons).  The script\niterates through about 500 records per second, so that\'s about a 7-8 minute runtime.\n\n`./bin/reduce_to_useful_inputs`\n\nThis produces a much smaller and manageable set of data, 8.9MB for the\nfinal sqlite file.  Here we can count the rows in it:\n\n```\nimport dataset\ndb = dataset.connect(\'sqlite:///./sqlite/useful_incidents.sqlite\')\ntable = db[\'incidents\']\nlen(table)\n# -> 196,574\n```\n\nSummary statistics can be extracted for a given column like so:\n\n```\nimport pandas as pd\nimport sqlite3\nconn = sqlite3.connect(\'./sqlite/useful_incidents.sqlite\')\ndata = pd.read_sql_query(""select PROP_LOSS from incidents"", conn)\ndata.describe()\n```\n\nPROP_LOSS\ncount  196574.000000\nmean     8914.580748\nstd     12933.652034\nmin         1.000000\n25%      1000.000000\n50%      3000.000000\n75%     10000.000000\nmax     60000.000000\n\nThe next step is to clean up the data so it can be analyzed reliably.  This\nmeans removing additional outliers from numeric fields and estimating missing\nvalues.\n\nThis script does about 500 records per second over 200k, so a bit less than 7\nminutes to run it:\n\n`./bin/clean_data`\n\nThis results in a remaining dataset of 192,405 records.  At this point\nwe take the data and make categorical fields one-hot encoded vectors,\nand feature-scale numeric fields to be from 0 to 1. We also use\nthe log of the prop loss rather than it\'s normal value to unskew it\nbefore trying to get a good regression.  Once again\nthere is a script for this:\n\n`./bin/normalize_data`\n\nThis is still 192k records, but it\'s only able to process about 200 records per\nsecond.  This means it\'s closer to 16 minutes to execute.\n\nA cleaned and normalized record looks like this:\n\n```\nimport dataset\ndb = dataset.connect(\'sqlite:///./sqlite/normalized_incidents.sqlite\')\ntable = db[\'incidents\']\ntable.find_one()\n```\n\nOrderedDict([(\'id\', 1),\n             (\'PROP_LOSS\', 3.30102999566), # log of 2000\n             (\'STATE_EXPENSE_0\', 0),\n             (\'STATE_EXPENSE_1\', 0),\n             (\'STATE_EXPENSE_2\', 1),\n             (\'INC_TYPE_0\', 0),\n             (\'INC_TYPE_1\', 0),\n             (\'INC_TYPE_2\', 0),\n             (\'INC_TYPE_3\', 1),\n             (\'INC_TYPE_4\', 0),\n             (\'INC_TYPE_5\', 0),\n             (\'INC_TYPE_6\', 0),\n             (\'AID_0\', 1),\n             (\'AID_1\', 0),\n             (\'AID_2\', 0),\n             (\'AID_3\', 0),\n             (\'HOUR_GROUP_0\', 1),\n             (\'HOUR_GROUP_1\', 0),\n             (\'HOUR_GROUP_2\', 0),\n             (\'HOUR_GROUP_3\', 0),\n             (\'CONTROLLED_TIME\', 0.09042553191489362),\n             (\'CLEAR_TIME\', 0.02976190476190476),\n             (\'SUPPRESSION_APPARATUS\', 0.0),\n             (\'SUPPRESSION_PERSONNEL\', 0.014925373134328358),\n             (\'PROP_VALUE\', 0.00021062965584656352),\n             (\'DETECTOR_FLAG\', 0),\n             (\'HAZMAT_RELEASE_0\', 1),\n             (\'HAZMAT_RELEASE_1\', 0),\n             (\'HAZMAT_RELEASE_2\', 0),\n             (\'HAZMAT_RELEASE_3\', 0),\n             (\'HAZMAT_RELEASE_4\', 0),\n             (\'MIXED_USE_0\', 1),\n             (\'MIXED_USE_1\', 0),\n             (\'MIXED_USE_2\', 0),\n             (\'MIXED_USE_3\', 0),\n             (\'MIXED_USE_4\', 0),\n             (\'HEAT_SOURCE\', 0),\n             (\'IGNITION_0\', 0),\n             (\'IGNITION_1\', 1),\n             (\'IGNITION_2\', 0),\n             (\'IGNITION_3\', 0),\n             (\'FIRE_SPREAD_0\', 1),\n             (\'FIRE_SPREAD_1\', 0),\n             (\'FIRE_SPREAD_2\', 0),\n             (\'FIRE_SPREAD_3\', 0),\n             (\'FIRE_SPREAD_4\', 0),\n             (\'FIRE_SPREAD_5\', 0),\n             (\'STRUCTURE_TYPE_0\', 1),\n             (\'STRUCTURE_TYPE_1\', 0),\n             (\'STRUCTURE_TYPE_2\', 0),\n             (\'STRUCTURE_TYPE_3\', 0),\n             (\'STRUCTURE_TYPE_4\', 0),\n             (\'STRUCTURE_TYPE_5\', 0),\n             (\'STRUCTURE_STATUS_0\', 1),\n             (\'STRUCTURE_STATUS_1\', 0),\n             (\'STRUCTURE_STATUS_2\', 0),\n             (\'STRUCTURE_STATUS_3\', 0),\n             (\'STRUCTURE_STATUS_4\', 0),\n             (\'SQUARE_FEET\', 0.0007094674556213017),\n             (\'AES_SYSTEM_0\', 1),\n             (\'AES_SYSTEM_1\', 0),\n             (\'AES_SYSTEM_2\', 0),\n             (\'AES_SYSTEM_3\', 0)])\n\n## training/validation split\n\nBefore we start analyzing, I\'m going to take some data (about 30,000 records)\nand split them off for a validation set.  Out of the 192k records, that means\nwe\'ll still have around 160k for training/crossvalidation.  Here\'s the splitter\nscript:\n\n`./bin/split_off_test_set`\n\nOn my machine this is processing about 200 rows per second, so for 192k records\nto get sorted that\'s about 16 minutes.   Afterwards you can check\nthe data split sizes like this:\n\nA cleaned and normalized record looks like this:\n\n```\nimport dataset\ntdb = dataset.connect(\'sqlite:///./sqlite/training_incidents.sqlite\')\nvdb = dataset.connect(\'sqlite:///./sqlite/validation_incidents.sqlite\')\nt_table = tdb[\'incidents\']\nv_table = vdb[\'incidents\']\nlen(t_table) # -> 162448\nlen(v_table) # -> 29957\n```\n\nAlgorithm selection is explored in\na jupyter notebook at\nAlgorithmExploration.ipynb, which\ncan be started and explored with:\n\n`jupyter notebook AlgorithmExploration.ipynb`\n\nHyperparameter tuning is done with the tune_params script in the bin\nfolder; it has the actual analysis step commented out, so you can uncomment\nthe oen you want to run; running _all_ of them at once would take some time.\nThe comments in that file contain the resulting best found parameters for\neach learner.\n'",Capstone project for Udacity Machine Learning Engineer nanodegree
https://github.com/datosgobar/presentacion-type-checking-python,"b""# presentacion-type-checking-python\n\nUna nueva forma de tener type checking en python (PEP 484), usando una notaci\xc3\xb3n vieja (PEP 3107) - PyConAR 2016. [Ver presentaci\xc3\xb3n](https://datosgobar.github.io/presentacion-type-checking-python)\n\n<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n## Indice\n\n- [Outline](#outline)\n- [Ejemplos](#ejemplos)\n- [Referencias](#referencias)\n- [Herramientas usadas en la presentaci\xc3\xb3n](#herramientas-usadas-en-la-presentaci%C3%B3n)\n- [Duraci\xc3\xb3n recomendada](#duraci%C3%B3n-recomendada)\n\n<!-- END doctoc generated TOC please keep comment here to allow auto update -->\n\n## Outline\n\n1. Function Annotations - PEP 3107 (2006)\n2. Type Hints - PEP 484 (2014)\n3. Librer\xc3\xadas de terceros para hacer type checking\n    * `mypy` en Python 3\n    * `mypy` en Python 2\n    * `typeguard` en Python 3 (con Jupyter)\n4. Tipos m\xc3\xa1s complejos\n\n## Ejemplos\n\n* **`foo_py2.py`**: Ejemplo de type checking para correr con `mypy` en Python 2 (`mypy --py2 examples/foo_py2.py`)\n* **`foo_py3.py`**: Ejemplo de type checking para correr con `mypy` en Python 3 (`mypy examples/foo_py3.py`)\n* **`Funciones anotadas (python 3).ipynb`**: Ejemplo para correr en jupyter con `typeguard`.\n\n## Referencias\n\n*Nota: varios ejemplos de la presentaci\xc3\xb3n fueron tomados de las referencias.*\n\n* [PEP 3107](https://www.python.org/dev/peps/pep-3107/): Esta PEP introduce una sintaxis para a\xc3\xb1adir anotaciones de metadata arbitrarias a funciones de Python.\n* [PEP 484](https://www.python.org/dev/peps/pep-0484/): Esta PEP introduce un m\xc3\xb3dulo provisional que define est\xc3\xa1ndares y herramientas para implementar el chequeo de tipado en funciones de python usando la sintaxis propuesta en PEP 3107 para anotar funciones.\n\n## Herramientas usadas en la presentaci\xc3\xb3n\n\n* [Cleaver: 30-sec slideshows for Hackers](https://github.com/jdan/cleaver). Se escribe en Markdown y renderiza en html.\n\n## Duraci\xc3\xb3n recomendada\n\n5 minutos\n""","Una nueva forma de tener type checking en python (PEP 484), usando una notación vieja (PEP 3107) - PyConAR 2016."
https://github.com/khwilson/pydata2014,"b""PyData2014 Materials\n====================\n\nThese are supplementary materials for my talk on Determining Skill Levels\nat PyData2NYC 2014.\n\nFootball\n--------\n\nThe football package parses and manipulates\n* cfb2013lines.csv\n* conferences.csv\n\n`cfb2013lines.csv` is a csv of the winners and losers of all college\nfootball games in 2013.\n\n`conferences.csv` is just a mapping from team to an abbreviation of their\nconference name (sorry for any inconsistencies; 2013 data is hard to get at\nstrangely).\n\nThe package itself has three main modules:\n* `base` for manipulating the data\n* `elo` for doing Elo's method on this data\n* `make_graph` for producing a win/lose directed dot graph, which you should\n  compile with `fdp`.\n\nTo use `make_graph`, simply run `python football/make_graph`.\n\nTo use `elo`, see the example in `Elo.ipynb`.\n\nIRT\n---\n\nThe IRT package contains a simple implementation of 1PL IRT with normal priors.\nIt contains a `simulate` function that allows you build data sets, and then\na `make_irt_neg_objective` function which you can use with `scipy.optimize` to\nlearn the latent parameters.\n\nFor an example of usage, see `IRT.ipynb`.\n\nTools\n-----\n\nI've also included some tools for doing finite-difference tests of jacobians\nand hessians. Doing finite-difference tests is *critical* for testing whether\nor not your fast implementations of these functions actually match your\nobjective. If they do not, then you're going to quickly get in trouble with\nline search algorithms.\n\nExercises\n---------\n\nSince this was an intermediate talk, I guess I can assign people who come here\nexercises:\n\n1. Here's a simple one. Fork this repo and add the gradients and\n   hessians of the elo and irt objectives to the packages. No credit if you do\n   not *test* these functions, which you can do with the finite-difference tests\n   available in the `tools` package.\n\n2. Then play around with the tools. Find other data sets or simulate larger\n   data sets. When do the tools break? What assumptions might break down?\n\n3. Be the BCS! Add priors to Elo's method to get your favorite team to the top.\n   Some simple ones you could try are:\n   - Put a normal prior centered at 1.0 on your favorite team's skill and a\n     prior centered at 0.0 for everyone else's. What happens (especially, what\n     happens to their conference)?\n   - Put a prior on a whole conference. What happens?\n   - Put a *nondiagonal* prior on the teams. For instance, try putting a prior\n     on every conference that the skills across the conference will be\n     normally distributed. How would you do that? Why might you think that's\n     a reasonable prior?\n\nLicense\n-------\n\nEverything in this repository is licensed under the Apache 2.0 license.\n""",Materials for PyData2014
https://github.com/ricardodeazambuja/IJCNN2017,"b""# Experiments used for the paper ~~accepted for~~ presented at [IJCNN2017](http://www.ijcnn.org/)\n# Short-Term Plasticity in a Liquid State Machine Biomimetic Robot Arm Controller\n\n## Abstract:\nBiological neural networks are able to control limbs in different scenarios, with high precision and robustness. As neural networks in living beings communicate through spikes, modern neuromorphic systems try to mimic them making use of spike-based neuron models. Liquid State Machines (LSM), a special type of Reservoir Computing system made of spiking units, when it was first introduced, had plasticity on an external layer and also through Short-Term Plasticity (STP) within the reservoir itself. However, most neuromorphic hardware currently available does not implement both Short-Term Depression and Facilitation and some of them don't support STP at all. In this work we test the impact of STP in an experimental way using a 2 degrees of freedom simulated robotic arm controlled by an LSM. Four trajectories are learned and their reproduction analysed with Dynamic Time Warping accumulated cost as the benchmark. The results from two different set-ups showed the use of STP in the reservoir was not computationally cost-effective for this particular robotic task.\n\n\n1) [Generation of trajectories](https://github.com/ricardodeazambuja/IJCNN2017/blob/master/2DofArm_simulation_data_generator-figures.ipynb)\n\n2) [Simulated 2DoF arm](https://github.com/ricardodeazambuja/IJCNN2017/blob/master/2DofArm_simulation_data_generator_and_physics.ipynb)\n\n3) [Generation of the training data](https://github.com/ricardodeazambuja/IJCNN2017/blob/master/2DofArm_simulation-Main.ipynb)\n\n4) [Linear Regression - readout weights](https://github.com/ricardodeazambuja/IJCNN2017/blob/master/2DofArm_simulation_linear_regression.ipynb)\n\n5) Testing:\n- [Set A](https://github.com/ricardodeazambuja/IJCNN2017/blob/master/2DofArm_simulation_testing_learned_readouts-A-STP_ON.ipynb)\n- [Set B](https://github.com/ricardodeazambuja/IJCNN2017/blob/master/2DofArm_simulation_testing_learned_readouts-B-STP_OFF.ipynb)\n- [Set C](https://github.com/ricardodeazambuja/IJCNN2017/blob/master/2DofArm_simulation_testing_learned_readouts-C-STP_ON.ipynb)\n- [Set D](https://github.com/ricardodeazambuja/IJCNN2017/blob/master/2DofArm_simulation_testing_learned_readouts-D-STP_OFF.ipynb)\n\n\n## OBS:  \n- [Results analysis 1](https://github.com/ricardodeazambuja/IJCNN2017/blob/master/___2DofArm_simulation_testing_analysis.ipynb)\n- [Results analysis 2](https://github.com/ricardodeazambuja/IJCNN2017/blob/master/___2DofArm_simulation_testing_learned_readouts-analysis-metric-individual-sets.ipynb)\n- [Results analysis 3](https://github.com/ricardodeazambuja/IJCNN2017/blob/master/___2DofArm_simulation_testing_learned_readouts-analysis.ipynb)\n- [Internal structure visualisation](https://github.com/ricardodeazambuja/IJCNN2017/blob/master/2DofArm_simulation_3D_printing_of_liquid_structure.ipynb)\n- [BEE SNN simulator](https://github.com/ricardodeazambuja/BEE)\n- [Dynamic Time Warping Library](https://github.com/ricardodeazambuja/DTW)\n\n## Preprint version:  \n- [IJCNN2017_draft.pdf](https://github.com/ricardodeazambuja/IJCNN2017/raw/master/IJCNN2017_draft.pdf)\n\n## Bibtex citation:\nhttps://github.com/ricardodeazambuja/IJCNN2017/blob/master/de_azambuja_stp_2017.bib\n\n## Final IEEE Xplore version:  \nhttp://ieeexplore.ieee.org/document/7966283/\n\n## Related works:\n- [Graceful Degradation under Noise on Brain Inspired Robot Controllers](https://github.com/ricardodeazambuja/ICONIP2016)\n- [Diverse, Noisy and Parallel: a New Spiking Neural Network Approach for Humanoid Robot Control](https://github.com/ricardodeazambuja/IJCNN2016)\n- [Neurorobotic Simulations on the Degradation of Multiple Column Liquid State Machines](https://github.com/ricardodeazambuja/IJCNN2017-2)\n- [Sensor Fusion Approach Using Liquid StateMachines for Positioning Control](https://github.com/ricardodeazambuja/I2MTC2017-LSMFusion)\n\n\n\n""",Short-Term Plasticity in a Liquid State Machine Biomimetic Robot Arm Controller
https://github.com/cvetakg/Traffic_Sign_Classifier_Jelena,b'# Traffic_Sign_Classifier_Jelena\n\nThis project is part of SDCND program - Term1\n',Traffic_Sign_Classifier
https://github.com/eduDorus/udacity_dl_project_4,b'# udacity_dl_project_4\nLanguage Translation\n',Language Translation
https://github.com/lorenzomartino86/titanic-survival,b'# titanic-survival\nA sample machine learning predictor for Kaggle competitions startup\n',A sample machine learning predictor for Kaggle competitions startup
https://github.com/jbrambleDC/datachallenge,"b'# Data Challenge\nThis is Cava Grills data challenge. Used to asses how potential interns technical skills are.\nIf you so choose, please complete the excersies and email us if you are interested in the position. \n\n## Data Interns\nIf you are applying for the Data Intern position, please complete the first part. Please submit all code, or direct us to the github repo you used to produce the results.\n\n## Business Intelligence\nIf you are applying for the BI position, please use SQL,MYSQL or any RDMS to query the first set. You are not required to do any plotting or further analysis. Simple summary statistics (counts,sums,averages) are required, and please demonstrate some advanced SQL.\n\n## Data Scientist\nIf you are applying for the DS position, please complete the entire excercise.\n'",This is our data challenge. Used to asses how potential interns technical skills are.
https://github.com/nishalad95/Langevin-Monte-Carlo,b'# Langevin-Monte-Carlo\nStatistical simulation of Brownian Motion using the Langevin Monte Carlo Markov Chain algorithm for a particle under the influence of a drift function. \n\nCurrently in progress!\n',Statistical simulation of Brownian Motion using the Langevin Monte Carlo Markov Chain algorithm for a particle under the influence of a drift function. 
https://github.com/avtomato/netology-homework,b'[Netology python course](http://netology.ru/programs/python) \n',:snake: Netology python course homeworks 
https://github.com/petethemeat/quora-competition,b'# quora-competition\nRepository for our final Data Science project\n',Repository for our final Data Science project
https://github.com/tomlouden/SPIDERMAN-paper,b'to sync overleaf version with master: git push overleaf master\nto sync master version with overleaf: git pull overleaf master\n\nsimple!\n',The first spiderman paper
https://github.com/wesleybeckner/wesleybeckner.github.io,"b'## Phantom for Jekyll\n\nA minimalist, responsive portfolio theme for [Jekyll](http://jekyllrb.com/) with Bootstrap.\n\n![preview](preview.jpg)\n\n[See it in action](http://jamigibbs.github.io/phantom/).\n\n## Fancy using it for your own site?\n\nHere are some steps to get you started:\n\n1. Clone this repo and cd into the directory:\n\n  ```bash\n  git clone https://github.com/jamigibbs/phantom.git your-dir-name && cd your-dir-name\n  ```\n\n2. Run:\n\n  ```bash\n  gem install bundler\n  bundle install\n  bundle exec jekyll serve\n  ```\n\n  You may need to append your commands with `sudo` if you\'re getting a permissions error.\n\n  _Don\'t have Jekyll yet? [Get `er installed then!](http://jekyllrb.com/docs/installation/)_\n\n3. Visit in your browser at:\n\n  `http://127.0.0.1:4000`\n\n## Launching with Github Pages :rocket:\n\nJekyll + Github pages is a marriage made in heaven. You can [use your own custom domain name](https://help.github.com/articles/setting-up-a-custom-domain-with-github-pages/) or use the default Github url (ie. http://username.github.io/repository) and not bother messing around with DNS settings.\n\n## Theme Features\n\n### Navigation\n\nNavigation can be customized in `_config.yml` under the `nav_item` key. Default settings:\n\n```yaml\nnav_item:\n    - { url: \'/\', text: \'Home\' }\n    - { url: \'/about\', text: \'About\' }\n```\n\nSet the `nav_enable` variable to false in `_config.yml` to disable navigation.\n\n### Contact Form\n\nYou can display a contact form within the modal window template. This template is already setup to use the [Formspree](https://formspree.io) email system. You\'ll just want to add your email address to the form in `/_includes/contact-modal.html`.\n\nPlace the modal window template in any place you\'d like the user to click for the contact form.\nThe template will display a link to click for the contact form modal window:\n\n```liquid\n{% include contact.html %}\n{% include contact-modal.html %}\n```\n\n### Animation Effects\n\nAnimations with CSS classes are baked into the theme. To animate a section or element, simply add the animation classes:\n\n```html\n<div id=""about-me"" class=""wow fadeIn"">\n  I\'m the coolest!\n</div>\n```\n\nFor a complete list of animations, see the [animation list](http://daneden.github.io/animate.css/).\n\n### Pagination\n\nBy default, pagination on the home page will activate after 10 posts. You can change this within `_config.yml`. You can add the pagination to other layouts with:\n\n```liquid\n  {% for post in paginator.posts %}\n    {% include post-content.html %}\n  {% endfor %}\n\n  {% include pagination.html %}\n```\n\nRead more about the [pagination plugin](http://jekyllrb.com/docs/pagination/).\n\n## Credit\n\n* Bootstrap, http://getbootstrap.com/, (C) 2011 - 2016 Twitter, Inc., [MIT](https://github.com/twbs/bootstrap/blob/master/LICENSE)\n\n* Wow, https://github.com/matthieua/WOW, (C) 2014 - 2016 Matthieu Aussaguel\n, [GPL](https://github.com/matthieua/WOW#open-source-license)\n\n* Animate.css, https://github.com/daneden/animate.css, (C) 2016 Daniel Eden, [MIT](https://github.com/daneden/animate.css/blob/master/LICENSE)\n'",wesley's github website
https://github.com/dustin-anthc/CCHRC,b'# CCHRC\nA repository of notebooks that we use for data analysis\n',A repository of notebooks that we use for data analysis
https://github.com/pattrickmiller/udacity,"b'""# udacity"" \n'",Udacity Projects
https://github.com/badphysics/velocity_selector,b'# velocity_selector\nvelocity selector for university physics II at RIT\n\nThis is to supplement the Velocity Selector activity in University Physics II at Rochester Insititue of Technology\n',velocity selector for university physics II at RIT
https://github.com/llSourcell/The_evolution_of_gradient_descent,"b'# The_evolution_of_gradient_descent\n\nThis is the code for ""The Evolution of Gradient Descent"" by Siraj Raval on Youtube\n\n## Coding Challenge - Due Date, Thursday June 8th at 12 PM PST\n\nThis weeks coding challenge is to write out the [Adam](https://stats.stackexchange.com/questions/220494/how-does-the-adam-method-of-stochastic-gradient-descent-work) optimization strategy from scratch. In the process you\'ll learn about all the other gradient descent variants and why Adam works so well. Bonus points if you add a visual element to it by plotting it in a Jupyter notebook. Good luck!\n\n## Overview\n\nThis is the code for [this](https://youtu.be/nhqo0u1a6fw) videon on Youtube by Siraj Raval. In the video, we go over the different optimizer options that Tensorflow gives us. Under the hood, they are all variants of gradient descent.\n\n## Dependencies\n\n* matplotlib\n* pyplot\n* numpy\n\ninstall missing dependencies with [pip](https://pip.pypa.io/en/stable/)\n\n## Usage\n\nRun `jupyter notebook` to see the code that compares gradient descent to stochastic gradient descent run in the browser. I\'ve also got 2 seperate python files, one for adadelta and one for the nesterov method. Run those straight from terminal with the python command. \n\n## Credits\n\nThe credits for this code go to [GRYE](https://github.com/GRYE) and [dtnewman](https://github.com/dtnewman/gradient_descent). I\'ve merely created a wrapper to get people started. \n\n'","This is the code for ""The Evolution of Gradient Descent"" by Siraj Raval on Youtube"
https://github.com/Davin-IBM/Proof-of-Technology,"b""# Proof-of-Technology\n\nProofs of Technology (PoT) are to determine feasible solutions to technical problems.\n\n## Contents:\n\n1. [Use IBM's Data Science Experience (DSX) for building a machine learning web application](DSX)\n""",Proofs of Technology (PoT) are to determine feasible solutions to technical problems.
https://github.com/ledrui/Advanced-Lane-Lines-detection,"b'## Advanced Lane Finding\n[![Udacity - Self-Driving Car NanoDegree](https://s3.amazonaws.com/udacity-sdc/github/shield-carnd.svg)](http://www.udacity.com/drive)\n\n\nIn this project, your goal is to write a software pipeline to identify the lane boundaries in a video, but the main output or product we want you to create is a detailed writeup of the project.  Check out the [writeup template](https://github.com/udacity/CarND-Advanced-Lane-Lines/blob/master/writeup_template.md) for this project and use it as a starting point for creating your own writeup.  \n\nCreating a great writeup:\n---\nA great writeup should include the rubric points as well as your description of how you addressed each point.  You should include a detailed description of the code used in each step (with line-number references and code snippets where necessary), and links to other supporting documents or external references.  You should include images in your writeup to demonstrate how your code works with examples.  \n\nAll that said, please be concise!  We\'re not looking for you to write a book here, just a brief description of how you passed each rubric point, and references to the relevant code :). \n\nYou\'re not required to use markdown for your writeup.  If you use another method please just submit a pdf of your writeup.\n\nThe Project\n---\n\nThe goals / steps of this project are the following:\n\n* Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n* Apply a distortion correction to raw images.\n* Use color transforms, gradients, etc., to create a thresholded binary image.\n* Apply a perspective transform to rectify binary image (""birds-eye view"").\n* Detect lane pixels and fit to find the lane boundary.\n* Determine the curvature of the lane and vehicle position with respect to center.\n* Warp the detected lane boundaries back onto the original image.\n* Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.\n\nThe images for camera calibration are stored in the folder called `camera_cal`.  The images in `test_images` are for testing your pipeline on single frames.  If you want to extract more test images from the videos, you can simply use an image writing method like `cv2.imwrite()`, i.e., you can read the video in frame by frame as usual, and for frames you want to save for later you can write to an image file.  \n\nTo help the reviewer examine your work, please save examples of the output from each stage of your pipeline in the folder called `ouput_images`, and include a description in your writeup for the project of what each image shows.    The video called `project_video.mp4` is the video your pipeline should work well on.  \n\nThe `challenge_video.mp4` video is an extra (and optional) challenge for you if you want to test your pipeline under somewhat trickier conditions.  The `harder_challenge.mp4` video is another optional challenge and is brutal!\n\nIf you\'re feeling ambitious (again, totally optional though), don\'t stop there!  We encourage you to go out and take video of your own, calibrate your camera and show us how you would implement this project from scratch!\n'",Detecting road lane using advanced Computer Vision techniques 
https://github.com/ishantiw/customer_insights,"b'\n     ,-----.,--.                  ,--. ,---.   ,--.,------.  ,------.\n    \'  .--./|  | ,---. ,--.,--. ,-|  || o   \\  |  ||  .-.  \\ |  .---\'\n    |  |    |  || .-. ||  ||  |\' .-. |`..\'  |  |  ||  |  \\  :|  `--, \n    \'  \'--\'\\|  |\' \'-\' \'\'  \'\'  \'\\ `-\' | .\'  /   |  ||  \'--\'  /|  `---.\n     `-----\'`--\' `---\'  `----\'  `---\'  `--\'    `--\'`-------\' `------\'\n    ----------------------------------------------------------------- \n\n\nWelcome to your Node.js project on Cloud9 IDE!\n\nThis chat example showcases how to use `socket.io` with a static `express` server.\n\n## Running the server\n\n1) Open `server.js` and start the app by clicking on the ""Run"" button in the top menu.\n\n2) Alternatively you can launch the app from the Terminal:\n\n    $ node server.js\n\nOnce the server is running, open the project in the shape of \'https://projectname-username.c9users.io/\'. As you enter your name, watch the Users list (on the left) update. Once you press Enter or Send, the message is shared with all connected clients.\n'",Customer Insights
https://github.com/gacafe/vala_tc,b'# vala_tc\n\n[Online](https://gacafe.github.com/vala_tc/content)\n',Introduction to Python for Librarians
https://github.com/Yurlungur/spectral-methods-demo,b'spectral-methods-demo\n=====================\nAuthor: Jonah Miller <jonah.maxwell.miller@gmail.com>\n\nA Demonstration of Spectral Methods in Python\n',A Demonstration of Spectral Methods in Python
https://github.com/jfnavarro21/Python,b'Use to run ipython notebook from anywhere <br>\n- /c/Python27/Scripts/./ipython notebook\n\n',"Work in numpy and pandas, plus simple coding algorithms"
https://github.com/russodanielp/sbv,b'# svb IMPROVER SysTox Challenge\n\n[More info](https://sbvimprover.com/challenge-4)',sbv Challenge
https://github.com/sophienchu/tutorials,b'# tutorials\ntrying to learn python\n\ndoing several tutorials to learn python\n',trying to learn python
https://github.com/lexieheinle/jour407homework,"b'#JOUR 407 Homework\n\nThis repository contains my homework for JOUR 407 Data Journalism class taught by Matt Waite at the University of Nebraska-Lincoln. The majority of the files involve data analysis by agate and documentation via Jupyter.\n\n##Contents\n###[DataNormalizationHomework](DataNormalizationHomework)\nThis folder contains a dirty data set of [leaking underground storage tanks] (http://www.deq.state.ne.us/lustsurf.nsf/pages/sssi ""NDEQ lust database"") from the Nebraska Department of Environmental Quality. I used [OpenRefine] (http://openrefine.org) to normalize the data before finding the top 20 owners using Agate.\n\n###[DataSmellsHomework](DataSmellsHomework)\nThis folder looked at data from the University of Nebraska-Lincoln Police Department. By using Agate\'s group_by and count functions, inconsistent locations and buildings were revealed. Additionally, outliers in stolen and damaged amounts were found using agate-stats.\n\n###[JoinHomework](JoinHomework)\nFrom Nebraska school data, I calculated the percent change in free &amp; reduced lunch and enrollment from 2013 to 2015. This assignment required joining agate tables and creating custom functions to round decimals.\n\n###[ChartHomework](ChartHomework)\nUsing [seaborn](https://stanford.edu/~mwaskom/software/seaborn/) a Python data visualization based on [matploblib](http://matplotlib.org), I constructed three different chart styles of the top 10 Nebraska counties for mountain lion sightings.\n\n###[ChartSecondHomework](ChartSecondHomework)\nFrom the political ads database, I attempted to split the events by subjects to see which ads/candidates are most likely to use certain ones. However, this quest wasn\'t successful due to program limitations.\n\n###[IndependentStory](IndependentStory)\nUsing [general election data](http://www.sos.ne.gov/elec/prev_elec/index.html) from the Nebraska Secretary of State, I analyzed the changes in the state\'s political party distribution from 2000 to 2014. From my analysis, I found that if the current growth trend of the independent party continues, it would overtake the Democratic party by 2020.\n\nTo find the party\'s distributions, I had to write custom functions to calculate the correct percents. The percent change was calculated using agate functions. Estimating the future growth was based on the instructions in [Numbers in the Newsroom](http://store.ire.org/products/numbers-in-the-newsroom-using-math-and-statistics-in-news-second-edition-e-version) and required defining new functions.\n\n###[CellPhoneStory](CellPhoneStory)\nTo gather the data for this story, I had to request records from the E911 departments in both [Nebraska](http://www.psc.nebraska.gov/ntips/ntips_e911.html) and [Iowa](http://homelandsecurity.iowa.gov/programs/e_911.html). I looked at each state\'s E911 fund, which is funded by surcharges on cellphones. In the article, I explored how emergency services adapted to cover cellphones and the future plans.\n\n###[WellDatabases](WellDatabases)\nUsing irrigated acres data from the [National Agriculture Statistics Service](http://quickstats.nass.usda.gov) in United States Department of Agriculture and registered groundwater well data from [Nebraska Department of Natural Resources](http://quickstats.nass.usda.gov), I built an Agate custom computation to find whether a well was active during a given year. Next, I found the number of active wells for each county and also listed the number of irrigated acres. Currently, the complied information is saved in the [wells_acres_total.csv](WellDatabases/wells_acres_total.csv).\n\n###[ClimateData](ClimateData)\nI grabbed different climate-related statistics from the [High Plains Regional Climate Center](http://climod.unl.edu/) for use in an in-depth story on Nebraska climate. Those statistics include the frost season lengths, mean temperatures, and snowfall sums. I also had to build a [scrapper](ClimateData/snrScrapper.py) to grab the monthly Lincoln temperatures from the [UNL climate and data website](http://snr.unl.edu/lincolnweather/data/monthly-observed-vs-normals.asp) and how they compare to 30-year normals.\n\n###[SpouseBusinessStory](SpouseBusinessStory)\nUsing the US Census\' [Survey of Business Owners](http://www.census.gov/econ/sbo/getdata.html) from 2012 and 2007, I looked at how the distribution of businesses that are equally owned and operated by both spouses has changed. Several industries including utilities have significantly dropped perhaps due to the Great Recession effects.\n\n###[StormData](StormData)\nFrom the [National Centers for Environmental Information](https://www.ncdc.noaa.gov/stormevents/), I grabbed all events from January 1950 to 2016. To grab only Nebraska events from the plethora of files, I wrote a quick Python [program](StormData/findNebraska.py) and looked into how each storm event has increased/decreased over the years.\n\n###[USstatesNaturalBeauty](USstatesNaturalBeauty)\nFrom a [lovely cleaned csv](https://www.dropbox.com/s/qfv8xn4cffwqtvg/naturalamenities.csv?dl=0) that used the [USDA\'s Natural Amenities Index](http://www.ers.usda.gov/data-products/natural-amenities-scale.aspx), I used QGIS to join those scores to a United States map. The [final map](USstatesNaturalBeauty/natural-amenities-map.png) shows the lower scoring counties in dark brown or lighter yellow, and the higher ranking in either dark teal or a lighter one.\n'",Agate-analyzed data for data journalism class
https://github.com/jkroso/HTTP.jl,b'# HTTP.jl\n\nA client and server side implementation of HTTP for Julia.\n\nTo use it you will need the [Kip](https://github.com/jkroso/Kip.jl) module system.\nTo test it you will need Docker and [Jest](https://github.com/jkroso/Jest.jl)\n',An HTTP client and server for Julia
https://github.com/thedanschmidt/unsupervised-stock-features,"b'unsupervised-stock-features\n==============================\n\nUnsupervised learning of stock maret price features. Dan Schmidt\'s final project, Big Data 2016 HMC Gu.\n\nProject Organization\n------------\n\n    \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 LICENSE\n    \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 Makefile           <- Makefile with commands like `make data` or `make train`\n    \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 README.md          <- The top-level README for developers using this project.\n    \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 data\n    \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 external       <- Data from third party sources.\n    \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 interim        <- Intermediate data that has been transformed.\n    \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 processed      <- The final, canonical data sets for modeling.\n    \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 raw            <- The original, immutable data dump.\n    \xe2\x94\x82\n    \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 docs               <- A default Sphinx project; see sphinx-doc.org for details\n    \xe2\x94\x82\n    \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 models             <- Trained and serialized models, model predictions, or model summaries\n    \xe2\x94\x82\n    \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),\n    \xe2\x94\x82                         the creator\'s initials, and a short `-` delimited description, e.g.\n    \xe2\x94\x82                         `1.0-jqp-initial-data-exploration`.\n    \xe2\x94\x82\n    \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 references         <- Data dictionaries, manuals, and all other explanatory materials.\n    \xe2\x94\x82\n    \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 reports            <- Generated analysis as HTML, PDF, LaTeX, etc.\n    \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 figures        <- Generated graphics and figures to be used in reporting\n    \xe2\x94\x82\n    \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.\n    \xe2\x94\x82                         generated with `pip freeze > requirements.txt`\n    \xe2\x94\x82\n    \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 src                <- Source code for use in this project.\n    \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 __init__.py    <- Makes src a Python module\n    \xe2\x94\x82   \xe2\x94\x82\n    \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 data           <- Scripts to download or generate data\n    \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 make_dataset.py\n    \xe2\x94\x82   \xe2\x94\x82\n    \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 features       <- Scripts to turn raw data into features for modeling\n    \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 build_features.py\n    \xe2\x94\x82   \xe2\x94\x82\n    \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 models         <- Scripts to train models and then use trained models to make\n    \xe2\x94\x82   \xe2\x94\x82   \xe2\x94\x82                 predictions\n    \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 predict_model.py\n    \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 train_model.py\n    \xe2\x94\x82   \xe2\x94\x82\n    \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 visualization  <- Scripts to create exploratory and results oriented visualizations\n    \xe2\x94\x82\xc2\xa0\xc2\xa0     \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 visualize.py\n    \xe2\x94\x82\n    \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 tox.ini            <- tox file with settings for running tox; see tox.testrun.org\n\n\n--------\n\n<p><small>Project based on the <a target=""_blank"" href=""https://drivendata.github.io/cookiecutter-data-science/"">cookiecutter data science project template</a>. #cookiecutterdatascience</small></p>\n'","My final project for Professor Gu's Mathematics of Big Data class, Fall 2016 Harvey Mudd College."
https://github.com/streety/djangodistrict-geoviews-bokeh-intro,b'# djangodistrict-geoviews-bokeh-intro\n\n## Lightning talk given at the django district January 2017 new tech event on geoviews and bokeh\n\n',Lightning talk given at the django district January 2017 new tech event on geoviews and bokeh
https://github.com/anxingle/ctc_example,"b'# ctc_example\ncopy from  [sjchoi86](https://github.com/sjchoi86/Tensorflow-101)\n## one layer LSTM to classify mnist data Set      \nhere is the network struct:      \n<img src=""https://github.com/anxingle/ctc_example/blob/master/img/rnn_mnist_look.jpg?raw=true"" width=""700"" height=""400"" >         \n               \n## we can use CTC to recognition OCR \n'",CTC tutorial of OCR recognition(tensorflow0.9)
https://github.com/cessor/showmanship,"b""# showmanship\nA small presentation framework. You don't need PowerPoint. \n""",A small presentation framework. You don't need PowerPoint. 
https://github.com/rpicatoste/carnd-term1-project2-traffic-sign-classifier,"b'## Project: Build a Traffic Sign Recognition Program\n\nThe project repo has:\n\n* The jupyter notebook, that can be run cell by cell to obtain the results.\n* An html version of the notebook with the code run already and the results obtained.\n* The required writeup as markdown file, explaining how the different rubric points were addresses.\n* The python files with the code for the whole project.\n* A folder with images required for the writeup.\n* The csv file with the sign names, required to run the code.\n\nNOTE: The pickled data given for the project is not included and can be downloaded from [here](https://d17h27t6h515a5.cloudfront.net/topher/2017/February/5898cd6f_traffic-signs-data/traffic-signs-data.zip).'","Self-Driving Car Nanodegree Project 2, a traffic sign classifier"
https://github.com/LukasMosser/Jupetro,b'# Jupetro\nA collection of oil&amp;gas related Jupyter notebooks\n\nTo start an interactive session of this repository click here: [![Binder](http://mybinder.org/badge.svg)](http://mybinder.org:/repo/lukasmosser/jupetro)\n\n## License\nAll notebooks are published under the GPL3 license.\n\nCreated by Lukas Mosser 2016.',A collection of oil&gas related Jupyter notebooks
https://github.com/oist/TQM-demostrations,b'# TQM-demostrations\nthings that I might want to share with others in TQM\n',things that I might want to share with others in TQM
https://github.com/abhshkdz/neural-vqa-attention,"b""# neural-vqa-attention\n\nTorch implementation of an attention-based visual question answering model ([Stacked Attention Networks for Image Question Answering, Yang et al., CVPR16][1]).\n\n![Imgur](http://i.imgur.com/VbqIRZz.png)\n\n1. [Train your own network](#train-your-own-network)\n    1. [Extract image features](#extract-image-features)\n    2. [Preprocess VQA dataset](#preprocess-vqa-dataset)\n    3. [Training](#training)\n2. [Use a pretrained model](#use-a-pretrained-model)\n    1. [Pretrained models and data files](#pretrained-models-and-data-files)\n    2. [Running evaluation](#running-evaluation)\n3. [Results](#results)\n\nIntuitively, the model looks at an image, reads a question, and comes up with an answer to the question and a heatmap of where it looked in the image to answer it.\n\nThe model/code also supports referring back to the image multiple times ([Stacked Attention][1]) before producing the answer. This is supported via a `num_attention_layers` parameter in the code (default = 1).\n\n**NOTE**: This is NOT a state-of-the-art model. Refer to [MCB][7], [MLB][8] or [HieCoAtt][9] for that.\nThis is a simple, somewhat interpretable model that gets decent accuracies and produces [nice-looking results](#results).\nThe code was written about ~1 year ago as part of [VQA-HAT][12], and I'd meant to release it earlier, but couldn't get around to cleaning things up.\n\nIf you just want to run the model on your own images, download links to pretrained models are given below.\n\n## Train your own network\n\n### Preprocess VQA dataset\n\nPass `split` as `1` to train on `train` and evaluate on `val`, and `2` to train on `train`+`val` and evaluate on `test`.\n\n```\ncd data/\npython vqa_preprocessing.py --download True --split 1\ncd ..\n```\n```\npython prepro.py --input_train_json data/vqa_raw_train.json --input_test_json data/vqa_raw_test.json --num_ans 1000\n```\n\n### Extract image features\n\nSince we don't finetune the CNN, training is significantly faster if image features are pre-extracted. We use image features from VGG-19. The model can be downloaded and features extracted using:\n\n```\nsh scripts/download_vgg19.sh\nth prepro_img.lua -image_root /path/to/coco/images/ -gpuid 0\n```\n\n### Training\n\n```\nth train.lua\n```\n\n## Use a pretrained model\n\n### Pretrained models and data files\n\nAll files available for download [here][10].\n\n- `san1_2.t7`: model pretrained on `train`+`val` with 1 attention layer (SAN-1)\n- `san2_2.t7`: model pretrained on `train`+`val` with 2 attention layers (SAN-2)\n- `params_1.json`: vocabulary file for training on `train`, evaluating on `val`\n- `params_2.json`: vocabulary file for training on `train`+`val`, evaluating on `test`\n- `qa_1.h5`: QA features for training on `train`, evaluating on `val`\n- `qa_2.h5`: QA features for training on `train`+`val`, evaluating on `test`\n- `img_train_1.h5` & `img_test_1.h5`: image features for training on `train`, evaluating on `val`\n- `img_train_2.h5` & `img_test_2.h5`: image features for training on `train`+`val`, evaluating on `test`\n\n### Running evaluation\n\n```\nmodel_path=checkpoints/model.t7 qa_h5=data/qa.h5 params_json=data/params.json img_test_h5=data/img_test.h5 th eval.lua\n```\n\nThis will generate a JSON file containing question ids and predicted answers. To compute accuracy on `val`, use [VQA Evaluation Tools][13]. For `test`, submit to [VQA evaluation server on EvalAI][14].\n\n## Results\n\n**Format**: sets of 3 columns, col 1 shows original image, 2 shows 'attention' heatmap of where the model looks, 3 shows image overlaid with attention. Input question and answer predicted by model are shown below examples.\n![](http://i.imgur.com/Q0byOyp.jpg)\n\nMore results available [here][3].\n\n### Quantitative Results\n\nTrained on `train` for `val` accuracies, and trained on `train`+`val` for `test` accuracies.\n\n#### VQA v2.0\n\n| Method                | val     | test    |\n| ------                | ---     | ----    |\n| SAN-1                 | 53.15   | 55.28   |\n| SAN-2                 | 52.82   | -       |\n| [d-LSTM + n-I][4]     | 51.62   | 54.22   |\n| [HieCoAtt][9]         | 54.57   | -       |\n| [MCB][7]              | 59.14   | -       |\n\n#### VQA v1.0\n\n| Method                | test-std    |\n| ------                | --------    |\n| SAN-1                 | 59.87       |\n| SAN-2                 | 59.59       |\n| [d-LSTM + n-I][4]     | 58.16       |\n| [HieCoAtt][9]         | 62.10       |\n| [MCB][7]              | 65.40       |\n\n## References\n\n- [Stacked Attention Networks for Image Question Answering][1], Yang et al., CVPR16\n- [Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering][11], Goyal and Khot et al., CVPR17\n- [VQA: Visual Question Answering][6], Antol et al., ICCV15\n\n\n## Acknowledgements\n\n- Data preprocessing script borrowed from [VT-vision-lab/VQA_LSTM_CNN][4]\n\n## License\n\n[MIT][2]\n\n\n[1]: https://arxiv.org/abs/1511.02274\n[2]: https://abhshkdz.mit-license.org/\n[3]: https://computing.ece.vt.edu/~abhshkdz/neural-vqa-attention/figures/\n[4]: https://github.com/VT-vision-lab/VQA_LSTM_CNN\n[5]: http://visualqa.org/download.html\n[6]: http://arxiv.org/abs/1505.00468\n[7]: https://github.com/akirafukui/vqa-mcb\n[8]: https://github.com/jnhwkim/MulLowBiVQA\n[9]: https://github.com/jiasenlu/HieCoAttenVQA\n[10]: https://computing.ece.vt.edu/~abhshkdz/neural-vqa-attention/pretrained/\n[11]: https://arxiv.org/abs/1612.00837\n[12]: https://computing.ece.vt.edu/~abhshkdz/vqa-hat/\n[13]: https://github.com/VT-vision-lab/VQA\n[14]: https://evalai.cloudcv.org/featured-challenges/1/overview\n""",:question: Attention-based Visual Question Answering in Torch
https://github.com/mutaihillary/Mytodo_app,"b'# Flask TODOapp\n\nThe example TODOapp developed using **Flask**, **Vertabelo**, **SQLAlchemy** and **html ,css**. <br>\nThe initial version is deployed to **Heroku**: http://flask-todoapp.herokuapp.com/.\n\n\n'","Todo App built using Flask, SQLAlchemy and intergrate with login API"
https://github.com/codrut3/Kaggle-notebooks,"b'# Kaggle-notebooks\nA collection of my Jupyter notebooks, created while analyzing data during Kaggle competitions.\n'","A collection of my Jupyter notebooks, created while analyzing data during Kaggle competitions."
https://github.com/DJMedhaug/Boggler,"b'<div class=""content"">\n\n<div class=""item"">\n  <h1> Boggle solver</h1>\n\n<h1>Purpose</h1>\n<p>\nPurpose:  This project introduces Python modules as a way of breaking more \ncomplex Python programs into smaller, simpler and more reusable parts. game_dict.py\nis a Python module that you must complete.  \nThe boggler project also \nprovides more practice with recursive depth-first search. \n</p>\n\n<p>In addition, this project gives a small introduction to object-oriented programming. \nboggle_board.py is a module that you \ndon\'t have to modify, but you do need to read and understand. The boggle_board module\ncreates a BoggleBoard class.  Your boggler program will create and use a single\nBoggleBoard object.  In the following project, you will create classes and objects \nyourself. \n</p>\n\n\n<h1>Pair Assignment</h1>\n  <p>Work together with one classmate.\n  Make sure you both have a solid understanding of the core\n  recursive algorithms in this program, including the way we\n  manage marking tiles that are already in use on the current\n  path.  (This is very typical of depth-first search algorithms.)\n  </p>\n</div>\n\n\n<div class=""item"">\n<h1>Boggle solver</h1>\n  <p>Boggle is a popular word game in which players search for\n  words that can be formed by the letters on a set of tiles\n  in a 4x4 grid.  <a\n  href=""http://en.wikipedia.org/wiki/Boggle"">Wikipedia has a brief\n  description.</a></p>\n  <img src=""https://upload.wikimedia.org/wikipedia/commons/f/f4/Boggle.jpg"" />\n\n  <p>This program reads a boggle board\n  and a dictionary of possible solutions, and prints\n  all the dictionary words that can be formed for the board, following\n  standard Boggle rules.  Words can be formed starting from any\n  position, continuing horizontally, vertically, or diagonally,\n  without using any tile twice in the same word. The list of found\n  words is printed in alphabetical order. \n  </p>\n\n  <h2>Requirements</h2>\n  <p>The behavior of your program in a shell (command line) should be exactly like this.\n  There is also a graphical display, which is not shown here, and is not graded.\n  </p>\n<code><pre>$ python3 boggler.py ""ammaglxxxxpaxxxh"" shortdict.txt \nalpha 2\ngamma 2\nTotal score:  4\nPress enter to end\n</pre></code>\n\n<code><pre>$ python3 boggler.py ""ammaglxxxxpaxxxh"" dict.txt\nalp 1\nalpha 2\ngal 1\ngamma 2\nhap 1\nlag 1\nlam 1\nmag 1\nmax 1\nTotal score:  11\nPress enter to end\n</pre></code>\n\n  <h2>Starting materials</h2>\n  <ul>\n    <li><a href=""/boggle/graphics.py"">graphics.py</a> (don\'t change this)</li>\n    <li><a href=""/boggle/boggle_board.py"">boggle_board.py</a> (don\'t change this)</li>\n    <li><a href=""/boggle/test_harness.py"">test_harness.py</a> (for testing)</li>    \n    <li><a href=""/boggle/grid.py"">grid.py</a> (for display, don\'t change this)</li>  \n    <li><a href=""/boggle/game_dict.py"">game_dict.py</a> You must complete this and turn it in</li>  \n    <li><a href=""/boggle/boggler.py"">boggler.py</a> You must complete this and turn it in</li>    \n    <li><a href=""/boggle/shortdict.txt"">shortdict.txt</a> A short list of words, \n        useful for testing</li>\n    <li><a href=""/boggle/dict.txt"">dict.txt</a> A more typical list of \n    approximately 40,000 common English words.  This is usually enough to beat \n    human boggle players. Serious players of Scrabble and crossword puzzles \n    use larger lists that include less common words.</li>\n    </ul>\n    </p>\n    \n    <h2>How to get started</h2>\n    <p>Start with the game dictionary.  I have included a pretty good test suite so that \n    you can test it separately from the rest of the boggler program. Notice the funny\n    code at the end of the file for testing whether the game_dict module is being \n    called as a main program.  This is so that you can test it like this: \n    </p>\n<code><pre>\n$ python3 game_dict.py shortdict.txt \n***FAILED***  First word in dictionar (alpha)  Expected: | 1 | but got | 0 |\n***FAILED***  Last word in dictionary (omega)  Expected: | 1 | but got | 0 |\n***FAILED***  Within dictionary (beta)  Expected: | 1 | but got | 0 |\n***FAILED***  Within dictionary (delta)  Expected: | 1 | but got | 0 |\n***FAILED***  Within dictionary (gamma)  Expected: | 1 | but got | 0 |\n***FAILED***  Prefix of first word (al)  Expected: | 2 | but got | 0 |\n***FAILED***  Prefix of last word (om)  Expected: | 2 | but got | 0 |\n***FAILED***  Prefix of interior word (bet)  Expected: | 2 | but got | 0 |\n***FAILED***  Prefix of interior word (gam)  Expected: | 2 | but got | 0 |\n***FAILED***  Prefix of interior word (del)  Expected: | 2 | but got | 0 |\n   Passed --  Before any word (aardvark)  result:  0\n   Passed --  After all words (zephyr)  result:  0\n   Passed --  Interior non-word (axe)  result:  0\n   Passed --  Interior non-word (carrot)  result:  0\n   Passed --  Interior non-word (hagiography)  result:  0\n***FAILED***  First word in dictionar (alpha)  Expected: | 1 | but got | 0 |\n***FAILED***  Last word in dictionary (omega)  Expected: | 1 | but got | 0 |\n   Passed --  Short word omitted (beta)  result:  0\n </pre></code>\n <p>You will know you probably have a working game dictionary when you can pass these\n tests.  (Writing executable test cases before writing the code that passes them is part \n of a practice called \n &ldquo;test-driven development,&rdquo; and is common in the so-called &ldquo;agile&rdquo; software\n development methodologies that are popular today.) \n </p>\n <p>I suggest writing the search function as a linear search first: Check each word \n in order, using the Python built-in method <kbd>startswith</kbd> to check for a \n prefix match. When you have a linear search function working, you can move on to \n the rest of the program and come back later to make the search function faster\n with binary search.</p>\n <p>When you have your game_dict module working, move on to boggler.py.  Use the \n game board module to represent the board.  (Duplicating any of the functionality of board.py \n within your boggler.py source file will be considered an error.) The places you will \n need to work on boggler.py are marked with FIXME comments.  The main one is the \n recursive <kbd>find_words</kbd> function.  I have included several FIXME comments \n to guide you.</p>\n <p>boggler.py includes a <kbd>dedup</kbd> function for putting a results list into \n sorted order, without duplicate words.  Instead of keeping the original results \n (with duplicates) in a list, you could use the Python set data structure.  If you do,\n you should put the words into the set from the <kbd>find_words</kbd> function, and \n then <kbd>dedup</kbd> should be replaced with a function that extracts a sorted \n list from the set.</p>\n <p><strong>Optional:</strong> When your boggler.py program is working satisfactorily is a good time to return to\n game_dict.py and change the linear search to a binary search.  This requires some careful\n thought:  If the binary search does not find an exact match, which dictionary word\n should you check to see if the word you were looking for could be a prefix of a word\n in the dictionary?  The included test suite will help you check whether you got it right. \n It is better to turn in a working boggler program that uses linear search, than a \n broken boggler program that uses binary search (especially if the binary search is \n what is broken). \n </p>\n    \n</div>\n<div class=""item"">\n  <h1>Grading rubric</h1>\n  <table width=""85%"" border=""0"" cellpadding=""2"">\n    <tr>\n      <th colspan=""2"" scope=""col"">Functional correctness</th>\n      <th width=""8%"" scope=""col"">&nbsp;</th>\n      <th width=""60%"" scope=""col"">&nbsp;</th>\n      <th width=""1%"" scope=""col"">&nbsp;</th>\n      <th width=""3%"" scope=""col"">40</th>\n    </tr>\n    <tr>\n      <td width=""5%"">&nbsp;</td>\n      <td width=""23%""><p>Exactly meets input/output spec</p>        </td>\n      <td>10</td>\n      <td>5 = minor discrepancy, 0 = ignored spec</td>\n      <td>&nbsp;</td>\n      <td>&nbsp;</td>\n    </tr>\n    <tr>\n      <td>&nbsp;</td>\n      <td>Correct results: Word finding</td>\n      <td>25</td>\n      <td><p>25 = works for all cases (finds all words, and nothing\n\telse), 18 = works for almost all cases, 12 = works for most\n\tcases (e.g., not when one word is a prefix of another), 5 = works for some cases</p></td>\n      <td>&nbsp;</td>\n      <td>&nbsp;</td>\n    </tr>\n    <tr>\n      <td>&nbsp;</td>\n      <td>Correct results: Scoring</td>\n      <td>5</td>\n      <td>5 points if all words of three or more letters are correctly\n      scored, and the correct total is shown</td>\n      <td>&nbsp;</td>\n      <td>&nbsp;</td>\n    </tr>\n    <tr>\n      <th colspan=""2"">Other requirements</th>\n      <td>&nbsp;</td>\n      <td>&nbsp;</td>\n      <td>&nbsp;</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <td>&nbsp;</td>\n      <td><p>Header docstring</p>        </td>\n      <td>10</td>\n      <td>10 = as specified, 7 = minor issue, 5 = as #comment, 0 = missing</td>\n      <td>&nbsp;</td>\n      <td>&nbsp;</td>\n    </tr>\n    <tr>\n      <td>&nbsp;</td>\n      <td>Function header docstrings</td>\n      <td>8</td>\n      <td>8 = good docstrings in all functions, 6 = minor problems, 4 = incorrect or multiple missing, 0 = docstrings not provided</td>\n      <td>&nbsp;</td>\n      <td>&nbsp;</td>\n    </tr>\n    <tr>\n      <td>&nbsp;</td>\n      <td>Correct use of modules</td>\n      <td>12</td>\n      <td><p>12 = modules used correctly, no violation of abstraction,\n\t8 = decomposition has some issues, such as misuse of global\n\tvariables, use of magic numbers instead of symbolic\n\tconstants,  0 = failure to use modules</p></td>\n      <td>&nbsp;</td>\n      <td>&nbsp;</td>\n    </tr>\n    <tr>\n      <td>&nbsp;</td>\n      <td>Program style and readability</td>\n      <td>10</td>\n      <td><p>10 Good variable names, indentation, etc --- very readable code, 8 = minor issues, such as inconsistent indentation, 5 = major issues that interfere with readability of code, 0 = unreadable mess </p></td>\n      <td>&nbsp;</td>\n      <td>&nbsp;</td>\n    </tr>\n    <tr>\n      <th colspan=""2"">Total</th>\n      <td>&nbsp;</td>\n      <td>&nbsp;</td>\n      <td>&nbsp;</td>\n      <td>80</td>\n    </tr>\n  </table>\n  <p>I can\'t anticipate all issues that may be encountered in grading,\n  so points may be deducted for other issues not listed in the\n  rubric.    A program that does not compile and run (e.g., because of a syntax error) starts with 0 points for functional correctness, but I may award some partial credit. </p>\n  <p>&nbsp;</p>\n  </div>\n'",Dana and Kevin's Boggler project.
https://github.com/kacerchio/CS505-final-project,"b'# CS505-final-project\n### *by Kristel Tan and Nisa Gurung*\n\nThe main goal of this project was to identify trends in movie attributes that correlate with a movie\xe2\x80\x99s success in respect to the genre that it belongs in. Movie attributes analyzed include the duration of a movie, its allocated budget, average actor similarity, title length, director hits, and plot keywords. For the purposes of this project, we defined a film\xe2\x80\x99s success by how much gross profit it made in the box office, independent of critic reviews and ratings. By analyzing this data, we expect to be able to make recommendations for such movie attributes and predictions of whether or not a given set of attributes will result in a highly profitable movie launch. \n\n#Datasets \n\nThe dataset to support this project was initially retrieved from \xe2\x80\x98kaggle.com\xe2\x80\x99. It was a CSV file of 5,000 movie attributes scraped from IMDB and \xe2\x80\x98the-numbers.com\xe2\x80\x99. We cleaned this dataset of entries that were missing relevant attributes. Because this step eliminated a significant amount of data, we manually scraped and cleaned more data from IMDB\xe2\x80\x99s top box office lists for the past 12 years. We then merged this data back with the original dataset, totaling roughly 6,000 that had all of the attributes we were looking to analyze. We removed movies that were missing column entries because we wanted to ensure that every movie could contribute to every analysis equally. \n\n#Techniques \n\nFor this project, we planned to use two different techniques to help identify movie attributes related to its success. These methods include multiple linear regression and k-means clustering. Multiple linear regression was a natural choice because it gives us an idea of whether or not certain independent variables are significant to any given genre and how strongly they correlate with its gross profit. In particular, we looked at the duration, title length, budget, number of hits the director has produced, and average actor similarity in relation to the gross profit it made. Applying an OLS linear regression function to this data provided us with coefficients and confidence intervals to identify correlations. \n\nFurthermore, we used k-means clustering to analyze a different kind of attribute, which were plot keywords. We chose to focus on evaluating a movie\xe2\x80\x99s plot keywords for the top \n1,000 profitable movies and least 1,000 profitable movies of our overall dataset. In other words, we clustered these movies by the most similar plot keywords that are associated with them, provided from IMDB\xe2\x80\x99s website. Clusters formed based upon words of similar themes and meaning. Individuals points stayed closer to the center of the cluster or strayed away from it depending upon how similar it was to the other plot keywords in that cluster. At a high level, this gave us an idea of what plot keywords formed the largest clusters, which also indicated what storylines movie critics and audiences found most interesting.\n\n#Scripts \nThe scripts for this project are data-collection.ipynb which is in the data folder and Regression.ipynb and Kmeans.ipynb. \n\nWe retrieved our movie_master_dataset.csv from data-collection.ipynb. Regression.ipynb runs multiple linear regression on five popular genres: action, thriller, drama, romance and comedies and also on three unpopular genres: documentary, history and musical. Kmeans.ipynb runs k-means clustering on the plot keywords of the top 1000 profitable movies and also the 1000 least profitable movies. '",This is a repository for analysis on the success of movie releases based upon IMDB movie data. 
https://github.com/bird-house/twitcher,"b'============================\nTwitcher: OWS Security Proxy\n============================\n\n.. image:: https://img.shields.io/badge/docs-latest-brightgreen.svg\n   :target: http://twitcher.readthedocs.io/en/latest/?badge=latest\n   :alt: Documentation Status\n   \n.. image:: https://zenodo.org/badge/DOI/10.5281/zenodo.3736114.svg\n   :target: https://doi.org/10.5281/zenodo.3736114\n   :alt: Zenodo DOI\n\n.. image:: https://github.com/bird-house/twitcher/actions/workflows/tests.yml/badge.svg?branch=master\n   :target: https://github.com/bird-house/twitcher/actions/workflows/tests.yml\n   :alt: GitHub Actions Status\n\n.. image:: https://img.shields.io/github/license/bird-house/twitcher.svg\n   :target: https://github.com/bird-house/twitcher/blob/master/LICENSE.txt\n   :alt: GitHub license\n\n.. image:: https://badges.gitter.im/bird-house/birdhouse.svg\n   :target: https://gitter.im/bird-house/birdhouse?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge\n   :alt: Join the chat at https://gitter.im/bird-house/birdhouse\n\n\nTwitcher (the bird-watcher)\n  *a birdwatcher mainly interested in catching sight of rare birds.* (`Leo <https://dict.leo.org/ende/index_en.html>`_).\n\nTwitcher is a security proxy for OWS services like Web Processing Services (WPS).\nThe proxy service uses OAuth2 access tokens to protect the OWS service access.\nIn addition one can also use X.509 certificates for client authentication.\n\nThe implementation is not restricted to WPS services.\nIt will be extended to more OWS services like WMS (Web Map Service)\nand might also be used for Thredds catalog services.\n\nTwitcher extensions:\n\n* `Magpie`_ is an AuthN/AuthZ service provided by the `PAVICS`_ project.\n* `Weaver`_  middleware by CRIM_. A reimplementation of an old `Twitcher fork <https://github.com/ouranosinc/twitcher/>`_\n  for workflow execution and a Swagger RESTful interface for Web Processing Services.\n\nTwitcher is implemented with the Python `Pyramid`_ web framework.\n\nYou can try Twitcher online using Binder, or view the notebooks on NBViewer.\n\n.. image:: https://mybinder.org/badge_logo.svg\n   :target: https://mybinder.org/v2/gh/bird-house/twitcher.git/master?filepath=notebooks\n   :alt: Binder Launcher\n   :height: 20\n\n.. image:: https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg\n   :target: https://nbviewer.jupyter.org/github/bird-house/twitcher/tree/master/notebooks/\n   :alt: NBViewer\n   :height: 20\n\nTwitcher is part of the `Birdhouse`_ project. The documentation is on `ReadTheDocs`_.\n\nTwitcher `Docker`_ images are also available for most recent tagged versions.\n\n.. _Birdhouse: http://birdhouse.readthedocs.io/en/latest/\n.. _Pyramid: http://www.pylonsproject.org\n.. _ReadTheDocs: http://twitcher.readthedocs.io/en/latest/\n.. _Magpie: https://github.com/Ouranosinc/Magpie\n.. _PAVICS: https://ouranosinc.github.io/pavics-sdi/index.html\n.. _Weaver: https://github.com/crim-ca/weaver\n.. _CRIM: https://www.crim.ca/en\n.. _Swagger: https://swagger.io/\n.. _Docker: https://cloud.docker.com/u/birdhouse/repository/docker/birdhouse/twitcher/general\n'",Security Proxy for Web Processing Services (WPS)
https://github.com/opentrv/ors,"b""# ors\nOpenTRV REST Server\n\nyou may need to set up the virtual python env\n\nsudo apt-get install python-virtualenv\n\nvirtualenv <directory name for virtualenv>\nsource <directory of virtualenv>/bin/activate\npip install django (I think I'm using 1.8.3)\npip install selenium\nthen python manage test\n\n""",OpenTRV REST Server
https://github.com/LibraryOfCongress/data-exploration,"b'# Library of Congress Data Exploration\n\n## About this repository\n\nThe data-exploration repository includes Jupyter notebooks and example scripts using openly available Library of Congress Digital Collections or records. Nearly all of these notebooks use the APIs available through https://www.loc.gov. \n\nFor more information on how to use the loc.gov APIs, see https://www.loc.gov/apis/ .  For more information about how to use the Jupyter notebooks available in this repository, see https://libraryofcongress.github.io/data-exploration/all-tutorials.html \n\nContact LC-labs@loc.gov for questions about these notebooks, or to suggest a new addition. \n'",Tutorials for working with Library of Congress collections data
https://github.com/Databingo/Databingo,b'# Databingo\ndatabooking examples\n',Databooking examples
https://github.com/mat-esp-2015/funcoes-douglas-paulo-julyana,"b'# Fun\xc3\xa7\xc3\xb5es e programa\xc3\xa7\xc3\xa3o defensiva\n\nParte do curso\n[Matem\xc3\xa1tica Especial I](http://www.leouieda.com/matematica-especial/)\nda [UERJ](http://www.uerj.br/).\n\nMinistrado por [Leonardo Uieda](http://www.leouieda.com/).\n\n## Objetivos\n\n* Aprender a utilizar fun\xc3\xa7\xc3\xb5es para manejar seu c\xc3\xb3digo\n* Desenvolver t\xc3\xa9cnicas para produzir c\xc3\xb3digo mais correto\n\n## Leitura recomendada\n\n* Li\xc3\xa7\xc3\xa3o de fun\xc3\xa7\xc3\xb5es do [Software Carpentry](http://software-carpentry.org/)\n  (vers\xc3\xa3o 4)\n* Li\xc3\xa7\xc3\xa3o de programa\xc3\xa7\xc3\xa3o defensiva do Software Carpentry\n  :http://swcarpentry.github.io/python-novice-inflammation/08-defensive.html\n* http://www.scipy-lectures.org/intro/language/functions.html\n\n## Prepara\xc3\xa7\xc3\xa3o\n\nUtilize o link enviado por e-mail para criar um reposit\xc3\xb3rio para seu grupo.\nCada membro do grupo deve clicar no link e selecionar o nome do grupo criado na\npr\xc3\xa1tica passada.\nN\xc3\xa3o se esque\xc3\xa7a de sair de sua conta **no github.com e no\nclassroom.github.com**.\n\n**Crie um arquivo** chamado `alunos.txt` com os **nomes completos** de todos os\nintegrantes do grupo. Inclua tamb\xc3\xa9m a **qual turma** pertencem.\n\nAs tarefas para serem feitas est\xc3\xa3o em um [Jupyter\nnotebook](http://jupyter.org/).\nPara utilizar o Jupyter, voc\xc3\xaa precisa iniciar um servidor de notebook\nno seu computador.\nAbra o bash e digite:\n\n    $ jupyter notebook\n\nEspere um pouco at\xc3\xa9 aparecer algo como:\n\n    [I 10:50:47.370 NotebookApp] Serving notebooks from local directory: /home/leo\n    [I 10:50:47.370 NotebookApp] 0 active kernels\n    [I 10:50:47.370 NotebookApp] The IPython Notebook is running at: http://localhost:8888/\n    [I 10:50:47.370 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n\nIsso deve abrir o seu navegador padr\xc3\xa3o tamb\xc3\xa9m em uma p\xc3\xa1gina no endere\xc3\xa7o\n[http://127.0.0.1:8888](http://127.0.0.1:8888).\nEssa p\xc3\xa1gina ir\xc3\xa1 te mostrar as pastas que est\xc3\xa3o em seu computador\n(a partir da pasta onde voc\xc3\xaa rodou `$ jupyter notebook`).\n\n## Tarefas\n\nSiga as instru\xc3\xa7\xc3\xb5es em no notebook disponibilizado no reposit\xc3\xb3rio.\nAs tarefas e **suas solu\xc3\xa7\xc3\xb5es devem estar contidas nesse notebook**.\nPor isso, fa\xc3\xa7a commits de suas mudan\xc3\xa7as ao notebook.\n\n**AVISO**: N\xc3\xa3o esque\xc3\xa7a de verificar se abriu o notebook no clone correto!\n\n**AVISO 2**: Ap\xc3\xb3s cada mudan\xc3\xa7a, `git add` + `git commit` + `git push`.\n\n**AVISO 3**: ANTES de come\xc3\xa7ar: `git pull origin master`\n\n## Dicas\n\n* Fa\xc3\xa7am muitos **commits**. Quanto mais melhor.\n* N\xc3\xa3o se esque\xc3\xa7a do **push**.\n* Utilize **mensagens de commit** descritivas. ""Completei a tarefa 1"" \xc3\xa9 melhor que\n  ""mudan\xc3\xa7a"".\n* Escolha nomes descritivos para **vari\xc3\xa1veis**. ""temperatura"" \xc3\xa9 melhor que ""a"".\n* **Descreva o que (e por que) voc\xc3\xaa fez** em coment\xc3\xa1rios e c\xc3\xa9lulas de texto.\n  Isso ser\xc3\xa1 muito \xc3\xbatil quando voc\xc3\xaa voltar a essa tarefa depois.\n* Preste aten\xc3\xa7\xc3\xa3o aos **detalhes**. Leia as instru\xc3\xa7\xc3\xb5es com aten\xc3\xa7\xc3\xa3o.\n\n## Checklist da avalia\xc3\xa7\xc3\xa3o\n\nUtilizaremos a lista abaixo para avaliar a sua solu\xc3\xa7\xc3\xa3o. Cada item poder\xc3\xa1\nreceber a nota ""total"" se atender perfeitamente ao crit\xc3\xa9rio, ""parcial"" (metade\nda nota) se atender parcialmente ao crit\xc3\xa9rio, ou ""zero"" se falhar ao crit\xc3\xa9rio.\nNote que a nota m\xc3\xa1xima (incluindo a b\xc3\xb4nus) \xc3\xa9 10.\n\n    - [] Mensagens de commit que explicam claramente a mudan\xc3\xa7a que foi feita\n      [total|parcial|zero] 0.5 pt\n    - [] Formata\xc3\xa7\xc3\xa3o do c\xc3\xb3digo apropriada.\n      Ex: `espacamento = (maximo - minimo)/(N - 1)`,\n      `indices = range(0, N, 1)`, `dado = dados[i + 1]` == BOM.\n      `espacamento=(maximo-minimo)/ (N-1)`, `indices=range (0,N,1)`,\n      `dado= dados [i+ 1]` == RUIM [total|parcial|zero] 0.5 pt\n    - [] Utilizar vari\xc3\xa1veis ao inv\xc3\xa9s de colocar n\xc3\xbamero ""na m\xc3\xa3o"".\n      Ex: `for i in range(0, N):`, `A[k][N - 1]` == BOM.\n      `for i in range(0, 39):`, `A[k][47]` == RUIM. [total|parcial|zero] 1 pt\n    - [] C\xc3\xb3digo com coment\xc3\xa1rios que explicam ""por que"" algo foi feito, n\xc3\xa3o s\xc3\xb3\n      ""o que"" foi feito [total|parcial|zero] 1 pt\n    - [] Nomes de vari\xc3\xa1veis descritivos. Ex: `temperatura`, `media_por_hora`,\n      `linha`, `somatorio` == bom. `a`, `var`, `meh`, `lista` == ruim.\n      [total|parcial|zero] 2 pt\n    - [] C\xc3\xb3digo produz a solu\xc3\xa7\xc3\xa3o correta (**exatamente** como deveria ser\n      impresso) [total|parcial|zero] 5 pt\n    - [] Tarefa b\xc3\xb4nus [total|parcial|zero] 1 pt extra (n\xc3\xa3o ser\xc3\xa1 considerado\n      caso a nota j\xc3\xa1 seja 10)\n\n## License\n\nAll content can be freely used and adapted under the terms of the\n[Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/).\n\n![Creative Commons License](https://i.creativecommons.org/l/by/4.0/88x31.png)\n'",funcoes-douglas-paulo-julyana created by Classroom for GitHub
https://github.com/elmasria/titanic-survival-exploration,"b""# Titanic Survival Exploration\n\n## Project Overview\n\nCreate decision functions that attempt to Accurately predict survival outcomes, for at least 80%, from the 1912 Titanic disaster based on each passenger's features, such as sex and age.\n\n### Install\n\n#### Software Requirements\n\n- [Python 2.7](https://www.python.org/download/releases/2.7/)\n- [NumPy](http://www.numpy.org/)\n- [Pandas](http://pandas.pydata.org)\n- [matplotlib](http://matplotlib.org/)\n- [scikit-learn](http://scikit-learn.org/stable/)\n- [Jupyter Notebook](http://ipython.org/notebook.html)\n\n### Run\n\nIn a terminal or command window, navigate to the top-level project directory `titanic_survival_exploration/`, run one of the following commands:\n\n```bash\njupyter notebook titanic_survival_exploration.ipynb\n```\nor\n```bash\nipython notebook titanic_survival_exploration.ipynb\n```\n\nThis will open the Jupyter Notebook software and project file in your web browser.\n\n### Data\n\nThe dataset used in this project is included as `titanic_data.csv`. This dataset is provided by Udacity and contains the following attributes:\n\n**Features**\n- `pclass` : Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n- `name` : Name\n- `sex` : Sex\n- `age` : Age\n- `sibsp` : Number of Siblings/Spouses Aboard\n- `parch` : Number of Parents/Children Aboard\n- `ticket` : Ticket Number\n- `fare` : Passenger Fare\n- `cabin` : Cabin\n- `embarked` : Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n\n**Target Variable**\n- `survival` : Survival (0 = No; 1 = Yes)\n\n\n""",Titanic Survival Exploration
https://github.com/ducha-aiki/pytorch-sift,"b""This is an differentiable [pytorch](https://github.com/pytorch/pytorch) implementation of SIFT patch descriptor. It is very slow for describing one patch, but quite fast for batch. It can be used for descriptop-based learning shape of affine feature.\n\n**UPD 08/2019** : pytorch-sift is added to [kornia](https://github.com/arraiyopensource/kornia) and available by `kornia.features.SIFTDescriptor`\n\nThere are different implementations of the SIFT on the web. I tried to match [Michal Perdoch implementation](https://github.com/perdoch/hesaff/blob/master/siftdesc.cpp), which gives high quality features for image retrieval [CVPR2009](http://cmp.felk.cvut.cz/~chum/papers/perdoch-cvpr09.pdf). However, on planar datasets, it is inferior to [vlfeat implementation](http://www.vlfeat.org/sandbox/api/sift.html).\nThe main difference is gaussian weighting window parameters, so I have made a vlfeat-like version too.  MP version weights patch center much more (see image below, left) and additionally crops everything outside the circular region. Right is vlfeat version\n\n\n![Michal Perdoch kernel](/img/mp_kernel.png)\n![vlfeat kernel](/img/vlfeat_kernel.png)\n\n\n\n```python\n\ndescriptor_mp_mode = SIFTNet(patch_size = 65,\n                        sigma_type= 'hesamp',\n                        masktype='CircularGauss')\n\ndescriptor_vlfeat_mode = SIFTNet(patch_size = 65,\n                        sigma_type= 'vlfeat',\n                        masktype='Gauss')\n\n```\nResults:\n\n![hpatches mathing results](/img/hpatches-results.png)\n\n\n```\nOPENCV-SIFT - mAP \n   Easy     Hard      Tough     mean\n-------  -------  ---------  -------\n0.47788  0.20997  0.0967711  0.26154\n\nVLFeat-SIFT - mAP \n    Easy      Hard      Tough      mean\n--------  --------  ---------  --------\n0.466584  0.203966  0.0935743  0.254708\n\nPYTORCH-SIFT-VLFEAT-65 - mAP \n    Easy      Hard      Tough      mean\n--------  --------  ---------  --------\n0.472563  0.202458  0.0910371  0.255353\n\nNUMPY-SIFT-VLFEAT-65 - mAP \n    Easy      Hard      Tough      mean\n--------  --------  ---------  --------\n0.449431  0.197918  0.0905395  0.245963\n\nPYTORCH-SIFT-MP-65 - mAP \n    Easy      Hard      Tough      mean\n--------  --------  ---------  --------\n0.430887  0.184834  0.0832707  0.232997\n\nNUMPY-SIFT-MP-65 - mAP \n    Easy     Hard      Tough      mean\n--------  -------  ---------  --------\n0.417296  0.18114  0.0820582  0.226832\n\n\n```\n    \nSpeed: \n- 0.00246 s per 65x65 patch - [numpy SIFT](https://github.com/ducha-aiki/numpy-sift)\n- 0.00028 s per 65x65 patch - [C++ SIFT](https://github.com/perdoch/hesaff/blob/master/siftdesc.cpp)\n- 0.00074 s per 65x65 patch - CPU, 256 patches per batch\n- 0.00038 s per 65x65 patch - GPU (GM940, mobile), 256 patches per batch\n- 0.00038 s per 65x65 patch - GPU (GM940, mobile), 256 patches per batch\n\n\n\n\nIf you use this code for academic purposes, please cite the following paper:\n\n```\n@InProceedings{AffNet2018,\n    title = {Repeatability Is Not Enough: Learning Affine Regions via Discriminability},\n    author = {Dmytro Mishkin, Filip Radenovic, Jiri Matas},\n    booktitle = {Proceedings of ECCV},\n    year = 2018,\n    month = sep\n}\n\n```\n\n""",PyTorch implementation of SIFT descriptor
https://github.com/sarineb/lebanon-refugee-data,b'# lebanon-refugee-data\nAnalyzing data from informal settlements of Syrian refugees in Lebanon\n',Analyzing data from informal settlements of Syrian refugees in Lebanon
https://github.com/bohare/MNIST-Classification,b'# MNIST-Classification\nClassifying standard MNIST dataset with NNs.\n\nTrying Neural Networks out on Git.\n',Classifying standard MNIST dataset with NNs.
https://github.com/ccopelan/ubcs3-exercises,"b'# ubcs3-exercises\n\nExercises for the UBC Scientific Software Seminar\n\n## Exercise 1. \n\nWrite a Jupyter notebook describing a project that you\'re interested in.\n\nMy notebook is called ""Exercise1Notebook""\n'",Exercises for the UBC Scientific Software Seminar
https://github.com/matthiaskoenig/sbmlutils,"b'.. image:: https://github.com/matthiaskoenig/sbmlutils/raw/develop/docs_builder/images/sbmlutils-logo-60.png\n   :align: left\n   :alt: sbmlutils logo\n\nsbmlutils: python utilities for SBML\n====================================\n\n.. image:: https://github.com/matthiaskoenig/sbmlutils/workflows/CI-CD/badge.svg\n   :target: https://github.com/matthiaskoenig/sbmlutils/workflows/CI-CD\n   :alt: GitHub Actions CI/CD Status\n\n.. image:: https://img.shields.io/pypi/v/sbmlutils.svg\n   :target: https://pypi.org/project/sbmlutils/\n   :alt: Current PyPI Version\n\n.. image:: https://img.shields.io/pypi/pyversions/sbmlutils.svg\n   :target: https://pypi.org/project/sbmlutils/\n   :alt: Supported Python Versions\n\n.. image:: https://img.shields.io/pypi/l/sbmlutils.svg\n   :target: http://opensource.org/licenses/LGPL-3.0\n   :alt: GNU Lesser General Public License 3\n\n.. image:: https://codecov.io/gh/matthiaskoenig/sbmlutils/branch/develop/graph/badge.svg\n   :target: https://codecov.io/gh/matthiaskoenig/sbmlutils\n   :alt: Codecov\n\n.. image:: https://readthedocs.org/projects/sbmlutils/badge/?version=latest\n   :target: https://sbmlutils.readthedocs.io/en/latest/?badge=latest\n   :alt: Documentation Status\n\n.. image:: https://zenodo.org/badge/55952847.svg\n   :target: https://zenodo.org/badge/latestdoi/55952847\n   :alt: Zenodo DOI\n\n.. image:: https://img.shields.io/badge/code%20style-black-000000.svg\n   :target: https://github.com/ambv/black\n   :alt: Black\n\n.. image:: http://www.mypy-lang.org/static/mypy_badge.svg\n   :target: http://mypy-lang.org/\n   :alt: mypy\n\nsbmlutils is a collection of python utilities for working with\n`SBML <http://www.sbml.org>`__ models implemented on top of\n`libsbml <http://sbml.org/Software/libSBML>`__ and other libraries\nwith source code available from `https://github.com/matthiaskoenig/sbmlutils <https://github.com/matthiaskoenig/sbmlutils>`__.\n\nFeatures include among others\n\n-  helper functions for model creation, manipulation, and annotation\n-  HTML reports of SBML models `https://sbml4humans.de <https://sbml4humans.de>`__.\n-  file converters (XPP)\n\nThe documentation is available on `https://sbmlutils.readthedocs.io <https://sbmlutils.readthedocs.io>`__. \nIf you have any questions or issues please `open an issue <https://github.com/matthiaskoenig/sbmlutils/issues>`__.\n\n.. image:: docs/presentations/reproducibility_center_2021/screenshot.png\n   :target: https://youtu.be/SxIq8qeXxD0?t=1261\n   :alt: sbmlutils introduction\n   :height: 200px\n\nHow to cite\n===========\n.. image:: https://zenodo.org/badge/55952847.svg\n   :target: https://zenodo.org/badge/latestdoi/55952847\n   :alt: Zenodo DOI\n\nContributing\n============\n\nContributions are always welcome! Please read the `contributing guidelines\n<https://github.com/matthiaskoenig/sbmlutils/blob/develop/.github/CONTRIBUTING.rst>`__ to\nget started.\n\nLicense\n=======\n\n* Source Code: `LGPLv3 <http://opensource.org/licenses/LGPL-3.0>`__\n* Documentation: `CC BY-SA 4.0 <http://creativecommons.org/licenses/by-sa/4.0/>`__\n\nThe sbmlutils source is released under both the GPL and LGPL licenses version 2 or\nlater. You may choose which license you choose to use the software under.\n\nThis program is free software: you can redistribute it and/or modify it under\nthe terms of the GNU General Public License or the GNU Lesser General Public\nLicense as published by the Free Software Foundation, either version 2 of the\nLicense, or (at your option) any later version.\n\nThis program is distributed in the hope that it will be useful, but WITHOUT ANY\nWARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A\nPARTICULAR PURPOSE. See the GNU General Public License for more details.\n\nFunding\n=======\nMatthias K\xc3\xb6nig is supported by the Federal Ministry of Education and Research (BMBF, Germany)\nwithin the research network Systems Medicine of the Liver (**LiSyM**, grant number 031L0054) \nand by the German Research Foundation (DFG) within the Research Unit Programme FOR 5151 \n""`QuaLiPerF <https://qualiperf.de>`__ (Quantifying Liver Perfusion-Function Relationship in Complex Resection - \nA Systems Medicine Approach)"" by grant number 436883643 and by grant number \n465194077 (Priority Programme SPP 2311, Subproject SimLivA).\nSBML4Humans was funded as part of `Google Summer of Code 2021 <https://summerofcode.withgoogle.com/>`__.\nMatthias K\xc3\xb6nig has received funding from the EOSCsecretariat.eu which has received funding \nfrom the European Union\'s Horizon Programme call H2020-INFRAEOSC-05-2018-2019, grant Agreement number 831644.\n\nInstallation\n============\n`sbmlutils` is available from `pypi <https://pypi.python.org/pypi/sbmlutils>`__ and \ncan be installed via:: \n\n    pip install sbmlutils\n\nRequirements\n------------\n`tkinter` is required which can be installed on linux via::\n\n    apt-get install python-tk\n    apt-get install python3-tk\n\nPlease see the respective installation instructions for your operating system.\n\nDevelop version\n---------------\nThe latest develop version can be installed via::\n\n    pip install git+https://github.com/matthiaskoenig/sbmlutils.git@develop\n\nOr via cloning the repository and installing via::\n\n    git clone https://github.com/matthiaskoenig/sbmlutils.git\n    cd sbmlutils\n    pip install -e .\n\nTo install for development use::\n\n    pip install -e .[development]\n\n\xc2\xa9 2017-2023 Matthias K\xc3\xb6nig\n'",Python utilities for SBML
https://github.com/richard-alexander/language-translation,b'# language-translation\nDeep learning: Simple model for language translation using encoder/decoder recurrent neural network (RNN)\n',Deep learning: Simple model for language translation using encoder/decoder recurrent neural network (RNN)
https://github.com/chrishokamp/constrained_decoding,"b'## Lexically Constrained Decoding with Grid Beam Search\n\nThis project is a reference implementation of Grid Beam Search (GBS) as presented in [Lexically Constrained Decoding For Sequence Generation](https://arxiv.org/abs/1704.07138).\n\nWe provide two sample implementations of translation models -- one using our framework for\nNeural Interactive Machine Translation, \nand another for models trained with [Nematus](https://github.com/rsennrich/nematus).\n\nNMT models trained with Nematus model work out of the box. This project can also be used as a general-purpose \nensembled decoder for Nematus models with or without constraints. \n\n### Quick Start\n\n```\ngit clone https://github.com/chrishokamp/constrained_decoding.git\ncd constrained_decoding\npip install -e .\n```\n\n\n#### Translating with a Nematus Model: A Full Example \n\nWe assume you\'ve already installed [Theano](http://deeplearning.net/software/theano/install_ubuntu.html) \n\nYou need to install the **theano** branch of [Nematus](https://github.com/EdinburghNLP/nematus/tree/theano) \n```\ngit clone https://github.com/EdinburghNLP/nematus.git\ncd nematus\ngit checkout theano\npython setup.py install\n``` \n\n\nNow download assets and run constrained translation\n```\n# CHANGE THIS TO A LOCAL PATH \nEXP_DIR=/data/mt_data/nematus_en-de_example\n\n# Download one of the Edinburgh pre-trained Nematus models from WMT 2016\nmkdir $EXP_DIR && cd $EXP_DIR\nLANG_PAIR=en-de\nwget -r --cut-dirs=2 -e robots=off -nH -np -R index.html* http://data.statmt.org/rsennrich/wmt16_systems/$LANG_PAIR/\n\ncd $EXP_DIR\n# get subword-nmt\ngit clone https://github.com/rsennrich/subword-nmt\nSUBWORD=$EXP_DIR/subword-nmt\n\n# get moses scripts for preprocessing\ngit clone https://github.com/marian-nmt/moses-scripts.git \nMOSES_SCRIPTS=$EXP_DIR/moses-scripts\n\nSRC=en\nTRG=de\n\n# Download the WMT 16 En-De test data\nTEST_DATA=$EXP_DIR/wmt_test\nmkdir $TEST_DATA && cd $TEST_DATA\nwget http://data.statmt.org/wmt16/translation-task/test.tgz\ntar xvf test.tgz\n\n# preprocess test data\n# SRC\ncat $TEST_DATA/test/newstest2016-ende-src.en.sgm | $MOSES_SCRIPTS/scripts/generic/input-from-sgm.perl | \\\n$MOSES_SCRIPTS/scripts/tokenizer/normalize-punctuation.perl -l $SRC | \\\n$MOSES_SCRIPTS/scripts/tokenizer/tokenizer.perl -l $SRC -penn | \\\n$MOSES_SCRIPTS/scripts/recaser/truecase.perl -model $EXP_DIR/$LANG_PAIR/truecase-model.$SRC | \\\n$SUBWORD/apply_bpe.py -c $EXP_DIR/$LANG_PAIR/$SRC$TRG.bpe > $TEST_DATA/newstest2016-$SRC.preprocessed\n\n# we\'ll use this as the reference when computing BLEU scores\n$MOSES_SCRIPTS/scripts/generic/input-from-sgm.perl < $TEST_DATA/test/newstest2016-ende-ref.de.sgm > $TEST_DATA/test/newstest2016-ende-ref.de\n\n# extract a small sample from the test set\ncat $TEST_DATA/newstest2016-$SRC.preprocessed | head -500 >  $TEST_DATA/newstest2016-$SRC.preprocessed.small\n\n\nMODEL_DIR=$EXP_DIR/$LANG_PAIR\n# Now open $MODEL_DIR/model.npz.json, editing the \'dictionaries\' key to point to the full path of the dictionaries\n# i.e. $EXP_DIR/$SRC_LANG-$TRG_LANG/vocab.en.json\n\n# for example, mine looks like this:\n#  ""dictionaries"": [\n#    ""/data/mt_data/nematus_en-de_example/en-de/vocab.en.json"",\n#    ""/data/mt_data/nematus_en-de_example/en-de/vocab.de.json""\n#  ],\n\n# your path to `constrained_decoding`\nGBS=~/projects/constrained_decoding\n\n# run translation without constraints \npython $GBS/scripts/translate_nematus.py \\\n  -m $MODEL_DIR/model.npz \\\n  -c $MODEL_DIR/model.npz.json \\\n  -i $TEST_DATA/newstest2016-$SRC.preprocessed.small \\\n  --alignments_output $TEST_DATA/newstest2016-$SRC.preprocessed.small.alignments \\\n  > $TEST_DATA/newstest2016-$SRC.preprocessed.small.baseline_translated\n\n\n# run translation with constraints \nCONSTRAINTS=$GBS/examples/nematus/wmt_16_en-de/sample.constraints.wmt-test.small.json\n\npython $GBS/scripts/translate_nematus.py \\\n  -m $MODEL_DIR/model.npz \\\n  -c $MODEL_DIR/model.npz.json \\\n  -i $TEST_DATA/newstest2016-$SRC.preprocessed.small \\\n  --constraints $CONSTRAINTS \\\n  --alignments_output $TEST_DATA/newstest2016-$SRC.preprocessed.small.constrained.alignments \\\n  > $TEST_DATA/newstest2016-$SRC.preprocessed.small.constrained_translated\n\n# Check BLEU Scores\n# postprocess baseline\ncat $TEST_DATA/newstest2016-$SRC.preprocessed.small.baseline_translated | sed \'s/\\@\\@ //g\' | \\\n$MOSES_SCRIPTS/scripts/recaser/detruecase.perl | \\\n$MOSES_SCRIPTS/scripts/tokenizer/detokenizer.perl -l $TRG \\\n> $TEST_DATA/newstest2016-$SRC.preprocessed.small.baseline_translated.postprocessed\n\n# postprocess constrained\ncat $TEST_DATA/newstest2016-$SRC.preprocessed.small.constrained_translated | sed \'s/\\@\\@ //g\' | \\\n$MOSES_SCRIPTS/scripts/recaser/detruecase.perl | \\\n$MOSES_SCRIPTS/scripts/tokenizer/detokenizer.perl -l $TRG \\\n> $TEST_DATA/newstest2016-$SRC.preprocessed.small.constrained_translated.postprocessed\n\n# get BLEU scores\n# we only used the first 500 lines \ncat $TEST_DATA/test/newstest2016-ende-ref.de | head -500 >  $TEST_DATA/test/newstest2016-ende-ref.de.small\n\n# baseline\n$MOSES_SCRIPTS/scripts/generic/multi-bleu.perl $TEST_DATA/test/newstest2016-ende-ref.de.small < $TEST_DATA/newstest2016-$SRC.preprocessed.small.baseline_translated.postprocessed\n\n# with constraints\n$MOSES_SCRIPTS/scripts/generic/multi-bleu.perl $TEST_DATA/test/newstest2016-ende-ref.de.small < $TEST_DATA/newstest2016-$SRC.preprocessed.small.constrained_translated.postprocessed\n\n```\n\n\n### Citing\n\nIf you use code or ideas from this project, please cite:\n\n```\n@InProceedings{hokamp-liu:2017:Long,\n  author    = {Hokamp, Chris  and  Liu, Qun},\n  title     = {Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search},\n  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},\n  month     = {July},\n  year      = {2017},\n  address   = {Vancouver, Canada},\n  publisher = {Association for Computational Linguistics},\n  pages     = {1535--1546},\n  url       = {http://aclweb.org/anthology/P17-1141}\n}\n```\n\n\n### Running Experiments\n\nFor PRIMT and Domain Adaptation experiments, the lexical constraints are stored in `*.json` files. \nThe format is `[[[c1_t1, c1_t2], [c2_t1, c2_t2, c2_t3], ...], ...]`: \nEach constraint is a list of tokens, and each segment has a list of constraints. The length of the \nouter list in the `*.json` should be the same as number of segments in the source data. If there are no constraints for a\nsegment, there should be an empty list. \n\n\n### Performance\n\nThe current implementation is pretty slow, and it gets slower the more constraints you add :disappointed:. \nThe GBS algorithm can be easily parallelized, because each cell in a column is independent of the others (see paper). \nHowever, implementing this requires us to make some assumptions about the underlying model, and would thus\nlimit the generality of the code base. If you have ideas about how to make things faster, please create an issue. \n\n### Features\n\nEnsembling and weighted decoding for Nematus models\n\n\n### Using the Prototype server\n\nWe provide a [very simple server](scripts/run_constrained_decoding_server.py) for convenience while prototyping. \n\n\n\n\n\n'",Lexically constrained decoding for sequence generation using Grid Beam Search
https://github.com/daedaluschan/HKAiportSchedule,b'# HKAiportSchedule\nHKAiportSchedule\n\nI am trying to write a show scrip to capture the arrival / departure schedule & actually delay for all the flight using HKIA (Hong Kong International Airport).\n\nThis is still in progress and feel free to provide your comment. :)\n\nLibraries used are quite common \n* lxml (for html parsing)\n* requests (for making HTTP calls)\n* pandas (for data manipulation)\n',HKAiportSchedule
https://github.com/nicolasalvarez/titanic_survival_exploration,"b'# Project 0: Introduction and Fundamentals\n## Titanic Survival Exploration\n\n### Install\n\nThis project requires **Python 2.7** and the following Python libraries installed:\n\n- [NumPy](http://www.numpy.org/)\n- [Pandas](http://pandas.pydata.org)\n- [matplotlib](http://matplotlib.org/)\n- [scikit-learn](http://scikit-learn.org/stable/)\n\nYou will also need to have software installed to run and execute an [iPython Notebook](http://ipython.org/notebook.html)\n\nUdacity recommends our students install [Anaconda](https://www.continuum.io/downloads), a pre-packaged Python distribution that contains all of the necessary libraries and software for this project.\n\n### Code\n\nTemplate code is provided in the notebook `titanic_survival_exploration.ipynb` notebook file. Additional supporting code can be found in `titanic_visualizations.py`. While some code has already been implemented to get you started, you will need to implement additional functionality when requested to successfully complete the project.\n\n### Run\n\nIn a terminal or command window, navigate to the top-level project directory `titanic_survival_exploration/` (that contains this README) and run **one** of the following commands:\n\n```bash\njupyter notebook titanic_survival_exploration.ipynb\n```\nor\n```bash\nipython notebook titanic_survival_exploration.ipynb\n```\n\nThis will open the iPython Notebook software and project file in your web browser.\n\n## Data\n\nThe dataset used in this project is included as `titanic_data.csv`. This dataset is provided by Udacity and contains the following attributes:\n\n- `survival` ? Survival (0 = No; 1 = Yes)\n- `pclass` ? Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n- `name` ? Name\n- `sex` ? Sex\n- `age` ? Age\n- `sibsp` ? Number of Siblings/Spouses Aboard\n- `parch` ? Number of Parents/Children Aboard\n- `ticket` ? Ticket Number\n- `fare` ? Passenger Fare\n- `cabin` ? Cabin\n- `embarked` ? Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n'",Project 0: Titanic Survival Exploration
https://github.com/NIEHS/orso,"b'# ORSO: Online Resource for Social Omics\n\nORSO (Online Resource for Social Omics) is a web application designed to help users find next generation sequencing (NGS) datasets relevant to their research interests. ORSO performs this task by creating and maintaining a network of datasets connected based on similarity in primary read coverage values and annotated metadata. An ORSO user can follow these connections to discover datasets with similar characteristics. ORSO is also a social network. Users may favorite datasets and follow other users, creating new connections that influence how datasets are recommended.\n\nFor instructions on how to use the ORSO web application, please see the [ORSO user manual](https://github.com/lavenderca/genomics-network/blob/master/docs/user_manual.md). We also provide documentation for installation and administration in the [ORSO developer manual](https://github.com/lavenderca/genomics-network/blob/master/docs/dev_manual.md).\n'",ORSO (Online Resource for Social Omics) is an web application that leverages a social network to connect life scientists to NGS data of interest.
https://github.com/1501213456/2016.M3.TQF-ML.FundPerformanceAttribution,"b'# 2016.M3.TQF-ML.FundPerformanceAttribution\n### Data:\n* [Benchmark_index.csv](https://github.com/1501213456/2016.M3.TQF-ML.FundPerformanceAttribution/blob/master/Benchmark_index.csv)\n* [Target_Active_Funds.csv](https://github.com/1501213456/2016.M3.TQF-ML.FundPerformanceAttribution/blob/master/Target_Active_Funds.csv)\n### Final Report:\n* [Final Report-Fund Performance Attribution and Prediction.pdf](https://github.com/1501213456/2016.M3.TQF-ML.FundPerformanceAttribution/blob/master/Final%20Report-Fund%20Performance%20Attribution%20and%20Prediction.pdf)<br>\n### Code:\n* [FundPerformanceAttribution.ipynb](https://github.com/1501213456/2016.M3.TQF-ML.FundPerformanceAttribution/blob/master/FundPerformanceAttribution.ipynb)\n\nThank you for this class, Prof. Choi! ^_^\n'",Fund Performance Attribution and Prediction
https://github.com/EricDoug/tensorflowDev,b'# tensorflowDev\ntensorflowDev\xe4\xb8\xbb\xe8\xa6\x81\xe8\xae\xb0\xe5\xbd\x95\xe6\x88\x91\xe7\x9a\x84\xe4\xb8\x80\xe4\xba\x9b\xe5\xad\xa6\xe4\xb9\xa0tensorflow\xe7\x9a\x84\xe4\xbb\xa3\xe7\xa0\x81\xe3\x80\x82\n\n## RNN\n',tensorflowDev主要记录我的一些学习tensorflow的代码。
https://github.com/Himanshu141/StreetSignRecognition,"b""# Street-Sign-Detection\n\nThe readme will be updated soon\n\nTo run the code, you'll have to change path names in the files.\n\nThe subfolders have the CNN models and the Cascades.\n\nThe csv files are the submissions.\n\nThe code may not be currently well documented. We will work on proper documentation soon.\n\n## Authors:\n\n[Ayush Sawarni](https://github.com/sawarniayush)\n\n[Himanshu Sharma](https://github.com/himanshu141)\n\n[Rishabh Joshi](https://github.com/rishabhjoshi)\n""",ML Hackathon by MapMyIndia and BITS ACM on Kaggle
https://github.com/chembl/autoencoder_ipython,"b'# autoencoder_ipython\nIpython notebook for blog post <a href=""http://chembl.blogspot.co.uk/2017/07/using-autoencoders-for-molecule.html"">entry</a>\n\nBased in:\n\n<strong>Link to the paper</strong><br />\n<a href=""https://arxiv.org/abs/1610.02415"">arXiv</a>\n\n<strong>Link to the repository</strong><br />\n<a href=""https://github.com/maxhodak/keras-molecules"">github</a>\n'",Ipython notebook for blog post entry
https://github.com/jwilber/python_nlp,b'init\n',Python nlp stuff with NLTK
https://github.com/openhealthcare/openspirometer,"b""Open-spirometer\n====\nA spirometer (http://en.wikipedia.org/wiki/Spirometry) is a device that measures your breathing. This is particularly helpful medically for the managment of chronic lung diseases such as asthma, pulmonary fibrosis, cystic fibrosis, and COPD.\n\nOpen-spirometer is special because it's design will be open and because it will capture and share, with user consent, high quality open spirometry data from day one. \n\nWe'd love you to get involved by using what we make, reporting bugs/suggesting improvements, and fixing bugs/making improvements.  Please refer to the CONTRIBUTING file.\n\nOpen-spirometer is an open (open governance/design/hardware/source) commercial health product by Open Health Care UK that we've not made yet.\n\n\nWhat problem does Open-spirometer solve?\n======\nOpen-spirometer aims to solve the paucity of high quality independant open spirometry data problem. We think patients and health care professionals would be empowered to make better decisions together if they had better data. At present spirometery data often lies locked up in proprietary spirometry devices and non-digital data formats. This is suboptimal. Why we need an open spirometer is also described in a blog post here http://www.openhealthcare.org.uk/?p=449\n\n\nOpen governance\n======\n\nRoadmap: \n\nDiscussion List: \n\nAccess: availability of the latest source code, developer\nsupport mechanisms, public roadmap, and transparency of\ndecision-making\n\nDevelopment: the ability of developers to influence the content\nand direction of the project\n\nDerivatives: the ability for developers to create and distribute\nderivatives of the source code in the form of spin-off\x0b projects,\nhandsets or applications.\n\nCommunity: a community structure that does not discriminate\nbetween developers\n\n(from http://www.visionmobile.com/blog/2011/07/the-open-governance-index-measuring-openness-from-android-to-webkit/)\n\nOpen source\n======\nGNU Affero GPLv3\n\nCommunications\n======\nhello@openhealthcare.org.uk\n\nhttp://www.openhealthcare.org.uk\n\nhttps://twitter.com/ohcuk\n\nchannel #ohc_dev on freenode\n\nhttps://docs.google.com/document/d/1gvlvsJGYwKLe5FtpEOpRt48q_SDEKZP6m3eb6Pu2tlc/edit\n\nDemo\n======\nnot yet\n""",Better open data with the open spirometer
https://github.com/dvm-shlee/PyNIT,"b'# PyNIT (Python NeuroImaging Toolkit)\n### Version: 0.2.1\n\n**Deprecation Warning:** \nPyNIT module is not maintained anymore, please use PyNIPT (https://github.com/dvm-shlee/pynipt) instead.'",Python based NeuroImaging Toolkit
https://github.com/wilkens/edinburgh-masterclass,"b""# Edinburgh Master Class \n\nMaterials for masterclass in quantitative humanities at Edinburgh, September 2016.\n\n**NB.** If you're reading these files on GitHub (rather than locally on your machine), you won't be able to change or run any of the code. To run your own copy, download this project from GitHub, make sure you have working Python 3 environment, then start a notebook server via the Anaconda Navigator or at the command line with `jupyter notebook`. More details in the `00 Intro and Setup.ipynb` notebook.\n\nDirectories are named according to contents:\n\n* **Data** contains input data used for analysis. It has two subfolders, **Texts** and **Other**, containing, respectively, plain text versions of literary texts and other types of input data.\n* **Notebooks** contains code for the course in the form of Jupyter notebooks\n* **Readings** contains supplemental articles for discussion\n* **Results** contains the output of various analyses\n* **Slides** contains a PDF of the opening lecture slides\n""","Materials for masterclass in quantitative humanities at Edinburgh, Sept 2016"
https://github.com/donkey-hotei/neuraldata,b'Exploring Neural Data\n---------------------\nProblem set solutions and notes for the Exploring Neural data course offered by Brown Univerity.\n\nThis course is an opprotunity to learn about neuroscience research and explore questions related to how brains work. It is an intoductory-level course that help the student understand real-life challenges faced by  neuroscientists as they work with the large amount of data that they collect from the brain.\n\n',Exploring Neural Data @ Brown
https://github.com/sujitpal/intro-dl-talk-code,b'# intro-dl-talk-code\nJupyter notebooks and code for Intro to DL talk at Genesys\n',Jupyter notebooks and code for Intro to DL talk at Genesys
https://github.com/Pmcmanus02/hello-world,b'# hello-world\nStarting my Git hub.\nWass up peeps? This is the begining of your great adventure!\n',Starting my Git hub.
https://github.com/cabaf/udacity-face-generation,b'# Generation of Face of Celibrities with GANs\nGenerating celebrity faces with GANs as final project for the udacity deep learning nanodegree.\n',Generating celebrity faces with GANs
https://github.com/yeshg/AI_Art,"b""# AI_Art\n\nTo run the code: need an installation of Caffe with built pycaffe libraries, as well as the python libraries numpy, scipy and PIL. For instructions on how to install Caffe and pycaffe, refer to the installation guide [here](http://caffe.berkeleyvision.org/installation.html). Before running the ipython notebooks, you'll also need to download the [bvlc_googlenet model](https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet), and insert the path of the pycaffe installation into ```pycaffe_path``` and the model path to the googlenet model into ```model_path```.\n\nThis code was based on the [deepdream code](https://github.com/google/deepdream) shared by Google, as well as the [code](https://github.com/kylemcdonald/deepdream/blob/master/dream.ipynb) by Kyle McDonald and Auduno's article and [code](https://github.com/auduno/deepdraw) on visualizations with GoogleNet. The idea of using bilateral filtering comes from Mike Tyka.\n\n[Image net class id and class table](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a) - image net class id to class\n""",Deepdream approach to generate high-resolution visualizations of Convolutional Neural Networks.
https://github.com/glouppe/tutorials-scikit-learn,"b'# Scikit-Learn tutorials\n\n1. Tutorial on machine learning and Scikit-Learn (beginner level).\n2. Tutorial on robust and calibrated estimators with Scikit-Learn (mid level)\n\nContact: <a href=""https://twitter.com/glouppe"">@glouppe</a> | BSD 3-clause license\n\n## Installation instructions\n\n1) [Download](https://www.continuum.io/downloads) and install the latest Anaconda distribution, coming with Python 3.5 and the full scientific Python stack. \n\n2) Install dependencies:\n```\nconda install numpy scipy scikit-learn jupyter matplotlib \n```\n\n3) Clone this repository and start Jupyter\n```\ngit clone https://github.com/glouppe/tutorial-scikit-learn.git\ncd tutorial-scikit-learn\njupyter notebook\n```\n\n## Launch on Binder without installing anything!\n[![Binder](http://mybinder.org/badge.svg)](http://mybinder.org/repo/glouppe/tutorial-scikit-learn)\n\n\n'",Scikit-Learn tutorials
https://github.com/spm2164/data-homework,b'# data-homework\n',homework for allison!
https://github.com/Biodun/Data-Science-Projects,b'# Data-Science-Projects\nCollection of data analyses on topics I find interesting\n\nBay Area Bike Share data analysis\n---\n[IPython Notebook](http://nbviewer.ipython.org/github/Biodun/Data-Science-Projects/blob/master/BayAreaBikeShare/Data_challenge_1.ipynb)\n',Personal data science projects I've done
https://github.com/jorghyq/Gwyddion-Utils,"b'# Gwyddion-Utils\nThis project is to provide a SPM data browser for fast image reviewing and saving (like Nanonis Scan Inspector). It is based on [Gwyddion](http://gwyddion.net/), using the Gwyddion python API.\n\nIt can be used independently as GUI to browse SPM image data, or started as a\nfunction in Gwyddion.\n\nIts biggest advantage is the fast review of the SPM data and fast saving\nfunction.\n\nCurrently, it can handle Nanonis .sxm file and Omicron .mtrx file.\n\n## Usage\n### Use as independent GUI\nTo use it independently, you need to have Gwyddion installed with pygwy.\n\nYou need to be able to use the gwyddion outside Gwyddion software by `import gwy`.\n\nThen you can simply run the SPMBrowser.py in **GwyBrowser/gwybrowser**.\n\n### Use within the Gwyddion\nTo use it within the Gwyddion, you need to have Gwyddion installed with pygwy.\n\nFor the python part, you need pygtk, numpy, matplotlib and re.\n\nPut the pygwy, ui and icon folders inside the **GwyBrowser** to the ~/.gwyddion file.\n'",Some modules for Gwyddions
https://github.com/esmatanis/Advanced_Lane_Lines_Finding,"b'## Advanced Lane Finding\nIn this project, our goal is to write a software pipeline to identify the lane boundaries in a video. Following steps were implemented to acheive the goal :\n\n* Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n* Apply a distortion correction to raw images.\n* Use color transforms, gradients, etc., to create a thresholded binary image.\n* Apply a perspective transform to rectify binary image (""birds-eye view"").\n* Detect lane pixels and fit to find the lane boundary.\n* Determine the curvature of the lane and vehicle position with respect to center.\n* Warp the detected lane boundaries back onto the original image.\n* Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.\n\nThe project consists of following files : \n\n* `calibrate.py` : The script for camera calibration.\n* `lane_detection.py` : The script for applying color and gradient thresholds, applying perspective transformation and detect lane pixels.\n* `pipeline.py` : The main script which performs lane detection analysis on video frames.    \n* `Project.ipynb` : IPython notebook with step-by-step description and execution of entire code. \n\n--\n\n### Camera calibration\nThe code for this step is contained in the file `calibrate.py` wherein the relevant class to handle all operations is called `calibration`. \nI start by reading all the chess board images using `calibration.add_image()` function. The ""object points"", which will be the (x, y, z) coordinates of the chessboard corners in the world, is fixed on the (x, y) plane at z=0, such that the object points are the same for each calibration image. Thus, `calibration.obj_points` is appended with a copy of the same coordinates every time I successfully detect all chessboard corners in a test image using the function `calibration.find_corners()`. `calibration.img_points` will be appended with the (x, y) pixel position of each of the corners in the image plane with each successful chessboard detection. Corners for all but three of the images were detected, here are four examples : \n\n![](output_images/calibration/corners.png)\n\nNote that three of the images did not have all the 9x6 corners which will be used to test calibration.\n\nI then used the output `calibration.obj_points` and `calibration.img_points` to compute the camera calibration and distortion coefficients in the function using the `cv2.calibrateCamera()` function (please see `calibration.calc_distortion()` for details). I applied this distortion correction to the test image using the `cv2.undistort()` function (please see `calibration.undistort_img()` for details) and obtained the following result for the three test images : \n\n![](output_images/calibration/undistorted.png)\n\n--\n\n### Thresholding\nI used a combination of color and gradient thresholds to split image into 7 binary channels (please see `lane_detection.split_channels()` function on line 35 of the file `lane_detection.py`). Images are first converted to HSV color space and 7 channels are selected : 4 of these channels are obtained by applying Sobel gradient along x-direction on \'S\' and \'V\' of HSV keeping the sign of threshold instead of taking absolute values i.e. positive and negative gradient thresholds. The \'S\' gradient allows us to pick up yellow line edges while \'V\' gradient thresholds pick both white and yellow edges. The other 3 channels are obtained by applying color thresholds, \'H\' color threshold for picking yellow and \'V\' threshold for picking white color (I used two thresholds for white in case one fails). Here are the results of applying gradient thresholds on \'V\' channel and white color selection: \n\n![](output_images/thresholding/white.png)     \n![](output_images/thresholding/white_challenge.png) \n\nFrom the above images, it can be seen that using positive and negative gradients allows us to differentiate between lane lines from shadow lines and other irrelevant road markings. Concretely, for lane lines, positive and negative threshold lines always appear in vicinity.\nHere are the results for applying gradient thresholds on \'S\' channel and yellow color selection: \n\n![](output_images/thresholding/yellow.png)     \n![](output_images/thresholding/yellow_challenge.png)\n\n--\n\n\n### Perspective transform\nThe code for my perspective transform includes a function called `lane_detection.warper()`, which appears on line 12 in the file `lane_detection.py`. The source and destination points were chosen as follows:\n\n| Source        | Destination   |\n| ------------- |:-------------:| \n| (225,700)     | (320,720)     |\n| (590,450)     | (320,-100)    |\n| (690,450)     | (960,-100)    |\n| (1055,700)    | (960,720)     |\n\nI verified that my perspective transform was working as expected by drawing the source and destination points onto test images and its warped counterpart to verify that the lines appear parallel in the warped image. \n\n![](output_images/perspective/perspective.png) \n\n--\n\n### Lane detection\nTwo approaches were implemented for finding lane lines. The first approach is the window sliding method which is used when prior lane information does not exist or missing. Although this appraoch is often more robust, it is also computationally time consuming. The other approach is searching for lane pixels in a target region determined by the previous window frame. The second approach is less compuationally intensive and is used for most of the video frames. \n\nThe window sliding method is implemented in `lane_detection.sliding_window()` function on line 231 of the file `lane_detection.py` using 15 windows for each side of the lane. Information from all 7 channels of the thresholding step is utilized. \nFor example, I require that positive gradient edges be accompanied by negative gradient edges, which allows for differentiating lane markings from shadows. For full selection criteria, see the function `lane_detection.get_good_pixels()` on line 107 of `lane_detection.py`.  Here are the results on a test image : \n\n![](output_images/detection/window_sliding.png) \nAs seen above, a more targeted search window is used when relevant pixels are detected. To demonstrate the working of our thresholding, here is one from a more challenging test image: \n\n![](output_images/detection/window_sliding_challenge.png)\n\nIn order to fit curves to ""good"" pixels, I have used `numpy.polyfit` function to fit a second order polynomial, implementation of which can be found in the function `lane_detection.curve_fit()` on line 348 of `lane_detection.py`. Concretely, the best fit curves are parametrized as follows :\n$$x = A y^2 + B y + C$$\nOnce the lane pixels have been detected using sliding window method, a targeted search is performed in subsequent video frames by focusing on region around the best fit curves from the previous frame (see `lane_detection.target_search()` on line 405 of `lane_detection.py` ). Here is an example : \n\n![](output_images/detection/target_search.png)\n\n\n--\n\n### Lane parameters\nTo calculate lane parameters, measurement units have to be changed from pixels to real world units such as meter. I use the following conversion : $k_x=3.7/700$ m/pixel in the x-direction and $k_y=30/720$ m/pixel.\n\n* Radius of curvature : The radius of curvature is implemented in `lane_line.calc_R()` function on line 72 of `pipeline.py`. Since, the original fit was performed in the pixel space, I use the following formula for conversion to real world space :\n\t$$ R = \\frac{(k_x^2 + k_y^2(2Ay+B)^2)^{3/2}}{2 k_x k_y |A|}$$ \n\t\n* To find the position of the vehicle with respect to the center, I calculate the x-coordinate of both the left and right lane lines with repect to the center at the base of the image. The average of the two x-coordinates is the position of the center of the road with respect to the camera center. \n\n-- \n\n### Final output\nTo store the history of video frames, I have implemented a class `lane_line` which can be found in `pipeline.py` on line 17. The final pipeline is implemented in the function `pipeline(img)` (also to be found in `pipeline.py` on line 132) which takes as input video frame `img`. In this function, I toggle between two methods for finding lane line pixels : window sliding method is used for the first few frames and every 10th frame while a targetted search is used when lane markings were found in the previous video frame. Several checks are performed to check if the best fit curves actually make sense :\n\n* Fit parameters are required to not be significantly different from previous frame.  \n* Left and right lane lines should not be diverging.\n* The base gap between left and right lines should not be too large or too small. \n* The residuals from curve fit (normalized by the number of pixels found) should not be too big. \n\n Further, I have implemented averaging of the fit parameters over last few iterations so as to avoid jittery lines. \n\nHere are a couple sample output images :   \n![](output_images/output/output.png)\n\nHere is the final output video : \n\n[![Project video](https://img.youtube.com/vi/kIUPB5wd10E/0.jpg)](https://www.youtube.com/watch?v=kIUPB5wd10E ""Project video"")   \n\nand here is one on a more challenging road : \n\n[![Project video](https://img.youtube.com/vi/A1_HX8_tke4/0.jpg)](https://www.youtube.com/watch?v=A1_HX8_tke4 ""Project video"") \n\n--\n\n### Discussion\nThe most challenging aspect of this project was the shadows and road markings, specially on the challenging video. I was able to deal with this issue by requiring that positive and negative gradients be in vicinity. The shadows and irrelavant road markings usually have either positive gradients or negative gardient edges but not both unlike the lane markings which have both. \n\nHowever, there is a lot of scope for improvement which I hope to address in the future (currently not implemented due to lack of time) :\n\n* Use of convolution to select hot pixels can help in removing outliers.\n* The consistency checks can be made more robust. In particular, currently they do not work well when roads are curvy as in the harder challenge video.\n* It might be worth exploring more thresholds such as gradient directions.  \n* Information about yellow and white markings can be easily included.     \n '","Developed a pipeline to process a video stream from a car driving on a highway in order to robustly identify individual lane lines from the road irrespective of curvature.  Primary languages/libraries: Python, OpenCV"
https://github.com/TheGoldenRatio/thegoldenratio.github.com,b'## This is my blog. ##\n\nDesign is heavily based off of the **[Leonids](http://renyuanz.github.io/leonids)** theme.\n',blog
https://github.com/kingsman142/Projects,"b'# Projects\n\n## Greyscaler\n\nThis repo contains my greyscale program, bitmaps.java - BinaryOutput.png and GreyOutput.png are the two output files associated with the program for the user to see sample output.  Also associated with the greyscale project are: cat.jpg, city.jpg, sunset.jpg, up.jpg, leaf.jpg, all of which are sample images for the user to test with.  However, any image can be used as long as it\'s in the same directory as bitmaps.java. The last two files greyValues.bmp and bitmap.bmp store the grey values and binary values respectively of the images.\n\n## Racing Game\n\nA game where you can drive around a racetrack with a racecar.  Written in C++, it utilizes the SFML 2.3.2 library for graphics.  In order to be ran, the respective files must be put into a project and linked correctly. NOT FINISHED\n\n## Random Terrain\n\nGenerates random terrain and displays it to the user in a JFrame.  Currently, the RandomTerrain.java file generates terrain using the midpoint displacement algorithm, so the terrain is one-dimensional. Its corresponding examples can be seen through MidpointDisplacementExamples.png.  In DiamondSquare.java, terrain is generated using the diamond-square algorithm, so the terrain is outputted to a two-dimensional bitmap but can easily be translated to 3 dimensions.  Its corresponding examples are stored in DiamondSquareExamples.png, with each heightmap being 512x512.\n\n## Networking\n\nContains two files, IRCClient.java and IRCServer.java.  First, run the server, and then the client.  A one way connection works where the client can send messages to the server and it will display the messages but the server won\'t respond to the client.  Make sure they\'re both on the same point using command-line arguments.  I was trying to make an IRC (Internet Relay Chat), but I can\'t figure out MultiCastSockets right now so I\'m delaying this project.\n\n## LWJGL-PC\n\nThis project is perhaps the most satisfying of the ones here.  It utilizes LWJGL (a java binding for OpenGL), with OpenGL shaders v3.30 Core to generate 3D random terrain.  In order to generate the terrain, I used my DiamondSquare.java file from the ""Random Terrain"" project to generate the values for each point.  Currently, using the diamond-square algorithm, the terrain is size 512x512.  With a few modifications, the terrain can not only show the grey value of each point, but can also show if the value should be at grass/ground level, hill level, or mountain top level (0-.33 = ground, .33-.66 = hill, .66-1.00 = mountain top) using green, grey, and white.\n\n## LWJGL\n\nMy 3D random terrain project; similar to the early stages of LWJGL-PC.  However, I believe OSX and Windows have different OpenGL drivers so they don\'t run identically.  I might have to double check that.\n\n## Videos\n\nMy archive of videos from projects.  Currently, there is only one video, and it is for the LWJGL-PC project to show users how the program looks with multiple tests and a couple of modifications.\n'",Some of my ongoing and finished projects
https://github.com/habinez/Google-ML-Recipes,b'# Google-ML-Recipes\nA guide to a friend following Machine Learning [Recipes] (https://www.youtube.com/watch?v=tNa99PG8hR8&list=PLOU2XLYxmsIIuiBfYad6rFYQU_jL2ryal&index=1) with Josh Gordon\n\n1. Week 0 : [Set up Environment](https://nbviewer.jupyter.org/github/habinez/Google-ML-Recipes/blob/master/00_Week%200%20Environment%20Set%20Up.ipynb)\n2. Week 1 : [Hello World](https://nbviewer.jupyter.org/github/habinez/Google-ML-Recipes/blob/master/01_Week%201%20Hello%20World.ipynb) - Machine Learning Recipes #1\n3. Week 1 : [Visualizing a Decision Tree](https://nbviewer.jupyter.org/github/habinez/Google-ML-Recipes/blob/master/02_Week%201%20Visualizing%20a%20Decision%20Tree.ipynb) - Machine Learning Recipes #2\n',A guide to a a friend following Machine Learning Recipes with Josh Gordon
https://github.com/mmalmrose/Continuum_flux_demo,b'# Continuum_flux_demo\nread in and plot some astronomical spectra while fitting components to the continuum\n',read in and plot some astronomical spectra while fitting components to the continuum
https://github.com/nicolasfauchereau/SAM,b'# Southern Annular Mode calculation\n\nThis notebook (calculates_SAM_index.ipynb) calculates the Southern Annular Mode index (SAM) as the principal \ncomponent associated with the first EOF coming from an EOF decomposition of the 850hPa\nmonthly anomalies (1981-2010 climatology) from the NCEP/NCAR (*aka* NCEP1) reanalysis dataset\n\nYou can visualize the notebook by following [this link](http://htmlpreview.github.io/?https://github.com/nicolasfauchereau/SAM/blob/master/calculates_SAM_index.html) \n',Calculates the Southern Annular Mode index (SAM) from NCEP/NCAR monthly anomalies
https://github.com/VeerpalBrar/ML-Algorithms-from-Scratch,b'### Implemented So far:\n- Linear Regression on Titanic Dataset as seen on Kaggle\n- 3-layer Neural Network on randomly generated data\n',Implement algorithms from scratch
https://github.com/toddhodes/ipynb,b'\nThis is from a music class utilizing ipython jupyter notebooks; looking into the extent of github integration with jupyter\n',"iphython note example, checking out the github integration"
https://github.com/hatshex/text-mining,b'# Miner\xc3\xada de texto | ITAM | Prof. V\xc3\xadctor Mijangos\n\n\xc2\xbfDe qu\xc3\xa9 va?\n* Introducci\xc3\xb3n\n  - Miner\xc3\xada de texto\n  - Datos estructurados y no estructurados\n* Lenguaje Natural\n  - Herramientas Ling\n  - Comportamiento estad\xc3\xadstico\n  - Representaci\xc3\xb3n textual\n  - Clasificaci\xc3\xb3n y agrupamiento de textos\n  - Recuperaci\xc3\xb3n de informaci\xc3\xb3n\n  - Modelado de t\xc3\xb3picos\n  - Aplicaciones\n\nBibliograf\xc3\xada\n* Foundations of Statistical Natural Language Processing. Christopher Manning and Hinrich Schuetze.\n* Natural Language Processing and Text Mining. Anne Kao and Stephen R. Poteet\n* https://sites.google.com/site/victormijangoscruz/cursos/metodos-analiticos-para-texto\n\nArt\xc3\xadculos interesantes\n* http://es.slideshare.net/ChristopherMoody3/word2vec-lda-and-introducing-a-new-hybrid-algorithm-lda2vec-57135994\n* http://nlpx.net/archives/330\n* https://datawarrior.wordpress.com/2016/02/15/lda2vec-a-hybrid-of-lda-and-word2vec/\n* http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/\n* http://insightdatascience.com/blog/thisplusthat_a_search_engine_that_lets_you_add_words_as_vectors.html\n* https://www.cs.bgu.ac.il/~elhadad/nlp16.html\n* http://rare-technologies.com/word2vec-tutorial/\n* http://mghassem.mit.edu/insights-word2vec/\n* https://www.youtube.com/watch?v=tdLmf8t4oqM\n* https://www.youtube.com/watch?v=T8tQZChniMk\n* https://www.youtube.com/watch?v=S75EdAcXHKk&nohtml5=False\n\n* Welcome to the 6th Lisbon Machine Learning School! http://lxmls.it.pt/2016/\n',Clase de minería de texto del ITAM
https://github.com/GGYIMAH1031/LPinPython,b'# LPinPython\nLinear Programming in Python (Using PuLP)\n',Linear Programming in Python (Using PuLP)
https://github.com/DmitryUlyanov/AGE,"b'This repository contains code for the paper\n\n**[""Adversarial Generator-Encoder Networks""](https://arxiv.org/abs/1704.02304)** (AAAI\'18) by *Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky*.\n\n![](data/readme_pics/age.png)\n\n## Pretrained models\n\nThis is how you can access the models used to generate figures in the paper.\n\n1) First install dev version of `pytorch 0.2` and make sure you have `jupyter notebook` ready.\n\n2) Then download the models with the script:\n```\nbash download_pretrained.sh\n```\n\n3) Run `jupyter notebook` and go through `evaluate.ipynb`.\n\nHere is an example of samples and reconstructions for `imagenet`, `celeba` and `cifar10` datasets generated with `evaluate.ipynb`.\n\n#### Celeba\n\n|Samples    |Reconstructions|\n|:---------:|:-------------:|\n|![](data/readme_pics/celeba_samples.png) | ![](data/readme_pics/celeba_reconstructions.png) |\n\n#### Cifar10\n\n|Samples    |Reconstructions|\n|:---------:|:-------------:|\n|![](data/readme_pics/cifar10_samples.png) | ![](data/readme_pics/cifar10_reconstructions.png) |\n\n#### Tiny ImageNet\n\n|Samples    |Reconstructions|\n|:---------:|:-------------:|\n|![](data/readme_pics/imagenet_samples.png) | ![](data/readme_pics/imagenet_reconstructions.png) |\n\n\n# Training\n\nUse `age.py` script to train a model. Here are the most important parameters:\n\n* `--dataset`: one of [celeba, cifar10, imagenet, svhn, mnist]\n* `--dataroot`: for datasets included in `torchvision` it is a directory where everything will be downloaded to; for imagenet, celeba datasets it is a path to a directory with folders `train` and `val` inside.\n* `--image_size`:\n* `--save_dir`: path to a folder, where checkpoints will be stored\n* `--nz`: dimensionality of latent space\n* `-- batch_size`: Batch size. Default 64.\n* `--netG`: `.py` file with generator definition. Searched in `models` directory\n* `--netE`: `.py` file with generator definition. Searched in `models` directory\n* `--netG_chp`: path to a generator checkpoint to load from\n* `--netE_chp`: path to an encoder checkpoint to load from\n* `--nepoch`: number of epoch to run\n* `--start_epoch`: epoch number to start from. Useful for finetuning.\n* `--e_updates`: Update plan for encoder. `<num steps>;KL_fake:<weight>,KL_real:<weight>,match_z:<weight>,match_x:<weight>`.\n* `--g_updates`: Update plan for generator. `<num steps>;KL_fake:<weight>,match_z:<weight>,match_x:<weight>`.\n\nAnd misc arguments:\n* `--workers`: number of dataloader workers.\n* `--ngf`: controlles number of channels in generator\n* `--ndf`: controlles number of channels in encoder\n* `--beta1`: parameter for ADAM optimizer\n* `--cpu`: do not use GPU\n* `--criterion`: Parametric `param` or non-parametric `nonparam` way to compute KL. Parametric fits  Gaussian into data, non-parametric is based on nearest neighbors. Default: `param`.\n* `--KL`: What KL to compute: `qp` or `pq`. Default is `qp`.\n* `--noise`: `sphere` for uniform on sphere or `gaussian`. Default `sphere`.\n* `--match_z`: loss to use as reconstruction loss in latent space. `L1|L2|cos`. Default `cos`.\n* `--match_x`: loss to use as reconstruction loss in data space. `L1|L2|cos`. Default `L1`.\n* `--drop_lr`: each `drop_lr` epochs a learning rate is dropped.\n* `--save_every`: controls how often intermediate results are stored. Default `50`.\n* `--manual_seed`: random seed. Default `123`.\n\n\nHere is `cmd` you can start with:\n\n### Celeba\nLet `data_root` to be a directory with two folders `train`, `val`, each with the images for corresponding split.\n\n```\npython age.py --dataset celeba --dataroot <data_root> --image_size 64 --save_dir <save_dir> --lr 0.0002 --nz 64 --batch_size 64 --netG dcgan64px --netE dcgan64px --nepoch 5 --drop_lr 5 --e_updates \'1;KL_fake:1,KL_real:1,match_z:0,match_x:10\' --g_updates \'3;KL_fake:1,match_z:1000,match_x:0\'\n```\n\nIt is beneficial to finetune the model with larger `batch_size` and stronger matching weight then:\n```\npython age.py --dataset celeba --dataroot <data_root> --image_size 64 --save_dir <save_dir> --start_epoch 5 --lr 0.0002 --nz 64 --batch_size 256 --netG dcgan64px --netE dcgan64px --nepoch 6 --drop_lr 5   --e_updates \'1;KL_fake:1,KL_real:1,match_z:0,match_x:15\' --g_updates \'3;KL_fake:1,match_z:1000,match_x:0\' --netE_chp  <save_dir>/netE_epoch_5.pth --netG_chp <save_dir>/netG_epoch_5.pth\n```\n\n### Imagenet\n\n```\npython age.py --dataset imagenet --dataroot /path/to/imagenet_dir/ --save_dir <save_dir> --image_size 32 --save_dir ${pdir} --lr 0.0002 --nz 128 --netG dcgan32px --netE dcgan32px --nepoch 6 --drop_lr 3  --e_updates \'1;KL_fake:1,KL_real:1,match_z:0,match_x:10\' --g_updates \'2;KL_fake:1,match_z:2000,match_x:0\' --workers 12\n```\n\nIt can be beneficial to switch to `256` batch size after several epochs.\n\n### Cifar10\n\n```\npython age.py --dataset cifar10 --image_size 32 --save_dir <save_dir> --lr 0.0002 --nz 128 --netG dcgan32px --netE dcgan32px --nepoch 150 --drop_lr 40  --e_updates \'1;KL_fake:1,KL_real:1,match_z:0,match_x:10\' --g_updates \'2;KL_fake:1,match_z:1000,match_x:0\'\n```\n\n---------------------\n\nTested with python 2.7.\n\nImplementation is based on pyTorch [DCGAN code](https://github.com/pytorch/examples/tree/master/dcgan).\n\n# Citation\n\nIf you found this code useful please cite our paper\n\n```\n@inproceedings{DBLP:conf/aaai/UlyanovVL18,\n  author    = {Dmitry Ulyanov and\n               Andrea Vedaldi and\n               Victor S. Lempitsky},\n  title     = {It Takes (Only) Two: Adversarial Generator-Encoder Networks},\n  booktitle = {{AAAI}},\n  publisher = {{AAAI} Press},\n  year      = {2018}\n}\n```\n'","Code for the paper ""Adversarial Generator-Encoder Networks"""
https://github.com/cornetj2/Star-1-Progress,b'# Star-1-Progress\nCurrent work on star 6224062.\n',Current work on star 6224062.
https://github.com/Zoha131/learning_pandas,"b'## Welcome to GitHub Pages\n\nYou can use the [editor on GitHub](https://github.com/Zoha131/learning_pandas/edit/master/README.md) to maintain and preview the content for your website in Markdown files.\n\nWhenever you commit to this repository, GitHub Pages will run [Jekyll](https://jekyllrb.com/) to rebuild the pages in your site, from the content in your Markdown files.\n\n### Markdown\n\nMarkdown is a lightweight and easy-to-use syntax for styling your writing. It includes conventions for\n\n```markdown\nSyntax highlighted code block\n\n# Header 1\n## Header 2\n### Header 3\n\n- Bulleted\n- List\n\n1. Numbered\n2. List\n\n**Bold** and _Italic_ and `Code` text\n\n[Link](url) and ![Image](src)\n```\n\nFor more details see [GitHub Flavored Markdown](https://guides.github.com/features/mastering-markdown/).\n\n### Jekyll Themes\n\nYour Pages site will use the layout and styles from the Jekyll theme you have selected in your [repository settings](https://github.com/Zoha131/learning_pandas/settings). The name of this theme is saved in the Jekyll `_config.yml` configuration file.\n\n### Support or Contact\n\nHaving trouble with Pages? Check out our [documentation](https://help.github.com/categories/github-pages-basics/) or [contact support](https://github.com/contact) and we\xe2\x80\x99ll help you sort it out.\n'",This is a practice repository. I have learnt git. So I am practicing it. Also learning pandas.
https://github.com/Nathaniel-Rodriguez/relcat,"b'Implements the relational categorization task from (Williams, 2008) and (Williams, 2013)\nUses Python 3.5.2+ (recommend using Anaconda)\n(optional) vpython 2\n(optional) jupyter notebook\nRequires CMA-ES library\n\nWilliams, P. L., Beer, R. D., & Gasser, M. (2008). An Embodied Dynamical Approach to Relational Categorization. Proceedings of the 30th Annual Conference of the Cognitive Science Society, 1, 223\xe2\x80\x93228.\n\nWilliams, P., & Beer, R. (2013). Environmental Feedback Drives Multiple Behaviors from the Same Neural Circuit. Advances in Artificial Life, ECAL 2013, 12, 268\xe2\x80\x93275. https://doi.org/10.7551/978-0-262-31709-2-ch041'",Implements Randall Beer's relational categorization task
https://github.com/lveronese/capstone,"b'# Machine Learning Engineer Nanodegree\n## Specializations\n## Project: Capstone Proposal and Capstone Project\n\n**Note**\n\nWelcome to my Capstone Project.\nIn order to correctly review the material, this README provides a few instructions about the documentation and code.\n\nThe proposal I submitted is available in the proposal.pdf file.\nI have provided both reviews I had, named: Proposal Udacity Review First.pdf and Proposal Udacity Review Final.pdf.\nThe url for the last proposal is https://review.udacity.com/#!/reviews/398679\n\nThe capstone project report is available as capstone_report.pdf. The report does NOT contain any code and has been written to provide a complete guide to the steps I followed in order to complete my project. The report follows the suggested capstone project structure as of the provided template.\nCompanion of the report is a jupyter notebook capstone_report.ipynb that contains all python code inside a version of the content in the report in order to be able to understand the context the code has been written in. The jupyter notebook is sufficient to have access to all results. So just run all the cells and the data will be loaded (from the data directory) and all code executed. The jupyter notebook is also available in exported html format for reader convenience.\n\nI used only standard libraries (I used the Anaconda installation with python 2.7), in particular: numpy, pandas, matplotlib, scipy.stats (for norm).\n\nThe reference folder contains additional papers I read in order to document myself about possibile solutions. They are referenced inside the report.\n\nThe data folder contains both JSON and CSV versions of the data samples I used for the report.\n\nI proofreaded only the capstone PDF report. Please forgive me for any typos you may encounter in the ipython notebook.\n\n'",Udacity Machine Learning Nanodegree Capstone Project
https://github.com/samjfalk/GA-DSI,"b""Within this folder you will find 7 weekly projects and a Capstone from General Assembly's Data Science Immersive. The projects are a representation of the work I completed while in the program\n""",Projects from General Assembly's Data Science Immersive Program
https://github.com/Prandtl/Bach,b'# Bach\n## How to use\nTo look at example results:\n```zsh\nmake runShowExample\n```\nto run hogwild:\n```zsh\nmake runHogwild\n```\nor to run and show:\n```zsh\nmake runShowHogwild\n```\n## Files\n* _example.cpp_ - simple gradient descent realization puts gradient descent path to _example.out_\n* _hogwild.cpp_ - hogwild parallel SGD. Puts output to _hogwild.out_\n* _ariadna.py_ - python script that takes filename as argument (*example.out* for example) and shows its contents\n* _toast.cpp_ - One of PETSc examples copied from the source without changes\n* _makefile_ - makefile to compile and run toast with PETSc\n',i want to get my degree.
https://github.com/manujeevanprakash/Webscraping1,b'Webscraping1\n============\n\nWeb scraping mini project -1 (countries)\n',Web scraping mini project -1 (countries)
https://github.com/datalabgit/datascience_cycle,"b""# Data Science Cycle\nHere you'll find the materials for the Data Science Cycle Workshop of September 2016. The topics covered in this workshop are:\n+ Feature Selection\n+ Feature Generation\n+ Unsupervised Learning: K-Means\n+ Supervised Learning: Classification\n\nThis workshop was made by: PhD. Andr\xc3\xa9s M\xc3\xa9ndez-V\xc3\xa1zquez \n\nThe DataLab Notebook (DataLab.ipynb) has the basic code that you can start working on. The voice.csv file contains the data you'll work with during the workshop and the DataLab.py file has all the code solved.\n\nAny further questions, you can contact DataLab Community on facebook: https://www.facebook.com/datalabmx \n""",The Data Science Cycle Workshop
https://github.com/Zurga/bearded-dangerzone,b'# bearded-dangerzone\nInfovis git repo\n',Infovis git repo
https://github.com/coreysery/reposcrape,"b""# Repository scrape\n\nClass project to track moving data.\nI'm using github and hopefully more sites, to track changes in popular repositories over time.\nChanges including, but not limited to:\n - Repo name\n - Author\n - Contributors\n - Stars\n - Forks\n - Watches\n - Languages used\n - Commits\n\n \n""",Class scrape project
https://github.com/alonsopg/automatic_summarization,b'# automatic_summarization\nSome tests and notes of how to use Automatic summarization libraries.\n',Some tests of Automatic summarization libraries
https://github.com/PatrickMockridge/SpringBoard-Data-Science-Intensive,b'# SpringBoard Data Science Intensive\nSpringBoard Data Science Intensive Course Project Repo\n\nThis repository represents the work completed for my Intensive introduction to Data Science at [Springboard.com](https://www.springboard.com) in early 2017\n\nThe course contains a variety of miniprojects on data munging and machine learning algorithms.\n\nMy Capstone report focussed on data related to the 2016 US Presidential Election:\n\n[2016 US Presidential Primary Debates](https://www.kaggle.com/kinguistics/2016-us-presidential-primary-debates)\n\n[Hillary and Donald Trump Tweets](https://www.kaggle.com/benhamner/clinton-trump-tweets)\n\n[2016 US Presidential Debates](https://www.kaggle.com/mrisdal/2016-us-presidential-debates)\n\n[Reddit Comments on the Presidential Inauguration](https://www.kaggle.com/amalinow/reddit-comments-on-presidential-inauguration)\n\nThe Introduction to my Final Project submission can be found on my personal blog [here](https://www.patrickmockridge.com/2017/06/02/springboard-data-science-intensive-introduction/)\n\nThe final project slide deck can be viewed [here](https://github.com/PatrickMockridge/SpringBoard-Data-Science-Intensive/blob/master/Capstone%20Project/Final%20Project/Capstone%20Slide%20Presentation.pdf)\n\n![Donald Trump Twitter WordCloud normalised for popularity](https://github.com/PatrickMockridge/SpringBoard-Data-Science-Intensive/blob/master/Capstone%20Project/Final%20Project/data/trump-colored-masked-twitter-wordcloud.png?raw=true)\n\n\n',SpringBoard Data Science Intensive Course Project Repo
https://github.com/DaveBackus/MFAP,"b""###MFAP\n\nMaterials for **Macroeconomic Foundations for Asset Prices** (ECON-UB-233), a course at NYU's Stern School of Business about the mathematics of asset prices and the macroeconomic foundations for them.   The repository includes TeX, Matlab, and Python files for notes, assignments, quizzes, etc.  The outputs (mostly pdf files) and a more complete description of the course are posted on the [course site]  (https://sites.google.com/site/nyusternmacrofoundations/home).\n\nSend comments and questions to Dave Backus at NYU:  db3@nyu.edu. \n\nPart of the #nyuecon collection.\n""","Macroeconomic Foundations for Asset Prices, an undergrad course at NYU"
https://github.com/Eehenhyu/DeepLearning,b'# DeepLearning\nDeep Learning implemented using Python & Tensorflow.\n\nDifferent Neural Net Architectures are used in different projects:\n\n**BasicNeuralNet:** plain Neural Network.\n\n**Sentiment\\_Analysis:** plain Neural Network.\n\n**Image\\_Classification:** Convolutional Neural Network.\n\n**GAN\\_Face\\_Generation:** Generative Adversarial Network.\n\n**Language\\_Translation:** Recurrent Neural Netowork.\n',Deep Learning using Python
https://github.com/niharikabalachandra/TimeSeries-MiniProject,b'# TimeSeries-MiniProject\nDealing with time series data using pandas\n',Dealing with time series data using pandas
https://github.com/anaulin/scratch,b'Code scratch repo.\n',Scratch pad
https://github.com/bgarcia7/ai_trainer,b'# ai_trainer\nYour next personal trainer\n',Your next personal trainer
https://github.com/LoLab-MSM/pydyno,"b'[![Codacy Badge](https://api.codacy.com/project/badge/Grade/4dc49b4309bc4f05911eee43f932591b)](https://app.codacy.com/app/ortega2247/tropical?utm_source=github.com&utm_medium=referral&utm_content=LoLab-VU/tropical&utm_campaign=Badge_Grade_Dashboard)\n[![Build Status](https://travis-ci.org/LoLab-VU/pydyno.svg?branch=master)](https://travis-ci.org/LoLab-VU/pydyno)\n[![Coverage Status](https://coveralls.io/repos/github/LoLab-VU/tropical/badge.svg?branch=master)](https://coveralls.io/github/LoLab-VU/tropical?branch=master)\n\n# PyDyNo\n\nPython Dynamic analysis of biochemical NetwOrks (PyDyNo) is an open source python library for the analysis of \nsignal execution in network-driven biological processes. PyDyNo supports the analysis of [PySB](http://pysb.org/)\nand [SBML](http://sbml.org/Main_Page) models.\n\n## Publications\n\nPrePrint: Probability-based mechanisms in biological networks with parameter uncertainty  \nOscar O. Ortega, Blake A. Wilson, James C. Pino, Michael W. Irvin, Geena V. Ildefonso, Shawn P. Garbett, Carlos F. Lopez\n\nbioRxiv 2021.01.26.428266; doi: https://doi.org/10.1101/2021.01.26.428266 \n\nThe preprint paper can be found [here](https://www.biorxiv.org/content/10.1101/2021.01.26.428266v1.full)\n\nJupyter notebooks with the code to reproduce the paper figures can be found [here](https://github.com/LoLab-VU/pydyno/tree/master/pydyno/examples/paper1)\n\n\n## Installation\n\n### From PyPI\n\n```bash\n> pip install pydyno\n```\n\n### Installing the latest unreleased version\n\n```bash\n> pip install git+git:https://github.com/LoLab-VU/pydyno.git\n```\n\n### Installing from source folder\n\n- Download and extract pydyno\n- Navigate into the pydyno directory\n- Install (Python is necessary for this step):\n\n```bash\n> python setup.py install\n```\n\n## How to use PyDyNo\n\n# Import libraries\n\n\n```python\nimport pydyno\nimport numpy as np\nfrom os.path import dirname, join\nfrom IPython.display import Image\nfrom pydyno.examples.double_enzymatic.mm_two_paths_model import model\nfrom pydyno.visualize_simulations import VisualizeSimulations\nfrom pydyno.discretization import PysbDomPath\nfrom pydyno.visualize_discretization import visualization_path\nfrom pysb.simulator import ScipyOdeSimulator\n```\n\n# Load the calibrated parameters and simulate the model with 100 different parameter sets\n\n\n```python\n# import calibrated parameters\nmodule_path = dirname(pydyno.__file__)\npars_path = join(module_path, ""examples"", ""double_enzymatic"", ""calibrated_pars.npy"")\npars = np.load(pars_path)\n```\n\n\n```python\n# define time for the simulation and simulate model\ntspan = np.linspace(0, 100, 101)\nsim = ScipyOdeSimulator(model, tspan=tspan).run(param_values=pars[:100])\n```\n\n# Visualize the dynamics of the model\n\n```python\nvt = VisualizeSimulations(model, sim, clusters=None)\nvt.plot_cluster_dynamics(components=[5])\n# This saves the figure in the local folder with the filename comp0_cluster0.png\n```\n![png](pydyno/examples/double_enzymatic/double_enzymatic_reaction_files/double_enzymatic_reaction_6_1.png)\n\n# Obtain the dominant paths for each of the simulations\xc2\xb6\n\n\n\n```python\ndp = PysbDomPath(model, sim)\nsignatures, paths = dp.get_path_signatures(\'s5\', \'production\',                                         depth=2, dom_om=1)\nsignatures.sequences.head()\n```\n\n# Obtain distance matrix and optimal number of clusters (execution modes)\n\n```python\nsignatures.dissimilarity_matrix()\nsignatures.silhouette_score_agglomerative_range(4)\n```\n\n```python\n# Select the number of cluster with highest silhouette score\nsignatures.agglomerative_clustering(2)\n```\n\n\n```python\n# Plot signatures\nsignatures.plot_sequences()\n# File is saved to the local directory with the filename modal.png\n```\n\n![png](pydyno/examples/double_enzymatic/double_enzymatic_reaction_files/double_enzymatic_reaction_13_0.png)\n\n```python\npaths\n```\n    {2: [OrderedDict([(\'s5\', [[\'s3\'], [\'s4\']])]),\n      OrderedDict([(\'s3\', [[\'s0\', \'s1\']]), (\'s4\', [[\'s0\', \'s2\']])])],\n     1: [OrderedDict([(\'s5\', [[\'s4\']])]), OrderedDict([(\'s4\', [[\'s0\', \'s2\']])])],\n     0: [OrderedDict([(\'s5\', [[\'s3\']])]), OrderedDict([(\'s3\', [[\'s0\', \'s1\']])])]}\n\n# Visualize execution modes\n[Graphviz](https://graphviz.org/download/) is necessary to obtain these visualizations\n```python\nvisualization_path(model, \n                   path=paths[0], \n                   target_node=\'s5\', \n                   type_analysis=\'production\', \n                   filename=\'path_0.png\')\n# Visualization is saved to local directory with the filename path0.png\n```\n\n![png](pydyno/examples/double_enzymatic/double_enzymatic_reaction_files/path_0.png)\n\n```python\nvisualization_path(model, \n                   path=paths[1], \n                   target_node=\'s5\', \n                   type_analysis=\'production\', \n                   filename=\'path_1.png\')\n# Visualization is saved to local directory with the filename path1.png\n```\n\n![png](pydyno/examples/double_enzymatic/double_enzymatic_reaction_files/path_1.png)\n\n```python\nvisualization_path(model, \n                   path=paths[2], \n                   target_node=\'s5\', \n                   type_analysis=\'production\', \n                   filename=\'path_2.png\')\n# Visualization is saved to local directory with the filename path2.png\n```\n\n![png](pydyno/examples/double_enzymatic/double_enzymatic_reaction_files/path_2.png)\n'",Tool that uses tropical algebra concepts to 'decompose' species trajectories in the protein-protein interactions that drive changes of concentration in time
https://github.com/adriansarno/autoencoder,b'# autoencoder\nA simple autoencoder in tensorflow.\n',A simple autoencoder in tensorflow.
https://github.com/virgodi/plda,"b'Parallelized Python/Cython implementation of Latent Dirichlet allocation\nFinal project for CS205 at Harvard University\nWritten by Charles Liu, Nicolas Drizard, and Virgile Audi\n\n# System Requirements:\n\nThis package was tested on OSX. We ran experiments on Python 2.7 with needed packages:\n\n- Numpy\n- Threading\n- Cython\n\nThe execution of the Cython scripts require a C compiler.\n\n# Installation:\n\nTo install the package, download the zip folder from the git repository. We are working to have a pip install link soon.\n\n# Documentation:\n\nThis Python package can be used to perform efficient topic modeling using Latent Dirichlet Allocation. More details on LDA can be found in the IPython notebook below. \n\nThe organisation of the package is as follow:\n\n - Two classes: \n    \n    * The *oviLDA* class to perform Online Variational Inference and the *cgsLDA* class to perform Collapsed Gibbs Sampling\n \n    * These 2 classes have identical methods, and only a few specific attributes change for inference purposes:\n \n        - Common attributes include:\n        \n       |    Attribute    |                        Type                       |                                                        Details                                                        |\n       |:---------------:|:-------------------------------------------------:|:---------------------------------------------------------------------------------------------------------------------:|\n       |    num_topics   |                        Int                        |                                                Number of topics desired                                               |\n       |   num_threads   |                        Int                        |                                      Number of threads needed for parallelisation                                     |\n       |      topics     | Array of dimensions: num_topics x len(vocabulary) | Each row representing a particular topic, after normalisation these can be treated as multinomial over the vocabulary |\n       |      gamma      |   Array of dimensions: len(corpus) x num_topics   |                               Each row representing the topic assignment for a document                               |\n       | _log_likelihood |                       Float                       |                                       Perplexity evaluated on the training data                                       |\n        \n        - OVI specific attributes:\n        \n        |    Attribute   |      Type      |                                Details                                |\n        |:--------------:|:--------------:|:---------------------------------------------------------------------:|\n        |   batch_size   |       Int      |             Number of document to consider in every batch             |\n        |       tau      |       Int      |    Parameter used to weight the first iterations of the algorithm     |\n        |      kappa     | Float: (0.5,1] | Parameter controlling the rate at which we forget previous iterations |\n        | max_iterations |       Int      |        Maximum number of iterations on one particular document        |\n        \n        - CGS specific attributes:\n        \n        |   Attribute   |   Type  |                                       Details                                        |\n        |:-------------:|:-------:|:------------------------------------------------------------------------------------:|\n        |   iterations  |   Int   |                          Number of sampling iterations                               |\n        |    damping    |   Int   | Likelihood full number of occurrences will be sampled. See notebook for more details |\n        | sync_interval |   Int   |      Parameter controlling how often threads aggregate topic distributions           |\n        |     alpha     |  Float  |                     Dirichlet prior parameter for document/topics                    |\n        |      beta     |  Float  |                      Dirichlet prior parameter for topics/words                      |\n        |  split_words  | Boolean |            Parallelization method used. See notebook for more details                |\n        \n        - Methods:\n        \n            * fit(dtm): fits the model for a particular corpus\n            \n            | Parameters |                    Type                   |        Details       |\n            |------------|:-----------------------------------------:|----------------------|\n            |     dtm    | array of dimensions: len(docs) x len(voc) | document term matrix |\n           \n            * transform(dtm): Transform new documents into a topic assignment matrix according to a previously trained model\n            \n            | Parameters |                    Type                   |                        Details                        |\n            |------------|:-----------------------------------------:|-------------------------------------------------------|\n            |     dtm    | array of dimensions: len(docs) x len(voc) | document term matrix (NO ZERO COLUMNS FOR CGS METHOD) |\n            \n            |   Return  |                     Type                    |        Details       |\n            |-----------|:-------------------------------------------:|----------------------|\n            |   gamma   | array of dimensions: len(docs) x num_topics |  Topic assignments   |\n        \n    \n -  Useful functions related to the LDA model in the LDAutil folder:\n    \n    * print_topic(model,vocabulary,num_top_words): prints the topics for a fitted LDA model\n    \n    |   Parameters  |                   Type                   |                              Details                              |\n    |:-------------:|:----------------------------------------:|:-----------------------------------------------------------------:|\n    |     model     |             cgsLDA or oviLDA             |                   A previously fitted LDA model                   |\n    |   vocabulary  | array of dimensions: 1 x len(vocabulary) | An array of strings ordered in the same way as the columns of DTM |\n    | num_top_words |                    Int                   |                  Number of wanted words per topic                 |\n    \n    * perplexity(model,dtm_test): computes the log-likelihood of the documents in dtm_test based on the\n    topic distribution already learned by the model\n    \n    | Parameters |                       Type                       |                                          Details                                         |\n    |:----------:|:------------------------------------------------:|:----------------------------------------------------------------------------------------:|\n    |    model   |                 cgsLDA or oviLDA                 |                               A previously fitted LDA model                              |\n    |   dtm_new  | array of dimensions: len(docs) x len(vocabulary) | A new DTM corresponding to the new documents on which we want to evaluate the perplexity |\n\n    |   Return   |  Type |                Details                |\n    |:----------:|:-----:|:-------------------------------------:| \n    | perplexity | float | Perplexity evaluated on new documents |\n\nMore details on these functions and what they actually evaluate are present in the Ipython notebook.\n    \n - A subset of the Reuters news dataset in the form of a document term matrix and the associated vocabulary.\n    \n\n# Test to run:\n\nFor you to test if your system is up to the requirements and to showcase the package in action, we included a Python test.py file.\n\nYou can run both versions of LDA by commenting and uncommenting respectively lines 36 and 39.\n\n# References:\n\n- The OVI code is based on Hoffman\'s 2010 paper [""Online Learning for Latent Dirichlet Allocation""](https://www.cs.princeton.edu/~blei/papers/HoffmanBleiBach2010b.pdf)\n- The CGS code relies on:\n    * [Efficient Collapsed Gibbs Sampling For Latent Dirichlet Allocation](http://jmlr.csail.mit.edu/proceedings/papers/v13/xiao10a/xiao10a.pdf) by Han Xiao and Thomas Stibor\n    * [Parallel Inference for Latent Dirichlet Allocation on Graphics Processing Units](https://www.cs.purdue.edu/homes/alanqi/papers/Parallel-Inf-LDA-GPU-NIPS.pdf) by Feng Yan, Ningyi Xu and Yuan (Alan) Qi\n'",Python Package to run Latent Dirichlet Allocation in Parallel
https://github.com/brookisme/gitnb,"b'## GITNB \n\n**GIT TRACKING FOR PYTHON NOTEBOOKS**\n\nA simple idea: GitNB doesn\'t actually track python notebooks. Instead, GitNB creates and updates python versions of your notebooks which are in turn tracked by git.\n\n1. [Quick Start](#quick)\n2. [But I\'m Lazy!!!](#lazy)\n3. [Install](#install)\n4. [Docs](#docs)\n5. [User Config](#config)\n\n_____\n<a name=\'quick\'></a>\n### QUICK START:\n\nThis quick-start is just an example. It looks long (due to bash-output) but its quick: 1-2 minutes tops.\n\nA. INITIALIZE GIT REPO\n\n```bash\ntest| $ tree\n.\n\xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 A-Notebook.ipynb\n\xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 A_BUGGY_NOTEBOOK.ipynb\n\xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 Py2NB.ipynb\n\xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 another_python_file.py\n\xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 some_python_file.py\n\xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 widget\n    \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 I\\ have\\ spaces\\ in\\ my\\ name.ipynb\n    \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 Notebook1.ipynb\n    \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 widget.py\n\n1 directory, 8 files\n\ntest| $ git init\nInitialized empty Git repository in /Users/brook/code/jupyter/gitnb/test/.git/\n\ntest| $ git add .\n\ntest| $ git commit -am ""Initial Commit: python files""\n[master (root-commit) b29b6c4] ...\n```\n\n\nB. INITIALIZE GitNB, ADD NOTEBOOKS TO GitNB TO BE TRACKED\n\n```bash\n\n# initialize gitnb\ntest|master $ gitnb init\n\ngitnb: INSTALLED \n   - nbpy.py files will be created/updated/tracked\n   - install user config with: $ gitnb configure\n\n# lets list our (untracked) notebooks\ntest|master $ gitnb list\n\ngitnb[untracked]:\n  Py2NB.ipynb\n  A-Notebook.ipynb\n  widget/I have spaces in my name.ipynb\n  A_BUGGY_NOTEBOOK.ipynb\n  widget/Notebook1.ipynb\n\n# adding an individual file\ntest|master $ gitnb add A_BUGGY_NOTEBOOK.ipynb \ngitnb: add (A_BUGGY_NOTEBOOK.ipynb | nbpy/A_BUGGY_NOTEBOOK.nbpy.py)\n\n# adding all the files in a directory\ntest|master $ gitnb add widget\ngitnb: add (widget/I have spaces in my name.ipynb | nbpy/I have spaces in my name.nbpy.py)\ngitnb: add (widget/Notebook1.ipynb | nbpy/Notebook1.nbpy.py)\n\n# the default directory for the python versions of the notebooks is nbpy/\ntest|master $ tree\n.\n\xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 A-Notebook.ipynb\n\xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 A_BUGGY_NOTEBOOK.ipynb\n\xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 Py2NB.ipynb\n\xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 another_python_file.py\n\xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 nbpy\n\xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 A_BUGGY_NOTEBOOK.nbpy.py\n\xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 I\\ have\\ spaces\\ in\\ my\\ name.nbpy.py\n\xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 Notebook1.nbpy.py\n\xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 some_python_file.py\n\xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 widget\n    \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 I\\ have\\ spaces\\ in\\ my\\ name.ipynb\n    \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 Notebook1.ipynb\n    \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 widget.py\n\n2 directories, 11 files\n\n# our list now conatins tracked and untracked notebooks\ntest|master $ gitnb list\n\ngitnb[tracked]:\n  widget/Notebook1.ipynb\n  widget/I have spaces in my name.ipynb\n  A_BUGGY_NOTEBOOK.ipynb\n\ngitnb[untracked]:\n  A-Notebook.ipynb\n  Py2NB.ipynb\n\n# note these files are now in our git repo\ntest|master $ git status\nOn branch master\nChanges to be committed:\n  (use ""git reset HEAD <file>..."" to unstage)\n\n  new file:   nbpy/A_BUGGY_NOTEBOOK.nbpy.py\n  new file:   nbpy/I have spaces in my name.nbpy.py\n  new file:   nbpy/Notebook1.nbpy.py\n\n# git commit the new nbpy.py versions\ntest|master $ git commit -am ""add nbpy.py versions of notebooks""\n[master 868b0a2] ...\n```\n\n\nC. QUICK LOOK AT A ""NBPY.PY"" VERSION OF A NOTEBOOK\n\n\n```bash\ntest|master $ cat nbpy/A_BUGGY_NOTEBOOK.nbpy.py \n\n\n""""""[markdown]\n## This is a notebook with bugs\n""""""\n\n\n""""""[code]""""""\nimport numpy as np\n""""""""""""\n\n\n""""""[code]""""""\ndef feature(food=True):\n    if foo:\n        return ""I am not a bug""\n    else:\n        return ""I told you I am not a bug""\n""""""""""""\n\n\n""""""[code]""""""\nprint(""Are you a bug?"")\nprint(feature(True))\n""""""""""""\n\n```\n\nD. UPDATE NBPY.PY FILE AFTER EDITING YOUR NOTEBOOK\n\nThat notebook is buggy ...[updating python notebook]... I just went to the python-notebook and fixed the bugs. Let\'s see what happened:\n\n```bash\n# note the changes have not appeared in our nbpy.py file\ntest|master $ git diff\n\n# however, we can see the changes with \'gitnb diff\'\ntest|master $ gitnb diff A_BUGGY_NOTEBOOK.ipynb\n\ngitnb[diff]: A_BUGGY_NOTEBOOK.ipynb[->nbpy.py] - nbpy/A_BUGGY_NOTEBOOK.nbpy.py\n--- +++ @@ -1,7 +1,7 @@ \n \n """"""[markdown]\n-## This is a notebook with bugs\n+## This is a notebook without bugs\n """"""\n \n \n@@ -11,7 +11,7 @@ \n \n """"""[code]""""""\n-def feature(food=True):\n+def feature(foo=True):\n     if foo:\n         return ""I am not a bug""\n     else:\n\n\n# we now use \'gitnb update\' to update the tracked files\n# this creates a new nbpy.py version and adds the changes\n# to the git repo\ntest|master $ gitnb update\n\n# now we can see the bug fixes with \'git diff\'\ntest|master $ git diff\ndiff --git a/nbpy/A_BUGGY_NOTEBOOK.nbpy.py b/nbpy/A_BUGGY_NOTEBOOK.nbpy.py\nindex e80204b..955b359 100644\n--- a/nbpy/A_BUGGY_NOTEBOOK.nbpy.py\n+++ b/nbpy/A_BUGGY_NOTEBOOK.nbpy.py\n@@ -1,7 +1,7 @@\n \n \n """"""[markdown]\n-## This is a notebook with bugs\n+## This is a notebook without bugs\n """"""\n \n \n@@ -11,7 +11,7 @@ import numpy as np\n \n \n """"""[code]""""""\n-def feature(food=True):\n+def feature(foo=True):\n     if foo:\n         return ""I am not a bug""\n     else:\n\n# commit the changes\ntest|master $ git commit -am ""fixed bug: i fixed .ipynb, gitnb fixed .nbpy.py""\n[master 812a4f0] ...\n```\n\nE. CREATE PYTHON-NOTEBOOK FROM NBPY.PY FILE\n\nFinally, lets say we actually need that buggy notebook after all\n\n```bash\ntest|master $ git checkout 868b0a2\nNote: checking out \'868b0a2\'.\n[... git detached head messaging ...]\nHEAD is now at 868b0a2... add nbpy.py versions of notebooks\n\n# create notebook from nbpy.py file\ntest|(HEAD detached at 868b0a2) $ gitnb tonb nbpy/A_BUGGY_NOTEBOOK.nbpy.py \n\n# the default directory for generated notebooks versions is nbpy/\ntest|(HEAD detached at 868b0a2) $ tree nbpy_nb\nnbpy_nb\n\xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 A_BUGGY_NOTEBOOK.nbpy.ipynb\n\n0 directories, 1 file\n```\n\nMy bugs are back!\n\n![nbpy_nb/A_BUGGY_NOTEBOOK.nbpy.ipynb](https://github.com/brookisme/gitnb/blob/master/buggy.png)\n\n_____\n<a name=\'lazy\'></a>\n### LAZY CONFIG:\n\nIf the [quick-start](#quick) seemed like too much how about this...\n\n```bash\n$ gitnb commit -am ""I just updated and commited every notebook in my project"" \n```\n\nHow in the what? Two things are going on here\n\n1. We are [commit](#commit)-ing with `gitnb commit` instead of `git commit`\n2. I\'ve [installed](#configure) the user config and set\n\n```bash\n# ./gitnb.config.yaml\n...\nGIT_ADD_ON_GITNB_UPDATE: True\nAUTO_TRACK_ALL_NOTEBOOKS: True\n...\n```\n\nNow each time I `gitnb commit`:\n\n* All new notebooks are [add](#add)-ed to be tracked by gitnb\n* All notebooks are [update](#update)-ed\n* All changes are added to the git repo\n* `git commit --allow-empty` is [called](#commit)\n\nNote: the `--allow-empty` flag is there because the at the time of the commit (before the nbpy.py files are generated there may or may not be changes to commit)\n\nHere\'s the super-quick-quick-start-example:\n```bash\ntest|master $ git init\nInitialized empty Git repository in /Users/brook/code/jupyter/gitnb/test/.git/\ntest| $ gitnb init\n\ngitnb: INSTALLED \n   - nbpy.py files will be created/updated/tracked\n   - install user config with: $ gitnb configure\n\ntest| $ gitnb configure\ngitnb: USER CONFIG FILE ADDED (./gitnb.config.yaml) \n```\n\n... go update gitnb.config.yaml ...\n\n```bash\ntest| $ gitnb commit -am ""Initial Commit with everything""\ngitnb: add (A-Notebook.ipynb | nbpy/A-Notebook.nbpy.py)\ngitnb: add (A_BUGGY_NOTEBOOK.ipynb | nbpy/A_BUGGY_NOTEBOOK.nbpy.py)\ngitnb: add (Py2NB.ipynb | nbpy/Py2NB.nbpy.py)\ngitnb: add (widget/I have spaces in my name.ipynb | nbpy/I have spaces in my name.nbpy.py)\ngitnb: add (widget/Notebook1.ipynb | nbpy/Notebook1.nbpy.py)\n[master (root-commit) cb8c106] ...\n\ntest|master $ tree\n.\n\xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 A-Notebook.ipynb\n\xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 A_BUGGY_NOTEBOOK.ipynb\n\xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 Py2NB.ipynb\n\xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 another_python_file.py\n\xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 gitnb.config.yaml\n\xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 nbpy\n\xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 A-Notebook.nbpy.py\n\xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 A_BUGGY_NOTEBOOK.nbpy.py\n\xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 I\\ have\\ spaces\\ in\\ my\\ name.nbpy.py\n\xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 Notebook1.nbpy.py\n\xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 Py2NB.nbpy.py\n\xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 some_python_file.py\n\xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 widget\n    \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 I\\ have\\ spaces\\ in\\ my\\ name.ipynb\n    \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 Notebook1.ipynb\n    \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 widget.py\n\n2 directories, 14 files\n```\n_____\n<a name=\'install\'></a>\n### INSTALL:\n\n###### pip:\n```bash\npip install gitnb\n```\n\n###### github:\n```\ngit clone https://github.com/brookisme/gitnb.git\ncd gitnb\nsudo pip install -e .\n```\n\n_____\n<a name=\'docs\'></a>\n### DOCS:\n\n```bash\n$ gitnb --help\nusage: gitnb [-h] {init,configure,list,update,add,remove,topy,tonb} ...\n```\n\n<a name=\'methods\'></a>\n###### methods:\n\n1. [init](#init): initialize gitnb for project\n2. [configure](#configure): install gitnb.config.yaml for user config\n3. [gitignore](#gitignore): add ipynb \\& gitnb files to gitignore\n4. [list](#list): list tracked notebooks or nbpy.py files\n5. [add](#add): begin tracking notebook\n6. [remove](#remove): stop tracking notebook\n7. [update](#update): update nbpy.py files with recent notebook edits\n8. [commit](#commit): update, followed by add, followed by `git commit`\n9. [diff](#diff): perform diff between current notebook version and last [update](#update)-ed version\n10. [topy](#topy): convert notebook to nbpy.py file (without [add](#add)-ing)\n11. [tonb](#tonb): convert nbpy.py file to python notebook\n\n_____\n<a name=\'init\'></a>\n\n###### init:\nInitialize Project:\n\n* `git init` required before `gitnb init`\n* installs .gitnb directory at the project root\n* creates or appends .git/hooks/pre-commit for auto-tracking config\n\n```bash\n$ gitnb init\n```\n([back to methods](#methods))\n\n_____\n<a name=\'configure\'></a>\n###### configure:\nInstall Config:\n\n* optional: only necesary if you want to change the [default config](https://github.com/brookisme/gitnb/blob/master/gitnb/default.config.yaml)\n* installs gitnb.config.yaml directory at the project root\n\n```bash\n$ gitnb configure\n```\n([back to methods](#methods))\n\n_____\n<a name=\'gitignore\'></a>\n###### gitignore:\nUpdate .gitignore:\n\n\n\nAppends (or creates) gitignore with the recommended settings. Namely,\n\n* notebooks: *.ipynb, .ipynb_checkpoints\n* gitnb: nbpy_nb/\n\n```bash\n$ gitnb gitignore\n```\n([back to methods](#methods))\n\n_____\n<a name=\'list\'></a>\n###### list:\nList Project Notebooks, or nbpy.py files\n\npositional arg (*type*):\n\n* (default) **all**: list tracked and untracked notebooks\n* **tracked**: list tracked notebooks\n* **untracked**: list untracked notebooks\n* **nbpy**: list nbpy.py files\n\n```bash\n$ gitnb list --help\nusage: gitnb list [-h] [type]\n\npositional arguments:\n  type        notebooks: ( all | tracked | untracked ), or nbpy\n```\n([back to methods](#methods))\n\n_____\n<a name=\'add\'></a>\n###### add:\nAdd notebook to gitnb:\n\n* converts notebook(s) to nbpy.py file(s)\n* adds notebook-nbpy pair to gitnb tracking list\n* performs a `git add` on nbpy.py file(s)\n* path: path to file or directory \n* destination_path: (optional) \n    * if path is a file path nbpy file will be at destination_path\n    * if destination_path is falsey (recommended) default path is used\n    * default path can be changed with [user config](#config)\n    * if path is a direcotry path, default config is always used\n\n```bash\n$ gitnb add --help\nusage: gitnb add [-h] path [destination_path]\n\npositional arguments:\n  path              path to ipynb file\n  destination_path  if falsey uses default destination path\n```\n([back to methods](#methods))\n\n_____\n<a name=\'remove\'></a>\n###### remove:\nRemove notebook from gitnb:\n\n* notebook will no longer be tracked\n* nbpy.py file will **not** be deleted\n* ipynb file will **not** be deleted\n\n```bash\n$ gitnb remove --help\nusage: gitnb remove [-h] path\n\npositional arguments:\n  path        path to ipynb file\n```\n([back to methods](#methods))\n\n_____\n<a name=\'update\'></a>\n###### update:\nUpdate nbpy files:\n\n* will update nbpy files with current content from your tracked notebooks\n* make sure your notebook has been saved!\n\n```bash\n$ gitnb update\n```\n([back to methods](#methods))\n\n_____\n<a name=\'commit\'></a>\n###### commit:\nUpdate and Commit:\n\n* if (UPDATE_ON_GITNB_COMMIT) perform \'gitnb update\'\n* call `git commit --allow-empty` with optional flags [a|m]:\n  - -a flag (add all - same as git commit -a)\n  - -m flag (add all - same as git commit -m)\n  - note: `--allow-empty` because the nbpy files are not yet updated\n\n```bash\n#\n# this line of code is equivalent to\n# - $ gitnb update\n# - $ git add .\n# - $ git commit --allow-empty -am ""COMMIT MESSAGE""\n#\n$ gitnb commit [-a] [-m ""COMMIT MESSAGE""]\n```\n([back to methods](#methods))\n\n_____\n<a name=\'diff\'></a>\n###### diff:\nDiff for recent changes.\n\nCreates a diff between the most recent nbpy.py version of the noteboook\nand the nbpy.py version of the notebook in its current state (the working copy).\n\n```bash\n$ gitnb diff <PATH-TO-NOTEBOOK(.ipynb)-FILE>\n```\n([back to methods](#methods))\n\n_____\n<a name=\'topy\'></a>\n###### topy:\nTo-Python:\n\n* converts notebook(s) to nbpy.py file(s)\n* similar to [add](#add) but does not gitnb or git track\n\n```bash\n$ gitnb topy --help\nusage: gitnb topy [-h] path [destination_path]\n\npositional arguments:\n  path              path to ipynb file\n  destination_path  if falsey uses default destination path\n```\n([back to methods](#methods))\n\n_____\n<a name=\'tonb\'></a>\n###### tonb:\nTo-Notebook:\n\n* creates new notebook from nbpy.py python file\n* great for collaborators!\n* great for recovering lost work!\n\n```bash\n$ gitnb tonb --help\nusage: gitnb tonb [-h] path [destination_path]\n\npositional arguments:\n  path              path to ipynb file\n  destination_path  if falsey uses default destination path\n```\n([back to methods](#methods))\n\n\n_____\n<a name=\'config\'></a>\n### USER CONFIG:\n\nThe [configure](#configure) method installs `gitnb.config.yaml` in your root directory.  This is a copy of the [default config](https://github.com/brookisme/gitnb/blob/master/gitnb/default.config.yaml). Note at anytime you can go back to the default configuration by simply deleting the user config file (`gitnb.config.yaml`).\n\nThere are comment-docs in the config file that should explain what each configuration control. However I thought I\'d touch a couple of the perhaps more interesting configurations here.\n\n##### LAZY INSTALL\n\nsee [But I\'m Lazy!](#lazy)\n\n##### GIT_ADD_ON_GITNB_ADD (defaults to True):\n\nIf True the [add](#add) method will perform a `git add` after creating the nbpy file and adding it to the gitnb tracking list.  You can set this to False if you want to explicity call `git add` yourself after looking over the file.\n\n##### UPDATE_ON_COMMIT (defaults to True):\n\nIf True, the gitnb [update](#update) method will automatically be called when performing a `git commit` (during pre-commit hook).\n\n##### AUTO_TRACK_ALL_NOTEBOOKS (defaults to False):\n\nIf True, `gitnb add .` (see [add](#add) method) will automatically be called when performing a `git commit` (during pre-commit hook). This will add all notebooks in your project to gitnb.\n\n_Note if the only thing that has changed is your notebooks, you\'ll still need to explicity call `gitnb update` or add the `--allow-empty` flag to your `git commit`._\n\n##### EXCLUDE_DIRS:\n\nA list of directories not to include when searching for notebooks\n\n##### OTHER:\n\nYou can also configure, default location for new files, if they include an indentifier (like \'nbpy\' in `somefile.nbpy.py`), spacing in nbpy files and more. Check the comment-docs for more info.\n\n\n'",git tracking for python notebooks
https://github.com/sebastianrosales/Rosales_Mendez_hw6,b'Rosales_Mendez_hw6\n==================\n\nTarea 6 Presa-Depredador y Particula en el Campo Magnetico\n',Tarea 6 Presa-Depredador y Particula en el Campo Magnetico
https://github.com/dncn123/WineNLPRecommender,b'# WineNLPRecommender\nNLP on descriptions of wines from online retailer.\n\ndata_collection.ipynb\n  - notebook used to scrap data for exercise\n  \npreprocessing.py \n  - class that can be used to preprocess data\n  \nWineNet.py\n  - class that can be used to create graph of wines\n  \nrecommendations.ipynb\n  - notebook in which preprocessing and network are used\n  \nWine t-SNE.ipynb \n  - notebook using t-SNE clustering - which worked like a dream...\n',NLP on descriptions of wines from online retailer.
https://github.com/wangjiahong/Titanic-Kaggle,"b""##Kaggle competition: Titanic Machine learning\n\nBest solution ranks top 4% on leaderboard. (187th out of 5535 teams)\n\n![Image of score on leaderboard](https://github.com/wangjiahong666/Titanic-Kaggle/blob/master/input/score_on_leaderboard.PNG)\n\n\nplay_a_random_music.py will play a music randomly from my local disk when my main algorithm is completed. This is a kind of notification for me. \n\n\n'\n""","A tutorial for Kaggle's Titanic: Machine Learning from Disaster competition. Demonstartes basic data munging, analysis, and visualization techniques. Shows examples of supervised machine learning techniques."
https://github.com/xujin1982/Two-Sigma-Connect-Rental-Listing-Inquiries,b'# Two-Sigma-Connect-Rental-Listing-Inquiries\nCode for Kaggle Two Sigma Connect Rental Listing Inquiries\n',Kaggle Two-Sigma-Connect-Rental-Listing-Inquiries
https://github.com/deepankverma/dlnd_1,b'# dlnd_1\nFirst Project of Deep Learning\n',First Project of Deep Learning
https://github.com/leonardoaraujosantos/LearningTorch,b'# LearningTorch\nPlace where I save all my experiments with torch\n',Doing deep learning experiments with torch
https://github.com/sketchychen/ga_datasci_cw,"b""# ga_datasci_cw\nthis is coursework, belonging to Rachel C., for GA Seattle's Data Science, 2016 Mar 15 - May 19\n""",Coursework for GA Seattle's Data Science 2016 Mar 15 - May 19
https://github.com/mlayeghi/activation-cost-functions,b'# Activation_Cost_Functions\n\nBrief descriptions and TensorFlow examples of activation and cost functions and their applications in Neural Networks.\n\n',Activation vs. Cost functions
https://github.com/stuti-madaan/Advanced-Predictive-Modeling,b'# Advanced-Predictive-Modeling\nCodes for deeper insights in Predictive Modeling\n',Codes for deeper insights in Predictive Modeling
https://github.com/akshay-sr/CarND-FindingLaneLines-P1,"b'#**Finding Lane Lines on the Road** \n<img src=""laneLines_thirdPass.jpg"" width=""480"" alt=""Combined Image"" />\n\nWhen we drive, we use our eyes to decide where to go.  The lines on the road that show us where the lanes are act as our constant reference for where to steer the vehicle.  Naturally, one of the first things we would like to do in developing a self-driving car is to automatically detect lane lines using an algorithm.\n\nIn this project you will detect lane lines in images using Python and OpenCV.  OpenCV means ""Open-Source Computer Vision"", which is a package that has many useful tools for analyzing images.  \n\n**Step 1:** Getting setup with Python\n\nTo do this project, you will need Python 3 along with the numpy, matplotlib, and OpenCV libraries, as well as Jupyter Notebook installed. \n\nWe recommend downloading and installing the Anaconda Python 3 distribution from Continuum Analytics because it comes prepackaged with many of the Python dependencies you will need for this and future projects, makes it easy to install OpenCV, and includes Jupyter Notebook.  Beyond that, it is one of the most common Python distributions used in data analytics and machine learning, so a great choice if you\'re getting started in the field.\n\nChoose the appropriate Python 3 Anaconda install package for your operating system <A HREF=""https://www.continuum.io/downloads"" target=""_blank"">here</A>.   Download and install the package.\n\nIf you already have Anaconda for Python 2 installed, you can create a separate environment for Python 3 and all the appropriate dependencies with the following command:\n\n`>  conda create --name=yourNewEnvironment python=3 anaconda`\n\n`>  source activate yourNewEnvironment`\n\n**Step 2:** Installing OpenCV\n\nOnce you have Anaconda installed, first double check you are in your Python 3 environment:\n\n`>python`    \n`Python 3.5.2 |Anaconda 4.1.1 (x86_64)| (default, Jul  2 2016, 17:52:12)`  \n`[GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)] on darwin`  \n`Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.`  \n`>>>`   \n(Ctrl-d to exit Python)\n\nrun the following commands at the terminal prompt to get OpenCV:\n\n`> pip install pillow`\n`> conda install -c https://conda.anaconda.org/menpo opencv3`\n\nthen to test if OpenCV is installed correctly:\n\n`> python`  \n`>>> import cv2`  \n`>>>`  \n(Ctrl-d to exit Python)\n\n**Step 3:** Installing moviepy  \n\nWe recommend the ""moviepy"" package for processing video in this project (though you\'re welcome to use other packages if you prefer).  \n\nTo install moviepy run:\n\n`>pip install moviepy`  \n\nand check that the install worked:\n\n`>python`  \n`>>>import moviepy`  \n`>>>`  \n(Ctrl-d to exit Python)\n\n**Step 4:** Opening the code in a Jupyter Notebook\n\nYou will complete this project in a Jupyter notebook.  If you are unfamiliar with Jupyter Notebooks, check out <A HREF=""https://jupyter-notebook-beginner-guide.readthedocs.io/en/latest/"" target=""_blank"">this link</A> to get started.\n\nJupyter is an ipython notebook where you can run blocks of code and see results interactively.  All the code for this project is contained in a Jupyter notebook. To start Jupyter in your browser, run the following command at the terminal prompt (be sure you\'re in your Python 3 environment!):\n\n`> jupyter notebook`\n\nA browser window will appear showing the contents of the current directory.  Click on the file called ""P1.ipynb"".  Another browser window will appear displaying the notebook.  Follow the instructions in the notebook to complete the project.  \n'",FindingLaneLines
https://github.com/Davidjohnwilson/notes,b'# Notes\n\nFor notes from meetups and conferences. Most of the time these will be saved as ipython/Jupyter notebooks.',Various notes from meetups and conferences.
https://github.com/twhyntie/bokeh_time_series,"b'# Plotting time series with Bokeh\nThis repository contains code, tools and Jupyter notebooks\nfor making time series plot using Bokeh.\n\nSee this [notebook](./pixel_rate_time_series_WEB.ipynb) for\na read-only (i.e. non-interactive) version of the\n[interactive notebook](./pixel_rate_time_series.ipynb).\n'",Code and tools for plotting time series using Bokeh.
https://github.com/RobertPRanney/Blog_Exploration,"b""# The Successful Blogger\n\n#### Data Science Immersive Capstone project\n---\n\n#### 5 Minute Presentation Slides\n[link to slidedeck](./presentation_slides.pdf)\n\n\n#### Long Version\n###### Project Motivation\nNear the beginning of the galvanize DSI program I started a blog for myself. I\nwanted a place to help promote myself as I try to transition careers. A place to\ncontribute my data science thoughts, and trip reports. Plus it also just seemed\nlike it could be fun. My fledging blog still needs lots of work, but I do have a\ndecent start on it.\n\n![[screenshot of blog]](./final_images/my_blog_photo.png)\n\n\n###### Is it possible to identify elements that contribute to a successful blog?\nWhile in the DSI program I was wondering if I could use some of my new data\nscience skills to identify elements of a blog that can lead to its success. Of\ncourse I would never be able to to identify everything that makes a blog do\nwell. It would be hard to capture the visual appeal of a blog, and it would be\nalmost impossible to capture aspects like self promotion. That being said I\nthought it was possible to capture some aspects.\n\n---\n##### Project Overview and Workflow\nTo do this analysis a single topic of blogs was chosen to limit the effect of\nbetter topics doing significantly differently. The executive overview is shown\nbelow. The project involved gathering as many blogs as I felt like, for this I\ngathered 2500 blogs. Then much cleaning and some topic filtering took place.\nThen the porblem is split into two sub problems, 1 to deal with identifying\nelements of successful posts, and one to identify elements of successful blogs.\n\n![Project Workflow: Executive Overview](./final_images/executive_overview.png)\n\nAll of the code to run this a analysis, or a similar analysis is contained in\nthe code folder of this repository. But there are quite a few steps that go into\nthe analysis, and the executive overview does not serve as good enough road map.\nThe current code some needs some reorganization, but in the meantime I will\nplace the current roadmap that I used will doing this project. This should serve\nas a passable guide understand how the functions work together.\n\n![Code Roadmap: Re-organization still needed](./final_images/code_roadmap.png)\n\n\n##### Data Collection\nData was collected from the wordpress api, which is super friendly and easy to\nuse. Allows pulling of a public blogs information (most information), plus all\nof blogs posts, and its comments. This can be done by knowing a blogs url to get\nthe unique ID. Amazingly there is no rate limiting or quota, and doesn't even\nrequire api keys. The less robust part of the data collection currently is how\nto collect the url links. I was going to scrape them from google search results,\nbut quickly abandoned this idea due to the massive problems of that approach.\nInstead I found a fairly efficient manual method to gather about 10,000 google\nsearch result links in about an hour using the chrome 'link clump' add on. This\ndeficiency is something I would like to address later given time. These 10,000\nyield 2500 blogs.\n\n##### Data storage\nA blog and all of its posts were pulled into a mongo db. This was done on an ec2\ninstance, but with only 2500 blogs it was only about 3 Gb of data so it could\nhave been done locally, but just for efficiency almost all analysis stayed up on\nthe instance.\n\n##### Filtering\nBlogs were filtered for having multiples authors, too many post (>2500) and not\nseeming relevant based on not a high enough precent of posts containing fitness\nkey words.\n\n##### Splitting\nFrom here posts are stripped out of blogs to model the things that contribute to\nsuccessful posts.\n\n##### Natural Language Processing\n![NLP processing pipeline](./final_images/nlp_pipe.png)\nBeautiful Soup was used to pull text from html, the text is cleaned, stripped,\nand lemmatized. The lemmatization metod chosen was the nltk wordnet lemmatizer.\nThis needs to be re-examined though due to the odd words that made it through.\nThe tokenized documents were converted to a tfidf matrix. The resulting matrix\nwas then reduced with NMF, for this analysis 30 was chosen. A longer anaylsis,\nor one done with more post, or no topic division could use more, as it was 30\nseemed to give reasonable topics, but the reconstruction accuracy shows that it\ncould have easier had more topics.\n![reconstruction error](./final_images/reconstruction_error.png)\n\n##### Topic analysis\nThe resulting topics can be looked at by running helpers.py and giving the topic\nnumber for example topic 5 gives the picture below.\n![latent topic 5](./final_images/words_5.png)\n\n\nSome of the posts that made it throuhg topic filtering were obviously not within\nthe realm of fitness though. For example one was very about german.\n![german latent features](./final_images/german_wc.png)\n\n\nlooking at the number of posts that fall into topic shows that most did fit into\nthe fitness topic\n![documents in each topic](./final_images/docs_by_topics.png)\n\n<br>\n##### Machine Learning: posts\n---\nLots of different models to try and predict the success of posts were done.\nframe work to gridsearch to gridsearch over and then save the best model is\nstored in grid_search_and_save.py. The best models were always the ensemble\nmodels. To deal with the 'skewed' success metric (i.e. 50% of posts have zero\nlikes and zero comments) the continuous variable was turned into a 4 categories.\nThe best model was a random forest, but the adaboost and gradient boost really\nwere very similar. The best one has a confusion matrix below.\n![Random Forest Confusion Matrix](./final_images/RFCConfMatr.png)\nThis is obviously far from perfect but it does identify the great traction class\nfairly well. So some things can be learned from this. Individually the features\ncan be examined to see there individual effects, not everything is important\nthough. Looking at the built in feature importance shows this.\n![Feature Importance](./final_images/post_feat_imp.png)\nSo we can ignore most of these, but lets look at the contribution of some more\ninteresting ones.\n\n![sklearn built in](./final_images/all_partials.png)\nThis is pretty nice to see trends, but It would be nice to see a few of these in\nterms of the actually probability of being in a particular class, specifically\nthe probability of being in the most successful class. This is accomplished with\na more custom script. Lets look at the effect of changing the number of tags\nsince it is the most interesting.\n\n![Effect of Number of Words](./final_images/pred_num_tags.png)\nJust by judicious use of the number of tags a post can have a 10% higher change\nof making it into the most successful class of posts. Tags are blogger\ndesignated keywords that identify the topic that a post belongs in. These are\nthen used in the internal topic listings for people to read posts in a certain\ntopic. Wordpress allows up to 20 but says if you use 'too many' then the post\nwill be listed less. This is to avoid users trying to abuse the topic listings\nsince a post can't really be in every topic. So it seems that 'too many' is\nabout 10. This was a nice point in the project, since the effect of this\nfeature is explainable. This shows that the model is most likely picking up on\nreal effects rather than trying to interpret noise.\n\n###### Summary of stuff a blogger can do to be more successful in posts\n* Always use 10 tags\n* Write longer posts (>1000 words) (might not generalize to other topics)\n* Add more images\n* Add more links to other sites\n* Use max number of categories\n* Post whenever (no noticable difference on weekends vs weekdays)\n\n<br>\n#### Machine Learning: Blogs\n---\nIdentifying successful blogs is more straight forward, just try to create a\nmodel to predict number of subscribers. After investigating a number of options\nthe best one turned out to be gradient boosting regressor, although like posts\nthe ensemble models on a whole did about the same, and did far better than other\nthings such as linear regression. Before fitting models outliers were tossed out\nsince the median was about 25 but the max was up at 25,000 subscribers. The\noveral accuracy of the model was not great, about .55 r2, and rmse of 14.\nVisually this looks quite bad.\n![Predicted vs. Actual](./final_images/pred_act_scatter.png)\nWell this is pretty bad, but lets see what can be learned anyways. First lets\nlook at feature importance.\n![feature importance for blogs](./final_images/blog_feature_importance1.png)\nMost of these make sense, if you have access to the number of likes for posts\nand number of comments for posts then making a guess at the number of\nsubscribers doesn't seem like a far stretch. Again lets look at individual\ncontributions.\n![some pdps](./final_images/pdp_selected_blogs.png)\nmost of these seem pretty sensible, or might just be noise. Only thing\nI found interesting was the effect of the gap between posts (but it is a minor\neffect). Shown on a more interpretable scale this is seen below.\n![effect of post gap](./final_images/pred_ave_gap.png)\nSo maybe putting more thought into a post a posting less than everyday is\nbeneficial, hard to tell, might just be some noise. Another worth while note is\nthat no topic really did any better for blogs once they were assigned a main\ntopic.\n\n\n##### Thanks For Reading\nIf you made it this far then thanks for reading, feel free to look at the code\nor anything else in the repo\n\nRR\n""","Analysis to identify features that lead to highly liked and commented posts, as well as highly subscribed blogs"
https://github.com/mjbrodzik/ExploringCETB,b'# ExploringCETB\nExamples for reading and understanding Calibrated Enhanced-Resolution Brightness Temperature (CETB) files\n',Examples for reading and understanding Calibrated Enhanced-Resolution Brightness Temperature (CETB) files
https://github.com/MohamedDabo/Pokemon-Data-Exploration,"b'# Pokemon-Catch-Em-All\nIn this project, I explored the world of Pokemon. Having grown up with the series, I thought it would be a good idea to analyze this dataset found on Kaggle.\n\nFirst I took a look at the different types of pokemon, then I digged a little deeper into battle performances by answering the following questions:\n\n    1. Who has the highest Combat Power (CP)?\n    2. Who has the lowest Combat Power (CP)?\n    3. Who has the highest Hit Points (HP)?\n    4. Who has the lowest Hit Points (HP)?\n    5. Who is the strongest Pokemon?\n'",Exploring the world of Pokemons!
https://github.com/shannonxfiles/scrape-fed-salaries,b'# scrape-fed-salaries\nscrape publicly available federal employee salaries into a postgres database\n',scrape publicly available federal employee salaries into a postgres database
https://github.com/lmarti/jupyterday-philly-19.05.2017,b'\n# Jupyter Notebooks in a Computational Intelligence Class\n\n![Python 3.x](https://img.shields.io/badge/python-3.x-green.svg)\n[![nbviwer](https://img.shields.io/badge/view%20in-nbviewer-orange.svg)](http://nbviewer.jupyter.org/github/lmarti/jupyterday-philly-19.05.2017/tree/master/)\n[![Binder](http://mybinder.org/badge.svg)](http://mybinder.org/)\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.495739.svg)](https://doi.org/10.5281/zenodo.495739)\n[![License: CC BY-NC-SA 4.0](https://img.shields.io/badge/license-CC%20BY--NC--SA%204.0-lightgrey.svg)](http://creativecommons.org/licenses/by-nc-sa/4.0/)\n\nSlides of my presentation at [#JupyterDayPhilly](http://jupyterday.blogs.brynmawr.edu).\n',"Slides for my talk ""Jupyter Notebooks in a Computational Intelligence/Machine Learning Class"""
https://github.com/sandiegopython/geekgirl,"b'geekgirl\n========\n\nSan Diego PyLadies hosted a workshop at the Geek Girl Tech Conference on\nSaturday June 21, 2014.\n\n\nIPython Notebook Files\n----------------------\n\nYou can view each of the files online using `nbviewer`_.\n\n* `part-1.ipynb`_: numbers, strings, variables, booleans, ""if statements""\n* `part-2.ipynb`_: lists and loops\n* `part-3.ipynb`_: defining functions and using modules\n\n\nGiving the talk\n---------------\n\nBefore giving the talk, IPython Notebook must be installed:\n\n.. code-block:: bash\n\n    $ pip install \'ipython[notebook]\'\n\nHow to run an IPython Notebook:\n\n.. code-block:: bash\n\n    $ ipython notebook part-1.ipynb\n\nAt the workshop we gave the talk like this:\n\n* We opened a Python shell on the projector monitor and an IPython notebook on the laptop monitor\n* One person read the notebook and typed into the Python shell\n* Another person stood in front of the audience and explained each step\n* Questions were answered ad-hoc, occasionally by Googling answers or typing at the terminal\n\n\nCopying\n-------\n\nYou can give this talk too!\n\nThis workshop is licensed under the GNU General Public License v2.  See LICENSE file for more details.\n\n\n.. _nbviewer: http://nbviewer.ipython.org/\n.. _part-1.ipynb: http://nbviewer.ipython.org/github/pythonsd/geekgirl/blob/master/part-1.ipynb\n.. _part-2.ipynb: http://nbviewer.ipython.org/github/pythonsd/geekgirl/blob/master/part-2.ipynb\n.. _part-3.ipynb: http://nbviewer.ipython.org/github/pythonsd/geekgirl/blob/master/part-3.ipynb\n'",GeekGirl Conference 2014
https://github.com/mehdi2407/CarND-LaneLines-P1,"b'# CarND-LaneLines-P1\nThis repository consists of the first project from the Udacity Self Driving Car Nanodegree. The pipeline of the algorithm and detailed reflection about the project could be viewed from writeup.md.\nAs perquisites to run this project, you need to install OpenCV package. \n\n'",This repo consists of the first project from the udacit self driving car nanodegree
https://github.com/abitofalchemy/DmChallenge,b'Social Topic Analyzer\n=====================\n\nData Mining\nFall 2014 - CSE 40647/60647\n\nProject Team\n------------\n\n* Salvador Aguinaga\n* Aastha Nigam\n\nFolder Contents\n---------------\n\n* Docs- Final project report and PPT Presenteation \n* Data - Datasets used\n* Scripts - Source code\n\n\n\niPython Notebook (Mac OS)\n-------------------------\n* ipython notebook --pylab inline\n\n\nProject Notes\n---------------------\n* Workin on Scripts/Salvador/D_GetTweetsWithUrls.py Data/toy.json\n* Working with Python Twitter and \n* Testing out Twython\n* Working with twython to get all users of wsbt in a loop that waits to collect more info in x amount of time intervals\n\nReferences:\n-----------\n* http://social-metrics.org/python-tutorial-1/\n* http://heuristically.wordpress.com/2011/04/08/text-data-mining-twitter-r/\n* http://stackoverflow.com/questions/19432202/twython-get-followers-list\n* http://stackoverflow.com/questions/11439164/storing-json-dictionaries-from-twitter-streaming-api-using-pymongo\n* http://unsupervisedlearning.wordpress.com/2014/07/06/scraping-your-twitter-homepage-with-python-and-mongodb/\n* http://nbviewer.ipython.org/github/EnricoGiampieri/EnricoGiampieri.github.io/blob/master/_ipython_notebooks/twitter.ipynb\n\t- limits \n* json\n-- https://freepythontips.wordpress.com/2013/08/08/storing-and-loading-data-with-json/\n\n* Visualization\n-- https://www.zotero.org/jwbaker/items/BPHDT5GT?fullsite=0\n\n\n',SCI Challenge Project
https://github.com/feici02/cookbook,b'# cookbook\nMy code snippets.\n',:notebook_with_decorative_cover: My code snippets.
https://github.com/spiningup/udacity-ml-student_intervention,"b'# Project 2: Supervised Learning\n## Building a Student Intervention System\n\n### Install\n\nThis project requires **Python 2.7** and the following Python libraries installed:\n\n- [NumPy](http://www.numpy.org/)\n- [Pandas](http://pandas.pydata.org)\n- [scikit-learn](http://scikit-learn.org/stable/)\n\nYou will also need to have software installed to run and execute an [iPython Notebook](http://ipython.org/notebook.html)\n\nUdacity recommends our students install [Anaconda](https://www.continuum.io/downloads), a pre-packaged Python distribution that contains all of the necessary libraries and software for this project. \n\n### Run\n\nIn a terminal or command window, navigate to the top-level project directory `student_intervention/` (that contains this README) and run one of the following commands:\n\n```ipython notebook student_intervention.ipynb```  \n```jupyter notebook student_intervention.ipynb```\n\nThis will open the iPython Notebook software and project file in your browser.\n\n## Data\n\nThe dataset used in this project is included as `student-data.csv`. This dataset has the following attributes:\n\n- `school` : student\'s school (binary: ""GP"" or ""MS"")\n- `sex` : student\'s sex (binary: ""F"" - female or ""M"" - male)\n- `age` : student\'s age (numeric: from 15 to 22)\n- `address` : student\'s home address type (binary: ""U"" - urban or ""R"" - rural)\n- `famsize` : family size (binary: ""LE3"" - less or equal to 3 or ""GT3"" - greater than 3)\n- `Pstatus` : parent\'s cohabitation status (binary: ""T"" - living together or ""A"" - apart)\n- `Medu` : mother\'s education (numeric: 0 - none,  1 - primary education (4th grade), 2 - 5th to 9th grade, 3 - secondary education or 4 - higher education)\n- `Fedu` : father\'s education (numeric: 0 - none,  1 - primary education (4th grade), 2 - 5th to 9th grade, 3 - secondary education or 4 - higher education)\n- `Mjob` : mother\'s job (nominal: ""teacher"", ""health"" care related, civil ""services"" (e.g. administrative or police), ""at_home"" or ""other"")\n- `Fjob` : father\'s job (nominal: ""teacher"", ""health"" care related, civil ""services"" (e.g. administrative or police), ""at_home"" or ""other"")\n- `reason` : reason to choose this school (nominal: close to ""home"", school ""reputation"", ""course"" preference or ""other"")\n- `guardian` : student\'s guardian (nominal: ""mother"", ""father"" or ""other"")\n- `traveltime` : home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)\n- `studytime` : weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)\n- `failures` : number of past class failures (numeric: n if 1<=n<3, else 4)\n- `schoolsup` : extra educational support (binary: yes or no)\n- `famsup` : family educational support (binary: yes or no)\n- `paid` : extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)\n- `activities` : extra-curricular activities (binary: yes or no)\n- `nursery` : attended nursery school (binary: yes or no)\n- `higher` : wants to take higher education (binary: yes or no)\n- `internet` : Internet access at home (binary: yes or no)\n- `romantic` : with a romantic relationship (binary: yes or no)\n- `famrel` : quality of family relationships (numeric: from 1 - very bad to 5 - excellent)\n- `freetime` : free time after school (numeric: from 1 - very low to 5 - very high)\n- `goout` : going out with friends (numeric: from 1 - very low to 5 - very high)\n- `Dalc` : workday alcohol consumption (numeric: from 1 - very low to 5 - very high)\n- `Walc` : weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)\n- `health` : current health status (numeric: from 1 - very bad to 5 - very good)\n- `absences` : number of school absences (numeric: from 0 to 93)\n- `passed` : did the student pass the final exam (binary: yes or no)\n'",Build a student intervention system by predicting whether a student will pass or fail an exam
https://github.com/deercoder/0-PhD,"b'# PhD\nResearch notes, codes, misc for my PhD study\n'","Research notes, codes, deep learning"
https://github.com/sanjeevbadgeville/Spark2-H2O-R-Zeppelin,"b'# Spark2-H2O-R-Zeppelin\nA stack for data mining using Spark2, H2O, R and Zeppelin running on Cloudera Hadoop Distribution\n\n# Spark2 Setup\n\n## Hadoop Version (tested with CDH5.8)\n$hadoop version\n  Hadoop 2.6.0-cdh5.11.0\n\n## Download Spark \nhttp://spark.apache.org/downloads.html\nDownload Spark: spark-2.2.0-bin-hadoop2.6.tgz\nwget http://apache.mirrors.ionfish.org/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.6.tgz\n\n## Extract Spark2 downloaded file\n- sudo mkdir /opt/spark\n- sudo chown -R cloudera:cloudera /opt/spark\n- cp /mnt/working/spark-2.2.0-bin-hadoop2.6.tgz /opt/spark\n- tar xvzf /opt/spark/spark-2.2.0-bin-hadoop2.6.tgz\n-  ln -s /opt/spark/spark-2.2.0-bin-hadoop2.6.tgz /opt/spark/current\n\n## Update conf/spark-env.sh\n- SPARK_HOME=/opt/spark/current\n- HADOOP_CONF_DIR=/etc/hive/conf\n\n## Update conf/spark-defaults.conf\n- spark.master                       yarn\n- spark.yarn.jars                    hdfs://localhost:8020/user/cloudera/spark-2.2.0-bin-hadoop2.6/*\n\n## Create HDFS folder /user/cloudera (if not present)\n- sudo -u hdfs hdfs dfs -mkdir /user/cloudera\n- sudo -u hdfs hdfs dfs -chown -R cloudera /user/cloudera\n- hdfs dfs -mkdir spark-2.2.0-bin-hadoop2.6\n- hdfs dfs -copyFromLocal jars/*  spark-2.2.0-bin-hadoop2.6\n\n## Test Spark2 Installation\n- $ ./bin/run-example SparkPi 10 --master yarn\n- $ ./bin/spark-shell --master yarn\n- $ ./bin/pyspark\nYarn cluster mode\n- $ ./bin/spark-submit --class org.apache.spark.examples.SparkPi     --master yarn     --deploy-mode cluster     --driver-memory 4g     --executor-memory 2g     --executor-cores 1     --queue thequeue     examples/jars/spark-examples*.jar     10\nYarn client mode\n- ./bin/spark-submit --class org.apache.spark.examples.SparkPi     --master yarn     --deploy-mode client     --driver-memory 4g     --executor-memory 2g     --executor-cores 1     --queue thequeue     examples/jars/spark-examples*.jar     10\n\n# H2O Sparkling-Water Setup\n\n## Download Sparkling-Water\nhttp://h2o-release.s3.amazonaws.com/sparkling-water/rel-2.0/0/sparkling-water-2.0.0.zip\n\n## Update sparkling-env.sh\n- SPARK_HOME=/opt/spark/current\n- MASTER=yarn\n\n## Copy Sparkling-Water Fat jar\n- cp /opt/sparkling-water/current/assembly/build/libs/sparkling-water-assembly_2.11-2.0.0-all.jar  /opt/spark/current/jars\n\n## Test Sparkling-Water Installation\n- /opt/spark/current/bin/spark-submit --master=yarn --class water.SparklingWaterDriver --conf ""spark.yarn.am.extraJavaOptions=-XX:MaxPermSize=384m -Dhdp.version=current""  --driver-memory=8G --num-executors=3 --executor-memory=3G --conf ""spark.executor.extraClassPath=-XX:MaxPermSize=384m -Dhdp.version=current""  /opt/spark/current/jars/sparkling-water-assembly_2.11-2.2.2-all.jar\n\n# Install R\n\n$ sudo yum install R\n$ sudo yum install libxml2-devel\n$ sudo yum install libcurl-devel\n\n- install.packages(\'knitr\', repos=""http://cran.rstudio.com/"",dependencies = TRUE)\n- install.packages(\'data.table\', repos=""http://cran.rstudio.com/"",dependencies = TRUE)\n- install.packages(\'curl\', repos=""http://cran.rstudio.com/"",dependencies=TRUE)\n- install.packages(""httr"", repos=""http://cran.rstudio.com/"", dependencies=TRUE)\n- install.packages(""plotly"", repos=""http://cran.rstudio.com/"", dependencies=TRUE)\n- install.packages(""devtools"", repos=""http://cran.rstudio.com/"", dependencies=TRUE)\n- devtools::install_github(""ropensci/plotly"")\n- devtools::install_github(\'ramnathv/rCharts\')\n\n# Apache Zeppelin\n\n## Download Zeppelin \nhttps://zeppelin.apache.org/download.html\nzeppelin-0.7.3-bin-all.tgz\nwget http://mirror.reverse.net/pub/apache/zeppelin/zeppelin-0.7.3/zeppelin-0.7.3-bin-all.tgz\n\n## Update /opt/zeppelin/current/conf/zeppelin-env.sh\n- export MASTER=yarn\n- export SPARK_HOME=/opt/spark/current\n- export SPARK_APP_NAME=zeppelin-cdh\n- export HADOOP_CONF_DIR=/etc/hive/conf\n- export SPARK_SUBMIT_OPTIONS=""--jars /opt/spark/current/jars/sparkling-water-assembly_2.11-2.2.2-all.jar""\n\n## Run Zeppelin with Spark2, Sparkling-water, and R\n- /opt/zeppelin/current/bin/zeppelin.sh -Pspark-2.2\n\n## Test Zeppelin Installation\nhttp://localhost:8080/#/\n\n- %spark\n- import org.apache.spark.sql._\n- val sqlContext = new SQLContext(sc)\n- import sqlContext.implicits._\n- import org.apache.spark.h2o._\n- val h2oContext = H2OContext.getOrCreate(sc) \n- import h2oContext._ \n\n- val df: DataFrame = sc.parallelize(1 to 1000, 100).map(v => IntHolder(Some(v))).toDF\n- val hf = h2oContext.asH2OFrame(df)\n- val newRdd = h2oContext.asDataFrame(hf)(sqlContext)\n\n# Oracle Access\n\n## Use the ojdbc7.jar in the lib folder as it has the file defaultConnectionProperties.properties file updated with oracle.jdbc.timezoneAsRegion=false\n\n## Use the Postgres Interpreter\n- postgresql.driver.name\toracle.jdbc.driver.OracleDriver\n- postgresql.max.result\t1000\n- postgresql.password\t    [PASSWORD]\n- postgresql.url\t        jdbc:oracle:thin:@[HOST_IP]:[HOST_PORT]:[SID]\n- postgresql.user\t        [USERNAME]\n\nDependencies\n/opt/zeppelin/current/lib/ojdbc7.jar\n\n## Add Oracle jdbc driver to Spark Interpreter\n\nDependencies\n/opt/zeppelin/current/lib/ojdbc7.jar\n\n## Test Zeppelin to access Oracle using %psql\n- %psql\n- SELECT * FROM DUAL\n\n## Test Zeppelin to access Oracle using %spark\n- %spark\n- sc.version\n-- val pdf = sqlContext.load(""jdbc"", Map(""url"" -> ""jdbc:oracle:thin:[USERNAME]/[PASSWORD]@[HOST_IP]:[HOST_PORT]:[SID]"", ""driver"" -> ""oracle.jdbc.driver.OracleDriver"", ""dbtable"" -> ""dual"") )\n- pdf.printSchema()\n- pdf.registerTempTable(""pdf"")\n\n- %sql SELECT count(*) FROM pdf\n\n# Vertica\n\n# Use the vertica-jdbc-8.0.0-1.jar in the lib folder\n\n## Use JDBC Interpreter\n- default.driver\tcom.vertica.jdbc.Driver\n- default.password\t[PASSWD]\n- default.url\tjdbc:vertica://[HOST]:[HOST_PORT]/[DB]?user=[USERNAME]&password=[PASSWD]\n- default.user\t[USERNAME]\n\nDependencies\n/opt/zeppelin/current/lib/vertica-jdbc-8.0.0-1.jar\n\n## Test Zeppelin to access Vertica using %jdbc\n- %jdbc\n- SELECT count(*) FROM [SCHEMA].[TABLE]\n\n## Test Zeppelin to access Vertica using %spark\n- %spark\n- sc.version \n- val pdfv = sqlContext.load(""jdbc"", Map(""url"" -> ""jdbc:vertica://[HOST]:[PORT]/[DB]?user=[USERNAME]&password=[PASSWD]"", ""driver"" -> ""com.vertica.jdbc.Driver"", ""dbtable"" -> ""[SCHEMA].[TABLE]"", ""fetchsize"" -> ""100"") )\n- pdfv.printSchema()\n- pdfv.registerTempTable(""pdfv"")\n\n- %sql SELECT * FROM pdfv\n\n## Impala\n/opt/zeppelin/current/lib/ojdbc7.jar\t\n/tmp/toSratch/2.5.36.1056/ImpalaJDBC41.jar\t\n/opt/zeppelin/current/lib/vertica-jdbc-8.0.0-1.jar\t\n/tmp/toSratch/2.5.36.1056/commons-logging-1.1.1.jar\t\n/tmp/toSratch/2.5.36.1056/hive_metastore.jar\t\n/tmp/toSratch/2.5.36.1056/hive_service.jar\t\n/tmp/toSratch/2.5.36.1056/httpclient-4.1.3.jar\t\n/tmp/toSratch/2.5.36.1056/httpcore-4.1.3.jar\t\n/tmp/toSratch/2.5.36.1056/libfb303-0.9.0.jar\t\n/tmp/toSratch/2.5.36.1056/libthrift-0.9.0.jar\t\n/tmp/toSratch/2.5.36.1056/log4j-1.2.14.jar\t\n/tmp/toSratch/2.5.36.1056/ql.jar\t\n/tmp/toSratch/2.5.36.1056/slf4j-api-1.5.11.jar\t\n/tmp/toSratch/2.5.36.1056/slf4j-log4j12-1.5.11.jar\t\n/tmp/toSratch/2.5.36.1056/TCLIServiceClient.jar\t\n/tmp/toSratch/2.5.36.1056/zookeeper-3.4.6.jar\n\n# References\nhttps://www.linkedin.com/pulse/running-spark-2xx-cloudera-hadoop-distro-cdh-deenar-toraskar-cfa\nhttps://github.com/h2oai/sparkling-water/blob/master/DEVEL.md#SparklingWaterZeppelin\nhttp://www.cloudera.com/documentation/enterprise/5-8-x/topics/cdh_ig_running_spark_on_yarn.html\n- find IPAddress: docker inspect [container_id] | grep IPAddress\n- sudo iptables -t nat -A DOCKER -p tcp --dport 8080 -j DNAT --to-destination [container_ip]:8080\n- sudo iptables -t nat -A POSTROUTING -s [container_ip] -j MASQUERADE -p tcp --dport 8080 -d [container_ip]\nRedhat 6.5 if yum install R fails\n""wget http://mirror.centos.org/centos/6/os/x86_64/Packages/lapack-devel-3.2.1-4.el6.x86_64.rpm\nwget http://mirror.centos.org/centos/6/os/x86_64/Packages/blas-devel-3.2.1-4.el6.x86_64.rpm\nwget http://mirror.centos.org/centos/6/os/x86_64/Packages/texinfo-tex-4.13a-8.el6.x86_64.rpm\nwget http://vault.centos.org/6.2/updates/x86_64/Packages/libicu-devel-4.2.1-9.1.el6_2.x86_64.rpm\nsudo yum localinstall *.rpm\n\n\n'","A stack for data mining using Spark2, H2O, R and Zeppelin running on Cloudera Hadoop Distribution"
https://github.com/data-henrik/idug2016_interactive_db2_reports,"b'# IDUG EMEA 2016 Conference, Brussels: Interactive DB2 Reports and Presentations\nConference Website: [IDUG EMEA 2016](http://www.idug.org/p/cm/ld/fid=862)\n\n### How to have fun with DB2 / dashDB and Jupyter Notebooks - interactive presentation and reports\nMy IDUG session E08 is about to following topics\n   * Getting Started with Jupyter Notebooks\n   * Interfaces to DB2 / dashDB\n   * Presentations with RISE\n   * The FUN I had and more...\n\nThe draft for my IDUG EMEA presentation is in the file [20161115_IDUG_InteractiveReports.ipynb](https://github.com/data-henrik/idug2016_interactive_db2_reports/blob/master/20161115_IDUG_InteractiveReports.ipynb)\n\n![](https://raw.githubusercontent.com/data-henrik/idug2016_interactive_db2_reports/master/201611idug_notebook.png)\n\n### Stuff I used\nFor my Jupyter Notebook-based presentation I made use of several products, libraries and technologies. Here is a list of those:\n* DB2 and dashDB\n* Jupyter Notebook: http://jupyter.org/\n* SQL Magic / ipython-sql: https://github.com/catherinedevlin/ipython-sql\n* RISE - Reveal.js Jupyter/IPython Slideshow Extension: https://github.com/damianavila/RISE\n* ibmdbpy: http://pythonhosted.org/ibmdbpy/\n* ibmdby sample notebooks: https://github.com/ibmdbanalytics/ibmdbpy-notebooks\n* Python interface to DB2 - ibm_db: https://github.com/ibmdb/python-ibmdb\n* Bokeh Visualization Library: http://bokeh.pydata.org/en/latest/\n* ...\n\nI started my journey with Jupyter Notebooks using the [Apache Spark service on Bluemix](https://console.ng.bluemix.net/catalog/services/apache-spark/) which includes notebooks.\n\n### Links:\nIn the past I already used and wrote about notebooks and DB2, here are some related links as well as to samples you might find useful:\n* CeBIT Weather: https://github.com/data-henrik/CeBIT-Weather and related blog at http://blog.4loeser.net/2016/03/coincidence-cebit-visitors-and-weather.html\n* Notes on Notebooks: http://blog.4loeser.net/2016/08/notes-on-notebooks-data-db2-and-bluemix.html\n* Sample notebooks provided for Apache Spark service on Bluemix: https://developer.ibm.com/clouddataservices/docs/spark/sample-notebooks/\n* Simple graphing with pandas: http://pbpython.com/simple-graphing-pandas.html\n'",How to have fun with DB2 / dashDB and Jupyter Notebooks - interactive presentation and reports
https://github.com/Sapphirine/twitter_based_eod_stock_market_price_predictor,"b'E6893 Big Data Analytics  \nColumbia University  \n12/22/2016  \nNan Zhao, Ben Zhu, Sabina Smajlaj  \nProfessor Chin-Yung Lin  \n\n#    EoD Price Predictor\n\n1. Data Source: \n      1. Twitter Dumps\n\n            Full Archive: https://archive.org/details/twitterstream\n\n            July, 2016: https://archive.org/details/archiveteam-twitter-stream-2016-07  \n\n            June, 2016: https://archive.org/details/archiveteam-twitter-stream-2016-06 \n\n            Auguest, 2016: https://archive.org/details/archiveteam-twitter-stream-2016-05\n\n      2. Stock Price\n\n            Quandl API: https://www.quandl.com/docs/api\n\n\n2. Tools Used: \n\n      Python, Sk-learn, Hadoop, Pandas, Numpy, JSON, Regular Expression\n\n3. User guide:\n\n      1. Data Fetching\n         get_rel_entries_bash and unzip_dir_bash are responsible for unzipping all data from different directories as well as sub directories. Each archive file is in form of Month-Day-Hour for subdirectories within the file. User shall run unzip_dir_bash and get_rel_entries_bash under Month folder to get 8 different csv for different company mentions.\n\n      2. Data Parsing, Cleaning and ML model\n        pivot_data.py, Quandl_data_parse.ipynb, Join_data_and_ML_model.ipynb\n        These files are responsible for cleaning, arregating data from multiple csv files and generate ML models for servers to provide predictions.\n\n      3. Web Hosting\n        All source code is within web_server folder. LINK to web app: http://e6893stockmarketpredictor.herokuapp.com \n        Web is written in Python with Flask infrastructure. \n  \n  \n'",Twitter Based EOD Stock Market Price Predictor
https://github.com/xpgeng/du4dama,b'# du4dama\na script of scratching comments\n',a script of scratching comments
https://github.com/jay-mahadeokar/deeplab-public-ver2,"b'## DeepLab v2\n\n### Introduction\n\nDeepLab is a state-of-art deep learning system for semantic image segmentation built on top of [Caffe](http://caffe.berkeleyvision.org).\n\nIt combines (1) *atrous convolution* to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks, (2) *atrous spatial pyramid pooling* to robustly segment objects at multiple scales with filters at multiple sampling rates and effective fields-of-views, and (3) densely connected conditional random fields (CRF) as post processing.\n\nThis distribution provides a publicly available implementation for the key model ingredients reported in our latest [arXiv paper](http://arxiv.org/abs/1606.00915).\nThis version also supports the experiments (DeepLab v1) in our ICLR\'15. You only need to modify the old prototxt files. For example, our proposed atrous convolution is called dilated convolution in CAFFE framework, and you need to change the convolution parameter ""hole"" to ""dilation"" (the usage is exactly the same). For the experiments in ICCV\'15, there are some differences between our argmax and softmax_loss layers and Caffe\'s. Please refer to [DeepLabv1](https://bitbucket.org/deeplab/deeplab-public/) for details.\n\nPlease consult and consider citing the following papers:\n\n    @article{CP2016Deeplab,\n      title={DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs},\n      author={Liang-Chieh Chen and George Papandreou and Iasonas Kokkinos and Kevin Murphy and Alan L Yuille},\n      journal={arXiv:1606.00915},\n      year={2016}\n    }\n\n    @inproceedings{CY2016Attention,\n      title={Attention to Scale: Scale-aware Semantic Image Segmentation},\n      author={Liang-Chieh Chen and Yi Yang and Jiang Wang and Wei Xu and Alan L Yuille},\n      booktitle={CVPR},\n      year={2016}\n    }\n\n    @inproceedings{CB2016Semantic,\n      title={Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs and a Discriminatively Trained Domain Transform},\n      author={Liang-Chieh Chen and Jonathan T Barron and George Papandreou and Kevin Murphy and Alan L Yuille},\n      booktitle={CVPR},\n      year={2016}\n    }\n\n    @inproceedings{PC2015Weak,\n      title={Weakly- and Semi-Supervised Learning of a DCNN for Semantic Image Segmentation},\n      author={George Papandreou and Liang-Chieh Chen and Kevin Murphy and Alan L Yuille},\n      booktitle={ICCV},\n      year={2015}\n    }\n\n    @inproceedings{CP2015Semantic,\n      title={Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs},\n      author={Liang-Chieh Chen and George Papandreou and Iasonas Kokkinos and Kevin Murphy and Alan L Yuille},\n      booktitle={ICLR},\n      year={2015}\n    }\n\n\nNote that if you use the densecrf implementation, please consult and cite the following paper:\n\n    @inproceedings{KrahenbuhlK11,\n      title={Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials},\n      author={Philipp Kr{\\""{a}}henb{\\""{u}}hl and Vladlen Koltun},\n      booktitle={NIPS},\n      year={2011}\n    }\n\n### Performance\n\n*DeepLabv2* currently achieves **79.7%** on the challenging PASCAL VOC 2012 semantic image segmentation task -- see the [leaderboard](http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=11&compid=6). \n\nPlease refer to our project [website](http://liangchiehchen.com/projects/DeepLab.html) for details.\n\n### Pre-trained models\n\nWe have released several trained models and corresponding prototxt files at [here](http://liangchiehchen.com/projects/DeepLab_Models.html). Please check it for more model details.\n\n### Experimental set-up\n\n1. The scripts we used for our experiments can be downloaded from this [link](https://ucla.box.com/s/4grlj8yoodv95936uybukjh5m0tdzvrf):\n    1. run_pascal.sh: the script for training/testing on the PASCAL VOC 2012 dataset. __Note__ You also need to download sub.sed script.\n    2. run_densecrf.sh and run_densecrf_grid_search.sh: the scripts we used for post-processing the DCNN computed results by DenseCRF.\n2. The image list files used in our experiments can be downloaded from this [link](https://ucla.box.com/s/rd9z2xvwsfpksi7mi08i2xqrj7ab4keb):\n    * The zip file stores the list files for the PASCAL VOC 2012 dataset.\n3. To use the mat_read_layer and mat_write_layer, please download and install [matio](http://sourceforge.net/projects/matio/files/matio/1.5.2/).\n\n### FAQ\n\nCheck [FAQ](http://liangchiehchen.com/projects/DeepLab_FAQ.html) if you have some problems while using the code.\n\n### How to run DeepLab\n\nThere are several variants of DeepLab. To begin with, we suggest DeepLab-LargeFOV, which has good performance and faster training time.\n\nSuppose the codes are located at deeplab/code\n\n1. mkdir deeplab/exper (Create a folder for experiments)\n2. mkdir deeplab/exper/voc12 (Create a folder for your specific experiment. Let\'s take PASCAL VOC 2012 for example.)\n3. Create folders for config files and so on.\n    1. mkdir deeplab/exper/voc12/config  (where network config files are saved.)\n    2. mkdir deeplab/exper/voc12/features  (where the computed features will be saved (when train on train))\n    3. mkdir deeplab/exper/voc12/features2 (where the computed features will be saved (when train on trainval))\n    4. mkdir deeplab/exper/voc12/list (where you save the train, val, and test file lists)\n    5. mkdir deeplab/exper/voc12/log (where the training/test logs will be saved)\n    6. mkdir deeplab/exper/voc12/model (where the trained models will be saved)\n    7. mkdir deeplab/exper/voc12/res (where the evaluation results will be saved)\n4. mkdir deeplab/exper/voc12/config/deeplab_largeFOV (test your own network. Create a folder under config. For example, deeplab_largeFOV is the network you want to experiment with. Add your train.prototxt and test.prototxt in that folder (you can check some provided examples for reference).)\n5. Set up your init.caffemodel at deeplab/exper/voc12/model/deeplab_largeFOV. You may want to soft link init.caffemodel to the modified VGG-16 net. For example, run ""ln -s vgg16.caffemodel init.caffemodel"" at voc12/model/deeplab_largeFOV.\n6. Modify the provided script, run_pascal.sh, for experiments. You should change the paths according to your setting. For example, you should specify where the caffe is by changing CAFFE_DIR. Note You may need to modify sub.sed, if you want to replace some variables with your desired values in train.prototxt or test.prototxt.\n7. The computed features are saved at folders features or features2, and you can run provided MATLAB scripts to evaluate the results (e.g., check the script at code/matlab/my_script/EvalSegResults).\n\n### Python\n\nSeyed Ali Mousavi has implemented a python version of run_pascal.sh (Thanks, Ali!). If you are more familiar with Python, you may want to take a look at [this](https://github.com/TheLegendAli/CCVL).'",This repo was mirrored from here: https://bitbucket.org/aquariusjay/deeplab-public-ver2
https://github.com/mparker2/seaborn_sinaplot,b'# Sinaplot for `matplotlib`/`seaborn`\nA python `matplotlib` implementation of the sinaplot\nA kind of hybrid between `sns.violinplot` and `sns.stripplot`.\nidea is from [Sinaplot R package](https://cran.r-project.org/web/packages/sinaplot/vignettes/SinaPlot.html)\n',A python implementation of the sinaplot using matplotlib and seaborn
https://github.com/gundamkeroro/DLND,b'# DLND\nThis is all my project of Udacity Deep Learning Nanodegree starts in May 25\n\n\n##Projects:\n* *Project 01:* [first-neural-network](https://github.com/gundamkeroro/DLND/tree/master/first-neural-network)\n* *Project 02:* [image-classification](https://github.com/gundamkeroro/DLND/tree/master/image-classification)\n* *Project 03:* [tv-script-generation](https://github.com/gundamkeroro/DLND/tree/master/tv-script-generation)\n* *Project 04:* [language-translation](https://github.com/gundamkeroro/DLND/tree/master/language-translation)\n* *Project 05:* [face_generation](https://github.com/gundamkeroro/DLND/tree/master/face_generation)',This is all my project of Udacity Deep Learning Nanodegree starts in May 25
https://github.com/ProfessorBrunner/rp-pds15,"b'# Practical Data Science\n\nUniversity of Illinois, Research Park    \nInstructor: Robert J. Brunner    \nSpring 2015  \n\n-----\n\nThe [Course Index](http://nbviewer.ipython.org/github/ProfessorBrunner/rp-pds15/blob/master/index.ipynb) page provides links to each week\'s lessons.\n\n-----\n\n<font color = ""red"">\nNote: This is a __draft__ version that will be revised as we progress through the course.\n</font>\n\n### Week 1: Introduction to Practical Data Science \n\nReview the course schedule and learning goals before posting a welcome\nmessage on the course Piazza. Next, learn about virtualization and the\nDocker engine and Docker container concept. In a breakout session,\ninstall the [Docker Engine](Week0/Docker.md) and the course Docker image\nbuilt for this _Practical Data Science_ course. Next, learn about source\ncode version control, and how to accomplish this by using the git tool.\nIn a breakout session, learn to work with git at the command line, and\nnavigate the github site. Review basic Unix concepts and gain experience\nworking at the Unix command prompt.\n\n### Week 2: Practical Command Line Data Science:  \n\nLearn about Unix, the Unix shell, and the Unix process model and\nfilesystem. Use the Docker technology by working at the Unix command\nprompt within the course Docker container in interactive mode. This will\nfocus on using Unix command line tools and techniques to work with data\nin the BASH shell\n\n### Week 3: Introduction to Python programming and the IPython Notebook\n\nLearn how to use the IPython notebook by using the course Docker\ncontainer in server mode. Also learn basic Python programming, python\ndata types, and file I/O, before finishing with a quick overview of the\nnumpy and scipy libraries.\n\n### Week 4: Exploring Data Through Visualizations:  \n \nLearn how to make data visualization by using Python, primarily from\nwithin the IPython notebook by using matplotlib and seaborn. This will\ninclude a discussion of scatter plots, linear regression and plotting,\nhistograms, box plots, and other advanced visualization concepts.\n\n<!-- Note: I have had to move this elsewhere, perhaps into Week 6\n\nUsing Python DataFrames (Pandas):  \n\nLearn about the _Data Frame_ concept and how to use it within Python by\nusing the Pandas library. This will include ways to load and work with\nlarge tabular data, and to performa basic data operations like cleaning,\ntransforming, merging, and reshaping.\n-->\n\n### Week 5: Using Databases:  \n\nLearn about database technology, before specifically focusing on\nrelational database management systems. This will include learning how\nto create database, and SQL DDL and DML to create, insert, update and\ndelete data. This will conclude with a discussion of accessing a\ndatabase from Python.\n\n### Week 6: Data Acquisition:  \n\nLearn about acquiring data from diverse sources including webpages,\nonline repositories, and social media. This will require a discussion of\nweb scraping, DOM tress, and JSON.\n\n### Week 7: Statistical & Machine Learning:  \n \nReview basic statistics and probability and learn how to compute\ndifferent random distributions  by using numpy and scipy routines. Next,\nlearn about machine learning and basic approaches to perform machine\nlearning by using the scikit_learn library in Python.\n\n### Week 8: Data Intensive Computing:  \n\nLearn about basic concepts in high performance computing and how to\nperform them in Python. Next, learn about cloud computing, including how\nDocker technology integrates into commercial clouds. Finally, a\ndiscussion of the standard Hadoop platform and its capabilities.\n\n-----\n\nThe _Practical Data Science_ course [License](LICENSE.md)\n\n-----\n'","Practical Data Science course notes offered at the University of Illinois, Research Park in Spring 2015"
https://github.com/rdemaria/pytimber,b'# Project now maintained in\n\nhttps://gitlab.cern.ch/scripting-tools/pytimber.git\n',Python Wrapping of CALS API
https://github.com/luizgh/intro_to_cnns,"b'# Introdu\xc3\xa7\xc3\xa3o \xc3\xa0 redes neurais convolucionais\n\nEsse reposit\xc3\xb3rio cont\xc3\xa9m um tutorial de redes neurais convolucionais usando as bibliotecas Theano e Lasagne. O foco \xc3\xa9 na implementa\xc3\xa7\xc3\xa3o de redes neurais usando essas bibliotecas.\n\nO tutorial \xc3\xa9 dividido em tr\xc3\xaas partes:\n* Introdu\xc3\xa7\xc3\xa3o \xc3\xa0 aprendizagem de m\xc3\xa1quina e ao Theano\n* Redes neurais convolucionais (CNNs)\n* Transfer Learning usando CNNs\n\nOs slides podem se consultados na pasta /slides. Exerc\xc3\xadcios para implementar os modelos s\xc3\xa3o propostos em Python, usando Ipython [notebooks](http://jupyter.org/)\n\n## Configura\xc3\xa7\xc3\xa3o necess\xc3\xa1ria\n\nOs exemplos foram feitos para serem executados em CPU, e requerem os seguintes programas/bibliotecas:\n* Python 2 (recomendo a distribui\xc3\xa7\xc3\xa3o anaconda: https://www.continuum.io/downloads)\n* Pacotes de python:\n   * scipy, jupyter, notebook, PIL, matplotlib (podem ser instalados usando ""conda"" (se estiver usando a distribui\xc3\xa7\xc3\xa3o anaconda) ou ""pip""\n   * Theano e Lasagne (ler instru\xc3\xa7\xc3\xb5es de instala\xc3\xa7\xc3\xa3o nesse link: https://lasagne.readthedocs.io/en/latest/user/installation.html)\n   \n'",Introdução à redes neurais convolucionais usando Theano + Lasagne
https://github.com/chusiang/ansible-jupyter.dockerfile,"b'# Docker image: Ansible on Jupyter Notebook\n\n[![Docker Hub](https://img.shields.io/badge/docker-ansible--jupyter-blue.svg)](https://hub.docker.com/r/chusiang/ansible-jupyter/) [![microbadger](https://images.microbadger.com/badges/image/chusiang/ansible-jupyter.svg)](https://microbadger.com/images/chusiang/ansible-jupyter ""Get your own image badge on microbadger.com"")\n\nA Docker image for run [Ansible][ansible_official] 2.x on [Jupyter Notebook][jupyter_official] 4.x (ipython notebook) with Browsers.\n\n[ansible_official]: https://www.ansible.com/\n[jupyter_official]: http://jupyter.org/\n\n## Supported tags and respective `Dockerfile` links\n\n- `alpine-3`, `latest` [*(alpine-3/Dockerfile)*][dockerfile_alpine-3]\n- `archlinux` [*(archlinux/Dockerfile)*][dockerfile_archlinux]\n- `centos-7` [*(centos-7/Dockerfile)*][dockerfile_centos-7]\n- `debian-9` [*(debian-9/Dockerfile)*][dockerfile_debian-9]\n- ~~`gentoo`~~ [*(gentoo/Dockerfile)*][dockerfile_gentoo]\n- `opensuse-42.3` [*(opensuse-42.3/Dockerfile)*][dockerfile_opensuse-42.3]\n- `ubuntu-18.04` [*(ubuntu-18.04/Dockerfile)*][dockerfile_ubuntu-18.04]\n\n[dockerfile_alpine-3]:      https://github.com/chusiang/ansible-jupyter.dockerfile/blob/master/alpine-3/Dockerfile\n[dockerfile_archlinux]:     https://github.com/chusiang/ansible-jupyter.dockerfile/blob/master/archlinux/Dockerfile\n[dockerfile_centos-7]:      https://github.com/chusiang/ansible-jupyter.dockerfile/blob/master/centos-7/Dockerfile\n[dockerfile_debian-9]:      https://github.com/chusiang/ansible-jupyter.dockerfile/blob/master/debian-9/Dockerfile\n[dockerfile_gentoo]:        https://github.com/chusiang/ansible-jupyter.dockerfile/blob/master/gentoo/Dockerfile\n[dockerfile_opensuse-42.3]: https://github.com/chusiang/ansible-jupyter.dockerfile/blob/master/opensuse-42.3/Dockerfile\n[dockerfile_ubuntu-18.04]:  https://github.com/chusiang/ansible-jupyter.dockerfile/blob/master/ubuntu-18.04/Dockerfile\n\n## Build image\n\n1. Get this project.\n\n    ```\n    $ git clone https://github.com/chusiang/ansible-jupyter.dockerfile.git\n    ```\n\n1. Go to workspace.\n\n    ```\n    $ cd ansible-jupyter.dockerfile/<IMAGE_TAG>/\n    ```\n\n1. Bunild the image.\n\n    ```\n    $ docker build -t chusiang/ansible-jupyter .\n    ```\n\n## Run container\n\n1. Get image.\n\n    ```\n    $ docker pull chusiang/ansible-jupyter\n    ```\n\n1. Run the container with daemon mode.\n\n    ```\n    $ docker run --name ansible-jupyter -P -d chusiang/ansible-jupyter\n    be8a15b9d4da5d24610c1fc738cb13086f01101e90f94640360d8d84892de772\n    ```\n\n1. Check container process.\n\n    ```\n    $ docker ps\n    CONTAINER ID        IMAGE                      COMMAND                  CREATED             STATUS              PORTS                     NAMES\n    be8a15b9d4da        chusiang/ansible-jupyter   ""docker-entrypoint.sh""   12 seconds ago      Up 11 seconds       0.0.0.0:32808->8888/tcp   ansible-jupyter\n    ```\n\n1. Enter container with command line.\n\n    ```\n    $ docker exec -it ansible-jupyter sh\n    / #\n    ```\n\n## Play Ansible on Jupyter\n\nNow, you can play the Ansible on Jupyter.\n\n1. Go jupyter web.\n\n    ```\n    # GNU/Linux\n    $ firefox http://localhost:32786\n\n    # macOS\n    $ open http://localhost:32786\n    ```\n\n    ![2016-11-20-ansible-jupyter1]\n\n1. Attach my example ==> [`ansible_on_jupyter.ipynb`][ansible_on_jupyter.ipynb].\n\n    ![2016-11-20-ansible-jupyter2]\n\n1. Remember use the `!` prefix to trigger system command.\n\nYou can see more detail at [\xe6\x80\x8e\xe9\xba\xbc\xe7\x94\xa8 Jupyter \xe6\x93\x8d\xe6\x8e\xa7 Ansible\xef\xbc\x9f(localhost) | \xe7\x8f\xbe\xe4\xbb\xa3 IT \xe4\xba\xba\xe4\xb8\x80\xe5\xae\x9a\xe8\xa6\x81\xe7\x9f\xa5\xe9\x81\x93\xe7\x9a\x84 Ansible \xe8\x87\xaa\xe5\x8b\x95\xe5\x8c\x96\xe7\xb5\x84\xe6\x85\x8b\xe6\x8a\x80\xe5\xb7\xa7](https://chusiang.gitbooks.io/automate-with-ansible/07.how-to-practive-the-ansible-with-jupyter1.html) .\n\nEnjoy it !\n\n[ansible_on_jupyter.ipynb]: https://github.com/chusiang/ansible-jupyter.dockerfile/blob/master/ipynb/ansible_on_jupyter.ipynb\n[2016-11-20-ansible-jupyter1]: https://cloud.githubusercontent.com/assets/219066/20463322/218f0c4a-af6b-11e6-9a95-2411ec7acb5f.png\n[2016-11-20-ansible-jupyter2]: https://cloud.githubusercontent.com/assets/219066/20463319/fa8c047c-af6a-11e6-96d6-f985096c9c8c.png\n\n## History\n\n### 2020\n\n* 12/12 Fixed Python 3 dependency problem on Alpine Linux v3.12, and stop support some EOL images.\n\n### 2018\n\n* 07/11 Add new images of `alpine-3.8`. Stop automated build image of `alpine-3.4`, `alpine-3.6` and `opensuse-42.2`.\n* 06/18 Add new images of `alpine-3.7`, `ubuntu-18.04`. Stop automated build image of `ubuntu-14.04`.\n* 01/10 Stop automated build images of `centos-6`, `debian-7` and `alpine-3.4_ansible-2.1`.\n\n### 2017\n\n* ??/?? Stop automated build images of `gentoo`, `opensuse-42.1` and `alpine-3.4`.\n\n## License\n\nCopyright (c) chusiang from 2016-2020 under the MIT license.\n'",Building the Docker image with Ansible and Jupyter.
https://github.com/srhrshr/word2vec-presentation,b'# word2vec-presentation\n',"Slidedeck for the presentation at the Big Data Conclave, VIT Chennai"
https://github.com/Syps/boston_data_react,b'# boston_data_react\n[Boston data visualization](https://github.com/Syps/boston_data) redone with React\n***\nhttps://www.nicksypteras.com/data/boston\n\n\n',Boston data visualization redone in React
https://github.com/ilanman/STA561,b'This repo holds my STA561 Machine Learning code and project. The project highlights randomized singular value decomposition methods compared to traditional SVD. There are some computational comparisons implemented in Python. The results show that RSVD methods are faster than SVD and power iterations significantly increase time to completion at a low accuracy cost. \n',Probability Machine Learning final project
https://github.com/linetools/linetools,"b'linetools\n=========\n\n[![astropy](http://img.shields.io/badge/powered%20by-AstroPy-orange.svg?style=flat)](http://www.astropy.org/)\n\nThis is a package for the analysis of 1d astronomical spectra,\nespecially quasar and galaxy spectra.\n\nCheck out the documentation at https://linetools.readthedocs.org/en/latest\n\nDevelopment status\n------------------\n[![Build Status](https://travis-ci.org/linetools/linetools.svg?branch=master)](https://travis-ci.org/linetools/linetools)\n[![Coverage Status](https://coveralls.io/repos/github/linetools/linetools/badge.svg?branch=master)](https://coveralls.io/github/linetools/linetools?branch=master)\n\nDOI\n---\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.168270.svg)](https://doi.org/10.5281/zenodo.168270)\n'","A package for the analysis of 1d astronomical spectra, especially quasar and galaxy spectra."
https://github.com/MollyZhang/Reinforcement_learning_NLP,"b'# Reinforcement_learning_NLP\nImplementing Reinforcement Learning to find the best dialogue strategy for a conversation agent (chatbot) by search for maximum award.\n\n\n# To record a converstaion, do:\n1. `git clone https://github.com/MollyZhang/Reinforcement_learning_NLP.git`\n2. `cd Reinforcement_learning_NLP`\n3. `cd RL`\n4. `python run.py`  \n\n\nIf you want to train and populate reward table based on the 300 conversations recorded,type f,if you want to try a new dialogue,type s,if you want to view the accuracy of the evaluation model,type e,if you want to view the reward table,type r,if you want to view the Q_table type q\n\n### Future improvements\n- Dealing with user saying gibberish like ""dfkjlskdfj""\n- Dealing user repeat itself\n- Dealing with user insult\n- Having strategy to mickmick user input. e.g. When user says ""yay!!!"", bot says""wow!!!!"". \n\n\n### A brief overview of the code\nWe have learnt currently from the 300 odd conversations and populated the Reward table based on the user evaluation metrics.\nThe first block initializes the variables and the Q_table and the R_table.We have 6 strategies and 18 state variables based on the 4 state metrics like (If the user utterance is a question or not,the length of the utterance,the sentiment of the uttterance and whether the utterance is at the beginning(first utterance of the user),we have thus created 18 combinations of these states.\n\nThe second block are all the utility functions used and called by the later blocks.The most prominent amongst them being the training() where we train and populate the Q_table.The logic of Q_learning is implemented here.\n\nThe third block populates the reward table according to whether the utterance is at the beginning(in this case,it is calculated according to 0.8*start+0.2* overall,while for the rest utterances,it is 0.4*engaging+04*interrupt+0.2*overall.\n\nThe fourth block is used for training where it calls the training() method.\n\nThe fifth block records the new conversations and poplates the strategies based on the Q_table and updates the Q_table.\nWork on evaluation is still in progress.\n'",Implementing Reinforcement Learning to find the best dialogue strategy for a conversation agent (chatbot) by searching for maximum award.
https://github.com/andy1li/udacity-dlnd,b'# My Projects for the Udacity Deep Learning Nanodegree Foundation Program\nhttps://profiles.udacity.com/u/andyli\n\n',My Projects for the Udacity Deep Learning Nanodegree Foundation Program
https://github.com/sgandavarapu/FakeNews,"b'# FakeNews\nW210 Capstone Project\n\n__data.zip__ contains html from the home pages of 14 fake news sources, extracted once per day from 02-18-2017 -> 03/07/2017. It also contains text files with JSON for all of the articles posted 02-18-2017 -> 02-28-2017 - for now only 7 of 14 sources are included.\n  \n__data_credible.zip__ contains txt file of full txt of 5 top articles extracted daily from 02-27-2017 -> 03-05-2017 from 8 credible sources.\n'",W210 Capstone Project
https://github.com/GiorgioBondi/SmartPillow,"b""# SmartPillow\nGiorgio Bond\xc3\xac, Marta Brunetti, Giulia Crocioni and Francesca Cunsolo's repository for our project during XPH2016 Hackaton @NECSTLab Politecnico di Milano sponsored by Xilinx\n""","Giorgio Bondì, Marta Brunetti, Giulia Crocioni and Francesca Cunsolo's repository for our project during XPH2016 Hackaton @NECSTLab Politecnico di Milano sponsored by Xilinx"
https://github.com/pynxton/course,b'Python course\n===============\n\nSee notebooks in `nbviewer <http://nbviewer.ipython.org/github/pynxton/course/tree/master/>`_\n\nBooked room for the course on **Thursday 4-5 pm** mostly in the Meadow room (EBI south building)\n\n\n========== ============ ====================================================================================\n date         where         \n========== ============ ====================================================================================\n 16 Oct     Meadow        `1.basics <http://nbviewer.ipython.org/github/pynxton/course/tree/master/>`_\n 23 Oct     Meadow        `1.basics <http://nbviewer.ipython.org/github/pynxton/course/tree/master/>`_\n 30 Oct     Meadow        `2.programming <http://nbviewer.ipython.org/github/pynxton/course/tree/master/>`_\n 6  Nov     Meadow        `2.programming <http://nbviewer.ipython.org/github/pynxton/course/tree/master/>`_  \n 13 Nov     Ickleton      `3.Matplotlib <http://nbviewer.ipython.org/github/pynxton/course/blob/master/3.%20Matplotlib.ipynb>`_\n 20 Nov     Ickleton      `4.Object Oriented in Python <http://nbviewer.ipython.org/github/pynxton/course/blob/master/4.%20Class%20and%20objects.ipynb>`_\n 27 Nov     Meadow        `5.numpy/scipy/pandas overview`\n 4 Dec      Ickleton      `6.Packaging <https://github.com/cokelaer/python_notes/blob/master/source/packaging.rst>`_\n========== ============ ====================================================================================\n\n',Python course
https://github.com/kristijensen/OOP,"b""# OOP\nI'm playing at defining classes\n""",I'm playing at defining classes
https://github.com/bwallace/Deep-PICO,"b'# Deep-PICO\n\nExperiments in deep (OK, shallow, but using embeddings) for PICO identification.\n\n##Requirements\n\npython2.7\n\nKeras\n```bash\n    $ pip install keras\n```\n\nscikit-learn\n```bash\n    $ pip install -U scikit-learn\n```\ngensim\n```bash\n    $ pip install gensim\n```\ntheano\n```bash\n    $ pip install theano\n```\nnltk\n```bash\n    $ pip install nltk\n```\ngeniatagger\n```bash\n    got to http://www.nactem.ac.uk/GENIA/tagger/\n    unzip: tar xvzf geniatagger.tar.gz\n    navigate to geniatagger and make\n    \n    install the python wrapper\n    pip install geniatagger-python\n```\nsklearn_crfsuite\n```bash\n    $ pip install sklearn_crfsuite\n```\npycrfsuite\n```bash\n    $ pip install pycrfsuite\n```\n\n\nInstalling tensorflow\n```bash\n    # Ubuntu/Linux 64-bit\n    $ sudo apt-get install python-pip python-dev\n    \n    # Mac OS X\n    $ sudo easy_install pip\n    # Ubuntu/Linux 64-bit, CPU only:\n    $ sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl\n    \n    # Ubuntu/Linux 64-bit, GPU enabled:\n    $ sudo pip install --upgrade https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.7.1-cp27-none-linux_x86_64.whl\n    \n    # Mac OS X, CPU only:\n    $ sudo easy_install --upgrade six\n    $ sudo pip install --upgrade https://storage.googleapis.com/tensorflow/mac/tensorflow-0.7.1-cp27-none-any.whl\n```\n\n##Usage\n\n###Running the Conditional Random Field Model\n```bash\n    $ python crf.py \n```\n\n###Command line arguments \n```bash\n    --w2v               # 1 or 0 whether to use word vectors or not as features\n    --iters             # number of iterations to train on    \n    --l1                # l1 regulerzation term\n    --l2                # l2 regulerzation term\n    --wiki              # 1 or 0 whether to use the word vectors trained on wikipedia and pubmed \n    --shallow_parse     # 1 or 0 whether to use standerd POS features\n    --words_before      # number of words to use as features that come before each token\n    --words_after       # number of words to use as features that come after each token\n    --grid_search       # 1 or 0 whether to search for optimal hyperparmeters with grid search\n```\n\n\n\n\n###Running the Convolutional or Standard Neural Network \nTo use the  Convolutional Neural Network or Standard Feed forward Neural Network\n\n```bash\n    $ python GroupCNNExperiment.py  \n```\n\n###Command line arguments\n\n```bash\n    --window_size       # the number of words to use as features \n    --wiki              # 1 or 0 Use the word vectors trained on pubmed and wikipedia \n    --n_feature_maps    # the numner of feature maps for the CNN only\n    --epochs            # number of epochs to train the model for\n    --undersample       # 1 or 0 whether to train the model with \n    --criterion         # the loss function\n    --optimizer         # optimization algorthim \n    --model             # nn or cnn | whether to use a Convotuonal or feed forward neural network \n    --genia             # 1 or 0\n    --tacc              # for personal use only or if you have access to TACC for some reason \n    --layers            # format <1,2,3,4> the numbers of hidden layers in the network\n```\n'",Deep-ish learning for PICO extraction from abstracts
https://github.com/SimonTong22/Baseball-Data-Explore,b'# Baseball-Data-Explore\n\nJupyter Notebook of Python Exploration of Baseball Data using statistics in numpy and pandas to understand the value of defense \n',Data Exploration of Baseball Stats
https://github.com/theideasmith/neuralmanifolds,"b'---\ntitle: Differential Equations and Modelling of Point Charges\ndate: 2017-2-2\nauthor: Akiva Lipshitz\nlayout: post\n---\n\nThis post continues my [initial thoughts on electrodynamics]({{site.url}}/2017/01/30/The-Electric-Field-Equation.html)\n\nParticles and their dynamics are incredibly fascinating, even wondrous. Give me some particles and some simple equations describing their interactions \xe2\x80\x93\xc2\xa0some very interesting things can start happening. \n\nCurrently studying electrostatics in my physics class, I am interested in not only the static force and field distributions but also in the dynamics of particles in such fields. To study the dynamics of electric particles is not an easy endeavor \xe2\x80\x93\xc2\xa0in fact the differential equations governing their dynamics are quite complex and not easily solved manually, especially by someone who lacks a background in differential equations. \n\nInstead of relying on our analytical abilities, we may rely on our computational abilities and numerically solve the differential equations. Herein I will develop a scheme for computing the dynamics of $n$ electric particles en masse. It will not be computationally easy \xe2\x80\x93\xc2\xa0the number of operations grows proportionally to $n^2$. For less than $10^4$ you should be able to simulate the particle dynamics for long enough time intervals to be useful. But for something like $10^6$ particles the problem is intractable. You\'ll need to do more than $10^12$ operations per iteration and a degree in numerical analysis. \n\n\n\n## Governing Equations \n\nGiven $n$ charges $q_1, q_2, ..., q_n$, with masses $m_1, m_2, ..., m_n$ located at positions $\\vec{r}_1, \\vec{r_2}, ..., \\vec{r}_n$, the force induced on $q_i$ by $q_j$ is given by \n\n$$\\vec{F}_{j \\to i} = k\\frac{q_iq_j}{\\left|\\vec{r}_i-\\vec{r}_j\\right|^2}\\hat{r}_{ij}$$\n\nwhere \n\n$$\\hat{r}_{ij} = \\vec{r}_i-\\vec{r}_j$$\n\nNow, the net *marginal* force on particle $q_i$ is given as the sum of the pairwise forces\n\n$$\\vec{F}_{N, i} = \\sum_{j \\ne i}{\\vec{F}_{j \\to i}}$$\n\nAnd then the net acceleration of particle $q_i$ just normalizes the force by the mass of the particle:\n\n$$\\vec{a}_i = \\frac{\\vec{F}_{N, i}}{m_i}$$\n\nTo implement this at scale, we\'re going to need to figure out a scheme for vectorizing all these operations, demonstrated below. \n\nWe\'ll be using `scipy.integrate.odeint` for our numerical integration. Below, the function `g(y, t, q, m, n, d, k)` is a function that returns the derivatives for all our variables at each iteration. We pass it to `odeint` and then do the integration. \n\n\n```python\nimport numpy as np\nimport numpy.ma as ma\nfrom scipy.integrate import odeint\nmag = lambda r: np.sqrt(np.sum(np.power(r, 2)))\n\ndef g(y, t, q, m, n,d, k):\n    """"""\n    n: the number of particles\n    d: the number of dimensions \n      (for fun\'s sake I want this \n      to work for k-dimensional systems)\n    y: an (n*2,d) dimensional matrix \n        where y[:n]_i is the position\n        of the ith particle and\n        y[n:]_i is the velocity of \n        the ith particle\n    qs: the particle charges\n    ms: the particle masses\n    k: the electric constant\n    t: the current timestamp\n    """"""\n#     r1, r2, dr1dt, dr2dt = np.copy(y.reshape((n*2,d)))\n#     F = -1./mag(r2-r1)**2\n\n#     dy = [\n#      dr1dt,\n#      dr2dt,\n#      (F)*(r1-r2),\n#      (F)*(r2-r1),\n#     ]\n    y = np.copy(y.reshape((n*2,d)))\n\n    # rj across, ri down\n    rs_from = np.tile(y[:n], (n,1,1))\n\n    # ri across, rj down\n    rs_to = np.transpose(rs_from, axes=(1,0,2))\n\n    # directional distance between each r_i and r_j\n    # dr_ij is the force from j onto i, i.e. r_i - r_j\n    dr = rs_to - rs_from\n\n    # Used as a mask\n    nd_identity = np.eye(n).reshape((n,n,1))\n\n    # Force magnitudes\n    drmag = ma.array(\n        np.sqrt(\n            np.sum(\n                np.power(dr, 2), 2)), \n        mask=nd_identity)\n\n    # Pairwise q_i*q_j for force equation\n    qsa = np.tile(q, (n,1))\n    qsb = np.tile(q, (n,1)).T\n    qs = qsa*qsb\n\n    \n    # Directional forces\n    Fs = (-qs/np.power(drmag,2)).reshape((n,n,1))\n\n    # Dividing by m to obtain acceleration vectors\n    a = np.sum(Fs*dr, 1)\n\n    # Sliding integrated acceleration\n    # (i.e. velocity from previous iteration)\n    # to the position derivative slot\n    y[:n] = np.copy(y[n:])\n\n    # Entering the acceleration into the velocity slot\n    y[n:] = np.copy(a)\n    # Flattening it out for scipy.odeint to work\n    return np.array(y).reshape(n*2*d)  \n\n\n```\n\nLet\'s define our time intervals, so that odeint knows which time stamps to iterate over. \n\n\n```python\nt_f = 10000\nt = np.linspace(0, 10, num=t_f)\n```\n\nSome other constants\n\n\n```python\n# Number of dimensions\nd = 2\n# Number of point charges\nn = 3\n# charge magnitudes, currently all equal to 1\nq = np.array([-10,0.2,-5])\n# masses\nm = np.ones(n)\n\n# The electric constant \n#    k=1/(4*pi*epsilon_naught)\n#    Right now we will set it to 1\n#    because for our tests we are choosing all q_i =1. \n#    Therefore, k*q is too large a number and causes \n#    roundoff errors in the integrator. \n# In truth:\n#    k = 8.99*10**9\n# But for now:\nk=1.\n```\n\nWe get to choose the initial positions and velocities of our particles. For our initial tests, we\'ll set up 3 particles to collide with eachother. \n\n\n```python\n\nr1i = np.array([-2., 0.0])\ndr1dti = np.array([3.,0.])\n\nr2i = np.array([20.,0.5])\ndr2dti = np.array([-3., 0.])\n\nr3i = np.array([11.,20])\ndr3dti = np.array([0, -3.])\n```\n\nAnd pack them into an initial state variable we can pass to odeint. \n\n\n```python\ny0 = np.array([r1i, r2i, r3i, dr1dti, dr2dti, dr3dti]).reshape(n*2*d)\n```\n\n## The Fun Part \xe2\x80\x93 Doing the Integration\n\nNow, we\'ll actually do the integration\n\n\n```python\n# Doing the integration\nyf = odeint(g, y0, t, args=(q,m,n,d,k)).reshape(t_f,n*2,d)\n\n```\n\n    /Library/Python/2.7/site-packages/ipykernel/__main__.py:60: RuntimeWarning: divide by zero encountered in divide\n\n\n\n```python\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n%matplotlib inline\n\nfig = plt.figure(figsize=(20,20))\n#ax = fig.add_subplot(111, projection=\'3d\')\nax = fig.add_subplot(111)\nys1 = yf[:,0,1]\nxs1 = yf[:,0,0]\n\n\nxs2 = yf[:,1,0]\nys2 = yf[:,1,1]\n\nxs3 = yf[:,2,0]\nys3 = yf[:,2,1]\n\nax.plot(xs1[1], ys1[1],\'bv\')     \nax.plot(xs1[-1], ys1[-1], \'rv\') \n\nax.plot(xs2[:1], ys2[:1], \'bv\')    \nax.plot(xs2[-1:], ys2[-1:], \'rv\') \n\nax.plot(xs3[:1], ys3[:1], \'bv\')    \nax.plot(xs3[-1:], ys3[-1:], \'rv\') \n# \n# minx = np.min(y[:,[0,2],0]) \n# maxx = np.max(y[:,[0,2],0]) \n\n# miny = np.min(y[:,[0,2],1]) \n# maxy = np.max(y[:,[0,2],1])\n                                         \nax.plot(xs1, ys1)                      \nax.plot(xs2, ys2)    \nax.plot(xs3, ys3)                      \n\n\n# plt.xlim(xmin=minx, xmax=maxx)\n# plt.ylim(ymin=miny, ymax=maxy)\n\nplt.title(""Paths of 3 Colliding Electric Particles"")\nplt.ion()\nplt.show()\n\n```\n\n[Download this post as an ipython notebook](https://gist.github.com/5f52c2bc414a744953e1c69970590f9d)\n\n## Videos\n<iframe width=""560"" height=""315"" src=""https://www.youtube.com/embed/kblQj2h0eJ0"" frameborder=""0"" allowfullscreen></iframe>\n<iframe width=""560"" height=""315"" src=""https://www.youtube.com/embed/B1RTjIBA6OQ"" frameborder=""0"" allowfullscreen></iframe>\n<iframe width=""560"" height=""315"" src=""https://www.youtube.com/embed/MG-n7eYxbvQ"" frameborder=""0"" allowfullscreen></iframe>\n\n## Path Plot\n\n'",My experimentation with neural networks as approximations of continuous high dimensional surfaces and the dynamics of particles therein. 
https://github.com/olemke/arts,"b""[![Build](https://github.com/atmtools/arts/workflows/Build/badge.svg?branch=master)](https://github.com/atmtools/arts/commits/master)\n\n\nWelcome to ARTS\n===============\n\nARTS is free software. Please see the file COPYING for details.\n\nIf you use data generated by ARTS in a scientific publication, then please\nmention this and cite the most appropriate of the ARTS publications that are\n\nsummarized on http://www.radiativetransfer.org/docs/\n\n[CONTRIBUTING.md](CONTRIBUTING.md) provides information on contributing\nto ARTS on GitHub.\n \nFor documentation, please see the files in the doc subdirectory.\n\nFor building and installation instructions please read below.\n\n\nDependencies\n------------\n\nBuild Prerequisites (provided by mambaforge):\n\n- gcc/g++ >=11 (or llvm/clang >=13)\n- cmake (>=3.18)\n- zlib\n- openblas\n- libc++ (only for clang)\n- libmicrohttpd (>=0.9, optional, for documentation server)\n- netcdf (optional)\n- Python3 (>=3.9)\n  - required modules:\n    docutils\n    lark-parser\n    matplotlib\n    netCDF4\n    numpy\n    pytest\n    scipy\n    setuptools\n    xarray\n- GUI (optional)\n    glfw\n    glew\n\nTo build the documentation you also need:\n\n- pdflatex (optional)\n- doxygen (optional)\n- graphviz (optional)\n\n\nBuilding ARTS\n-------------\n\nThe following instructions assume that you are using mambaforge as a build environment.  The installer is available at\n[the project's Github page](https://github.com/conda-forge/miniforge#mambaforge).\n\nUse the provided `environment-dev-{linux,mac}.yml` files to create a conda\nenvironment with all required dependencies. The environment will be called\n`pyarts-dev`:\n\nLinux:\n```\nmamba env create -f environment-dev-linux.yml\n```\n\nmacOS:\n```\nmamba env create -f environment-dev-mac.yml\n```\n\nHere are the steps to use `cmake` to build ARTS.\n\n```\nmamba activate pyarts-dev\ncd arts\ncmake --preset=default-gcc-mamba  # On macOS use default-clang-mamba\ncmake --build build -jX\n```\n\nX is the number of parallel build processes.\n**X=Number of Cores** gives you usually the fastest compilation time.\n\nWARNING: The compilation is very memory intensive. If you have 16GB of RAM,\ndon't use more than 6-8 cores. With 8GB, don't use more than 2-3 cores.\n\nDevelopment install of the PyARTS Python package:\n\n```\npython3 -m pip install --user -e build/python\n```\n\nYou only have to do the python package install once.\nIf the ARTS source has changed, update the PyARTS package by running:\n\n```\ncmake --build build -jX --target pyarts\n```\n\n\nBuild configurations\n--------------------\n\nBy default, ARTS is built in release mode with optimizations enabled and\nassertions and debugging symbols turned off.\n\nWhenever you change the configuration, remove your build directory first:\n\n```\nrm -rf build\n```\n\nTo build with assertions and debugging symbols use:\n\n```\ncmake --preset=default-gcc-mamba -DCMAKE_BUILD_TYPE=RelWithDebInfo\n```\n\nThis configuration offers a good balance between performance and debugging\ncapabilities. Since this still optimizes out many variables, it can be\nnecessary for some debugging cases to turn off all optimizations. For those\ncases, the full debug configuration can be enabled. Note that ARTS runs a lot\nslower in this configuration:\n\n```\ncmake --preset=default-gcc-mamba -DCMAKE_BUILD_TYPE=Debug\n```\n\n\nInstalling PyARTS\n-----------------\n\nTo install the PyARTS Python package, you need to build it and install it with\npip. Create your build directory and configure ARTS with cmake as described in\nthe previous sections. Then, run the following commands inside your build\ndirectory:\n\n```\ncmake --build build --target pyarts\npython3 -m pip install --user -e build/python\n```\n\nThis will not mess with your system's Python installation.\nA link to the pyarts package is created in your home directory, usually\n`$HOME/.local/lib/python3.X/site-packages/pyarts.egg-link`.\n\nYou don't need to reinstall the package with pip after updating ARTS.\nYou only need to run `cmake --build build --target pyarts` again.\n\n\nTests\n-----\n\n'cmake --build build --target check' will run several test cases to ensure that\nARTS is working properly. Use 'check-all' to run all available controlfiles,\nincluding computation time-intensive ones.\n\nSome tests depend on the arts-xml-data package. cmake automatically looks if it\nis available in the same location as ARTS itself. If necessary, a custom path\ncan be specified.\n\n```\ncmake --preset=default-gcc-mamba -DARTS_XML_DATA_PATH=/home/myname/arts-xml-data\n```\n\nIf arts-xml-data cannot be found, those tests are ignored.\n\nBy default, 4 tests are executed in parallel.\nIf you change the number of concurrently run test, you can add this option to your `cmake --preset=....` call:\n\n```\n-DTEST_JOBS=X\n```\n\nX is the number of tests that should be started in parallel.\n\nYou can also use the ctest command directly to run the tests:\n\nFirst, change to the `build` directory:\n```\ncd build\n```\n\nThis runs all test with 4 jobs concurrently:\n```\nctest -j4\n```\n\nTo run specific tests, use the -R option and specify part of the test case name\nyou want to run. The following command will run all tests that have 'ppath' in\ntheir name, e.g. arts.ctlfile.fast.ppath1d ...:\n\n```\nctest -R ppath\n```\n\nTo see the output of ARTS, use the -V option:\n\n```\nctest -V -R fast.doit\n```\n\nBy default, ctest will not print any output from ARTS to the screen. The option\n--output-on-failure can be passed to ctest to see output in the case an error\noccurs. If you want to always enable this, you can set the environment variable\nCTEST_OUTPUT_ON_FAILURE:\n\n```\nexport CTEST_OUTPUT_ON_FAILURE=1\n```\n\n\nNative build\n------------\n\nTo squeeze out every last drop of performance, you can also build a version\nspecifically optimized for your machine's processor:\n\n```\n-DCMAKE_BUILD_TYPE=Native\n```\n\nThis option should make the executable slightly faster, more so on better\nsystems, but not portable. Note that since this build-mode is meant for\nfast-but-accurate computations, some IEEE rules will be ignored. For now only\ncomplex computations are IEEE incompatible running this mode of build.\n\n\nOptional features\n-----------------\n\nFeatures that rely on Fortran code located in the 3rdparty\nsubdirectory are enabled by default, but can be disabled by passing the\nfollowing option to the `cmake --preset=...` command:\n\n```\n-DENABLE_FORTRAN=0\n```\n\nThis disables Disort, Fastem and Tmatrix.\n\nIf necessary, certain Fortran modules can be selectively disabled:\n\n```\n-DNO_DISORT=1\n```\nor\n```\n-DENABLE_FORTRAN=1 -DNO_TMATRIX=1\n```\n\nIMPORTANT: Only gfortran is currently supported.\nAlso, a 64-bit system is required (size of long type must be 8 bytes).\n\n\nEnable NetCDF: The basic matpack types can be read from NetCDF files, if NetCDF\nsupport is enabled:\n\n```\ncmake --preset=default-gcc-mamba -DENABLE_NETCDF=1\n```\n\nPrecompiled headers: PCH can speed up builds significantly. However, it hampers\nthe ability for ccache to properly skip unnecessary compilations, potentially\nincreasing rebuild times. Tests have shown that it only speeds up the build\nconsiderably for Clang, but not for GCC.\n\n```\ncmake --preset=default-clang-mamba -DENABLE_PCH=1 ..\n```\n\nIf you enable PCH and also use ccache, you need to set the `CCACHE_SLOPPINESS`\nenvironment variable properly:\n\n```\nexport CCACHE_SLOPPINESS=pch_defines,time_macros\n```\n\n\nDisabling features\n------------------\n\nDisable assertions: `-DNO_ASSERT=1`\n\nDisable OpenMP: `-DNO_OPENMP=1`\n\nDisable the built-in documentation server: `-DNO_DOCSERVER=1`\n\n\nccache support\n--------------\n\nThe build utilizes ccache automatically when available, it can be\nturned of with the option `-DENABLE_CCACHE=1`\n```\n\nFor details see https://ccache.samba.org/\n\n\nValgrind profiling\n------------------\n\nThe callgrind plugin included in valgrind is the recommended profiling method\nfor ARTS.\n\nDue to limitations of valgrind, you need to disable the tmatrix code\n(-DNO_TMATRIX=1) when compiling ARTS with Fortran support.\n\nCertain things should be taken into account when calling ARTS with valgrind.\nSince recursion (cycles) will lead to wrong profiling results it is\nimportant to use the following settings to obtain profile data for ARTS:\n\n```\nvalgrind --tool=callgrind --separate-callers=10 --separate-recs=3 arts -n1 ...\n```\n\nFor detail on these options consult the valgrind manual:\n\nhttp://valgrind.org/docs/manual/cl-manual.html#cl-manual.cycles\n\n-n1 should be passed to ARTS because parallelisation can further scew the\nresults. Since executing a program in valgrind can lead to 50x slower\nexecution, it is recommended to create a dedicated, minimal controlfile for\nprofiling.\n\nAfter execution with valgrind, the resulting callgrind.out.* file can be\nopened in kcachegrind[1] for visualization. It is available as a package for\nmost Linux distributions.\n\nNote that you don't have to do a full ARTS run. You can cancel the program\nafter some time when you think you have gathered enough statistics.\n\n[1] https://kcachegrind.github.io/\n\n\nLinux perf profiling\n--------------------\n\nThe [Performance Counters for Linux](https://perf.wiki.kernel.org/) offer a\nconvenient way to profile any program with basically no runtime overhead.\nProfiling works for all configurations (Debug, RelWithDebInfo and Release). To\nensure that the calltree can be analyzed correctly, compile ARTS with frame\npointers. This has minimal impact on performance. Use the following preset to\nenable this setting:\n\n```\ncmake --preset=perf-gcc-mamba\n```\n\nPrepend the perf command to your arts call to record callgraph information:\n\n```\nperf record -g src/arts MYCONTROLFILE.arts\n```\n\nThis can also be applied to any test case:\n\n```\nperf record -g ctest -R TestDOIT$\n```\n\nAfter recording, use the report command to display an interactive view of the\nprofiling information:\n\n```\nperf report -g graph,0.5,callees\n```\n\nThis will show a reverse call tree with the percentage of time spent in each\nfunction. The function tree can be expanded to expose the calling functions.\n\n""",The Atmospheric Radiative Transfer Simulator
https://github.com/sumitbinnani/AIND-Planning,"b'\n# Implement a Planning Search\n\n# [My Heuristic Analysis Report can be found here](./heuristic_analysis.pdf).\n\n## Synopsis\n\nThis project includes skeletons for the classes and functions needed to solve deterministic logistics planning problems for an Air Cargo transport system using a planning search agent. \nWith progression search algorithms like those in the navigation problem from lecture, optimal plans for each \nproblem will be computed.  Unlike the navigation problem, there is no simple distance heuristic to aid the agent. \nInstead, you will implement domain-independent heuristics.\n![Progression air cargo search](images/Progression.PNG)\n\n- Part 1 - Planning problems:\n\t- READ: applicable portions of the Russel/Norvig AIMA text\n\t- GIVEN: problems defined in classical PDDL (Planning Domain Definition Language)\n\t- TODO: Implement the Python methods and functions as marked in `my_air_cargo_problems.py`\n\t- TODO: Experiment and document metrics\n- Part 2 - Domain-independent heuristics:\n\t- READ: applicable portions of the Russel/Norvig AIMA text\n\t- TODO: Implement relaxed problem heuristic in `my_air_cargo_problems.py`\n\t- TODO: Implement Planning Graph and automatic heuristic in `my_planning_graph.py`\n\t- TODO: Experiment and document metrics\n- Part 3 - Written Analysis\n\n## Environment requirements\n- Python 3.4 or higher\n- Starter code includes a copy of [companion code](https://github.com/aimacode) \nfor the Stuart Russel/Norvig AIMA text.  \n\n\n## Project Details\n### Part 1 - Planning problems\n#### READ: Stuart Russel and Peter Norvig text:\n\n""Artificial Intelligence: A Modern Approach"" 3rd edition chapter 10 *or* 2nd edition Chapter 11 on Planning, available [on the AIMA book site](http://aima.cs.berkeley.edu/2nd-ed/newchap11.pdf) sections: \n\n- *The Planning Problem*\n- *Planning with State-space Search*\n\n#### GIVEN: classical PDDL problems\n\nAll problems are in the Air Cargo domain.  They have the same action schema defined, but different initial states and goals.\n\n- Air Cargo Action Schema:\n```\nAction(Load(c, p, a),\n\tPRECOND: At(c, a) \xe2\x88\xa7 At(p, a) \xe2\x88\xa7 Cargo(c) \xe2\x88\xa7 Plane(p) \xe2\x88\xa7 Airport(a)\n\tEFFECT: \xc2\xac At(c, a) \xe2\x88\xa7 In(c, p))\nAction(Unload(c, p, a),\n\tPRECOND: In(c, p) \xe2\x88\xa7 At(p, a) \xe2\x88\xa7 Cargo(c) \xe2\x88\xa7 Plane(p) \xe2\x88\xa7 Airport(a)\n\tEFFECT: At(c, a) \xe2\x88\xa7 \xc2\xac In(c, p))\nAction(Fly(p, from, to),\n\tPRECOND: At(p, from) \xe2\x88\xa7 Plane(p) \xe2\x88\xa7 Airport(from) \xe2\x88\xa7 Airport(to)\n\tEFFECT: \xc2\xac At(p, from) \xe2\x88\xa7 At(p, to))\n```\n\n- Problem 1 initial state and goal:\n```\nInit(At(C1, SFO) \xe2\x88\xa7 At(C2, JFK) \n\t\xe2\x88\xa7 At(P1, SFO) \xe2\x88\xa7 At(P2, JFK) \n\t\xe2\x88\xa7 Cargo(C1) \xe2\x88\xa7 Cargo(C2) \n\t\xe2\x88\xa7 Plane(P1) \xe2\x88\xa7 Plane(P2)\n\t\xe2\x88\xa7 Airport(JFK) \xe2\x88\xa7 Airport(SFO))\nGoal(At(C1, JFK) \xe2\x88\xa7 At(C2, SFO))\n```\n- Problem 2 initial state and goal:\n```\nInit(At(C1, SFO) \xe2\x88\xa7 At(C2, JFK) \xe2\x88\xa7 At(C3, ATL) \n\t\xe2\x88\xa7 At(P1, SFO) \xe2\x88\xa7 At(P2, JFK) \xe2\x88\xa7 At(P3, ATL) \n\t\xe2\x88\xa7 Cargo(C1) \xe2\x88\xa7 Cargo(C2) \xe2\x88\xa7 Cargo(C3)\n\t\xe2\x88\xa7 Plane(P1) \xe2\x88\xa7 Plane(P2) \xe2\x88\xa7 Plane(P3)\n\t\xe2\x88\xa7 Airport(JFK) \xe2\x88\xa7 Airport(SFO) \xe2\x88\xa7 Airport(ATL))\nGoal(At(C1, JFK) \xe2\x88\xa7 At(C2, SFO) \xe2\x88\xa7 At(C3, SFO))\n```\n- Problem 3 initial state and goal:\n```\nInit(At(C1, SFO) \xe2\x88\xa7 At(C2, JFK) \xe2\x88\xa7 At(C3, ATL) \xe2\x88\xa7 At(C4, ORD) \n\t\xe2\x88\xa7 At(P1, SFO) \xe2\x88\xa7 At(P2, JFK) \n\t\xe2\x88\xa7 Cargo(C1) \xe2\x88\xa7 Cargo(C2) \xe2\x88\xa7 Cargo(C3) \xe2\x88\xa7 Cargo(C4)\n\t\xe2\x88\xa7 Plane(P1) \xe2\x88\xa7 Plane(P2)\n\t\xe2\x88\xa7 Airport(JFK) \xe2\x88\xa7 Airport(SFO) \xe2\x88\xa7 Airport(ATL) \xe2\x88\xa7 Airport(ORD))\nGoal(At(C1, JFK) \xe2\x88\xa7 At(C3, JFK) \xe2\x88\xa7 At(C2, SFO) \xe2\x88\xa7 At(C4, SFO))\n```\n\n#### TODO: Implement methods and functions in `my_air_cargo_problems.py`\n- `AirCargoProblem.get_actions` method including `load_actions` and `unload_actions` sub-functions\n- `AirCargoProblem.actions` method\n- `AirCargoProblem.result` method\n- `air_cargo_p2` function\n- `air_cargo_p3` function\n\n#### TODO: Experiment and document metrics for non-heuristic planning solution searches\n* Run uninformed planning searches for `air_cargo_p1`, `air_cargo_p2`, and `air_cargo_p3`; provide metrics on number of node expansions required, number of goal tests, time elapsed, and optimality of solution for each search algorithm. Include the result of at least three of these searches, including breadth-first and depth-first, in your write-up (`breadth_first_search` and `depth_first_graph_search`). \n* If depth-first takes longer than 10 minutes for Problem 3 on your system, stop the search and provide this information in your report.\n* Use the `run_search` script for your data collection: from the command line type `python run_search.py -h` to learn more.\n\n>#### Why are we setting the problems up this way?  \n>Progression planning problems can be \nsolved with graph searches such as breadth-first, depth-first, and A*, where the \nnodes of the graph are ""states"" and edges are ""actions"".  A ""state"" is the logical \nconjunction of all boolean ground ""fluents"", or state variables, that are possible \nfor the problem using Propositional Logic. For example, we might have a problem to \nplan the transport of one cargo, C1, on a\nsingle available plane, P1, from one airport to another, SFO to JFK.\n![state space](images/statespace.png)\nIn this simple example, there are five fluents, or state variables, which means our state \nspace could be as large as ![2to5](images/twotofive.png). Note the following:\n>- While the initial state defines every fluent explicitly, in this case mapped to **TTFFF**, the goal may \nbe a set of states.  Any state that is `True` for the fluent `At(C1,JFK)` meets the goal.\n>- Even though PDDL uses variable to describe actions as ""action schema"", these problems\nare not solved with First Order Logic.  They are solved with Propositional logic and must\ntherefore be defined with concrete (non-variable) actions\nand literal (non-variable) fluents in state descriptions.\n>- The fluents here are mapped to a simple string representing the boolean value of each fluent\nin the system, e.g. **TTFFTT...TTF**.  This will be the state representation in \nthe `AirCargoProblem` class and is compatible with the `Node` and `Problem` \nclasses, and the search methods in the AIMA library.  \n\n\n### Part 2 - Domain-independent heuristics\n#### READ: Stuart Russel and Peter Norvig text\n""Artificial Intelligence: A Modern Approach"" 3rd edition chapter 10 *or* 2nd edition Chapter 11 on Planning, available [on the AIMA book site](http://aima.cs.berkeley.edu/2nd-ed/newchap11.pdf) section: \n\n- *Planning Graph*\n\n#### TODO: Implement heuristic method in `my_air_cargo_problems.py`\n- `AirCargoProblem.h_ignore_preconditions` method\n\n#### TODO: Implement a Planning Graph with automatic heuristics in `my_planning_graph.py`\n- `PlanningGraph.add_action_level` method\n- `PlanningGraph.add_literal_level` method\n- `PlanningGraph.inconsistent_effects_mutex` method\n- `PlanningGraph.interference_mutex` method\n- `PlanningGraph.competing_needs_mutex` method\n- `PlanningGraph.negation_mutex` method\n- `PlanningGraph.inconsistent_support_mutex` method\n- `PlanningGraph.h_levelsum` method\n\n\n#### TODO: Experiment and document: metrics of A* searches with these heuristics\n* Run A* planning searches using the heuristics you have implemented on `air_cargo_p1`, `air_cargo_p2` and `air_cargo_p3`. Provide metrics on number of node expansions required, number of goal tests, time elapsed, and optimality of solution for each search algorithm and include the results in your report. \n* Use the `run_search` script for this purpose: from the command line type `python run_search.py -h` to learn more.\n\n>#### Why a Planning Graph?\n>The planning graph is somewhat complex, but is useful in planning because it is a polynomial-size approximation of the exponential tree that represents all possible paths. The planning graph can be used to provide automated admissible heuristics for any domain.  It can also be used as the first step in implementing GRAPHPLAN, a direct planning algorithm that you may wish to learn more about on your own (but we will not address it here).\n\n>*Planning Graph example from the AIMA book*\n>![Planning Graph](images/eatcake-graphplan2.png)\n\n### Part 3: Written Analysis\n#### TODO: Include the following in your written analysis.  \n- Provide an optimal plan for Problems 1, 2, and 3.\n- Compare and contrast non-heuristic search result metrics (optimality, time elapsed, number of node expansions) for Problems 1,2, and 3. Include breadth-first, depth-first, and at least one other uninformed non-heuristic search in your comparison; Your third choice of non-heuristic search may be skipped for Problem 3 if it takes longer than 10 minutes to run, but a note in this case should be included.\n- Compare and contrast heuristic search result metrics using A* with the ""ignore preconditions"" and ""level-sum"" heuristics for Problems 1, 2, and 3.\n- What was the best heuristic used in these problems?  Was it better than non-heuristic search planning methods for all problems?  Why or why not?\n- Provide tables or other visual aids as needed for clarity in your discussion.\n\n## Examples and Testing:\n- The planning problem for the ""Have Cake and Eat it Too"" problem in the book has been\nimplemented in the `example_have_cake` module as an example.\n- The `tests` directory includes `unittest` test cases to evaluate your implementations. All tests should pass before you submit your project for review. From the AIND-Planning directory command line:\n    - `python -m unittest tests.test_my_air_cargo_problems`\n    - `python -m unittest tests.test_my_planning_graph`\n- The `run_search` script is provided for gathering metrics for various search methods on any or all of the problems and should be used for this purpose.\n\n\n'",Deterministic logistics planning problems for an Air Cargo transport system using a planning search agent
https://github.com/codebasics/py,"b'# py\nRepository to store sample Python programs.\n\nThis repository is meant for beginners to assist them in their learning of Python. The repository covers a wide range of algorithms and other programs, and would prove immensely helpful for everybody interested in Python programming.\n\nIf this is your first time coding in Python, I would love to suggest you begin from the [Basics](https://github.com/codebasics/py/tree/master/Basics). They are simple to understand and hopefully will prove fun to you.\n\nYou can also pay a visit to my very own [Youtube channel](https://www.youtube.com/channel/UCh9nVJoWXmFb7sLApWGcLPQ).\n\nContributions to the repository are welcome.\n\n[![CodeBasics](https://yt3.ggpht.com/ytc/AAUvwnihwx4a5idwBTE5JFpXHb-ykyh-i1gXtFiGJYV1=s176-c-k-c0x00ffffff-no-rj)](https://www.youtube.com/channel/UCh9nVJoWXmFb7sLApWGcLPQ).\n\n#### Happy coding!\n'",Repository to store sample python programs for python learning
https://github.com/lisaleemcb/ouroboros,b'ouroboros\n=========\n\nAssociated code for Active Feedback Resonator axion detector prototype\n',Associated code for active feedback resonator axion detector prototype
https://github.com/saverkamp/measure-metadata-workshop,"b""## Measuring Your Metadata -- Code4Lib 2016 Pre-Conference Workshop\nMonday, March 7  \n1:30-4:30pm  \nChemical Heritage Foundation  \nRoom: Ullyot S  \n\n\nCoordinators: Shawn Averkamp, Sara Rubinow, Matt Miller, Josh Hadro   \n\nTools and standards abound for creating and enriching metadata, but measuring, monitoring, and managing metadata for the long haul can be a daunting task. What tools are out there to assess the shape of our metadata? How can visualizations show us the gaps or flaws in our description? What can web traffic analytics tell us about the value of our metadata? What is quality, really? We certainly don\xe2\x80\x99t have all the answers, but together we can workshop the questions. Specific topics will be driven by the interest of attendees. The organizers will bring examples of their own work at NYPL in visualization, data analysis with Python, and Google analytics assessment and invite participants to bring their own tools and strategies to share in group discussion, short demos, and hands-on breakout sessions. Takeaways will include: exposure to approaches and tools in use in the field and an expanded network of commiserators to help you through your next metadata audit.\n\nSchedule:  \n1:30 - 1:40 Introductions  \n1:40 - 3:00 [Presentations](https://docs.google.com/spreadsheets/d/1ob4imuFCMi3fMkoIjlASBYWVdDd4qTVJj5xeG2eLEVM/edit#gid=634347005)  \n3:00 - 3:10 Break  \n3:10 - 4:10 [Hands-on sessions](https://docs.google.com/spreadsheets/d/1ob4imuFCMi3fMkoIjlASBYWVdDd4qTVJj5xeG2eLEVM/edit#gid=634347005)  \n4:10 - 4:30 Reporting back and discussion  \n\n## Before you attend\nYou'll get the most out of your hands-on sessions if you install the necessary applications ahead of time. If you're planning to participate in the following hands-on sessions, please try to come prepared!  \n\n### Using Python to assess metadata quality in MODS\nFor this hands-on session, we'll be using [Jupyter (IPython) notebook](http://jupyter.org/) to walk through some simple functions and scripts. We'll also be working with [lxml](http://lxml.de/), a third-party Python library for parsing and manipulating XML. Fortunately, both of these are already included in the [Anaconda Python distribution](https://www.continuum.io/why-anaconda). We strongly recommend [installing Anaconda](http://docs.continuum.io/anaconda/install) for this workshop. It also comes bundled with [pandas](http://pandas.pydata.org/) and all of its dependencies, so it will be useful to have it you're interested in learning more about data analysis.  \n\n### A beginner's guide to metadata analysis in Python with pandas\nFor this hands-on session, we'll be using [Jupyter (IPython) notebook](http://jupyter.org/) to explore basic data analysis with [pandas](http://pandas.pydata.org/), a Python data analysis library. Fortunately, both IPython notebook and pandas (as well as two additionally-necessary packages, [numpy](http://www.numpy.org/) and [matplotlib](http://matplotlib.org/)) are already included in the [Anaconda Python distribution](https://www.continuum.io/why-anaconda). We strongly recommend [installing Anaconda](http://docs.continuum.io/anaconda/install) for this session. It also comes bundled with [lxml](http://lxml.de/), a third-party Python library for parsing and manipulating XML, so it will be useful to have if you're interested in learning more about using Python to parse your XML data.\n\n### Visualizing your metadata with d3\nThis hands-on session will be a beginner's intro to the d3 visualization library. We will use it to try to render metadata quality results which allows quick visual analysis. All you will need for this session are the examples and data provided in the d3_viz folder, a text editor and a web browser.\n\n## Notes and Resources\nDuring the workshop, we'll take collaborative notes and share favorite resources in [this Google Doc](http://bit.ly/MeasureMetadataC4L16). We invite you to contribute!  \n""",Workshop materials for Code4Lib 2016 pre-conference: http://2016.code4lib.org/workshops/Measuring-Your-Metadata
https://github.com/weihuayi/fealpy,"b'# FEALPy: Finite Element Analysis Library in Python\n\n[![Join the chat at https://gitter.im/weihuayi/fealpy](https://badges.gitter.im/weihuayi/fealpy.svg)](https://gitter.im/weihuayi/fealpy?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n![Python package](https://github.com/weihuayi/fealpy/workflows/Python%20package/badge.svg)\n![Upload Python Package](https://github.com/weihuayi/fealpy/workflows/Upload%20Python%20Package/badge.svg)\n\n![](./FEALPY.png)\n\nWe want to develop an efficient and easy to use finite element software\npackage to support our teach and research work. \n\nWe still have lot work to do. \n\n\xe5\x85\xb3\xe4\xba\x8e FEALPy \xe7\x9a\x84\xe4\xb8\xad\xe6\x96\x87\xe5\xb8\xae\xe5\x8a\xa9\xe4\xb8\x8e\xe5\xae\x89\xe8\xa3\x85\xe4\xbf\xa1\xe6\x81\xaf\xe8\xaf\xb7\xe6\x9f\xa5\xe7\x9c\x8b\xef\xbc\x9a\n[FEALPy \xe5\xb8\xae\xe5\x8a\xa9\xe4\xb8\x8e\xe5\xae\x89\xe8\xa3\x85](https://www.weihuayi.cn/fealpy/docs/zh/start/fealpy-install)\n\n# Installation\n\n## Common\n\nTo install the latest release from PyPi, use\n```bash\npip install -U fealpy\n``` \n\nIf you have no `root` access on Linux/MacOS, please try \n```bash\npython -m pip install -U fealpy\n```\n\nUsers in China can install FEALPy from mirrors such as:\n- [Aliyun](https://developer.aliyun.com/mirror/pypi)\n- [Tsinghua](https://mirrors.tuna.tsinghua.edu.cn/help/pypi/)\n- [USTC](https://lug.ustc.edu.cn/wiki/mirrors/help/pypi)\n\n## From Source\n\n```bash\ngit clone https://github.com/weihuayi/fealpy.git\ncd fealpy\npip install .\n```\n\nFor developers, please use `pip install -e .` to install it in develop mode.\n\nOn Linux system such as Ubuntu or Fedora, or MacOS, maybe you should use `pip3 install -e .` to install it in\ndevelop mode.\n\n## Uninstallation\n\n```bash\npip uninstall fealpy\n```\n\n## Warning \nThe sparse pattern of the matrix `A` generated by `FEALPy` may not be the same as the theoretical pattern, since there exists nonzero values that are close to machine precision due to rounding. If you care about the sparse pattern of the matrix, you can use the following commands to eliminate them\n```python\neps = 10**(-15)\nA.data[ np.abs(A.data) < eps ] = 0\nA.eliminate_zeros()\n```\n\n## Docker\n\nTo be added.\n\n## Reference and Acknowledgement\n\nWe thank Dr. Long Chen for the guidance and compiling a systematic documentation for programming finite element methods.\n* http://www.math.uci.edu/~chenlong/programming.html\n* https://github.com/lyc102/ifem\n\n\n## Citation\n\nPlease cite `fealpy` if you use it in your paper\n\nH. Wei and Y. Huang, FEALPy: Finite Element Analysis Library in Python, https://github.com/weihuayi/fealpy, *Xiangtan University*, 2017-2023.\n\n```bibtex\n@misc{fealpy,\n\ttitle = {FEALPy: Finite Element Analysis Library in Python. https://github.com/weihuayi/fealpy},\n\turl = {https://github.com/weihuayi/fealpy},\n\tauthor = {Wei, Huayi and Huang, Yunqing},\n    institution = {Xiangtan University},\n\tyear = {Xiangtan University, 2017-2023},\n}\n```\n\n\n\n\n\n\n\n\n\n'",Finite Element Analysis Library in Python
https://github.com/docathon/sphinx-template,"b""# sphinx_template\n\nMaterials to help you go from zero to full documentation as quickly as possible.\n\nThis repository contains the following directories:\n\n* docs/\n  * The Sphinx source of the documentation.\n* my_package/\n  * An example Python package.  We'll grab docstrings from this package and\n  generate documentation for it.\n* examples/\n  * Visual examples that illustrate the use of `my_package`.  These examples\n  will be turned into a gallery.\n\nOf particular interest may be the `docs/source/conf.py` file, which is used to\nconfigure Sphinx.\n\nThe documentation may be built by doing:\n\n```\ncd docs\nmake html\n```\n""",A template for a fully-functioning sphinx deployment
https://github.com/ubcs3/2016-Fall,"b""## UBC Scientific Software Seminar\n\nThe UBC Scientific Software Seminar is inspired by [Software Carpentry](http://software-carpentry.org/) and its goal is to help students, graduates, fellows and faculty at UBC develop software skills for science.\n\n### Fall 2016: Machine Learning in Python with scikit-learn\n\n#### OUTLINE\n\n* What are the **learning goals**?\n  * To learn how to use [scikit-learn](http://scikit-learn.org) to solve machine learning problems\n  * To master [Python](https://www.python.org/) programming for scientific computing\n  * To learn mathematics and statistics applied to data science and machine learning\n  * To meet and collaborate with other students and faculty interested in scientific computing\n* What **software tools** are we going to use?\n  * [scikit-learn](http://scikit-learn.org/): machine learning in Python\n  * [SciPy Stack](http://scipy.org/): scientific computing with [NumPy](http://www.numpy.org/), [SciPy](http://scipy.org/), [matplotlib](http://matplotlib.org/) and [pandas](http://pandas.pydata.org/)\n  * [Python](https://www.python.org/)\n  * [Jupyter Notebooks](http://jupyter.org/): execute code with accompanying text, markdown and LaTeX all in the browser\n  * [Git/GitHub](https://github.com/): manage projects locally from the command line with Git and collaborate online with GitHub\n* What **scientific topics** will we study?\n  * Machine learning fundamentals (following [tutorials](http://scikit-learn.org/stable/tutorial/basic/tutorial.html) provided by [scikit-learn.org](http://scikit-learn.org/)):\n    * Regression, classification, clustering, dimensionality reduction\n  * Special topics:\n    * [Natural Language Processing](http://www.nltk.org/)\n* Where do we start? What are the **prerequisites**?\n  * UBCS3 Fall 2016 is a continuation of [UBCS3 Summer 2016](https://github.com/ubcs3/2016-Summer) which included:\n    * Bash shell\n    * Git/GitHub\n    * Python programming\n    * SciPy stack: NumPy, Scipy, matplotlib and pandas\n    * Basic examples using scikit-learn\n  * Calculus, linear algebra, probability and statistics\n* Who is the target **audience**?\n  * Everyone is invited!\n  * If the outline above is at your level, perfect! Get ready to write a lot of code!\n  * If the outline above seems too intimidating, come anyway! You'll learn things just by being exposed to new tools and ideas, and meeting new people!\n  * If you have experience with all the topics outlined above, come anyway! You'll become more of an expert by participating as a helper/instructor!\n\n#### SCHEDULE\n\nFall 2016 will consist of weekly 1-hour meetings held from October until mid-December. The regular scheduled time is Friday 1-2pm (with additional hour 3-4pm for those who cannot attend 1-2pm).\n\n* Week 1 - Friday October 7 - 1-2pm - LSK 121 [[Notes](2016-10-07-notes.ipynb)]\n  * Overview of machine learning problems\n  * Exploring the scikit-learn documentation\n  * Getting to know the scikit-learn API\n  * First examples with builtin example datasets\n* Week 2 - Friday October 14 - 1-2pm - LSK 121 [[Notes](2016-10-14-notes.ipynb)]\n  * Regression Example: Diabetes dataset\n    * A closer look at least squares linear regression calculations\n    * Can we improve R2? Let's create more features\n    * Splitting the dataset: Training data and testing data\n  * Classification Example: Hand-written digits dataset\n    * K-nearest neighbors classifier\n    * Evaluating the model\n* Week 3 - Friday October 21 - 1-2pm - LSK 121 [[Notes](2016-10-21-notes.ipynb)]\n  * Dimensionality reduction\n  * Principal component analysis\n  * Visualizing the digits dataset\n  * Linear algebra behind principal component analysis\n* Week 4 - Friday October 28 - 1-2pm - LSK 121 [[Notes](2016-10-28-notes.ipynb)]\n  * PCA revisted\n    * Visualizing principal components\n  * Unsupervised learning\n    * Clustering with K-means\n    * Digits dataset: How many different kinds of 1s are there?\n    * Combining KMeans with PCA\n* Week 5 - Friday November 4 - 1-2pm - LSK 121 [[Notes](2016-11-04-notes.ipynb)]\n  * Kernel density estimation and Gaussian processes - Presented by [@sempwn](https://github.com/sempwn)\n* *Remembrance Day* - No meeting November 11\n* Week 6 - Friday November 18 - 1-2pm - **UCLL 109**\n  * Natural Language Processing with nltk: Movie Review Classification - Presented by [@dbhaskar92](https://github.com/dbhaskar92)\n* Week 7 - Friday November 25 - 1-2pm - **UCLL 109** [[Notes](2016-11-25-notes.ipynb)]\n  * Natural Language Processing with nltk: Movie Review Classification (Continued)\n    * Working with nltk movie review dataset\n    * Using regular expressions to remove punctuation and stopwords\n    * Creating feature vectors from movie reviews\n    * Applying a Naive Bayes classifier""",UBC Scientific Software Seminar: Machine Learning in Python with scikit-learn
https://github.com/rtlee9/SIC-list,"b""# SIC codes for download -- open source edition\n\n[![Build Status](https://travis-ci.org/rtlee9/SIC-list.svg?branch=master)](https://travis-ci.org/rtlee9/SIC-list)\n[![Coverage Status](https://coveralls.io/repos/github/rtlee9/SIC-list/badge.svg?branch=)](https://coveralls.io/github/rtlee9/SIC-list?branch=)\n[![Code Climate](https://codeclimate.com/github/rtlee9/SIC-list/badges/gpa.svg)](https://codeclimate.com/github/rtlee9/SIC-list)\n[![license](https://img.shields.io/badge/license-Apache_2.0-blue.svg)](LICENSE)\n[![PyPI](https://img.shields.io/badge/python-2.7-blue.svg)](https://www.python.org/download/releases/2.7/)\n\nThis repo provides lists of four-digit SIC codes scraped from the websites of two government agencies: the [SEC](https://www.sec.gov/info/edgar/siccodes.htm) and [OSHA](https://www.osha.gov/pls/imis/sic_manual.html). The cleaned lists can be downloaded [here](https://raw.githubusercontent.com/rtlee9/SIC-list/master/data/sec_combined.csv) and [here](https://raw.githubusercontent.com/rtlee9/SIC-list/master/data/osha_combined.csv), respectively, and refresh instructions can be found below.\n\n## Background\nThe Standard Industrial Classification (SIC) is a system used to classify businesses by their primary business activity, or industry. The SIC system was created in the 1930's and has since been [replaced](https://www.census.gov/eos/www/naics/faqs/faqs.html#q8) as the industry classification system for Federal statistical agencies; however, it is still widely used by many businesses and by some government agencies.\n\n## Authoritative sources\n\nSIC codes were once maintained and assigned by the US government. I've found that only two government agencies currently publish a list of SIC codes and descriptions:\n\n| Source | Version | Use case |\n| ------ | ------- | -------- |\n| [Occupational Safety & Health Administration (OSHA)](https://www.osha.gov/pls/imis/sic_manual.html) | 1987 SIC manual | Unknown |\n| [U.S. Securities and Exchange Commission (SEC)](https://www.sec.gov/info/edgar/siccodes.htm) | No version provided, but the SEC website indicates the webpage was last modified January 25, 2015 | Used in [EDGAR](https://www.sec.gov/edgar/searchedgar/companysearch.html) electronic filings |\n\nThe SIC codes provided by the SEC generally align with those provided by OSHA; however, OSHA's SIC manual is more comprehensive -- it contains many more SIC codes than does the SEC's list.\n\n## Other sources\n\nThere are a number of online sources that provide SIC codes and descriptions, though I've found none that provide all of the following:\n* The source of their data\n* Their code, if relevant\n* Machine readable data\n\nTaken together, these are important for assessing data quality and reliability. The purpose of this repository is to provide SIC codes in adherence with these standards.\n\n## Usage\n\nThe latest data can be found in the root directory. To refresh:\n\n1. Install Python 2.7\n1. Install python requirements: `$ pip install -r requirements.txt`\n1. From the command line run `$ python src/main.py`\n\n## License\n[Apache License 2.0](LICENSE)\n""",List of SIC codes and descriptions from authoritative sources
https://github.com/t-lanigan/CarND-LaneLines-P1,"b'#**Finding Lane Lines on the Road** \n<img src=""laneLines_thirdPass.jpg"" width=""480"" alt=""Combined Image"" />\n\nWhen we drive, we use our eyes to decide where to go.  The lines on the road that show us where the lanes are act as our constant reference for where to steer the vehicle.  Naturally, one of the first things we would like to do in developing a self-driving car is to automatically detect lane lines using an algorithm.\n\nIn this project you will detect lane lines in images using Python and OpenCV.  OpenCV means ""Open-Source Computer Vision"", which is a package that has many useful tools for analyzing images.  \n\n**Step 1:** Getting setup with Python\n\nTo do this project, you will need Python 3 along with the numpy, matplotlib, and OpenCV libraries, as well as Jupyter Notebook installed. \n\nWe recommend downloading and installing the Anaconda Python 3 distribution from Continuum Analytics because it comes prepackaged with many of the Python dependencies you will need for this and future projects, makes it easy to install OpenCV, and includes Jupyter Notebook.  Beyond that, it is one of the most common Python distributions used in data analytics and machine learning, so a great choice if you\'re getting started in the field.\n\nChoose the appropriate Python 3 Anaconda install package for your operating system <A HREF=""https://www.continuum.io/downloads"" target=""_blank"">here</A>.   Download and install the package.\n\nIf you already have Anaconda for Python 2 installed, you can create a separate environment for Python 3 and all the appropriate dependencies with the following command:\n\n`>  conda create --name=yourNewEnvironment python=3 anaconda`\n\n`>  source activate yourNewEnvironment`\n\n**Step 2:** Installing OpenCV\n\nOnce you have Anaconda installed, first double check you are in your Python 3 environment:\n\n`>python`    \n`Python 3.5.2 |Anaconda 4.1.1 (x86_64)| (default, Jul  2 2016, 17:52:12)`  \n`[GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)] on darwin`  \n`Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.`  \n`>>>`   \n(Ctrl-d to exit Python)\n\nrun the following commands at the terminal prompt to get OpenCV:\n\n`> pip install pillow`  \n`> conda install -c https://conda.anaconda.org/menpo opencv3`\n\nthen to test if OpenCV is installed correctly:\n\n`> python`  \n`>>> import cv2`  \n`>>>`  \n(Ctrl-d to exit Python)\n\n**Step 3:** Installing moviepy  \n\nWe recommend the ""moviepy"" package for processing video in this project (though you\'re welcome to use other packages if you prefer).  \n\nTo install moviepy run:\n\n`>pip install moviepy`  \n\nand check that the install worked:\n\n`>python`  \n`>>>import moviepy`  \n`>>>`  \n(Ctrl-d to exit Python)\n\n**Step 4:** Opening the code in a Jupyter Notebook\n\nYou will complete this project in a Jupyter notebook.  If you are unfamiliar with Jupyter Notebooks, check out <A HREF=""https://www.packtpub.com/books/content/basics-jupyter-notebook-and-python"" target=""_blank"">Cyrille Rossant\'s Basics of Jupyter Notebook and Python</A> to get started.\n\nJupyter is an ipython notebook where you can run blocks of code and see results interactively.  All the code for this project is contained in a Jupyter notebook. To start Jupyter in your browser, run the following command at the terminal prompt (be sure you\'re in your Python 3 environment!):\n\n`> jupyter notebook`\n\nA browser window will appear showing the contents of the current directory.  Click on the file called ""P1.ipynb"".  Another browser window will appear displaying the notebook.  Follow the instructions in the notebook to complete the project.  \n'",Finding lanes lines on the road in Video Streaming
https://github.com/jimod/deeplearning-meetup-dublin,b'# deeplearning-meetup-dublin\nRepository for any code for the deep learning meetups currently contains \n\n# Meetup @ Intercom\n\n## Intro to Neural Nets -- from shallow to deep \n\n\n\n\n',Repository for any code for the deep learning meetups
https://github.com/sofianhw/spark-sql,b'# spark-sql\ndata ingestion with spark\n\nWill update soon\n\nhttp://spark.apache.org/docs/latest/\n',data ingestion with spark
https://github.com/JonathanReeve/dataviz-workshop,"b'# dataviz-workshop\nMaterials for a workshop in text analysis and visualization, originally given at Columbia University in April 2016. The \n'","Materials for a workshop in text analysis and visualization, originally given at Columbia University in April 2016. "
https://github.com/srikanth261/sentiment140,b'# sentiment140\nTwitter sentiment analysis\n',Twitter sentiment analysis
https://github.com/xn8812/caffe,"b'# Caffe\n\nCaffe is a deep learning framework made with expression, speed, and modularity in mind.\nIt is developed by the Berkeley Vision and Learning Center ([BVLC](http://bvlc.eecs.berkeley.edu)) and community contributors.\n\nCheck out the [project site](http://caffe.berkeleyvision.org) for all the details like\n\n- [DIY Deep Learning for Vision with Caffe](https://docs.google.com/presentation/d/1UeKXVgRvvxg9OUdh_UiC5G71UMscNPlvArsWER41PsU/edit#slide=id.p)\n- [Tutorial Documentation](http://caffe.berkeleyvision.org/tutorial/)\n- [BVLC reference models](http://caffe.berkeleyvision.org/model_zoo.html) and the [community model zoo](https://github.com/BVLC/caffe/wiki/Model-Zoo)\n- [Installation instructions](http://caffe.berkeleyvision.org/installation.html)\n\nand step-by-step examples.\n\n[![Join the chat at https://gitter.im/BVLC/caffe](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/BVLC/caffe?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\nPlease join the [caffe-users group](https://groups.google.com/forum/#!forum/caffe-users) or [gitter chat](https://gitter.im/BVLC/caffe) to ask questions and talk about methods and models.\nFramework development discussions and thorough bug reports are collected on [Issues](https://github.com/BVLC/caffe/issues).\n\nHappy brewing!\n\n## License and Citation\n\nCaffe is released under the [BSD 2-Clause license](https://github.com/BVLC/caffe/blob/master/LICENSE).\nThe BVLC reference models are released for unrestricted use.\n\nPlease cite Caffe in your publications if it helps your research:\n\n    @article{jia2014caffe,\n      Author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},\n      Journal = {arXiv preprint arXiv:1408.5093},\n      Title = {Caffe: Convolutional Architecture for Fast Feature Embedding},\n      Year = {2014}\n    }\n'",for testing
https://github.com/nkern/Astro_9,"b'### Course Repository for Astro 9\n\nSee the <a href=""https://nkern.github.io/Astro_9/index.html"">course webpage</a> for more details.\n'","Python Programming in Astronomy, Summer 2017"
https://github.com/andrew-reece/cs171-final-project,"b'This link contains our entire project, including screencast and process book.\n<br /><br />\nhttp://people.fas.harvard.edu/~reece/171/seenet/'",Evolution of human social networks (MIT Reality Mining data)
https://github.com/pottava/mxnet-char-lstm,"b'# \xe8\xa4\x87\xe6\x95\xb0\xe3\x83\xac\xe3\x82\xa4\xe3\x83\xa4 LSTM \xe3\x83\x8d\xe3\x83\x83\xe3\x83\x88\xe3\x83\xaf\xe3\x83\xbc\xe3\x82\xaf\xef\xbc\x88MXNet \xe5\xae\x9f\xe8\xa3\x85\xef\xbc\x89\n\n## \xe3\x83\x81\xe3\x83\xa5\xe3\x83\xbc\xe3\x83\x88\xe3\x83\xaa\xe3\x82\xa2\xe3\x83\xab\n\n\xe3\x81\xbe\xe3\x81\x9a\xe3\x81\xaf\xe6\x89\x8b\xe8\xa8\xb1\xe3\x81\xae\xe7\x92\xb0\xe5\xa2\x83\xe3\x81\xa7\xe3\x81\xa9\xe3\x82\x93\xe3\x81\xaa\xe5\xad\xa6\xe7\xbf\x92\xe3\x83\xbb\xe6\x8e\xa8\xe8\xab\x96\xe3\x81\x8c\xe3\x81\xa7\xe3\x81\x8d\xe3\x82\x8b\xe3\x81\xae\xe3\x81\x8b\xe3\x82\x92\xe8\xa9\xa6\xe3\x81\x97\xe3\x81\xa6\xe3\x81\xbf\xe3\x81\xbe\xe3\x81\x97\xe3\x82\x87\xe3\x81\x86\xe3\x80\x82  \n\n### 1. docker-compose \xe3\x81\xae\xe3\x82\xa4\xe3\x83\xb3\xe3\x82\xb9\xe3\x83\x88\xe3\x83\xbc\xe3\x83\xab\n\n\xe4\xbb\xa5\xe4\xb8\x8b\xe3\x81\xae\xe3\x83\xaa\xe3\x83\xb3\xe3\x82\xaf\xe3\x81\x8b\xe3\x82\x89\xe3\x80\x81Docker \xe3\x82\x92\xe3\x82\xa4\xe3\x83\xb3\xe3\x82\xb9\xe3\x83\x88\xe3\x83\xbc\xe3\x83\xab\xe3\x81\x97\xe3\x81\xa6\xe3\x81\x8f\xe3\x81\xa0\xe3\x81\x95\xe3\x81\x84\xe3\x80\x82\n\n- [Mac](https://docs.docker.com/docker-for-mac/install/#download-docker-for-mac)\n- [Windows](https://docs.docker.com/docker-for-windows/install/)\n\n\xe3\x82\xbf\xe3\x83\xbc\xe3\x83\x9f\xe3\x83\x8a\xe3\x83\xab\xe3\x81\xa7\xe3\x80\x81docker-compose \xe3\x82\x82\xe5\x90\x8c\xe6\x99\x82\xe3\x81\xab\xe3\x82\xa4\xe3\x83\xb3\xe3\x82\xb9\xe3\x83\x88\xe3\x83\xbc\xe3\x83\xab\xe3\x81\x95\xe3\x82\x8c\xe3\x81\x9f\xe3\x81\x93\xe3\x81\xa8\xe3\x82\x92\xe7\xa2\xba\xe8\xaa\x8d\xe3\x81\x97\xe3\x81\xbe\xe3\x81\x99\xe3\x80\x82\n\n```\n$ docker-compose -v\n\ndocker-compose version 1.11.2, build dfed245\n```\n\n### 2. docker-compose \xe3\x81\xa7 Jupyter \xe3\x82\x92\xe8\xb5\xb7\xe5\x8b\x95\n\n\xe4\xbb\xa5\xe4\xb8\x8b\xe3\x81\xae\xe3\x82\xb3\xe3\x83\x9e\xe3\x83\xb3\xe3\x83\x89\xe3\x81\xa7 Jupyter \xe3\x82\x92\xe8\xb5\xb7\xe5\x8b\x95\xe3\x81\x97\xe3\x81\xbe\xe3\x81\x99\n\n```\n$ docker-compose up\n\nCreating network ""mxnetcharlstm_default"" with the default driver\nCreating jupyter\nAttaching to jupyter\njupyter    | [I 20:36:10.489 NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret\njupyter    | [I 20:36:10.507 NotebookApp] Serving notebooks from local directory: /root/notebook\njupyter    | [I 20:36:10.507 NotebookApp] 0 active kernels\njupyter    | [I 20:36:10.509 NotebookApp] The Jupyter Notebook is running at: http://0.0.0.0:8888/?token=ac29066ff4b4e131ea28317aed8b63069ba9a5ae410e2d18\njupyter    | [I 20:36:10.509 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\njupyter    | [C 20:36:10.510 NotebookApp]\njupyter    |\njupyter    |     Copy/paste this URL into your browser when you connect for the first time,\njupyter    |     to login with a token:\njupyter    |         http://0.0.0.0:8888/?token=ac29076ff4b4e131ea28317aed8b63069ba9a5ae410e2d17\n```\n\n### 3. \xe3\x83\x96\xe3\x83\xa9\xe3\x82\xa6\xe3\x82\xb6\xe3\x81\xa7 Juputer \xe3\x82\x92\xe9\x96\x8b\xe3\x81\x8d\xe3\x81\xbe\xe3\x81\x99\n\n[http://localhost:8888](http://localhost:8888)\n\n\xe3\x83\x88\xe3\x83\xbc\xe3\x82\xaf\xe3\x83\xb3\xe3\x82\x92\xe8\xa6\x81\xe6\xb1\x82\xe3\x81\x95\xe3\x82\x8c\xe3\x82\x8b\xe3\x81\xae\xe3\x81\xa7\xe3\x80\x81\xe3\x82\xbf\xe3\x83\xbc\xe3\x83\x9f\xe3\x83\x8a\xe3\x83\xab\xe3\x81\xab\xe8\xa1\xa8\xe7\xa4\xba\xe3\x81\x95\xe3\x82\x8c\xe3\x81\xa6\xe3\x81\x84\xe3\x82\x8b\xe3\x83\x88\xe3\x83\xbc\xe3\x82\xaf\xe3\x83\xb3\xe3\x82\x92\xe5\x85\xa5\xe5\x8a\x9b\xe3\x81\x97\xe3\x81\xbe\xe3\x81\x99\xe3\x80\x82  \n`tutorials.ipynb` \xe3\x82\x92\xe3\x82\xaf\xe3\x83\xaa\xe3\x83\x83\xe3\x82\xaf\xe3\x81\x97\xe3\x81\xa6\xe9\x96\x8b\xe3\x81\x8d\xe3\x80\x81\xe3\x83\x81\xe3\x83\xa5\xe3\x83\xbc\xe3\x83\x88\xe3\x83\xaa\xe3\x82\xa2\xe3\x83\xab\xe3\x82\x92\xe9\x96\x8b\xe5\xa7\x8b\xe3\x81\x97\xe3\x81\xbe\xe3\x81\x99\xe3\x80\x82\n\n### 4. \xe7\x92\xb0\xe5\xa2\x83\xe3\x81\xae\xe7\xa0\xb4\xe6\xa3\x84\n\n\xe3\x83\x81\xe3\x83\xa5\xe3\x83\xbc\xe3\x83\x88\xe3\x83\xaa\xe3\x82\xa2\xe3\x83\xab\xe3\x81\x8c\xe7\xb5\x82\xe3\x82\x8f\xe3\x81\xa3\xe3\x81\x9f\xe3\x82\x89\xe3\x80\x81\xe3\x82\xbf\xe3\x83\xbc\xe3\x83\x9f\xe3\x83\x8a\xe3\x83\xab\xe3\x81\xab\xe3\x82\x82\xe3\x81\xa9\xe3\x82\x8a `Ctrl + C` \xe3\x81\xa7\xe3\x83\xad\xe3\x82\xb0\xe7\x9b\xa3\xe8\xa6\x96\xe3\x81\x8b\xe3\x82\x89\xe6\x8a\x9c\xe3\x81\x91  \n\xe4\xbb\xa5\xe4\xb8\x8b\xe3\x81\xae\xe3\x82\xb3\xe3\x83\x9e\xe3\x83\xb3\xe3\x83\x89\xe3\x81\xa7\xe5\xae\x8c\xe5\x85\xa8\xe3\x81\xab Jupyter \xe3\x82\x92\xe5\x81\x9c\xe6\xad\xa2\xe3\x81\x97\xe3\x81\xbe\xe3\x81\x97\xe3\x82\x87\xe3\x81\x86\xe3\x80\x82\n\n```\n$ docker-compose down -v\n\nStopping jupyter ... done\nRemoving jupyter ... done\nRemoving network mxnetcharlstm_default\n```\n'",A multilayer LSTM network with MXNet.
https://github.com/SujathaSubramanian/Machine-Learning--UW,b'\n### Machine Learning Specialization\n\nThese are course related working and projects for the 6 course specialization on Machine Learning\nfrom University of Washigton\n\n\n',Coursework ML Specialization
https://github.com/harshadss/my-presentations,b'# Presentations\n\nContains code files and presentation files for all the workshops and \npresentations that I conduct.\n',"Contains codes, presentation files for all the workshops, presentations that I conduct"
https://github.com/reubano/lambdaconf-tutorial,"b'[![Binder](http://mybinder.org/badge.svg)](http://beta.mybinder.org/v2/gh/reubano/lambdaconf-tutorial/master)\n\n# lambdaconf-tutorial\n\nMaterials for the LambdaConf tutorial, ""A Functional Programming approach to data  processing in Python"".\nPresentation slides can be found at https://speakerdeck.com/reubano/a-functional-programming-approach-to-data-processing-in-python\n\n## Preparation\n\nThis is an interactive (hands-on) tutorial. As such, you can choose to follow along [locally](#local) on your laptop, or [remotely](#remote) in a [binder notebook](http://beta.mybinder.org/v2/gh/reubano/lambdaconf-tutorial/master).\n\n### Local\n\nClone this repo\n\n```bash\ngit clone https://github.com/reubano/lambdaconf-tutorial.git\ncd lambdaconf-tutorial\n```\n\nMake sure you have a recent version of [Python 3 and pip 3](http://docs.python-guide.org/en/latest/starting/installation/)\n\n```bash\n# follow directions for your platform from the above link above\n\npython3 --version\n# Python 3.6.1\n\npip3 install --upgrade pip\npip3 --version\n# pip 9.0.1 from ...\n```\n\n(Optional) Setup and activate a [virtual environment](http://docs.python-guide.org/en/latest/dev/virtualenvs/#virtualenvironments-ref)\n\n```bash\npip3 install virtualenv\nvirtualenv lambdaconf-tutorial\nsource lambdaconf-tutorial/bin/activate\n```\n\n(Optional) install [iPython](https://ipython.readthedocs.io/en/stable/interactive/tutorial.html#the-four-most-helpful-commands), an enhanced Python shell\n\n```bash\n# if you are working in a virtualenv\npip install ipython\n\n# if you are *not* in a virtualenv\npip3 install --user ipython\n```\n\nInstall the packages to be used during the workshop\n\n```bash\n# if you are working in a virtualenv\npip install riko==0.51.0\n\n# if you are *not* in a virtualenv\npip3 install --user riko==0.51.0\n```\n\nStart the interactive Python\n\n```bash\n# if you installed iPython\nipython\n\n# if you did *not* install iPython\npython3\n```\n\nYou should now be in an interactive shell that looks something like this:\n\n```\nPython 3.6.1 (default, May 24 2017, 01:02:17)\n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)] on darwin\nType ""help"", ""copyright"", ""credits"" or ""license"" for more information.\n>>>\n```\n\nIn this interactive shell, you can enter any valid Python and immediately see the result.\n\n```\n>>> 1 + 3\n4\n>>> x = 1 + 3\n>>> x\n4\n```\n\nTo play around with the code, view the following files in your text editor of your choice.\n\n- [Presentation](https://github.com/reubano/lambdaconf-tutorial/blob/master/presentation.py): use to follow along as I talk, and reference during the exercises\n- [Exercises](https://github.com/reubano/lambdaconf-tutorial/blob/master/exercises.py): test your own code by typing `python3 exercises.py` in the terminal (*NOT* a python shell)\n- [Solutions](https://github.com/reubano/lambdaconf-tutorial/blob/master/solutions.py): see how the solution should look by typing `python3 solutions.py` in the terminal (*NOT* a python shell)\n\n### Remote\n\nTo play around with the code, [visit mybinder](http://beta.mybinder.org/v2/gh/reubano/lambdaconf-tutorial/master) and select the appropriate notebook.\n\n- Presentation (`presentation.ipynb`): use to follow along as I talk, and reference during the exercises\n- Exercises (`exercises.ipynb`): test your own code by typing directly into your browser\n- Solutions (`solution.ipynb`): interact with the solution and view the intermediate results\n\n## A few tips\n\n- if you are new to Python, browse through the [Python tutorial](https://docs.python.org/3.6/tutorial/index.html)\n- if you are new to meza, checkout the [meza readme](https://github.com/reubano/meza/blob/master/README.rst#hello-world)\n'","LambdaConf ""A Functional Programming Approach To Data Processing In Python"" tutorial materials "
https://github.com/jacobeisenstein/gt-nlp-class,"b'CS 4650 and 7650\n==========\n\n(**Note about registration**: registration is currently restricted to students pursuing CS degrees for which this course is an essential requirement. Unfortunately, the enrollment is already at the limit of the classroom space, so this restriction is unlikely to be lifted.)\n\n- **Course**: Natural Language Understanding\n- **Instructor**: Jacob Eisenstein\n- **Semester**: Spring 2018\n- **Time**: Mondays and Wednesdays, 3:00-4:15pm\n- **TAs**: Murali Raghu Babu, James Mullenbach, Yuval Pinter, Zhewei Sun\n- [Schedule](https://docs.google.com/spreadsheets/d/1BuvRjPhfHmy7XAfpc5KoygdfqI3Cue3bbmiO6yYuX_E/edit?usp=sharing)\n- [Recaps](https://docs.google.com/document/d/1loefqZhmOaF2mP8yQPEx91jZ7BHylWixVtYlFhpIlGM/edit?usp=sharing) from previous classes\n\nThis course gives an overview of modern data-driven techniques for natural language processing. The course moves from shallow bag-of-words models to richer structural representations of how words interact to create meaning. At each level, we will discuss the salient linguistic phemonena and most successful computational models. Along the way we will cover machine learning techniques which\nare especially relevant to natural language processing.\n\n- [Readings](#readings)\n- [Grading](#grading)\n- [Help](#help)\n- [Policies](#policies)\n\n# Learning goals\n<a name=""learning""/>\n\n- Acquire the fundamental linguistic concepts that are relevant to language technology. This goal will be assessed in the short homework assignments and the exams.\n- Analyze and understand state-of-the-art algorithms and statistical techniques for reasoning about linguistic data. This goal will be assessed in the exams and the assigned projects.\n- Implement state-of-the-art algorithms and statistical techniques for reasoning about linguistic data. This goal will be assessed in the assigned projects.\n- Adapt and apply state-of-the-art language technology to new problems and settings. This goal will be assessed in assigned projects.\n- (7650 only) Read and understand current research on natural language processing. This goal will be assessed in assigned projects.\n\n# Readings #\n<a name=""readings""/>\n\nReadings will be drawn mainly from my [notes](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes). Additional readings may be assigned from published papers, blogposts, and tutorials.\n\n## Supplemental textbooks ##\n\nThese are completely optional, but might deepen your understanding of the material.\n\n- [Speech and Language Processing](http://www.amazon.com/Speech-Language-Processing-2nd-Edition/dp/0131873210/) is the textbook most often used in NLP courses. It\'s a great reference for both the linguistics and algorithms we\'ll encounter in this course. Several chapters from the upcoming [third edition](https://web.stanford.edu/~jurafsky/slp3/) are free online.\n- [Natural Language Processing with Python](http://www.amazon.com/Natural-Language-Processing-Python-Steven/dp/0596516495)\nshows how to do hands-on work with Python\'s Natural Language Toolkit (NLTK), and also brings a strong linguistic perspective.\n- [Schaum\'s Outline of Probability and Statistics](http://www.amazon.com/Schaums-Outline-Probability-Statistics-Edition/dp/007179557X/ref=pd_sim_b_1?ie=UTF8&refRID=1R57HWNCW6EEWD1ZRH4C) can help you review the probability and statistics that we use in this course.\n\n# Grading\n<a name=""grading""/>\n\nThe graded material for the course will consist of:\n\n- Seven short homework assignments, of which you must do six. Most of these involve performing linguistic annotation on some text of your choice. The purpose is to get a basic understanding of key linguistic concepts. Each assignment should take less than an hour. Each homework is worth 2 points (12 total). (Many of these homeworks are implemented at **quizzes** on Canvas.)\n- Four assigned problem sets. These involve building and using NLP techniques which are at or near the state-of-the-art. The purpose is to learn how to implement natural language processing software, and to have fun. These assignments must be done individually. Each problem set is worth ten points (48 total). Students enrolled in CS 7650 will have an additional, research-oriented component to the problem sets.\n- An in-class midterm exam, worth 20 points, and a final exam, worth 20 points. The purpose of these exams is to assess understanding of the core theoretical concepts, and to encourage you to review and synthesize your understanding of these concepts. \n\nBarring a personal emergency or an institute-approved absence, you must take each exam on the day indicated in the schedule. Job interviews and travel plans are generally not a reason for an institute-approved absence. See [here](https://registrar.gatech.edu/info/institute-approved-absence-form-for-students) for more information on GT policy about absences.\n\n## Late policy\n\nProblem sets will be accepted up to 72 hours late, at a penalty of 2 points per 24 hours. (Maximum score after missing the deadline: 10/12; maximum score 24 hours after the deadline: 8/12, etc.)  It is usually best just to turn in what you have at the due date. Late homeworks will not be accepted. This late policy is intended to ensure fair and timely evaluation.\n\n# Getting help\n<a name=""help""/>\n\n## Office hours\n\nMy office hours follow Wednesday classes (4:15-5:15PM) and take place in class when available.\n\nTA office hours are in CCB commons (1st floor) unless otherwise announced on Piazza.\n- Murali: Friday   10AM-11AM\n- James:  Thursday 11AM-12PM\n- Yuval:  Tuesday  3PM-4PM\n- Zhewei: Monday   1PM-2PM\n\n## Online help\n\nPlease use Piazza rather than personal email to ask questions. This helps other students, who may have the same question. Personal emails may not be answered. If you cannot make it to office hours, please use Piazza to make an appointment. It is unlikely that I will be able to chat if you make an unscheduled visit to my office. The same is true for the TAs.\n\n# Class policies\n<a name=""policies""/>\n\nAttendance will not be taken, but **you are responsible for knowing what happens in every class**. If you cannot attend class, make sure you check up with someone who was there.\n\nRespect your classmates and your instructor by preventing distractions. This means be on time, turn off your cellphone, and save side conversations for after class. If you can\'t read something I wrote on the board, or if you think I made a mistake in a derivation, please raise your hand and tell me!\n\n**Using a laptop in class is likely to reduce your education attainment**. This has been documented by multiple studies, which are nicely summarized in the following article:\n\n- https://www.nytimes.com/2017/11/22/business/laptops-not-during-lecture-or-meeting.html\n\nI am not going to ban laptops, as long as they are not a distraction to anyone but the user. But I suggest you try pen and paper for a few weeks, and see if it helps.\n\n## Prerequisites\n<a name=""prerequisites""/>\n\nThe official prerequisite for CS 4650 is CS 3510/3511, ""Design and Analysis of Algorithms."" This prerequisite is essential because understanding natural language processing algorithms requires familiarity with dynamic programming, as well as automata and formal language theory: finite-state and context-free languages, NP-completeness, etc. While course prerequisites are not enforced for graduate students, prior exposure to analysis of algorithms is very strongly recommended.\n\nFurthermore, this course assumes:\n\n- Good coding ability, corresponding to at least a third or fourth-year undergraduate CS major. Assignments will be in Python.\n- Background in basic probability, linear algebra, and calculus.\n\nPeople sometimes want to take the course without having all of these\nprerequisites. Frequent cases are:\n\n- Junior CS students with strong programming skills but limited theoretical and mathematical background,\n- Non-CS students with strong mathematical background but limited programming experience.\n\nStudents in the first group suffer in the exam and don\'t understand the lectures, and students in the second group suffer in the problem sets. My advice is to get the background material first, and\nthen take this course.\n\n## Collaboration policy\n\nOne of the goals of the assigned work is to assess your individual progress in meeting the learning objectives of the course. You may discuss the homework and projects with other students, but your work must be your own -- particularly all coding and writing. For example:\n\n### Examples of acceptable collaboration\n\n- Alice and Bob discuss alternatives for storing large, sparse vectors of feature counts, as required by a problem set.\n- Bob is confused about how to implement the Viterbi algorithm, and asks Alice for a conceptual description of her strategy.\n- Alice asks Bob if he encountered a failure condition at a ""sanity check"" in a coding assignment, and Bob explains at a conceptual level how he overcame that failure condition.\n- Alice is having trouble getting adequate performance from her part-of-speech tagger. She finds a blog page or research paper that gives her some new ideas, which she implements.\n\n### Examples of unacceptable collaboration\n\n- Alice and Bob work together to write code for storing feature counts.\n- Alice and Bob divide the assignment into parts, and each write the code for their part, and then share their solutions with each other to complete the assignment.\n- Alice or Bob obtain a solution to a previous year\'s assignment or to a related assignment in another class, and use it as the starting point for their own solutions.\n- Bob is having trouble getting adequate performance from his part-of-speech tagger. He finds source code online, and copies it into his own submission.\n- Alice wants to win the Kaggle competition for a problem set. She finds the test set online, and customizes her submission to do well on it.\n\nSome assignments will involve written responses. Using other people\xe2\x80\x99s text or figures without attribution is plagiarism, and is never acceptable.\n\nSuspected cases of academic misconduct will be (and have been!) referred to the Honor Advisory Council. For any questions involving these or any other Academic Honor Code issues, please consult me, my teaching assistants, or http://www.honor.gatech.edu.\n'","Course materials for Georgia Tech CS 4650 and 7650, ""Natural Language"""
https://github.com/psi4/psi4,"b'# <img src=""https://github.com/psi4/psi4media/blob/master/logos-psi4/psi4square.png"" height=150>\n\n| **Status** | [![Azure DevOps builds](https://img.shields.io/azure-devops/build/psi4/e80489d7-9619-4512-8e7b-255e355b3ab8/1?logo=azure%20devops)](https://dev.azure.com/psi4/psi4/_build?definitionId=1) [![Codecov coverage](https://img.shields.io/codecov/c/github/psi4/psi4.svg?logo=Codecov&logoColor=white)](https://codecov.io/gh/psi4/psi4) |\n| :------ | :------- |\n| **Latest Release** | [![Last release tag](https://img.shields.io/github/release/psi4/psi4.svg)](https://github.com/psi4/psi4/releases)  [![Commits since release](https://img.shields.io/github/commits-since/psi4/psi4/v1.8.svg)](https://github.com/psi4/psi4/releases/tag/v1.8) [![python](https://img.shields.io/badge/python-3.8%2C%203.9%2C%203.10%2C%203.11-blue.svg)](https://psicode.org/psi4manual/master/introduction.html#supported-systems) |\n| **Communication** | [![User site](https://img.shields.io/badge/home-Psi4-5077AB.svg)](https://psicode.org/) [![docs latest](https://img.shields.io/badge/docs-latest-5077AB.svg?logo=read%20the%20docs)](https://psicode.org/psi4manual/master/index.html) [![chat on forum](https://img.shields.io/badge/chat-on_forum-808493.svg?logo=Discourse&logoColor=white)](http://forum.psicode.org/) [![dev chat on slack](https://img.shields.io/badge/dev_chat-on_slack-808493.svg?logo=slack)](https://join.slack.com/t/psi4/shared_invite/zt-5s36s4rb-SQH6_AWyfWOqlKYN3cFs4Q) |\n| **Foundation** | [![license](https://img.shields.io/github/license/psi4/psi4.svg)](https://opensource.org/licenses/LGPL-3.0) [![platforms](https://img.shields.io/badge/Platforms-Linux%2C%20MacOS%2C%20MacOS%20Silicon%2C%20Windows%2C%20Windows%20WSL-orange.svg)](https://psicode.org/psi4manual/master/introduction.html#supported-systems) [![python](https://img.shields.io/badge/python-3.8%2C%203.9%2C%203.10%2C%203.11-blue.svg)](https://psicode.org/psi4manual/master/introduction.html#supported-systems) |\n| **Installation** | [![obtain latest](https://img.shields.io/badge/obtain-latest-green.svg)](https://psicode.netlify.com/installs/latest) [![Conda](https://img.shields.io/conda/v/conda-forge/psi4.svg)](https://anaconda.org/conda-forge/psi4) [![Anaconda-Server Badge](https://anaconda.org/conda-forge/psi4/badges/latest_release_relative_date.svg)](https://anaconda.org/conda-forge/psi4) |\n| **Demo** | [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/psi4/psi4/56fbc7787af67dabdf1897d0dfe4263d8d97e241?urlpath=lab%2Ftree%2Fdoc%2Fsphinxman%2Fsource%2Fpsiapi.ipynb) |\n\n<!--  -->\n<!-- [![Last release date](https://img.shields.io/github/release-date/psi4/psi4.svg)](https://github.com/psi4/psi4/releases) -->\n<!-- [![Anaconda-Server Badge](https://anaconda.org/psi4/psi4/badges/version.svg)](https://anaconda.org/psi4/psi4) -->\n\n<!--<a href=""https://psi4.slack.com/messages""> <img src=""https://img.shields.io/badge/dev_chat-on_slack-808493.svg"" /></a>\n<a href=""mailto:psi4aiqc+slackinvite@gmail.com?subject=request slack invite (incl. who, where, email)""> <img src=""https://img.shields.io/badge/dev_chat-invite-808493.svg"" /></a> -->\n\n<!--[![Anaconda-Server Badge](https://anaconda.org/psi4/psi4/badges/installer/conda.svg)](https://anaconda.org/psi4/psi4) \n[![Anaconda-Server Badge](https://anaconda.org/psi4/psi4/badges/platforms.svg)](https://anaconda.org/psi4/psi4) -->\n\n<!--\n| **PR Activity** | \n[![commit activity](https://img.shields.io/github/commit-activity/y/psi4/psi4.svg)](https://github.com/psi4/psi4/graphs/contributors) \n[![issues-pr-closed](https://img.shields.io/github/issues-pr-closed-raw/psi4/psi4.svg)](https://github.com/psi4/psi4/pulls)\n-->\n\nPsi4 is an open-source suite of *ab initio* quantum chemistry programs\ndesigned for efficient, high-accuracy simulations of\nmolecular properties. We routinely perform computations with >2500 basis functions on multi-core machines.\n\nWith computationally demanding portions written in C++, exports\nof many C++ classes into Python via Pybind11, and a flexible Python driver, Psi4\nstrives to be friendly to both users and developers.\n\n* **Users\' Website**  www.psicode.org\n\n* **Downloading and Installing Psi4** https://psicode.org/psi4manual/master/build_faq.html (for the CMake adept, see [CMakeLists.txt](CMakeLists.txt#L27)\n\n* **Manual**  [http://bit.ly/psi4manual](https://psicode.org/psi4manual/master/index.html) (built nightly from master branch) or https://psicode.org/psi4manual/1.4.0/index.html (last release)\n\n* **Tutorial** https://psicode.org/psi4manual/master/tutorial.html for Psithon (``psi4 job.in``), https://psicode.org/psi4manual/master/psiapi.html for PsiAPI (``python job.py``)\n\n* **Forum** http://forum.psicode.org\n\n* **Communication & Support** https://psicode.org/psi4manual/master/introduction.html#technical-support\n\n* **GitHub**  https://github.com/psi4/psi4 (authoritative repository)\n\n* **Continuous Integration Status** [![Azure DevOps builds](https://img.shields.io/azure-devops/build/psi4/e80489d7-9619-4512-8e7b-255e355b3ab8/1/master.svg?logo=azure%20devops)](https://dev.azure.com/psi4/psi4/_build?definitionId=1) on Linux and Windows\n\n* **Anaconda**  https://anaconda.org/psi4 (binary available for Linux, Mac, Mac Silicon, Windows, and WSL Windows [![Binstar Badge](https://anaconda.org/psi4/psi4/badges/downloads.svg)](https://anaconda.org/psi4/psi4) ) [![Binstar Badge](https://anaconda.org/conda-forge/psi4/badges/downloads.svg)](https://anaconda.org/conda-forge/psi4) ) [instructions](https://psicode.org/psi4manual/master/conda.html#how-to-install-a-psi4-binary-with-the-psi4conda-installer-download-site)\n\n* **Coverage** Python and C++ source code lines hit by running most of the test suite. [![codecov](https://img.shields.io/codecov/c/github/psi4/psi4.svg?logo=Codecov&logoColor=white)](https://codecov.io/gh/psi4/psi4)\n\n* **Interested Developers**  https://psicode.org/developers.php (replacement page needed) (welcome to fork psi4/psi4 and follow [GitHub contribution procedure](https://psicode.org/psi4manual/master/build_obtaining.html#faq-githubworkflow)) [![PRs welcome](https://img.shields.io/badge/PRs-welcome-yellow.svg)](http://makeapullrequest.com)\n\n* **Sample Inputs**  http://www.psicode.org/psi4manual/master/testsuite.html (also in [`samples/`](samples))\n\n* **Download Tarball** https://github.com/psi4/psi4/releases \n\n<!--* **Build Dashboard** https://testboard.org/cdash/index.php?project=Psi\n\n* **YouTube Channel** https://www.youtube.com/psitutorials-->\n\n\nLicense [![license](https://img.shields.io/github/license/psi4/psi4.svg)](https://opensource.org/licenses/LGPL-3.0)\n=======\n\nPsi4: an open-source quantum chemistry software package\n\nCopyright (c) 2007-2023 The Psi4 Developers.\n\nThe copyrights for code used from other parties are included in\nthe corresponding files.\n\nPsi4 is free software; you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, version 3.\n\nPsi4 is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU Lesser General Public License for more details.\n\nYou should have received a copy of the GNU Lesser General Public License along\nwith Psi4; if not, write to the Free Software Foundation, Inc.,\n51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n\nThe full text of the GNU Lesser General Public License (version 3) is included in the\nCOPYING.LESSER file of this repository, and can also be found\n[here](https://www.gnu.org/licenses/lgpl.txt).\n\n\nCitation [![doi](https://img.shields.io/badge/doi-10.1063/5.0006002-5077AB.svg)](https://doi.org/10.1063/5.0006002)\n========\n\nThe journal article reference describing Psi4 is:\n\nD. G. A. Smith, L. A. Burns, A. C. Simmonett, R. M. Parrish,\nM. C. Schieber, R. Galvelis, P. Kraus, H. Kruse, R. Di Remigio,\nA. Alenaizan, A. M. James, S. Lehtola, J. P. Misiewicz, M. Scheurer,\nR. A. Shaw, J. B. Schriber, Y. Xie, Z. L. Glick, D. A. Sirianni,\nJ. S. O\'Brien, J. M. Waldrop, A. Kumar, E. G. Hohenstein,\nB. P. Pritchard, B. R. Brooks, H. F. Schaefer III, A. Yu. Sokolov,\nK. Patkowski, A. E. DePrince III, U. Bozkaya, R. A. King,\nF. A. Evangelista, J. M. Turney, T. D. Crawford, C. D. Sherrill,\n""Psi4 1.4: Open-Source Software for High-Throughput Quantum Chemistry"",\nJ. Chem. Phys. 152(18) 184108 (2020).\n\n* [![doi](https://img.shields.io/badge/doi-10.1021/acs.jctc.7b00174-5077AB.svg)](https://doi.org/10.1021/acs.jctc.7b00174) for Psi4 v1.1\n* [![doi](https://img.shields.io/badge/doi-10.1021/acs.jctc.8b00286-5077AB.svg)](https://doi.org/10.1021/acs.jctc.8b00286) for Psi4NumPy\n* [![doi](https://img.shields.io/badge/doi-10.1002/wcms.93-5077AB.svg)](https://doi.org/10.1002/wcms.93) for Psi4 alpha releases\n* [![doi](https://img.shields.io/badge/doi-10.1002/jcc.20573-5077AB.svg)](https://doi.org/10.1002/jcc.20573) for Psi3\n'",Open-Source Quantum Chemistry – an electronic structure package in C++ driven by Python
https://github.com/sanghaisubham/Tic-Tac-Toe-Game,b'# Tic-Tac-Toe-Game\nA Common Tic Tac Toe Game Played Between the User and the System with the System Starting the game\n',A Common Tic Tac Toe Game Played Between the User and the System with the System Starting the game
https://github.com/Cyberdog52/Project,"b""\nBefore you start with the project:\n\n1. \tDownload validation, test and train sets and put them in this directory\n2. \tRun tf_record_to_numpy 3 times for each set (train, test, validation)\n\tThis will create pkl files in the same directory\n\tYou can delete the tf_record files now, don't need them anymore\n3.\tMerge the 18 validation pkl files into train folder. Rename them by hand as if they were train files (from 59 to 76)\n\tThere should now be 76 dataTrain pkl files in the train directory\n\tYou can delete the validation folder now, don't need them anymore\n4. \tRun produce_masked_inputs.py twice for train and test\n\tYou can delete dataTrain and dataTest, from now use newTrain and newTest\n\n\nImportant things:\n\nSome videos are shorter than 50 frames, interpolate them to 50\nIt is better to delete the first and last images of a video than keeping all\nSome segmentation images are blank, do not segment if they are blank\n""",UIE
https://github.com/mingot/lung_cancer_ds_bowl,"b'# Lung Cancer Data Science Bowl 2017\n\n## Introduction\nRepository for the Vila del Pingui team for the Data Science Bowl 2017 (Feb2017 to Apr2017). The competetition ($1M in prizes) was about predicting early stage lung cancer from CT Scan images. The training set was 1397 + 200 patients and the test 500 patients. The result is an ensemble of 3 convolutional neural networks (resnet) for feature generation and xgboost for final ensemble.  \n\nThe team ended in 34th position of 2000 teams (top 2%) with the best model scoring in the 17th position.\n\n## Index\nAccess to latest results of each team and to documentation\n\n  1. Preprocessing and datasets (README TBD)\n    1. [Utils (git)](https://github.com/mingot/lung_cancer_ds_bowl/tree/master/src/utils) \n    2. [Bad segmentation spreadsheet (gdocs)](https://docs.google.com/spreadsheets/d/15wi07edzdVLqpnviPI4qhO5gOA_Ve6MQ7RCE92YGERE/edit#gid=0)\n    3. Preprocessed v3 (AWS): /mnt/hd2/preprocessed3 \n  2. DL ([README](https://github.com/mingot/lung_cancer_ds_bowl/blob/master/src/jc_dl/README.md))\n    1. Slices: TBD\n    2. Segmentation: TBD\n  3. Final model (README TBD)\n    1. New features: TBD\n    2. XGBoost: TBD\n    3. Final learner - submission: TBD\n  4. Literature: \n    1. Preprocessing ([google drive](https://drive.google.com/drive/folders/0BwtD1eiXdLQRVXo4aXFYVDVZNHM))\n    2. DL ([google drive](https://drive.google.com/drive/folders/0BwtD1eiXdLQRMlhvTzJsZjNkeWs))\n    3. Features ([google drive](https://drive.google.com/drive/folders/0BwtD1eiXdLQRS1d3eVlwSVpCblU))\n\n## References quick start\nBasic references to understand the problem and the data:\n\n 1. [Video] (https://www.youtube.com/watch?v=-XUKq3B4sdw) how to detect a lung cancer from a physician perspective (15 min).\n 2. Notebooks (Kaggle Kernels) Understand the data set and dealing with DICOM files. \n   1. [Preprocessing tutorial](https://www.kaggle.com/gzuidhof/data-science-bowl-2017/full-preprocessing-tutorial): understanding DICOM files, pixel values, standarization, ...\n   2.  [Exploratory data analysis](https://www.kaggle.com/anokas/data-science-bowl-2017/exploratory-data-analysis): basic exploration of the given data set\n 3. [Kaggle tutorial] (https://www.kaggle.com/c/data-science-bowl-2017/details/tutorial) with code for training a CNN using the U-net network for medical image segmentation. Based on the external LUNA data set (annotated).\n 4. [TensorFlow ppt] (https://docs.google.com/presentation/d/1TVixw6ItiZ8igjp6U17tcgoFrLSaHWQmMOwjlgQY9co/edit#slide=id.p) for quickstart (focused on convnets) and code included. After it, you can take the [official TF tutorial](https://www.tensorflow.org/tutorials/deep_cnn/) as the sample code.\n\n## Quickstart\n [TBD]\n 1 - Download the repo:\n ```\n $ git clone https://github.com/mingot/lung_cancer_ds_bowl.git\n ```\n 2 - Create [virtual enviroment](http://docs.python-guide.org/en/latest/dev/virtualenvs/) (see `virtualenvwrapper`) and install python requirements\n```\n$ mkvirtualenv lung\n$(lung) pip install -r requirements.txt\n```\n\n## Jupyter\n - Estan ja instalats els paquets de `requirements.txt` amb el kernel de python2.\n - Cada usuari pot fer git pull/commit/push desde un ssh o amb `!git commit` .. desde la consola de jupyter. No demana contrasenya, el usuari queda identificat amb el email\n - Cada usuari t\xc3\xa9 el seu directori `~/lung_cancer_ds_bowl` privat per ell excepte la carpeta `~/lung_cancer_ds_bowl/data` que es compartida per tots.\n - Tots els usuaris tenen perm\xc3\xads de sudo aix\xc3\xad que si cal instalar paquets poden fer servir `!sudo pip install` paquet desde jupyter i aix\xc3\xad ser\xc3\xa0n accesibles per tots.\n\n## Available datasets\nSee docs/ \n\n## Preprocessing\nThe preprocessed images are stored at `/mnt/hd2/preprocessed/`. To open the compressed files from python use the following instruction:\n`np.load(file)[\'arr_0\']`. There is one file per patient. Eah file is a numpy array of 4 dimensions: `[type,slice,height,width]`. The dimension `type` contains the preprocessed image at index 0, the lung segmentation at index 1, and when available (luna dataset) the nodules segmentation at index 2. All the images have dimensions `height` and `weight` dimensions of 512x512.\n\n## General guidelines\n - The analysis files should start with the author initials.\n - Avoid storing files >50Mb in Git. In particular, images from data folder should be outside the git repository.\n\n## File structure\n\n\n```\n\xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 README.md          <- The top-level README for developers using this project.\n\xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 data\n\xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 external       <- Data from third party sources.\n\xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 interim        <- Intermediate data that has been transformed.\n\xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 processed      <- The final, canonical data sets for modeling.\n\xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 raw            <- The original, immutable data dump.\n\xe2\x94\x82\n\xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 docs               <- A default Sphinx project; see sphinx-doc.org for details\n\xe2\x94\x82\n\xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 models             <- Trained and serialized models, model predictions, or model summaries\n\xe2\x94\x82\n\xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),\n\xe2\x94\x82                         the creator\'s initials, and a short `-` delimited description, e.g.\n\xe2\x94\x82                         `1.0-jqp-initial-data-exploration`.\n\xe2\x94\x82\n\xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 references         <- Data dictionaries, manuals, and all other explanatory materials.\n\xe2\x94\x82\n\xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 reports            <- Generated analysis as HTML, PDF, LaTeX, etc.\n\xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 figures        <- Generated graphics and figures to be used in reporting\n\xe2\x94\x82\n\xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.\n\xe2\x94\x82                         generated with `pip freeze > requirements.txt`\n\xe2\x94\x82\n\xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 src                <- Source code for use in this project.\n\xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 __init__.py    <- Makes src a Python module\n\xe2\x94\x82   \xe2\x94\x82\n\xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 data           <- Scripts to download or generate data\n\xe2\x94\x82   \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 make_dataset.py\n\xe2\x94\x82   \xe2\x94\x82\n\xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 features       <- Scripts to turn raw data into features for modeling\n\xe2\x94\x82   \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 build_features.py\n\xe2\x94\x82   \xe2\x94\x82\n\xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 models         <- Scripts to train models and then use trained models to make\n\xe2\x94\x82   \xe2\x94\x82   \xe2\x94\x82                 predictions\n\xe2\x94\x82   \xe2\x94\x82   \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 predict_model.py\n\xe2\x94\x82   \xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 train_model.py\n\xe2\x94\x82   \xe2\x94\x82\n\xe2\x94\x82   \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 visualization  <- Scripts to create exploratory and results oriented visualizations\n\xe2\x94\x82       \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 visualize.py\n\xe2\x94\x82\n\xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 tox.ini            <- tox file with settings for running tox; see tox.testrun.org\n```\n\n## Troubleshoot\n\n### ""Could not find a version that satisfies the requirement SimpleITK==0.10.0""\n\nThe solution is to manually download the egg from the [official website](https://itk.org/SimpleITKDoxygen/html/PyDownloadPage.html) and install it with `easy_install`.\n\n### ""Fatal Python error: PyThreadState_Get: no current thread""\n```\n>>> import SimpleITK as sitk\n""Fatal Python error: PyThreadState_Get: no current thread""\n```\nThe solution is to relink the `_SimpleITK.so`:\n```\n$ otool -L ~/virtualenvs/lung/lib/python2.7/site-packages/SimpleITK/_SimpleITK.so \n/Users/mingot/virtualenvs/lung/lib/python2.7/site-packages/SimpleITK/_SimpleITK.so:\n\t/System/Library/Frameworks/Python.framework/Versions/2.7/Python (compatibility version 2.7.0, current version 2.7.1)\n\t/System/Library/Frameworks/CoreFoundation.framework/Versions/A/CoreFoundation (compatibility version 150.0.0, current version 635.19.0)\n\t/usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 159.1.0)\n\t/usr/lib/libstdc++.6.dylib (compatibility version 7.0.0, current version 52.0.0)\n$ sudo install_name_tool -change /System/Library/Frameworks/Python.framework/Versions/2.7/Python ~/virtualenvs/lung/.Python ~/virtualenvs/lung/lib/python2.7/site-packages/SimpleITK/_SimpleITK.so\n```\n'",Data Science Bowl 2017 for lung cancer prediction with Keras
https://github.com/NYC-OPCCR/2014-07-15,b'2014-07-15\n==========\n\nDREAM about solving AML\n',DREAM about solving AML
https://github.com/rodgzilla/fractal_GAN,b'# fractal_GAN\nA deep learning project where the goal is to train a generator to produce plausible fractal images.\n',A deep learning project where the goal is to train a generator to produce plausible fractal images.
https://github.com/akalathil/Eng100D_Project,"b'# GET REQUESTS #\n\nList out types of affliction\nhttps://eng100d-project.herokuapp.com/list\n  >>\n  ```\n  [\n    ""disease"", \n    ""injury"", \n    ""mental_health"", \n    ""nutrition""\n    ...\n  ]\n  ```\nList all afflictions\nhttps://eng100d-project.herokuapp.com/list/all\n  >>\n  ```\n  [\n    {\n      ""type"": ""disease"",\n      ""affliction"": ""ARI/Lower respiratory tract Infection"",\n      ""name"": ""ARI/Lower respiratory tract Infection"",\n      ""date"": 1497093051529,\n      ""views"": 0,\n      ""description"": ""ARI/Lower respiratory tract Infection"",\n      ""source"": ""OpenNepal"",\n      ""uploader"": ""Original""\n    },\n    {\n      ""type"": ""disease"",\n      ""affliction"": ""Asthama"",\n      ""name"": ""Asthama"",\n      ""date"": 1497093051529,\n      ""views"": 0,\n      ""description"": ""Asthama"",\n      ""source"": ""Original""\n    },...\n  ]\n```\nList out afflictions with underneath type of affliction\nhttps://eng100d-project.herokuapp.com/list/{type of affliction}\n\n  ex. https://eng100d-project.herokuapp.com/list/disease\n  >>\n  ```\n  [\n    ""ARI/Lower respiratory tract Infection"",\n    ""Asthama"",\n    ""Bronchitis"",\n    ""Confirmed Meningitis"",\n    ""HIV/AIDS"",\n    ""Leprosy"",\n    ""Pneumonia"",\n    ...\n  ]\n  ```\nSee Full Affliction Profile\nhttps://eng100d-project.herokuapp.com/list/{type of affliction}/{affliction}\n```\nNOTE: Replace ""/"" with %2F\n```\n  ex. https://eng100d-project.herokuapp.com/list/disease/HIV%2FAIDS\n  >>\n  ```\n{\n    ""Data"": {\n      ""cols"": [\n        ""Name"",\n        ""Year"",\n        ""Value"",\n        ""Latitude"",\n        ""Longtitude""\n      ],\n      ""rows"": ""[ {\\""0\\"":\\""Taplejung\\"",\\""1\\"":\\""2013\\\\/14\\"",\\""2\\"":0,\\""Latitude\\"":27.6257485,\\""Longitude\\"":87.7763333},                \n                {\\""0\\"":\\""Panchthar\\"",\\""1\\"":\\""2013\\\\/14\\"",\\""2\\"":0,\\""Latitude\\"":27.2036401,\\""Longitude\\"":87.8156715},\n               ...]""\n    },\n    ""info"": {\n    ""name"": ""HIV/AIDS"",\n    ""date"": 1497093051529,\n    ""views"": 0,\n    ""description"": ""HIV/AIDS"",\n    ""source"": ""OpenNepal"",\n    ""uploader"": ""Uploader""\n    }\n}\n```\nGet info of affliction\nhttps://eng100d-project.herokuapp.com/info/{type of affliction}/{affliction}\n\n  ex.  https://eng100d-project.herokuapp.com/info/disease/HIV%2FAIDS\n>>\n```\n{\n    ""name"": ""HIV/AIDS"",\n    ""date"": 1497503262232,\n    ""views"": 0,\n    ""description"": ""HIV/AIDS"",\n    ""source"": ""OpenNepal"",\n    ""uploader"": ""Original""\n}\n```\n\nGet both rows and cols of affliction\nhttps://eng100d-project.herokuapp.com/data/{type of affliction}/{affliction}\n\n  ex. https://eng100d-project.herokuapp.com/data/disease/HIV%2FAIDS\n>>\n```\n{\n  ""cols"": [\n    ""Name"",\n    ""Year"",\n    ""Value"",\n    ""Latitude"",\n    ""Longtitude""\n  ],\n  ""rows"": ""[\n            {\\""0\\"":\\""Taplejung\\"",\\""1\\"":\\""2013\\\\/14\\"",\\""2\\"":0,\\""Latitude\\"":27.6257485,\\""Longitude\\"":87.7763333},\n            {\\""0\\"":\\""Panchthar\\"",\\""1\\"":\\""2013\\\\/14\\"",\\""2\\"":0,\\""Latitude\\"":27.2036401,\\""Longitude\\"":87.8156715}\n            ...]""\n}\n```\n\nGet cols of affliction\nhttps://eng100d-project.herokuapp.com/data/{type of affliction}/{affliction}/cols\n\n  ex. https://eng100d-project.herokuapp.com/data/disease/HIV%2FAIDS/cols\n  >>\n  ```\n  [\n    ""Name"",\n    ""Year"",\n    ""Value"",\n    ""Latitude"",\n    ""Longtitude""\n  ]\n  ```\n\nGet rows of affliction\nhttps://eng100d-project.herokuapp.com/data/{type of affliction}/{affliction}/rows\n  \n  ex. https://eng100d-project.herokuapp.com/data/disease/HIV%2FAIDS/rows\n  >>\n```\n  ""[\n    {\\""0\\"":\\""Taplejung\\"",\\""1\\"":\\""2013\\\\/14\\"",\\""2\\"":0,\\""Latitude\\"":27.6257485,\\""Longitude\\"":87.7763333},\n    {\\""0\\"":\\""Panchthar\\"",\\""1\\"":\\""2013\\\\/14\\"",\\""2\\"":0,\\""Latitude\\"":27.2036401,\\""Longitude\\"":87.8156715},\n    {\\""0\\"":\\""Ilam\\"",\\""1\\"":\\""2013\\\\/14\\"",\\""2\\"":6,\\""Latitude\\"":26.9111769,\\""Longitude\\"":87.9236747},\n    ...\n  ]""\n```\n\n# POST REQUEST #\nEdit Column Labels\nhttps://eng100d-project.herokuapp.com/edit/cols/{type of affliction}/{affliction}\n>> Send JSON in this format:\n```\n[\n  ""COLUMN_1_NAME"",\n  ""COLUMN_2_NAME"",\n  ""COLUMN_3_NAME"",\n  ...\n]\n```\n\nEdit Rows of an Affliction\nhttps://eng100d-project.herokuapp.com/edit/rows/{type of affliction}/{affliction}\n>> Send JSON in this format:\n```\n{\t\t\t\n  ""0"": ""NAME""\n  ""1"": ""YEAR""\n  ""2"": ""VALUE""\n  ""index"": ""INDEX_IN_ROW_ARRAY""\n}\n```\n\nEdit Info of Affliction\nhttps://eng100d-project.herokuapp.com/edit/info/{type of affliction}/{affliction}\n>> Send JSON in this format:\n```\nNOTE date is calculated with \nvar d=new Date();\nvar n=d.getTime();\n```\n```\n{\n  ""name"":""NAME"",\n  ""date"":""DATE"",\n  ""views"":""NUMBER_OF_VIEWS"",\n  ""description"":""SOME_DESCRIPTION"",\n  ""source"": ""SOME_SOURCE"",\n  ""uploader"":""SOME_UPLOADER""\n}\n```\n\nEdit the type of an Affliction\nhttps://eng100d-project.herokuapp.com/edit/type/{type of affliction}/{affliction}\n>> Send JSON in this format:\n```\n{\n  ""type"": ""NEW_TYPE""\n}\n```\n\nAdd Row to the Table of an Affliction\nhttps://eng100d-project.herokuapp.com/add/row/{type of affliction}/{affliction}\n>> Send JSON in this format:\n```\n{\n  ""0"":""NAME_OF_DISTRICT"",\n  ""1"":""YEAR_OF_DATASET"",\n  ""2"":""VALUE"",\n  ""Latitude"":""LATITUDE_OF_DISTRICT"",\n  ""Longitude"":""LONGITUDE_OF_DISTRICT""\n}\n```\nedit Data field\nhttps://eng100d-project.herokuapp.com/edit/data{type of affliction}/{affliction}\n>> Send JSON in this format:\n```\n{\n  info:{\n    ""name"":""X"",\n    ""date"":Y,\n    ""views"":Z,\n    ""description"":""new description"",\n    ""source"":""source"",\n    ""uploader"":""SOME_UPLOADER""\n  },\n  Data:{\n    cols:[col1,col2,col3...],\n    rows:[\n      {\n        ""0"":Name,\n        ""1"":Year,\n        ""2"":Value,\n        ""Latitude"":Lat,\n        ""Longitude"":Long\n      },\n      {\n        ""0"":Name,\n        ""1"":Year,\n        ""2"":Value,\n        ""Latitude"":Lat,\n        ""Longitude"":Long\n      },\n      ...\n    ]\n  }\n}\n```\nAdd a new affliction\nhttps://eng100d-project.herokuapp.com/add/affliction/{type of affliction}\n>> Send JSON in this format:\n\n```\n{\n    ""name"":""NAME_OF_AFFLICTION"",\n    ""description"": ""DESCRIPTION_OF_AFFLICTION""\n}\n```\nDelete Aflliction\nhttps://eng100d-project.herokuapp.com/delete/affliction/{type of affliction}\n>> Send JSON in this format:\n```\n{}\n```\n'",This project is our Eng100D project for onestep
https://github.com/beckernick/wikipedia_pageviews,"b""# wikipedia_pageviews\nThe Election's Effect on Candidates's Wikipedia Page Views\n""",The Election's Effect on Candidate's Wikipedia Page Views
https://github.com/darshan-b/Machine-Learning-Regression,"b'# Machine-Learning-Regression\nConcepts implemented from scratch include: Gradient Descent, Lasso Regression and Ridge Regression\n'","Concepts implemented from scratch include: Gradient Descent, Lasso Regression and Ridge Regression"
https://github.com/UCRclyman/P177_Homework04,b'# P177_Homework04\nHomework 04\n',Homework 04
https://github.com/parantapag/IBD4Health2017,b'# IBD4Health2017\nLecture materials for IBD4Health 2017 summer school\n',Lecture materials for IBD4Health 2017 summer school
https://github.com/Clique-CS109/project,"b""# Read me - Flowchart\n\nTo fully enjoy this repository, we recommend you to read files in the following order:\n\n 1. [`README.md`](README.md) - this file\n 2. [`FinalProject_ProcessBook_General.ipynb`](FinalProject_ProcessBook_General.ipynb) - a summary of our method and result\n 3. [`Data_Exploration.ipynb`](Data_Exploration.ipynb) - data exploration and refinement\n \n\t##### The following three notebooks use files in [`data`](data) folder.\n 4. [`Base_Model.ipynb`](Base_Model.ipynb) - codes for baseline method\n 5. [`BookSim.ipynb`](BookSim.ipynb) - codes for kNN method with book similarity\n 6.  [`userkNN.ipynb`](userkNN.ipynb) - codes for kNN method with user similarity\n\t- In order to run this notebook (faster) you need files in [`knn_file`](knn_file) folder.\n\nAlso, please visit our cool [website - clique-cs109.github.io](http://clique-cs109.github.io)! Don't forget to watch our cool [screencast video](https://youtu.be/Zjm2z99ru_E) too.""",The main repository for Team Clique CS109 project
https://github.com/mwaskom/nipype_concepts,b'Nipype Concepts\n===============\n\nThis collection of [IPython](http://ipython.org/)\nnotebooks should provide an introduction to some of the \nmain concepts central to using [Nipype](http://nipy.sourceforge.net/nipype/)\nfor neuroimaging analysis. \n\nStatic HTML Links\n-----------------\n\n- [Interfaces](http://nbviewer.ipython.org/urls/raw.github.com/mwaskom/nipype_concepts/master/interfaces.ipynb)\n- [Workflows](http://nbviewer.ipython.org/urls/raw.github.com/mwaskom/nipype_concepts/master/workflows.ipynb)\n- [Iteration](http://nbviewer.ipython.org/urls/raw.github.com/mwaskom/nipype_concepts/master/iteration.ipynb)\n\nRequirements\n------------\n\n- [IPython](http://ipython.org/)\n- [Nipype](http://nipy.sourceforge.net/nipype/)\n- [FSL](http://fsl.fmrib.ox.ac.uk/fsl/fslwiki/)\n- [SPM](http://www.fil.ion.ucl.ac.uk/spm/) (Optional)\n\nLicense\n-------\n\nSimplified BSD\n',Tutorial notebooks for Nipype
https://github.com/toshikurauchi/eyeswipe2-analysis,b'# eyeswipe2-analysis\nResults and analysis from eyeswipe2 experiment\n',Results and analysis from eyeswipe2 experiment
https://github.com/miroli/ML-training,b'##Machine learning training\n\nJupyter notebooks for teaching myself machine learning.\n',Machine learning training with Jupyter
https://github.com/antingithub/JupyterWorkFlow,b'# JupyterWorkFlow\nJupyter Work Flow Example\n',Jupyter Work Flow Example
https://github.com/AlexandreCaron/12_steps_to_NS,b'This repository contains source code and files (text/images) regarding my progress in the CFD Python: 12 steps to Navier-Stokes blog by Lorena A. Barba.\n\nURL of blog is : http://lorenabarba.com/blog/cfd-python-12-steps-to-navier-stokes/\n',CFD Python: 12 steps to Navier-Stokes
https://github.com/0xLiso/Introduccion_ML,b'# Introduccion_ML\nUna peque\xc3\xb1a introduccion al Machine Learning\n',Una pequeña introduccion al Machine Learning
https://github.com/ashu2012/udacitycCarND,b'# udacitycCarND\nhttps://classroom.udacity.com/nanodegrees/nd013\n\nProject 1\n',https://classroom.udacity.com/nanodegrees/nd013
https://github.com/jenyquist/FLASH,b'# FLASH\nAn Ipython notebook implementation of A Computer Program for Flow-Log Analysis of Single Holes (FLASH)\n',An Ipython notebook implementation of A Computer Program for Flow-Log Analysis of Single Holes (FLASH)
https://github.com/PacktPublishing/Apache-Spark-2-for-Beginners,"b'#Apache Spark 2 for Beginners\r\nThis is the code repository for [Apache Spark 2 for Beginners](https://www.packtpub.com/big-data-and-business-intelligence/apache-spark-2-beginners?utm_source=github&utm_medium=repository&utm_campaign=9781785885006), published by Packt. It contains all the supporting project files necessary to work through the book from start to finish.\r\n##Instructions and Navigations\r\nAll of the code is organized into folders. Each folder starts with a number followed by the application name. For example, Chapter02.\r\n\r\n## Software and Hardware List\r\n| Chapter number | Software required (with version) | Free/Proprietary | If proprietary, can code testing be performed using a trial version | If proprietary, then cost of the software | Download links to the software | Hardware specifications | OS required |\r\n| -------- | -------- | -------- | -------- | -------- | -------- | -------- | -------- |\r\n| All | Apache Spark 2.0.0 | Free | NA | NA | http://spark.apache.org/downloads.html | X86 | UNIX or MacOSX |\r\n| 6 | Apache Kafka 0.9.0.0 | Free | NA | NA | http://www.sublimetext.com/3 | X86 | UNIX or MacOSX |\r\n\r\n\r\n## Detailed installation steps (software-wise)\r\nThe steps should be listed in a way that it prepares the system environment to be able to test the codes of the book.\r\n###1. Apache Spark:\r\na. Download Spark version mentioned in the table<br>\r\nb. Build Spark from source or use the binary download and follow the detailed instructions given in the page http://spark.apache.org/docs/latest/building-spark.html<br>\r\nc. If building Spark from source, make sure that the R profile is also built and the instructions to do that is given in the link given inthe step b.<br>\r\n###2. Apache Kafka\r\na. Download Kafka version mentioned in the table<br>\r\nb. The \xe2\x80\x9cquick start\xe2\x80\x9d section of the Kafka documentation gives the instructions to setup Kafka.\r\nhttp://kafka.apache.org/documentation.html#quickstart<br>\r\nc. Apart from the installation instructions, the topic creation and the other Kafka setup pre-requisites have been covered in detail in the chapter of the book<br>\r\n\r\n\r\nThe code will look like the following:\r\n```\r\nPython 3.5.0 (v3.5.0:374f501f4567, Sep 12 2015, 11:00:19)\r\n[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin\r\n```\r\n\r\nSpark 2.0.0 or above is to be installed on at least a standalone machine to run the code samples and do further activities to learn more about the subject. For Spark Stream Processing, Kafka needs to be installed and configured as a message broker with its command line producer producing messages and the application developed using Spark as a consumer of those messages.\r\n\r\n##Related Products\r\n* [Scala Design Patterns](https://www.packtpub.com/application-development/scala-design-patterns?utm_source=github&utm_medium=repository&utm_campaign=9781785882500)\r\n\r\n* [Machine Learning with Spark](https://www.packtpub.com/big-data-and-business-intelligence/machine-learning-spark?utm_source=github&utm_medium=repository&utm_campaign=9781783288519)\r\n\r\n* [Quickstart Apache Axis2](https://www.packtpub.com/web-development/quickstart-apache-axis2?utm_source=github&utm_medium=repository&utm_campaign=9781847192868)\r\n###Suggestions and Feedback\r\n[Click here](https://docs.google.com/forms/d/e/1FAIpQLSe5qwunkGf6PUvzPirPDtuy1Du5Rlzew23UBp2S-P3wB-GcwQ/viewform) if you have any feedback or suggestions.\r\n'","Apache Spark 2 for Beginners, published by Packt"
https://github.com/justinmklam/python-resources,"b""# Python Resources\nExample code snippets in python so I don't forget everything after not using it for 6 months.\n\n# How To Use\n+ Open the desired iPython notebook\n+ Look through examples\n+ ???\n+ Profit\n""",Python resources so I don't forget everything after not using it for 6 months.
https://github.com/akshat7/Kaggle-Digit-Recognizer,"b'# Kaggle-Digit-Recognizer\nKaggle submission of the Digit Recognizer challenge using multiple ML Algorithms and their result comparisons.\n\n## Analysis of Different Algorithms\n\nMy first submission was using K-Nearest Neighbour Algorithm (taking K = 3), which gave an accuracy of 96.857%. The accuracy went up to 96.943% when applying PCA before KNN.\n\nThe next submission was made by using PCA with SVM, which gave an accuracy of 98.057%, much better than the previous predictions.\n\nNext prediction was made using decision tree, which brought the accuracy down to 85.286%.\n\n## Dataset\nThe Training and Testing Dataset can be downloaded from Kaggle itself:\n\nhttps://www.kaggle.com/c/digit-recognizer \n\n## Python Libraries Used\n* Scikit-Learn\n* Pandas\n* Numpy\n'",Kaggle submission of the Digit Recognizer challenge using multiple ML Algorithms and their result comparisons.
https://github.com/BibMartin/crossfolium,"b""# crossfolium\n\nA plugin to add [crossfilters](https://square.github.io/crossfilter) in\n[folium](https://github.com/python-visualization/folium).\n\nThere's no documentation, but you can browse the\n[example gallery](http://nbviewer.jupyter.org/github/bibmartin/crossfolium/tree/master/examples/).\n\n""",A plugin to add crossfilters in folium
https://github.com/birlrobotics/bnpy,"b'## **bnpy** : Bayesian nonparametric machine learning for python.\r\n\r\n* [About](#about)\r\n* [Project Website (Read the Docs)](https://bnpy.readthedocs.io/en/latest/)\r\n* [Example Gallery](#example-gallery)\r\n* [Quick Start](#quick-start)\r\n* [Installation](#installation)\r\n* [Team](#team)\r\n* [Academic References](#academic-references)\r\n* * [NIPS 2015: HDP-HMM paper](#nips-2015-hdp-hmm-paper)\r\n* * [AISTATS 2015: HDP topic models](#aistats-2015-hdp-topic-model-paper)\r\n* * [NIPS 2013: DP mixture models](#nips-2013-dp-mixtures-paper)\r\n\r\n# About\r\nThis python module provides code for training popular clustering models on large datasets. We focus on Bayesian nonparametric models based on the Dirichlet process, but also provide parametric counterparts. \r\n\r\n**bnpy** supports the latest online learning algorithms as well as standard offline methods. Our aim is to provide an inference platform that makes it easy for researchers and practitioners to compare models and algorithms.\r\n\r\n### Supported probabilistic models (aka allocation models)\r\n\r\n* Mixture models\r\n    * `FiniteMixtureModel` : fixed number of clusters\r\n    * `DPMixtureModel` : infinite number of clusters, via the Dirichlet process\r\n\r\n* Topic models (aka admixtures models)\r\n    * `FiniteTopicModel` : fixed number of topics. This is Latent Dirichlet allocation.\r\n    * `HDPTopicModel` : infinite number of topics, via the hierarchical Dirichlet process\r\n    \r\n* Hidden Markov models (HMMs)\r\n    * `FiniteHMM` : Markov sequence model with a fixture number of states\r\n    *  `HDPHMM` : Markov sequence models with an infinite number of states\r\n\r\n* **COMING SOON**\r\n    * grammar models\r\n    * relational models\r\n\r\n### Supported data observation models (aka likelihoods)\r\n\r\n* Multinomial for bag-of-words data\r\n    * `Mult`\r\n* Gaussian for real-valued vector data\r\n    * `Gauss` : Full-covariance \r\n    * `DiagGauss` : Diagonal-covariance\r\n    * `ZeroMeanGauss` : Zero-mean, full-covariance\r\n* Auto-regressive Gaussian\r\n    * `AutoRegGauss`\r\n\r\n### Supported learning algorithms:\r\n\r\n* Expectation-maximization (offline)\r\n    * `EM`\r\n* Full-dataset variational Bayes (offline)\r\n    * `VB`\r\n* Memoized variational (online)\r\n    * `moVB`\r\n* Stochastic variational (online)\r\n    * `soVB`\r\n\r\nThese are all variants of *variational inference*, a family of optimization algorithms. We plan to eventually support sampling methods (Markov chain Monte Carlo) too.\r\n\r\n# Example Gallery\r\n\r\nYou can find many examples of **bnpy** in action in our curated [Example Gallery](https://bnpy.readthedocs.io/en/latest/examples/).\r\n\r\nThese same demos are also directly available as Python scrips inside the [examples/ folder of the project Github repository](https://github.com/bnpy/bnpy/tree/master/examples).\r\n\r\n# Quick Start\r\n\r\nYou can use **bnpy** from a command line/terminal, or from within Python. Both options require specifying a dataset, an allocation model, an observation model (likelihood), and an algorithm. Optional keyword arguments with reasonable defaults allow control of specific model hyperparameters, algorithm parameters, etc.\r\n\r\nBelow, we show how to call bnpy to train a 8 component Gaussian mixture model on a default toy dataset stored in a .csv file on disk. In both cases, log information is printed to stdout, and all learned model parameters are saved to disk.\r\n\r\n## Calling from the terminal/command-line\r\n\r\n```\r\n$ python -m bnpy.Run /path/to/my_dataset.csv FiniteMixtureModel Gauss EM --K 8 --output_path /tmp/my_dataset/results/\r\n```\r\n\r\n## Calling directly from Python\r\n\r\n```\r\nimport bnpy\r\nbnpy.run(\'/path/to/dataset.csv\',\r\n         \'FiniteMixtureModel\', \'Gauss\', \'EM\',\r\n         K=8, output_path=\'/tmp/my_dataset/results/\')\r\n\r\n```\r\n\r\n## Advanced examples\r\n\r\nTrain Dirichlet-process Gaussian mixture model (DP-GMM) via full-dataset variational algorithm (aka ""VB"" for variational Bayes).\r\n\r\n```\r\npython -m bnpy.Run /path/to/dataset.csv DPMixtureModel Gauss VB --K 8\r\n```\r\n\r\nTrain DP-GMM via memoized variational, with birth and merge moves, with data divided into 10 batches.\r\n\r\n```\r\npython -m bnpy.Run /path/to/dataset.csv DPMixtureModel Gauss memoVB --K 8 --nBatch 10 --moves birth,merge\r\n```\r\n\r\n## Quick help\r\n```\r\n# print help message for required arguments\r\npython -m bnpy.Run --help \r\n\r\n# print help message for specific keyword options for Gaussian mixture models\r\npython -m bnpy.Run /path/to/dataset.csv FiniteMixtureModel Gauss EM --kwhelp\r\n```\r\n\r\n# Installation\r\n\r\nTo use **bnpy** for the first time, follow the documentation\'s [Installation Instructions](https://bnpy.readthedocs.io/en/latest/installation.html).\r\n\r\n# Team\r\n\r\n### Lead developer\r\n\r\nMike Hughes  \r\nWebsite: [www.michaelchughes.com](http://www.michaelchughes.com)\r\n\r\nPost-doctoral researcher (Aug. 2016 - present)  \r\nSchool of Engineering and Applied Sciences  \r\nHarvard University  \r\n\r\n### Faculty adviser\r\n\r\nErik Sudderth  \r\nAssistant Professor  \r\nBrown University, Dept. of Computer Science  \r\nWebsite: [http://cs.brown.edu/people/sudderth/](http://cs.brown.edu/people/sudderth/)\r\n\r\n### Contributors \r\n\r\n* Soumya Ghosh\r\n* Dae Il Kim\r\n* Geng Ji\r\n* William Stephenson\r\n* Sonia Phene\r\n* Gabe Hope\r\n* Leah Weiner\r\n* Alexis Cook\r\n* Mert Terzihan\r\n* Mengrui Ni\r\n* Jincheng Li\r\n\r\n# Academic References\r\n\r\n## Conference publications based on BNPy\r\n\r\n#### NIPS 2015 HDP-HMM paper\r\n\r\n> Our NIPS 2015 paper describes inference algorithms that can add or remove clusters for the sticky HDP-HMM.\r\n\r\n* ""Scalable adaptation of state complexity for nonparametric hidden Markov models."" Michael C. Hughes, William Stephenson, and Erik B. Sudderth. NIPS 2015.\r\n[[paper]](http://michaelchughes.com/papers/HughesStephensonSudderth_NIPS_2015.pdf)\r\n[[supplement]](http://michaelchughes.com/papers/HughesStephensonSudderth_NIPS_2015_supplement.pdf)\r\n[[scripts to reproduce experiments]](http://bitbucket.org/michaelchughes/x-hdphmm-nips2015/)\r\n\r\n#### AISTATS 2015 HDP topic model paper\r\n\r\n> Our AISTATS 2015 paper describes our algorithms for HDP topic models.\r\n\r\n* ""Reliable and scalable variational inference for the hierarchical Dirichlet process."" Michael C. Hughes, Dae Il Kim, and Erik B. Sudderth. AISTATS 2015.\r\n[[paper]](http://michaelchughes.com/papers/HughesKimSudderth_AISTATS_2015.pdf)\r\n[[supplement]](http://michaelchughes.com/papers/HughesKimSudderth_AISTATS_2015_supplement.pdf)\r\n[[bibtex]](http://cs.brown.edu/people/mhughes/papers/HughesKimSudderth-AISTATS2015-MemoizedHDP-bibtex.txt)\r\n\r\n#### NIPS 2013 DP mixtures paper\r\n\r\n> Our NIPS 2013 paper introduced memoized variational inference algorithm, and applied it to Dirichlet process mixture models.\r\n\r\n* ""Memoized online variational inference for Dirichlet process mixture models."" Michael C. Hughes and Erik B. Sudderth. NIPS 2013.\r\n[[paper]](http://michaelchughes.com/papers/HughesSudderth_NIPS_2013.pdf)\r\n[[supplement]](http://michaelchughes.com/papers/HughesSudderth_NIPS_2013_supplement.pdf)\r\n[[bibtex]](http://cs.brown.edu/people/mhughes/papers/HughesSudderth-NIPS2013-MemoizedDP-bibtex.txt)\r\n\r\n## Workshop papers\r\n\r\n> Our short paper from a workshop at NIPS 2014 describes the vision for **bnpy** as a general purpose inference engine.\r\n\r\n* ""bnpy: Reliable and scalable variational inference for Bayesian nonparametric models.""\r\nMichael C. Hughes and Erik B. Sudderth. Probabilistic Programming Workshop at NIPS 2014.\r\n[[paper]](http://michaelchughes.com/papers/HughesSudderth_NIPSProbabilisticProgrammingWorkshop_2014.pdf)\r\n\r\n\r\n# Target Audience\r\n\r\nPrimarly, we intend **bnpy** to be a platform for researchers. \r\nBy gathering many learning algorithms and popular models in one convenient, modular repository, we hope to make it easier to compare and contrast approaches. We also hope that the modular organization of **bnpy** enables researchers to try out new modeling ideas without reinventing the wheel.\r\n'",bnpy trains nonparameteric markov switching process in python through a simple and useful api.
https://github.com/niudd/dps,b'dps = Deep Player Score \xef\xbc\x88\xe4\xb8\x8d\xe6\x98\xafdamage per second\xef\xbc\x81\xef\xbc\x89\n\xe8\xb6\xb3\xe7\x90\x83\xe7\x90\x83\xe5\x91\x98\xe8\xaf\x84\xe5\x88\x86\xe7\xb3\xbb\xe7\xbb\x9f\xef\xbc\x8c\xe5\xbb\xba\xe7\xab\x8b\xe5\x9c\xa8\xe6\xb7\xb1\xe5\xba\xa6\xe7\x9a\x84\xe8\xb6\xb3\xe7\x90\x83\xe6\x95\xb0\xe6\x8d\xae\xe7\xbb\x93\xe6\x9e\x84\xe4\xb8\x8a\xe3\x80\x82\n\n\nmodel.py\xe5\xae\x9e\xe7\x8e\xb0\xe4\xba\x86\xe8\xaf\xa5\xe6\x96\xb9\xe6\xb3\x95\xe3\x80\x82\n',dps = Deep Player Score (Not Damage Per Second!)
https://github.com/tgvaughan/bacter,"b'bacter\n======\n\n[![Build Status](https://github.com/tgvaughan/bacter/workflows/Unit%2Fintegration%20tests/badge.svg)](https://github.com/tgvaughan/bacter/actions?query=workflow%3A%22Unit%2Fintegration+tests%22)\n\nBacter is a [BEAST 2](http://www.beast2.org)  package which facilitates\ninference of a (restricted kind of) ancestral recombination graph (ARG) and\nrelated parameters from a sequence alignment.  It is based on the model\ndescribed in [Didelot et al.\'s 2010 Genetics paper][1].\n\nThis archive contains the source code of the package and is therefore of\nprimary interest to programmers.  For installation and usage instructions, as\nwell as links to tutorials and other documentation, please visit the project\nhome page hosted at http://tgvaughan.github.io/bacter.\n\nArchive Contents\n----------------\n\n* `/examples` : Example XML files, simulated data for the tutorial and a\n  Jupyter notebook with implementation validation details. (You can view this\n  notebook [online][2].) \n* `/lib` : Required libraries.\n* `/src` : Java source code.\n* `/test` : Java source code (unit tests).\n* `/templates` : BEAUti templates.\n* `version.xml` : BEAST package version file.\n* `build.xml` : Ant build script.\n* `.gitignore` : Causes SCM to ignore certain files\n* `.travis.yml` : Control file for Travis CI server\n* `Dockerfile`: Used by Travis to build a reproducible test environment.\n* `COPYING` : Software license.\n* `README.md` : This file.\n\nBuilding package from source\n----------------------------\n\nTo build this package from source, ensure you have the following installed:\n\n* Java JDK v1.8 \n* Apache Ant v1.9 or later\n* An internet connection\n\nThe internet connection is required since the build script downloads the most\nrecent version of the BEAST 2 source to build the package against.\nAssuming both Java and Ant are on your execution path and your CWD is the root of\nthis archive, simply type ""ant"" from the command line to build the package.\nThis may take up to a minute due to the script fetching the BEAST source, and\nthe resulting binary will be left in the `/dist` directory.\nTo run the unit tests, use ""ant test"".\n\nLicense\n-------\n\nThis software is free (as in freedom). You are welcome to use it, modify it,\nand distribute your modified versions provided you extend the same courtesy to\nusers of your modified version.  Specifically, it is made available under the\nterms of the GNU General Public License version 3, which is contained in his\ndirectory in the file named COPYING.\n\nAcknowledgements\n----------------\n\nWork on this project is made possible by the support of the following institutions:\n\n* [The Allan Wilson Centre for Molecular Ecology and Evolution](http://www.allanwilsoncentre.ac.nz)\n\n* [The Royal Society of New Zealand\'s Marsden Fund](http://www.royalsociety.org.nz/programmes/funds/marsden/)\n\n* [The University of Auckland](http://auckland.ac.nz)\n\n* [Massey University](http://www.massey.ac.nz)\n\n[1]: http://www.genetics.org/content/186/4/1435\n[2]: http://nbviewer.jupyter.org/github/tgvaughan/bacter/blob/master/examples/Validation.ipynb\n'",Bacterial phylogenetics in BEAST 2.
https://github.com/jpdeleon/ircs,"b'# IRCS\nCommand-line implementation of basic data reduction of near-infrared images with polarimetry.\nThe data is from the infrared camera and spectrograph (IRCS) instrument on board the 8-m Subaru telescope in Hawaii.\n\n## Progress\n* 2017/03/20: basic scripting\n* 2017/04/24: created setup.py\n* 2017/04/26: added crop and bgsub\n* 2017/04/28: added calflat.py\n\n## Installation\nThis assumes that you have an environment with at least python 2.7 installed.\nIf not, use conda to create an environment called `ircs_pol`:\n```shell\nconda create -n ircs_pol python-2.7 matplotlib numpy\nsource activate ircs_pol```\n\n\nNow, clone and then install `ircs`  inside the environment:\n\n1. Clone this repository\n```shell\ngit clone git@github.com:jpdeleon/ircs.git```\n\n\n2. `cd` into the proper directory and install\n```shell\ncd /ircs\npython setup.py install```\n\n\n## Sample run\n\n### Part 1 ircs-imaging\n```shell\n$ ircs-imaging -h```\n\n\n### Part 2 ircs-polarimetry\n```shell\n$ ircs-polarimetry -h```\n\n\nSee also other plotting helper functions in /ircs/utils.py.\n\nTO DO: \n1. define input/output directories using a .yaml\n2. implement low-level control\n3. upgrade to classes\n'",Basic data reduction of near infrared images with polarimetry specific for IRCS--the infrared camera and spectrograph instrument in the Subaru telescope
https://github.com/wstrinz/infopipes,"b""#'Infopipes' prototyping\n\nCheckout [auto_tagger.rb.ipynb](auto_tagger.rb.ipynb)\n""","Use existing services to feed, tag, organize and read news (wip)"
https://github.com/mat-esp-2016/integracao-numerica-oceano_saq,"b'# Pr\xc3\xa1tica de Integra\xc3\xa7\xc3\xa3o Num\xc3\xa9rica\n\nParte do curso\n[Matem\xc3\xa1tica Especial I](http://www.leouieda.com/matematica-especial/)\nda [UERJ](http://www.uerj.br/).\n\nMinistrado por [Leonardo Uieda](http://www.leouieda.com/).\n\n## Objetivos\n\n* Aprender as regras de ret\xc3\xa2ngulos e trap\xc3\xa9zios para integra\xc3\xa7\xc3\xa3o num\xc3\xa9rica\n* Aplicar os conceitos de programa\xc3\xa7\xc3\xa3o em Python aprendidos at\xc3\xa9 agora\n\n## Prepara\xc3\xa7\xc3\xa3o\n\nUtilize o link enviado por e-mail para criar um reposit\xc3\xb3rio para seu grupo.\nUtilizaremos os **mesmos grupos** da pr\xc3\xa1tica passada.\nCada membro do grupo deve clicar no link e selecionar o nome do grupo criado na\npr\xc3\xa1tica passada.\nN\xc3\xa3o se esque\xc3\xa7a de sair de sua conta **no github.com e no\nclassroom.github.com**.\n\n**Crie um arquivo** chamado `alunos.txt` com os **nomes completos** de todos os\nintegrantes do grupo. Inclua tamb\xc3\xa9m a **qual turma** pertencem.\n\nAs tarefas para serem feitas est\xc3\xa3o em um [Jupyter\nnotebook](http://jupyter.org/).\nPara utilizar o Jupyter, voc\xc3\xaa precisa iniciar um servidor de notebook\nno seu computador.\nAbra o bash e digite:\n\n    $ jupyter notebook\n\nEspere um pouco at\xc3\xa9 aparecer algo como:\n\n    [I 10:50:47.370 NotebookApp] Serving notebooks from local directory: /home/leo\n    [I 10:50:47.370 NotebookApp] 0 active kernels\n    [I 10:50:47.370 NotebookApp] The IPython Notebook is running at: http://localhost:8888/\n    [I 10:50:47.370 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n\nIsso deve abrir o seu navegador padr\xc3\xa3o tamb\xc3\xa9m em uma p\xc3\xa1gina no endere\xc3\xa7o\n[http://127.0.0.1:8888](http://127.0.0.1:8888).\nEssa p\xc3\xa1gina ir\xc3\xa1 te mostrar as pastas que est\xc3\xa3o em seu computador\n(a partir da pasta onde voc\xc3\xaa rodou `$ jupyter notebook`).\n\n## Tarefas\n\nSiga as instru\xc3\xa7\xc3\xb5es em `tarefa-integracao.ipynb`.\nAs tarefas e **suas solu\xc3\xa7\xc3\xb5es devem estar contidas nesse notebook**.\nPor isso, fa\xc3\xa7a commits de suas mudan\xc3\xa7as ao notebook.\n\n**AVISO**: N\xc3\xa3o esque\xc3\xa7a de verificar se abriu o notebook no clone correto!\n\n**AVISO 2**: Ap\xc3\xb3s cada mudan\xc3\xa7a, `git add` + `git commit` + `git push`.\n\n**AVISO 3**: ANTES de come\xc3\xa7ar: `git pull origin master`\n\n## Dicas\n\n* Fa\xc3\xa7am muitos **commits**. Quanto mais melhor.\n* N\xc3\xa3o se esque\xc3\xa7a do **push**.\n* Utilize **mensagens de commit** descritivas. ""Completei a tarefa 1"" \xc3\xa9 melhor que\n  ""mudan\xc3\xa7a"".\n* Escolha nomes descritivos para **vari\xc3\xa1veis**. ""temperatura"" \xc3\xa9 melhor que ""a"".\n* **Descreva o que (e por que) voc\xc3\xaa fez** em coment\xc3\xa1rios e c\xc3\xa9lulas de texto.\n  Isso ser\xc3\xa1 muito \xc3\xbatil quando voc\xc3\xaa voltar a essa tarefa depois.\n* Preste aten\xc3\xa7\xc3\xa3o aos **detalhes**. Leia as instru\xc3\xa7\xc3\xb5es com aten\xc3\xa7\xc3\xa3o.\n\n## Leitura recomendada\n\n* https://en.wikipedia.org/wiki/Numerical_integration\n* http://www.if.ufrj.br/~sandra/MetComp/2012-2/\n* https://en.wikibooks.org/wiki/Numerical_Methods/Numerical_Integration\n'",integracao-numerica-oceano_saq created by GitHub Classroom
https://github.com/Obeyed/udacity,b'Various exercises from the online courses on udacity.com\n\n# Content\n\n- Deep learning course (https://classroom.udacity.com/courses/ud730)\n',Exercises from Udacity courses
https://github.com/LuisM78/python_machine_scripts,b'# python_machine_scripts\na place to store my python machine learning scripts\n',a place to store my python machine learning scripts
https://github.com/pmorissette/klink,"b"".. image:: http://pmorissette.github.io/klink/_static/logo.png\n\n.. image:: https://github.com/pmorissette/klink/workflows/Build%20Status/badge.svg\n    :target: https://github.com/pmorissette/klink/actions/\n\n.. image:: https://codecov.io/gh/pmorissette/klink/branch/master/graph/badge.svg\n    :target: https://codecov.io/pmorissette/klink\n\n.. image:: https://img.shields.io/pypi/v/klink\n    :alt: PyPI\n    :target: https://pypi.org/project/klink/\n\n.. image:: https://img.shields.io/pypi/l/klink\n    :alt: PyPI - License\n    :target: https://pypi.org/project/klink/\n\n\nklink - A Simple & Clean Sphinx Theme\n=====================================\n\nKlink is a **simple** and **clean** theme for creating `Sphinx docs\n<http://sphinx-doc.org/>`__. It is heavily inspired by the beautiful `jrnl theme\n<https://github.com/maebert/jrnl>`__. It also supports embedding `IPython\nNotebooks <http://ipython.org/notebook.html>`__ which can be mighty useful.\n\nFor a live demo, please visit `our docs <http://pmorissette.github.io/klink/>`__.\n\nOptions\n-------\n\nHere are the theme options. They should be added to the html_theme_options in\nyour **conf.py** file.\n\n* **github**\n    The github address of the project. The format is name/project\n    (pmorissette/klink).\n* **logo**\n    The logo file. Assumed to be in the _static dir. Default is logo.png. The logo\n    should be 150x150.\n* **analytics_id**\n    Your Google Analytics id (usually starts with UA-...)\n\nIPython Notebook Integration\n----------------------------\n\nWith the klink helper function **convert_notebooks()**, all notebooks will be\nconverted to .rst so that they can be included in your docs. This includes all\noutput including images. It\xe2\x80\x99s a very convenient way to create Python docs! \n\nAll you have to do is create notebooks within your source directory (same directory\nas your conf.py file). Then, you add a call to klink.convert_notebooks() in your\nconf.py. You can also mix in **Mardown** cells or **Raw NBConvert** cells in\nyour workbook. These will be converted to rst as well. \n\nIf you use the Raw NBConvert type cells, add a blank line at the start. There\nseems to be a bug in the rst conversion and if the cell does not begin with a\nblank line, you may run into some issues. \n\nUsing a Raw NBConvert cell with rst text inside is convenient, especially if you\nwant to have links to other parts of your Sphinx docs. \n\nInstallation\n------------\n\nAssuming you have pip installed:\n\n.. code:: sh\n\n    $ pip install klink\n\nThat's it.\n\nUsage\n-----\n\nIn your docs' **conf.py** file, add the following:\n\n.. code:: python\n\n    import klink\n\n    html_theme = 'klink'\n    html_theme_path = [klink.get_html_theme_path()]\n    html_theme_options = {\n        'github': 'yourname/yourrepo',\n        'analytics_id': 'UA-your-number-here',\n        'logo': 'logo.png'\n    }\n\nKlink also comes with a useful helper function that allows you to integrate an\nIPython Notebook into a .rst file. It basically converts the Notebook to .rst\nand copies the static data (images, etc) to your _static dir. \n\nIf you have IPython Notebooks that you would like to integrate, use the\nfollowing code to your **conf.py**:\n\n.. code:: python\n\n    klink.convert_notebooks()\n\nOnce the conversion is done, you will have a .rst file with the same name as\neach one of your notebooks.\n\n\n*NOTE: Place your notebooks in your docs' source dir.*\n\nNow all you have to do is use the **include** command to insert them into your\ndocs.\n\n\nCustomization\n-------------\n\nObviously, some of you will want to customize the theme. The easiest way to\nachieve this is to clone the repo into your _themes folder (create it if it does\nnot exist in your docs' source dir). To change the style, I recommend editing\nthe LESS files themselves. You will also need lessc to convert from less to css.\nSee the css command in the Makefile for an example. \n\nYou may also want to explore the option of using **git subtree**. Here is a good\n`intro tutorial <http://makingsoftware.wordpress.com/2013/02/16/using-git-subtrees-for-repository-separation/>`__.\n\nYou will also need to change your conf.py file. The following settings should\nwork::\n\n    html_theme = 'klink'\n    html_theme_path = ['_themes']\n    html_theme_options = {\n        'github': 'yourname/yourrepo',\n        'analytics_id': 'UA-your-number-here',\n        'logo': 'logo.png'\n    }\n""",A Simple and Clean Sphinx Docs Theme
https://github.com/batra-mlp-lab/VT-F15-ECE6504-HW0,"b""# [ECE 6504 Deep Learning for Perception](https://filebox.ece.vt.edu/~f15ece6504/)\n\n## Homework 0\n\n### Part 1: Getting Started with ECE6504\n\nIn this course, we will be using python considerably (most assignments will need a good amount of python).\n\n#### Anaconda\n\nAlthough many distributions of python are available, we recommend that you use the [Anaconda Python](https://store.continuum.io/cshop/anaconda/). Here are the advantages of using Anaconda:\n\n- Easy seamless install of [python packages](http://docs.continuum.io/anaconda/pkg-docs) (most come standard)\n- It does not need root access to install new packages\n- Supported by Linux, OS X and Windows\n- Free!\n\nWe suggest that you use either Linux (preferably Ubuntu) or OS X.\nFollow the instructions [here](http://docs.continuum.io/anaconda/install) to install Anaconda python.\nRemember to make Anaconda python the default python on your computer.\nCommon issues are addressed here in the  [FAQ](http://docs.continuum.io/anaconda/faq).\n\n**TODO**\n\nInstall Anaconda python.\n\n#### Python\nIf you are comfortable with python, you can skip this section!\n\nIf you are new to python and have sufficient programming experience in using languages like C/C++, MATLAB, etc., you should be able to grasp the basic workings of python necessary for this course easily.\n\nWe will be using the [Numpy](http://www.numpy.org/) package extensively as it is the fundamental package for scientific computing providing support for array operations, linear algebra, etc. A good tutorial to get you started is [here](http://cs231n.github.io/python-numpy-tutorial/). For those comfortable with the operations of MATLAB, [this](http://sebastianraschka.com/Articles/2014_matlab_vs_numpy.html) might prove useful.\n\nFor some assignments, we will be using the [IPython notebook](http://ipython.org/notebook.html). IPython is a command shell for interactive computing developed primarily for python. The notebook is a useful environment where text can be embedded with code enabling us to set a flow while you do the assignments.\nIf you have installed Anaconda and made it your default python, you should be able to start the IPython notebook environment with:\n\n**TODO**\n\n```sh\nipython notebook\n```\n\nThe IPython notebook files have `.ipynb` extension which you should be able to open now by navigating to the right directory.\n\n### Part 2: Starting homework 0\n\nThis homework is a warm up for the rest of the course. As part of this homework you will:\n\n- Implement a Multi-Class Support Vector Machine (SVM)\n    - vectorized loss function **4 points**\n    - vectorized gradient computation **4 points**\n- Implement Softmax Regression (SR)\n    - vectorized loss function **4 points**\n    - vectorized gradient computation **4 points**\n- Implement Stochastic Gradient Descent **2 points**\n- Tune the hyper parameters using Spearmint **2 points**\n\nYou will train the classifiers on images in the [CIFAR-10 dataset](http://www.cs.toronto.edu/~kriz/cifar.html). The CIFAR-10 is a toy dataset with 60000 images of size 32 X 32, belonging to 10 classes. You need to start with `svm.ipynb` first to implement the SVM and then go ahead with `softmax.ipynb` to implement logistic regression.\n\nThis homework is based on [assignment 1](http://cs231n.github.io/assignment1/) of the CS231n course at Stanford.\n\n**TODO**\n\nDownload the starter code [here](https://github.com/batra-mlp-lab/VT-F15-ECE6504-HW0/archive/1.0.zip).\n\n#### Getting the dataset\n\nMake sure you are connected to the internet. Navigate to the `f15ece6504/data` folder and run the following:\n\n**TODO**\n\n```sh\n./get_datasets.sh\n```\n\nThis script will download the python version of the database for you and put it in `f15ece6504/data/cifar-10-batches-py` folder.\n\n#### Getting Spearmint\n\nAs part of this homework, you will be using Spearmint to tune the hyper-parameters like learning rate, regularization strength, etc. Spearmint is a software package to perform Bayesian optimization. It is designed to automatically run experiments in a manner that iteratively adjusts a number of parameters so as to minimize some objective in as few runs as possible.\n\nIf you have Anaconda installed, setting up Spearmint should be pretty straightforward. You can find installation and usage instructions [here](https://github.com/HIPS/Spearmint). You need to use the command line interface to work with Spearmint. To get an idea of how Spearmint works, look up the [examples](https://github.com/HIPS/Spearmint/tree/master/examples/simple) provided in the `Spearmint/examples` folder.\n\n**TODO**\n\nInstall Spearmint. Follow instructions in the IPython notebook.\n\n**Deliverables**\n\nRun the `collectSubmission.sh` script and upload the resulting zip file.\n\n### Part 3: SVM and Logistic Regression\n\nAs you might already know, both SVM and SR classifiers are linear models but they use different loss functions (hinge loss in SVMs vs softmax loss in SR). Here is a brief summary of the classifiers and if you need a detailed tutorial to brush up your knowledge, [this](http://cs231n.github.io/linear-classify/) is a nice place.\n\nBefore we go into the details of a classifier, let us assume that our training dataset consists of \\\\(N\\\\) instances \\\\(x\\_i \\in \\mathbb{R}^D \\\\) of dimensionality \\\\(D\\\\). \nCorresponding to each of the training instances,\nwe have labels \\\\(y\\_i \\in \\{1,2,\\dotsc ,K \\}\\\\), where \\\\(K\\\\) is the number of classes. \nIn this homework, we are using the CIFAR-10 database where \\\\(N=50,000\\\\), \\\\(K=10\\\\), \\\\(D= 32 \\times 32 \\times 3\\\\) \n(image of size  \\\\(32 \\times 32\\\\) with \\\\(3\\\\) channels - Red, Green, and Blue).\n\nClassification is the task of assigning a label to the input from a fixed set of categories or classes. A classifier consists of two important components:\n\n**Score function:** This maps every instance \\\\(x_i\\\\) to a vector \\\\(p\\_i\\\\) of dimensionality \\\\(K\\\\). Each of these entries represent the class scores for that image. Both SVM and Logistic Regression have a linear score function given by:\n\n\\\\[ p\\_i = f(x\\_i;W,b) \\\\]\n\nwhere,\n\n\\\\[ f(x;W,b) = Wx + b \\\\]\n\nHere, W is a matrix of weights of dimensionality \\\\(K \\times D\\\\) and b is a vector of bias terms of dimensionality \\\\(K \\times 1\\\\). The process of training is to find the appropriate values for W and b such that the score corresponding to the correct class is high. In order to do this, we need a function that evaluates the performance. Using this evaluation as feedback, the weights can be updated in the right 'direction' to improve the performance of the classifier.\n\nWe make a minor modification to the notation before proceeding further. The bias term can be incorporated within the weight matrix W making it of dimensionality \\\\(K \\times (D+1)\\\\). The \\\\(i^{th}\\\\) row of the weight matrix W is represented as a column vector \\\\(w\\_i\\\\) so that \\\\(p \\_i^j = w \\_j^Tx\\_i\\\\). The superscript j denotes the \\\\(j^{th}\\\\) element of \\\\(p\\_i\\\\), the score vector corresponding to \\\\(x\\_i\\\\).\n\n**Loss function:** This function quantifies the correspondence between the predicted scores and ground truth labels.\nThe loss of the SVM is given by:\n\n\\\\[ L = \\frac{1}{N}\\sum\\_{i=1}^{N}\\sum\\_{j \\neq y\\_i} \\bigg[\\max \\big(0, p\\_i^j - p\\_i^{y\\_i} + \\Delta \\big) \\bigg] \\\\]\n\nHere, \\\\(\\Delta\\\\) is the margin. The loss function penalizes when the correct class is not greater than all the other scores by at least \\\\(\\Delta\\\\).\nThe loss of the Logistic Regression is given by:\n\n\\\\[ L = - \\frac{1}{N}\\sum\\_{i=1}^{N}\\log \\bigg( \\frac{e^{p\\_i^{y\\_i}}}{\\sum\\_j e^{p^j\\_i}} \\bigg) \\\\]\n\nIf the weights are allowed to take values as high as possible, the model can overfit to the training data. To prevent this from happening a regularization term \\\\(R(W)\\\\) is added to the loss function. The regularization term is the squared some of the weight matrix \\\\(W\\\\). Mathematically,\n\n\\\\[ R(W) = \\sum\\_{k}\\sum\\_{l}W\\_{k,l}^2 \\\\]\n\nThe regularization term \\\\(R(W)\\\\) is usually multiplied by the regularization strength \\\\(\\lambda\\\\) before adding it to the loss function. \\\\(\\lambda\\\\) is a hyper parameter which needs to be tuned so that the classifier generalizes well over the training set.\n\nThe next step is to update the weight parts such that the loss is minimized. This is done by Stochastic Gradient Descent (SGD). The weight update is done as:\n\n\\\\[ W := W - \\eta \\nabla L \\\\]\n\nHere, \\\\(\\nabla L\\\\) is the gradient of the loss function and the factor \\\\(\\eta\\\\) is the learning rate. SGD is usually performed by computing the gradient w.r.t. a randomly selected batch from the training set.\nThis method is more efficient than computing the gradient w.r.t the whole training set before each update is performed.\n\nReferences:\n\n1. [CS231n Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu)\n\n---\n\n&#169; 2015 Virginia Tech""",ECE6504 Homework 0
https://github.com/camillescott/avida-modularity,"b'# Evolution of Genetic Modularity in the Absence of Recombination\n\nCode for data generation, analysis, and manuscript for our study of the evolution of modularity in the absence of recombination in digital organisms using the Avida platform.\n'",Study on the evolution of genetic modularity in the absence of recombination
https://github.com/mat-esp/minimos-quadrados,"b'# M\xc3\xa9todo dos M\xc3\xadnimos Quadrados\n\nParte do curso\n[Matem\xc3\xa1tica Especial I](http://www.leouieda.com/matematica-especial/)\nda [UERJ](http://www.uerj.br/).\n\nMinistrado por [Leonardo Uieda](http://www.leouieda.com/).\n\n## Objetivos\n\n* Aprender a formula\xc3\xa7\xc3\xa3o matricial do m\xc3\xa9todo dos m\xc3\xadnimos quadrados (MMQ)\n* Treinar o uso de fun\xc3\xa7\xc3\xb5es e programa\xc3\xa7\xc3\xa3o defensiva\n* Utilizar o MMQ para ajustar uma reta a dados ruidosos\n* Generalizar o MMQ para qualquer fun\xc3\xa7\xc3\xa3o linear\n\n## Leitura recomendada\n\n* https://en.wikipedia.org/wiki/Least_squares\n* https://en.wikipedia.org/wiki/Linear_least_squares_%28mathematics%29\n* http://www.mat.ufmg.br/gaal/aplicacoes/quadrados_minimos.pdf\n* https://pt.wikipedia.org/wiki/M%C3%A9todo_dos_m%C3%ADnimos_quadrados\n\n## Prepara\xc3\xa7\xc3\xa3o\n\nUtilize o link enviado por e-mail para criar um reposit\xc3\xb3rio para seu grupo.\nCada membro do grupo deve clicar no link e selecionar o nome do grupo criado na\npr\xc3\xa1tica passada.\nN\xc3\xa3o se esque\xc3\xa7a de sair de sua conta **no github.com e no\nclassroom.github.com**.\n\n**Crie um arquivo** chamado `alunos.txt` com os **nomes completos** de todos os\nintegrantes do grupo. Inclua tamb\xc3\xa9m a **qual turma** pertencem.\n\n## Avalia\xc3\xa7\xc3\xa3o das pr\xc3\xa1ticas\n\nUtilizaremos a lista abaixo para avaliar a solu\xc3\xa7\xc3\xa3o entregue de cada pr\xc3\xa1tica.\n\n- [ ] Mensagens de commit que explicam claramente a mudan\xc3\xa7a que foi feita\n  [total|parcial|zero] 0.5 pt\n- [ ] Formata\xc3\xa7\xc3\xa3o do c\xc3\xb3digo apropriada.\n  Ex: `espacamento = (maximo - minimo)/(N - 1)`,\n  `indices = range(0, N, 1)`, `dado = dados[i + 1]` == BOM.\n  `espacamento=(maximo-minimo)/ (N-1)`, `indices=range (0,N,1)`,\n  `dado= dados [i+ 1]` == RUIM [total|parcial|zero] 0.5 pt\n- [ ] Utilizar vari\xc3\xa1veis ao inv\xc3\xa9s de colocar n\xc3\xbamero ""na m\xc3\xa3o"".\n  Ex: `for i in range(0, N):`, `A[k][N - 1]` == BOM.\n  `for i in range(0, 39):`, `A[k][47]` == RUIM. [total|parcial|zero] 1 pt\n- [ ] C\xc3\xb3digo com coment\xc3\xa1rios que explicam ""por que"" algo foi feito, n\xc3\xa3o s\xc3\xb3\n  ""o que"" foi feito [total|parcial|zero] 1 pt\n- [ ] Nomes de vari\xc3\xa1veis descritivos. Ex: `temperatura`, `media_por_hora`,\n  `linha`, `somatorio` == BOM. `a`, `var`, `meh`, `lista` == RUIM.\n  [total|parcial|zero] 2 pt\n- [ ] C\xc3\xb3digo produz a solu\xc3\xa7\xc3\xa3o correta (**exatamente** como especificado em\n  ""Resultado esperado"") [total|parcial|zero] 5 pt\n- [ ] Tarefa b\xc3\xb4nus [total|parcial|zero] 1 pt extra (n\xc3\xa3o ser\xc3\xa1 considerado\n  caso a nota j\xc3\xa1 seja 10)\n\nCada crit\xc3\xa9rio de avalia\xc3\xa7\xc3\xa3o poder\xc3\xa1 receber pontua\xc3\xa7\xc3\xa3o:\n\n* Total: se atender **perfeitamente** ao crit\xc3\xa9rio\n* Parcial: (metade da nota) se atender parcialmente ao crit\xc3\xa9rio\n* Zero: se falhar ao crit\xc3\xa9rio\n\nNote que a nota m\xc3\xa1xima, incluindo a tarefa b\xc3\xb4nus, \xc3\xa9 10.\nCada grupo ter\xc3\xa1 acesso a corre\xc3\xa7\xc3\xa3o de sua solu\xc3\xa7\xc3\xa3o.\n\n## Entrega das solu\xc3\xa7\xc3\xb5es\n\nA solu\xc3\xa7\xc3\xa3o de cada pr\xc3\xa1tica ser\xc3\xa1 um reposit\xc3\xb3rio no [Github](http://github.com/)\ncom o c\xc3\xb3digo feito pelos alunos (criado a partir dos templates no\n""Cronograma"").\nA entrega das solu\xc3\xa7\xc3\xb5es ser\xc3\xa1 feita criando uma\n[Issue](https://guides.github.com/features/issues/)\nno reposit\xc3\xb3rio da disciplina\n[leouieda/matematica-especial](https://github.com/leouieda/matematica-especial/).\nUtilize o link abaixo para ir direto para as Issues:\n\nhttps://github.com/leouieda/matematica-especial/issues\n\nCada grupo deve criar uma Issue para entragar cada pr\xc3\xa1tica.\nA issue dever\xc3\xa1 seguir o padr\xc3\xa3o abaixo:\n\n* T\xc3\xadtulo: Deve conter o nome da pr\xc3\xa1tica seguido dos nomes dos integrantes do\n  grupo e a qual turma pertecem (caso haja mais de uma). Ex: ""Pr\xc3\xa1tica\n  Integra\xc3\xa7\xc3\xa3o: Bilbo, John, Arthur - Turma 1""\n* Corpo: Deve conter o link para o reposit\xc3\xb3rio do grupo (ex:\n  `https://github.com/mat-esp-uerj/integracao-sociedade-42`) e qualquer\n  coment\xc3\xa1rio que achar necess\xc3\xa1rio (ex: problemas com os commits, erros que foram\n  arrumados depois, dificuldades, etc).\n\n## License\n\nAll content can be freely used and adapted under the terms of the\n[Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/).\n\n![Creative Commons License](https://i.creativecommons.org/l/by/4.0/88x31.png)\n'",Método dos mínimos quadrados
https://github.com/vyachegrinko/Python_for_Data_Science,"b'# Python for Data Science\n\nThis short primer on [Python](http://www.python.org/) is designed to provide a rapid ""on-ramp"" for computer programmers who are already familiar with basic concepts and constructs in other programming languages to learn enough about Python to effectively use open-source and proprietary Python-based machine learning and data science tools.\n\nThe primer is spread across a collection of [IPython Notebooks](http://ipython.org/notebook.html), and the easiest way to use the primer is to [install IPython Notebook](http://ipython.org/install.html) on your computer. You can also [install Python](https://www.python.org/downloads/), and manually copy and paste the pieces of sample code into the Python interpreter, as the primer only makes use of the Python standard libraries.\n\nThere are four versions of the primer. Three versions contain the entire primer in a single notebook:\n\n* Single IPython Notebook (cleared output cells): [Python_for_Data_Science_clean.ipynb](Python_for_Data_Science_clean.ipynb)\n* Single IPython Notebook (filled output cells): [Python_for_Data_Science_clean.ipynb](Python_for_Data_Science_all.ipynb)\n* Single web page (HTML): [Python_for_Data_Science_all.html](Python_for_Data_Science_all.html)\n\nThe other version divides the primer into 5 separate notebooks:\n\n* [Introduction](1_Introduction.ipynb)\n* [Data Science: Basic Concepts](2_Data_Science_Basic_Concepts.ipynb)\n* [Python: Basic Concepts](3_Python_Basic_Concepts.ipynb)\n* [Using Python to Build and Use a Simple Decision Tree Classifier](4_Python_Simple_Decision_Tree.ipynb)\n* [Next Steps](5_Next_Steps.ipynb)\n\nThere are several exercises included in the notebooks. Sample solutions to those exercises can be found in two Python source files:\n\n* [`simple_ml.py`](simple_ml.py): a collection of simple machine learning utility functions\n* [`simple_decision_tree.py`](simple_decision_tree.py): a Python class to encapsulate a simplified version of a popular machine learning model\n\nThere are also 2 data files, based on the [mushroom dataset](https://archive.ics.uci.edu/ml/datasets/Mushroom) in the UCI Machine Learning Repository, used for coding examples, exploratory data analysis and building and evaluating decision trees in Python:\n\n* [`agaricus-lepiota.data`](agaricus-lepiota.data): a machine-readable list of examples or instances of mushrooms, represented by a comma-separated list of attribute values\n* [`agaricus-lepiota.attributes`](agaricus-lepiota.attributes): a machine-readable list of attribute names and possible attribute values and their abbreviations\n\n## Change Log\n\n2015-07-26\n\n* Updated `all` and `clean` notebooks with additional cells base on PyData Seattle 2015 tutorial Q&A\n\n2015-07-21\n\n* Updated 5 subnotebooks for Python 3 compatibility\n* Changed file name of `SimpleDecisionTree.py` to `simple_decision_tree.py` (class name is unchanged)\n* Reordered introduction of `defaultdict` and `Counter` containers\n* Reordered the values returned by `classification_accuracy()`\n* Added more examples of formatted printing via `str.format()`\n* Various and sundry other minor changes to prepare for [tutorial](http://seattle.pydata.org/schedule/presentation/8/) at [PyData Seattle 2015](http://seattle.pydata.org/)\n\n2015-02-23\n\n* Added attribution for suggested changes to accommodate Python 3 to [Nick Coghlan](https://twitter.com/ncoghlan_dev)\n\n2015-02-22\n\n* Added `from __future__ import print_function, division` for Python 3 compatibility\n* Updated `simple_ml.py` and `SimpleDecisionTree.py` to also use Python 3 `print_function` and `division`\n* Replaced `xrange()` (Python 2 only) with `range()` (Python 2 or 3)\n* Replaced `dict.iteritems()` (Python 2 only) with `dict.items()` (Python 2 or 3)\n* Changed [""call by reference""](https://en.wikipedia.org/wiki/Evaluation_strategy#Call_by_reference) to [""call by sharing""](https://en.wikipedia.org/wiki/Evaluation_strategy#Call_by_sharing)\n* Added `isinstance()` (and reference to duck typing) to section on `type()`\n* Added variable for `delimiter` rather than hard-coding `\'|\'` character\n* Cleaned up various cells'",training module
https://github.com/dangall/Deep-Learning-course,b'# Deep Learning course content\nContains notebook exercises for a Deep Learning course from Udacity\n',Contains notebook exercises for a Deep Learning course from Udacity
https://github.com/JuliaX/cajun-talks,b'julia-cajun-talks\n=================\n',Talk materials for the Cambridge Area Julia User Network
https://github.com/akshaykumarpal/Stock-Market-Movement-Prediction-using-News-Feed-Python-and-R,"b'# Stock Market Movement Prediction using News-Feed in Python and R programming\n\nEnvironment: Python 3.5.2, Anaconda Navigator (1.3.1), R (3.3.1) and R Studio (1.0.136)\n\nWe had taken news feeds from different sources and build a model on a scale of 1 to 4 using sentiment analysis. Also, we took stock data from Dow Jones investors and made the data scaled between 0 to 1 so that we can apply Neural Network. Then, combining these data our final dataset was from past 2008 to 2016 years containing 25 topics for each month. Now we also applied logistic regression to make an indexing of the class label where our objective becomes to predict whether the stock price increases or decreases as \xe2\x80\x980\xe2\x80\x99 and \xe2\x80\x981\xe2\x80\x99 respectively. \n\nWe have applied both Linear Regression and Polynomial Regression to find relation between the change in stock market price vs. the volume as well as sentiment of news articles. We applied our training data on two classification methods such as Logistic Regression and Multi-Layer Perceptron (MLP) to estimate the movement of change in stock market price, volume and the sentiments of news articles. We trained our model on the basis of TF-IDF (Term Frequency \xe2\x80\x93 Inverse Document Frequency) sparse matrix using both unigram and bigram scores.\n\nWe saw Bigram model performs better than Unigram model for both Multi-Layer Perceptron and Logistic Regression. Also, we found that Multi-Layer Perceptron gives us the better accuracy than Logistic Regression.\n'",This project is about Stock Market Movement Prediction using News-Feed in Python and R (Deep Learning and Sentiment Analysis)
https://github.com/jimpala/msci-hep,"b'# msci-hep\nCode for HEP data science work at UCL, Summer 2016.\n'",Code for HEP data science work at UCL.
https://github.com/danielfrg/coursera-intro-ds,b'coursera-intro-ds\n=================\n\nCoursera intro to Data science - Spring 2013\n',Coursera intro to Data science - Spring 2013
https://github.com/CedricVallee/pythonKNearestNeighbors,"b'# pythonKNearestNeighbors\nThis repository contains code for the class IEOR265 ""Learning and Optimization"".\n'","This repository contains code for the class IEOR265 ""Learning and Optimization""."
https://github.com/PythonWorkshop/scalable-sklearn,"b""# Exploring scikit-learn with dask for scaling out computation on large data\n\n## tl;dr\n\nHere, you'll find demonstrations of scalable sklearn with [dask](http://dask.pydata.org/en/latest/) for out-of-core computation on large and complex datasets.  Dask uses task graphs (which are even modifiable) to scale out computation onto disk (out-of-core).  In this way both the computation and amount of data can be scaled in a big way which is really nice for ML.\n\n\n## Blurb\n\nIt\xe2\x80\x99s becoming increasingly important to scale up machine learning and deep learning computation either using a common solution in a cluster of GPUs or out-of-core computation on a single machine with enough local disk storage, which is rarely a problem these days.  Dask is a new library built on python that through out-of-core processes in task graphs can handle large datasets (gbs - tbs) for resource hungry computation.  It can do all this on a single PC/laptop given enough disk.\n\n## Outline\n\n1. Skimage to convert to numeric\n* Standard scaling of data\n* (Optional) clean up noise\n* Image classification with \n  * MLP setup (using sklearn 0.18.dev0)\n  * use dask and the partial_fit for MLP\n* Visualize task graph\n* Try it with gridsearchcv for hyperparameter tuning\n\n\n""",Demonstrations of scalable sklearn with dask for out-of-core computation.
https://github.com/abhishekanand/DS4UX,"b'# DS4UX\nData Science for User Experience Researchers [Details](http://blogs.uw.edu/hcde/2016/02/12/new-ms-course-data-science-for-user-experience-researchers/) \n\nNew MS Course: Data Science for User Experience Researchers\nHCDE 598\nData Science for User Experience Researchers\nSuccess in many UX related roles, particularly user research, require workers to possess an understanding of data science concepts and to have facility with the tools of data analysis. This course introduces students to widely-adopted programming and data science tools to give them the skills to use data to answer questions about the characteristics, behaviors, and needs of people who use a wide variety of products.\n\nThis course has students working with real data from real users. It is built around scenarios that are directly relevant to performing user research in industry, such as:\n\n* identifying user segments (e.g. power users) and popular content\n* manipulating very large datasets (too big for Excel!)\n* performing data visualization and statistical analysis using code (not GUIs)\n* implementing experimental designs such as A/B tests and funnel analysis\n\nThe goal of the course is to provide students with a basic grasp of programming and data science concepts using tools that they can reuse elsewhere. No previous programming experience is required, or even expected.  While the course is framed around user research, the use-cases we will work with are relevant to a wide variety of non-engineering roles in software development and the broader technology industry.\n\n**Upon completion of this course, students should be able to:**\n\n* Write or modify a program to collect a dataset from Wikipedia or the City of Seattle\xe2\x80\x99s open data portal (Data.Seattle.gov)\n* Effectively read web API documentation and write Python software to parse and understand a new and unfamiliar JSON-based web API.\n* Understand database schemas and use MySQL to extract user data from relational databases.\n* Use web-based data to effectively answer a substantively interesting question and to present this data effectively in the context of both a formal presentation and a written report.\n'",Data Science for User Experience Researchers
https://github.com/datakop/rabbitmq-notebook-examples,b'## RabbitMQ iPython Examples\n[Original tutorials](https://www.rabbitmq.com/getstarted.html)\n\n#### Install environment\n\n```sh\nvirtualenv .mqEnv\n. ./.mqEnv/bin/activate\npip install -r requirements.txt --upgrade\n```\n\n#### Ramp up RabbitMQ(in docker)\n```sh\ndocker pull rabbitmq:3-management\ndocker run -d  --hostname local_rabbitmq --name local_rabbitmq -p 15672:15672 -p 15674:15674 -p 5672:5672 rabbitmq:3-management\n```\n\n#### How to use\nI Run iPython notebook\n```sh\n. ./.mqEnv/bin/activate\n./.mqEnv/bin/ipython notebook notebooks # it will automatically open browser with notebooks.\n ```\nII Visit RabbitMQ managment site `http://DOCKER_IP:15672/` to check how MQ works.\n\nTo get docker ip run `docker-machine ip default`\n\n##### ToDo:\n- Remote procedure call (RPC) example. [link](https://www.rabbitmq.com/tutorials/tutorial-six-python.html)\n\n### Links:\n- [Docker](https://www.docker.com/whatisdocker)\n- [RabbitMQ](https://www.rabbitmq.com/features.html)\n- [iPyhton Notebook](http://ipython.org/notebook.html)',RabbitMQ example inside iPython Notebook.
https://github.com/anne4180/my_first_python_rep,b'# my_first_python_rep\nmy first python repository\n2020\n',my first python repository
https://github.com/yanyanmountainview/student_intervention,"b'# Project 2: Supervised Learning\n## Building a Student Intervention System\n\n### Install\n\nThis project requires **Python 2.7** and the following Python libraries installed:\n\n- [NumPy](http://www.numpy.org/)\n- [Pandas](http://pandas.pydata.org)\n- [scikit-learn](http://scikit-learn.org/stable/)\n\nYou will also need to have software installed to run and execute an [iPython Notebook](http://ipython.org/notebook.html)\n\nUdacity recommends our students install [Anaconda](https://www.continuum.io/downloads), i pre-packaged Python distribution that contains all of the necessary libraries and software for this project. \n\n### Code\n\nTemplate code is provided in the notebook `student_intervention.ipynb` notebook file. While some code has already been implemented to get you started, you will need to implement additional functionality when requested to successfully complete the project.\n\n### Run\n\nIn a terminal or command window, navigate to the top-level project directory `student_intervention/` (that contains this README) and run one of the following commands:\n\n```ipython notebook student_intervention.ipynb```  \n```jupyter notebook student_intervention.ipynb```\n\nThis will open the iPython Notebook software and project file in your browser.\n\n## Data\n\nThe dataset used in this project is included as `student-data.csv`. This dataset has the following attributes:\n\n- `school` ? student\'s school (binary: ""GP"" or ""MS"")\n- `sex` ? student\'s sex (binary: ""F"" - female or ""M"" - male)\n- `age` ? student\'s age (numeric: from 15 to 22)\n- `address` ? student\'s home address type (binary: ""U"" - urban or ""R"" - rural)\n- `famsize` ? family size (binary: ""LE3"" - less or equal to 3 or ""GT3"" - greater than 3)\n- `Pstatus` ? parent\'s cohabitation status (binary: ""T"" - living together or ""A"" - apart)\n- `Medu` ? mother\'s education (numeric: 0 - none,  1 - primary education (4th grade), 2 -\xe2\x82\xac\xe2\x80\x9c 5th to 9th grade, 3 - secondary education or 4 -\xe2\x82\xac\xe2\x80\x9c higher education)\n- `Fedu` ? father\'s education (numeric: 0 - none,  1 - primary education (4th grade), 2 - 5th to 9th grade, 3 - secondary education or 4 -\xe2\x82\xac\xe2\x80\x9c higher education)\n- `Mjob` ? mother\'s job (nominal: ""teacher"", ""health"" care related, civil ""services"" (e.g. administrative or police), ""at_home"" or ""other"")\n- `Fjob` ? father\'s job (nominal: ""teacher"", ""health"" care related, civil ""services"" (e.g. administrative or police), ""at_home"" or ""other"")\n- `reason` ? reason to choose this school (nominal: close to ""home"", school ""reputation"", ""course"" preference or ""other"")\n- `guardian` ? student\'s guardian (nominal: ""mother"", ""father"" or ""other"")\n- `traveltime` ? home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)\n- `studytime` ? weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)\n- `failures` ? number of past class failures (numeric: n if 1<=n<3, else 4)\n- `schoolsup` ? extra educational support (binary: yes or no)\n- `famsup` ? family educational support (binary: yes or no)\n- `paid` ? extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)\n- `activities` ? extra-curricular activities (binary: yes or no)\n- `nursery` ? attended nursery school (binary: yes or no)\n- `higher` ? wants to take higher education (binary: yes or no)\n- `internet` ? Internet access at home (binary: yes or no)\n- `romantic` ? with a romantic relationship (binary: yes or no)\n- `famrel` ? quality of family relationships (numeric: from 1 - very bad to 5 - excellent)\n- `freetime` ? free time after school (numeric: from 1 - very low to 5 - very high)\n- `goout` ? going out with friends (numeric: from 1 - very low to 5 - very high)\n- `Dalc` ? workday alcohol consumption (numeric: from 1 - very low to 5 - very high)\n- `Walc` ? weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)\n- `health` ? current health status (numeric: from 1 - very bad to 5 - very good)\n- `absences` ? number of school absences (numeric: from 0 to 93)\n- `passed` ? did the student pass the final exam (binary: yes or no)\n'","Based on students' performance data,  developed a model that predicted the likelihood that a given student will pass, quantifying whether an intervention is necessary."
https://github.com/pszjmb1/fun_with_metropolis_algorithm,"b""# fun_with_metropolis_algorithm\nDeveloping intuition behind MCMC sampling. \n\nBased on [Thomas Wiecki's fantastic tutorial](http://twiecki.github.io/blog/2015/11/10/mcmc-sampling/).\n""",Developing intuition behind MCMC sampling
https://github.com/pbrusco/crystal-learn,"b'## An sklearn-like machine-learning library for Crystal\n\nExample (that can be found in examples folder)\n```crystal\nrequire ""csv""\nrequire ""../knn""\nrequire ""../trees""\nrequire ""../array""\nrequire ""../ml""\n\nputs ""Loading IRIS dataset""\n\nx, y = ML.load_floats_csv(""iris.csv"")\nputs ""Shapes: X: #{x.shape}, y: #{y.shape}""\n\ndef folds_accuracy(clf, x, y, *, n_folds k)\n  accuracies = [] of Float64\n  folds = ML.kfold_cross_validation(n_folds: k, dataset_size: y.size)\n\n  folds.each do |train_index, test_index|\n    x_train, y_train = x[train_index], y[train_index]\n    x_test, y_test = x[test_index], y[test_index]\n\n    clf.fit(x_train, y_train)\n    y_pred = clf.predict(x_test)\n\n    accuracies << ML.accuracy(y_test, y_pred)\n  end\n  accuracies.mean\nend\n\nputs ""------------------ KNN -------------------""\n\n(5..150).step(10).each do |n|\n  clf = ML::Classifiers::KNeighborsClassifier(typeof(x.first.first), typeof(y.first)).new(n_neighbors: n)\n  folds_acc = folds_accuracy(clf, x, y, n_folds: 10).round(2)\n  puts ""10-folds accuracy #{folds_acc} (KNN - #{n} neighbors)""\nend\n\nputs ""------------------ TREES -------------------""\n\n(2..15).each do |max_depth|\n  clf = ML::Classifiers::DecisionTreeClassifier(typeof(x.first.first), typeof(y.first)).new(max_depth: max_depth)\n  folds_acc = folds_accuracy(clf, x, y, n_folds: 10).round(2)\n  puts ""10-folds accuracy #{folds_acc} (DecisionTreeClassifier - max_depth: #{max_depth})""\nend\n  # uncomment to vizualize the tree:\n  # clf.show_tree(%w(sepal_length sepal_width petal_length petal_width species))\nputs\n\n```\n\n### Output:\n```\nLoading IRIS dataset\nShapes: X: {150, 4}, y: 150\n------------------ KNN -------------------\n10-folds accuracy 0.97 (KNN - 5 neighbors)\n10-folds accuracy 0.96 (KNN - 15 neighbors)\n10-folds accuracy 0.95 (KNN - 25 neighbors)\n10-folds accuracy 0.93 (KNN - 35 neighbors)\n10-folds accuracy 0.95 (KNN - 45 neighbors)\n10-folds accuracy 0.94 (KNN - 55 neighbors)\n10-folds accuracy 0.91 (KNN - 65 neighbors)\n10-folds accuracy 0.88 (KNN - 75 neighbors)\n10-folds accuracy 0.81 (KNN - 85 neighbors)\n10-folds accuracy 0.62 (KNN - 95 neighbors)\n10-folds accuracy 0.43 (KNN - 105 neighbors)\n10-folds accuracy 0.43 (KNN - 115 neighbors)\n10-folds accuracy 0.56 (KNN - 125 neighbors)\n10-folds accuracy 0.28 (KNN - 135 neighbors)\n10-folds accuracy 0.25 (KNN - 145 neighbors)\n------------------ TREES -------------------\n10-folds accuracy 0.89 (DecisionTreeClassifier - max_depth: 2)\n10-folds accuracy 0.93 (DecisionTreeClassifier - max_depth: 3)\n10-folds accuracy 0.93 (DecisionTreeClassifier - max_depth: 4)\n10-folds accuracy 0.93 (DecisionTreeClassifier - max_depth: 5)\n10-folds accuracy 0.93 (DecisionTreeClassifier - max_depth: 6)\n10-folds accuracy 0.93 (DecisionTreeClassifier - max_depth: 7)\n10-folds accuracy 0.93 (DecisionTreeClassifier - max_depth: 8)\n10-folds accuracy 0.95 (DecisionTreeClassifier - max_depth: 9)\n10-folds accuracy 0.94 (DecisionTreeClassifier - max_depth: 10)\n10-folds accuracy 0.93 (DecisionTreeClassifier - max_depth: 11)\n10-folds accuracy 0.93 (DecisionTreeClassifier - max_depth: 12)\n10-folds accuracy 0.95 (DecisionTreeClassifier - max_depth: 13)\n10-folds accuracy 0.91 (DecisionTreeClassifier - max_depth: 14)\n10-folds accuracy 0.95 (DecisionTreeClassifier - max_depth: 15)\n\n```\n'",Machine Learning in Crystal 
https://github.com/yanyu711141/6_12_2016_SoftwareCarpentry,b'# 6_12_2016_SoftwareCarpentry\ncode from the workshop\n\n#\n##\n###\n',code from the workshop
https://github.com/jasonamyers/pycon2014,"b'Slideshow available on [My Blog](http://www.jasonamyers.com/pycon2014/#/)\n\nYou can view a PDF version of the slides at\n[SlideShare](http://www.slideshare.net/jamdatadude/sql-alchemy-core-an-introduction).\n\nIf you want to run the slides, you must have node installed then:\n* type ``npm install``\n* type ``grunt serve``\n* open ``http://127.0.0.1:8000/``\n\nIf you want to access the ipython notebook for this presentation:\n* create a virtualenv (For example: ``mkvirtualenv sacore``)\n* type ``pip install -r requirements.txt``\n* type ``ipython notebook``\n'",My Intro to SQLAlchemy Core Slides
https://github.com/cgerson/bike-share-analysis,"b'# bike-share-analysis\n\nI used NYC citibike <a href=""http://www.citibikenyc.com/system-data"" target=""_blank"">system data</a> to implement a classification system to predict routes of citi-bikers given starting station, gender & age of user, day of the week, and time of day.\n\n\n####Data:\n- Merged NYC boundaries geometry file (<a href = ""http://catalog.opendata.city/dataset/nyc-neighborhood-tabulation-areas-polygon"" target=""_blank"">source</a>) with station lat/lon data in CartoDB to link each station to its associated neighborhood. (see nyc_neighborhoods.csv)\n- Downloaded Dec 2014, Jan & Feb 2015 citibike data and merged with nyc_neighborhoods to find associated neighborhoods for each bike route. (see citibike_dec.csv, citibike_jan.csv & citibike_feb.csv)\n\n####Models:\n- Data was merged and cleaned (see citibike notebook)\n- Ran different classification algorithms to predict destination <i>neighborhood</i> and scored each on accuracy and precision. Made sure models sampled evenly from each class, given class imbalance. Each destination neighborhood is either correct or incorrect, so accuracy is an effective metric. (see citibike_models notebook)\n\n####Results:\n- See my <a href = ""http://cgerson.github.io/citimodels/"" target=""_blank"">blog post</a> for a write up of results and a copy of the final presentation'",Analysis of open bikeshare data
https://github.com/toyota790/Twitter_PanamaPapers_Analysis,"b'# Twitter Panama Papers Analysis\nThis project is using the Logstash to get data from Twitter. Then use the PySpark K-Means algorithm to clustering.\n\n##Architecture\n![Architecture](Architecture.png)\n\n##Environment\n* Anaconda version: 4.0.8\n* Python version: 2.7.11 \n* iPython version: 4.1.2\n* Spark version: 1.5.2\n* NLTK version: 3.2\n* Pandas version: 0.18.1\n* Scikit-learn version: 0.17.1\n* Snow Ball Stemmer version: 1.2.1\n* Bokeh version: 0.11.1\n* Logstash version: 2.3.1\n* Elasticsearch version: 2.3.1\n* JAVA Version 8 Update 77\n\n##Data Collection\n* Logstash to Elasticsearch ([Twitter Streamming API](https://dev.twitter.com/rest/public))<br>\n(Note: You can use the Python to crawl the data, it use the [Twitter REST API](https://dev.twitter.com/rest/public). Reference code is available in my Github [repository](https://github.com/toyota790/Twitter2JSON).)\n* Data Format: CSV\n* Search Keyword:\n<ol>\n\t<li>""#panamapapers""</li>\n\t<li>""panamapapers""</li>\n\t<li>""panama paper""</li>\n\t<li>""the panama paper""</li>\n</ol>\n\n##Data Source\n* 514 attributes\n* Data Size\n\t* Total: 200000 (484 MB)\n\t* Training dataset: 20000\n* Time\n\t* Start: Sun Apr 10 16:18:35 +0000 2016\n\t* End: Wed Apr 13 18:32:27 +0000 2016\n\n##Data Cleaning\n* URL\n\t* https, http\n* Emoji\n\t* UCS-4, UCS-2\n* Alphabet\n\t* a, c, l, etc.\n* Stop word\n\t*  NLTK\xe2\x80\x99s list of English stop words\n* Punctuation\n\t* dot, question mark, etc.\n##Feature Selection\n* Stemming\n\t* Panamapapers -> panamapap\n\t* Family -> famili\n\t* Link -> link\n* Tokenizing \n* TF-IDF\n\t* 2000 features\n\n##Data Modeling\n* K-means++ Clustering Algorithm\n\t* K=4\n* The size of each cluster:\n\t* Cluster 0: 5158\n\t* Cluster 1: 964\n\t* Cluster 2: 13233\n\t* Cluster 3: 645\n\n##Visualization\n* Bokeh (Two dimensions)\n![Bokeh Result] (Result/Bokeh_Result.png)\n\n* Word Cloud - Cluster 0\n![Word Cloud Cluster 0] (Result/WordCloud_cluster0.png)\n\n* Word Cloud - Cluster 1\n![Word Cloud Cluster 1] (Result/WordCloud_cluster1.png)\n\n* Word Cloud - Cluster 2\n![Word Cloud Cluster 2] (Result/WordCloud_cluster2.png)\n\n* Word Cloud - Cluster 3\n![Word Cloud Cluster 3] (Result/WordCloud_cluster3.png)\n\n* Plotly (Three dimensions)\n![Plotly Result] (Result/Plotly_Result.png)\n'",This project is using the Logstash to get data from Twitter. Then use the PySpark K-Means algorithm to clustering.
https://github.com/phgcarva/atai-notebooks-machine-learning,b'# atai-notebooks-machine-learning\nA series of homeworks done for the course CK0146 - Advanced Topics in Artificial Inteligence (Machine Learning)\n',A series of homeworks done for the course CK0146 - Advanced Topics in Artificial Inteligence (Machine Learning)
https://github.com/mbmilligan/msi-ipython-nb-ex,"b'[![Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/mbmilligan/msi-ipython-nb-ex/master)\n\nPython Examples for MSI\n=======================\n\nThe IPython notebooks in this repository accompany the Python for Scientific Computing workshop hosted by the Minnesota Supercomputing Institute. Visit https://www.msi.umn.edu/tutorials/python-scientific-computing for details and a PDF copy of the slides.\n\nGetting started\n---------------\n\nYou can clone a copy of this repository using git, for example:\n\n    git clone https://github.com/mbmilligan/msi-ipython-nb-ex\n    \nIf you have a web browser and are connected to the UMN network, you can access our JupyterHub server at: [notebooks.msi.umn.edu](https://notebooks.msi.umn.edu)\n\nIf you are at a command line at the MSI, the commands\n\n    module load python\n    cd msi-ipython-nb-ex\n    ipython notebook\n\nwill start the Jupyter notebook system in your web browser. From there, you can open the saved IPython notebooks (files with the extension .ipynb) by clicking on them.\n\nIf you do not have access to a Python environment right now, you can click on the Binder badge above to run these examples through [Mybinder.org](https://mybinder.org/) in a temporary environment. \n'",Example IPython notebooks for MSI
https://github.com/JonathanToro/Fish_Image_Classification,b'# Catching Fish With Neural Nets\n## Goal: Use computer vision and CNNs to accurately detect fish species in an image\n\nMy main approach in tackling this Kaggle competition was by using Convolutional Neural Networks. I used two different pretrained neural networks and architectures provided by Google. My best results was by using the InceptionV3 model and augmenting the test data so that the model had multiple tries to classify each image correctly. I hosted the modeling on Amazon Web Services by using their g2.8x instance which provided me with 4 gpus to work with. The code to parallelize the training is provided above. At the time of the submission of my best model it ranked top 5% on Kaggle.\n\nPlease refer to my [blog][1] for more information.\n\n[1]: https://jonathantoro.github.io/Catching-Fish-With-Neural-Nets/\n',Kaggle competition code that uses CNNs for image classification. Ranked top 5% at the time of submission
https://github.com/aprilnovak/coupling-moose,"b'MOOSE\n=====\n\n[![Build status](https://www.moosebuild.org/idaholab/moose/master/branch_status.svg)](https://www.moosebuild.org/repo/idaholab/moose/)\n\nThe Multiphysics Object-Oriented Simulation Environment (MOOSE) is a finite-element, multiphysics framework primarily developed by [Idaho National Laboratory](http://www.inl.gov). It provides a high-level interface to some of the most sophisticated [nonlinear solver technology](http://www.mcs.anl.gov/petsc/) on the planet. MOOSE presents a straightforward API that aligns well with the real-world problems scientists and engineers need to tackle. Every detail about how an engineer interacts with MOOSE has been thought through, from the installation process through running your simulation on state of the art supercomputers, the MOOSE system will accelerate your research.\n\nSome of the capability at your fingertips:\n\n* Fully-coupled, fully-implicit multiphysics solver\n* Dimension independent physics\n* Automatically parallel (largest runs >100,000 CPU cores!)\n* Modular development simplifies code reuse\n* Built-in mesh adaptivity\n* Continuous and Discontinuous Galerkin (DG) (at the same time!)\n* Intuitive parallel multiscale solves (see videos below)\n* Dimension agnostic, parallel geometric search (for contact related applications)\n* Flexible, plugable graphical user interface\n* ~30 plugable interfaces allow specialization of every part of the solve\n\nMore Information\n================\n\n**For more information, including installation instructions, please see the official website: [http://mooseframework.org](http://mooseframework.org)**\n\nContributing\n============\n\nFor information on how to contribute code changes to MOOSE please [see the mooseframework.org wiki](http://mooseframework.org/wiki/Contributing/).\n'",Local MOOSE repo holding common functionalities that in the future should be put in a MOOSE module
https://github.com/andridns/tf-cifar10,"b""# tf-CIFAR10\n\nA Convolutional Neural Network to classify CIFAR-10 image dataset using TensorFlow.\n\n## Files\n\nTo open the main code, simply open [`tf-cifar10.ipynb`](https://github.com/adsasmita/tf-cifar10/blob/master/tf-cifar10.ipynb) on any desktop browser, or you can download and run the cells in a Python 3 environment. The code is presented in a [Jupyter Notebook](https://github.com/jupyter/notebook) / iPython notebook for readability purposes.\n\n\n## Overview\n\nThis project classifies images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html). The dataset consists of airplanes, dogs, cats, and other objects. We will preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.\n\n![title](img/cifar10-1.png)\n\n## Data\n\nThe dataset is broken into batches to prevent our machine from running out of memory. The CIFAR-10 dataset consists of 5 batches, named data_batch_1, data_batch_2, etc.. Each batch contains the labels and images that are one of the following:\nairplane\nautomobile\nbird\ncat\ndeer\ndog\nfrog\nhorse\nship\ntruck\n\n## Convolutional Neural Network Architecture\n\n![title](img/cnn1.png)\n\nThis code builds combination of convolutional neural network, max pooling, dropout, and fully connected layers.  At the end, there will be a test on the neural network's predictions on sample images.\n\n![title](img/cnn2.png)\n\n## Result\n\nThe final accuracy is about 67%, which is much better than pure guessing (pure guessing on CIFAR-10 dataset will stochastically yield 10% accuracy).\n\n![title](img/result.png)\n\n## Dependencies\n\nThis project requires **Python 3** and the following Python libraries installed:\n\n* [TensorFlow](https://www.tensorflow.org/get_started/get_started)\n* [NumPy](http://www.numpy.org/)\n* [tqdm](https://pypi.python.org/pypi/tqdm) - Progress Meter\n* [pickle](https://docs.python.org/3/library/pickle.html)\n* [helper](https://pypi.python.org/pypi/helper)\n\n\n\n\n""",Convolutional Neural Network to classify CIFAR-10 image dataset using TensorFlow
https://github.com/fsfelix/audio-thumbnailing,"b'# audio-thumbnailing\nRepository for my scientific initiation project on audio thumbnailing. \n\n## Usage Requirements\n        * python 3\n        * jupyter-notebook\n        * numpy\n        * librosa\n\n## References\n* Cooper, Matthew L., and Jonathan Foote. ""Automatic Music Summarization via Similarity Analysis."" ISMIR. 2002.\n* M\xc3\xbcller, Meinard, Peter Grosche, and Nanzhu Jiang. ""A Segment-Based Fitness Measure for Capturing Repetitive Structures of Music Recordings."" ISMIR. 2011.\n* Bartsch, Mark A., and Gregory H. Wakefield. ""Audio thumbnailing of popular music using chroma-based representations."" IEEE Transactions on multimedia 7.1 (2005): 96-104.\n'",Repository for my scientific initiation project on audio thumbnailing. 
https://github.com/tmhughes81/student_intervention,"b'# Project 2: Supervised Learning\n## Building a Student Intervention System\n\n### Install\n\nThis project requires **Python 2.7** and the following Python libraries installed:\n\n- [NumPy](http://www.numpy.org/)\n- [Pandas](http://pandas.pydata.org)\n- [scikit-learn](http://scikit-learn.org/stable/)\n\nYou will also need to have software installed to run and execute an [iPython Notebook](http://ipython.org/notebook.html)\n\nUdacity recommends our students install [Anaconda](https://www.continuum.io/downloads), a pre-packaged Python distribution that contains all of the necessary libraries and software for this project. \n\n### Code\n\nTemplate code is provided in the notebook `student_intervention.ipynb` notebook file. While some code has already been implemented to get you started, you will need to implement additional functionality when requested to successfully complete the project.\n\n### Run\n\nIn a terminal or command window, navigate to the top-level project directory `student_intervention/` (that contains this README) and run one of the following commands:\n\n```ipython notebook student_intervention.ipynb```  \n```jupyter notebook student_intervention.ipynb```\n\nThis will open the iPython Notebook software and project file in your browser.\n\n## Data\n\nThe dataset used in this project is included as `student-data.csv`. This dataset has the following attributes:\n\n- `school` : student\'s school (binary: ""GP"" or ""MS"")\n- `sex` : student\'s sex (binary: ""F"" - female or ""M"" - male)\n- `age` : student\'s age (numeric: from 15 to 22)\n- `address` : student\'s home address type (binary: ""U"" - urban or ""R"" - rural)\n- `famsize` : family size (binary: ""LE3"" - less or equal to 3 or ""GT3"" - greater than 3)\n- `Pstatus` : parent\'s cohabitation status (binary: ""T"" - living together or ""A"" - apart)\n- `Medu` : mother\'s education (numeric: 0 - none,  1 - primary education (4th grade), 2 - 5th to 9th grade, 3 - secondary education or 4 - higher education)\n- `Fedu` : father\'s education (numeric: 0 - none,  1 - primary education (4th grade), 2 - 5th to 9th grade, 3 - secondary education or 4 - higher education)\n- `Mjob` : mother\'s job (nominal: ""teacher"", ""health"" care related, civil ""services"" (e.g. administrative or police), ""at_home"" or ""other"")\n- `Fjob` : father\'s job (nominal: ""teacher"", ""health"" care related, civil ""services"" (e.g. administrative or police), ""at_home"" or ""other"")\n- `reason` : reason to choose this school (nominal: close to ""home"", school ""reputation"", ""course"" preference or ""other"")\n- `guardian` : student\'s guardian (nominal: ""mother"", ""father"" or ""other"")\n- `traveltime` : home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)\n- `studytime` : weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)\n- `failures` : number of past class failures (numeric: n if 1<=n<3, else 4)\n- `schoolsup` : extra educational support (binary: yes or no)\n- `famsup` : family educational support (binary: yes or no)\n- `paid` : extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)\n- `activities` : extra-curricular activities (binary: yes or no)\n- `nursery` : attended nursery school (binary: yes or no)\n- `higher` : wants to take higher education (binary: yes or no)\n- `internet` : Internet access at home (binary: yes or no)\n- `romantic` : with a romantic relationship (binary: yes or no)\n- `famrel` : quality of family relationships (numeric: from 1 - very bad to 5 - excellent)\n- `freetime` : free time after school (numeric: from 1 - very low to 5 - very high)\n- `goout` : going out with friends (numeric: from 1 - very low to 5 - very high)\n- `Dalc` : workday alcohol consumption (numeric: from 1 - very low to 5 - very high)\n- `Walc` : weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)\n- `health` : current health status (numeric: from 1 - very bad to 5 - very good)\n- `absences` : number of school absences (numeric: from 0 to 93)\n- `passed` : did the student pass the final exam (binary: yes or no)\n'",udacity Project #2: Student Intervention
https://github.com/srv902/MonoDepthCNN,"b'# MonoDepthCNN\nA sample take on the CNN based approach to find the depth map of a monocular image.\n\ndatasolver.m\nIt preprocesses the RAW NYUV2 dataset across different classes. The order of execution according to what I have done of the matlab script is given below. The mentioned matlab scripts are available freely under NYUV2-dataset-toolbox. \n\n1. Get the matched depth and rgb images using get_synched_frames.m\n2. After obtaining the images, apply project_depth_map.m . It aligns the depth map onto the rgb image.\n3. crop_image.m to select only those portions of the image which have valid depth data.\n4. fill_depth_cross_bf.m to fill the missing depth values which incorporates cross bilateral filter...\n'",A sample take on the CNN based approach to find the depth map of a monocular image.
https://github.com/interactivetech/DeepStyleArtClassifier,"b'# DeepStyleArtClassifier\n## Specialization Project, Cornell Tech\n\n\nUsing Keras and Tensorflow to develop a classifier that can classify different art styles.\nWe will be leveraging weights from VGG16, VGG19, Inceptionv3, and ResNet50\n\n\n## Installation\n\npip install -r requirements.txt\nipython notebook\n\n\n### NOTE: I would recommend using virtualenv\n\nlinks that helped this project:\n\nLoading the models: http://www.pyimagesearch.com/2017/03/20/imagenet-vggnet-resnet-inception-xception-keras/\nTraining new model with VGG Model weights: https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n'",CornellTech specialization project
https://github.com/DessimozLab/treeCl,"b'# ``treeCl`` - Phylogenetic Tree Clustering\n\n![Travis build status](https://travis-ci.org/kgori/treeCl.svg?branch=master)\n\n``treeCl`` is a python package for clustering gene families by\nphylogenetic similarity. It takes a collection of alignments, infers their phylogenetic trees,\nand clusters them based on a matrix of between-tree distances. Finally, it calculates a single representative tree for each cluster.\n\nYou can read the paper [here](https://dx.doi.org/10.1093/molbev/msw038)\n\n## Installation\n\n#### Preparing dependencies\n\nIf your system already has python 2.7, cython, numpy and a C++11-capable compiler (e.g. gcc >= 4.7), then you\'re ready to install.\n\nThe remaining python dependencies will be automatically installed during the build process.\n\n#### External dependencies\n\nTo be able to build trees, treeCl needs to call on some external software. The choices are RAxML, PhyML, FastTree or PLL (using [pllpy](https://github.com/kgori/pllpy)). If any of these are installed, available in your path, and keep the standard names they were installed with, they should work.\n\n#### Installing ``treeCl``\nAll remaining dependencies will be installed automatically using pip\n\n    pip install treeCl\n\n\n\n## Example Analysis\n``` python\n\nimport treeCl\n\n""""""\nThe first point of call is the treeCl.Collection class. \nThis handles loading your data, and calculating the trees \nand distances that will be used later.\n\nThis is how to load your data. This should be a directory\nfull of sequence alignments in fasta \'*.fas\' or phylip\n\'*.phy\' formats. These can also be zipped using gzip or \nbzip2, treeCl will load them directly.\n""""""\nc = treeCl.Collection(input_dir=\'input_dir\', file_format=\'phylip\')\n\n""""""\nNow it\'s time to calculate some trees. The simplest way to \ndo this is\n""""""\nc.calc_trees()\n\n""""""\nThis uses RAxML to infer a tree for each alignment. We can \npass arguments to RAxML using keywords.\n""""""\nc.calc_trees(executable=\'raxmlHPC-PTHREADS-AVX\',  # specify raxml binary to use\n             threads=8,  # use multithreaded raxml\n             model=\'PROTGAMMAWAGX\',  # this model of evolution\n             fast_tree=True)  # use raxml\'s experimental fast tree search option\n\n""""""\nWe can use PhyML instead of RAxML. Switching programs is \ndone using a TaskInterface\n""""""\n\nphyml = treeCl.tasks.PhymlTaskInterface()\nc.calc_trees(task_interface=phyml)\n\n""""""\nPhyML doesn\'t support multithreading, but treeCl can run \nmultiple instances using JobHandlers\n""""""\n\nthreadpool = treeCl.parutils.ThreadpoolJobHandler(8)  # external software can be run in parallel\n                                              # using a threadpool.\n                                              \nc.calc_trees(jobhandler=threadpool, task_interface=phyml)\n\n""""""\nTrees are expensive to calculate. Results can be cached to disk, \nand reloaded.\n""""""\nc.write_parameters(\'cache\')\nc = treeCl.Collection(input_dir=\'input_dir\', param_dir=\'cache\')\n\n""""""\nOnce trees have been calculated, we can measure all the \ndistances between them. treeCl implements Robinson-Foulds (rf), \nweighted Robinson-Foulds (wrf), Euclidean (euc), and \ngeodesic (geo) distances.\n""""""\ndm = c.get_inter_tree_distances(\'geo\')  \n\n# Alternatively\nprocesses = treeCl.parutils.ProcesspoolJobHandler(8)  # with pure python code, it is better to use processpools to parallelise for speed\ndm = c.get_inter_tree_distances(\'geo\', \n                                jobhandler=processes, \n                                batchsize=100)  # jobs are done in batches to\n                                                # reduce overhead\n\n""""""\nHierarchical Clustering\n""""""\nhclust = treeCl.Hierarchical(dm)\npartition = hclust.cluster(3)  # partition into 3 clusters\n\n# To use different linkage methods\nfrom treeCl.clustering import linkage\npartition = hclust.cluster(3, linkage.AVERAGE)\npartition = hclust.cluster(3, linkage.CENTROID)\npartition = hclust.cluster(3, linkage.COMPLETE)\npartition = hclust.cluster(3, linkage.MEDIAN)\npartition = hclust.cluster(3, linkage.SINGLE)\npartition = hclust.cluster(3, linkage.WARD)  # default, Ward\'s method\npartition = hclust.cluster(3, linkage.WEIGHTED)\n\n""""""\nSpectral Clustering\n""""""\nspclust = treeCl.Spectral(dm)\npartition = spclust.cluster(3)\n\n# Alternative calls\nfrom treeCl.clustering import spectral, methods\nspclust.cluster(3, algo=spectral.SPECTRAL, method=methods.KMEANS) # these are the defaults\nspclust.cluster(3, algo=spectral.KPCA, method=methods.GMM) # alternatives use kernel PCA and a Gaussian Mixture Model\n\n# Getting transformed coordinates\nspclust.spectral_embedding(2) # spectral embedding in 2 dimensions\nspclust.kpca_embedding(3) # kernel PCA embedding in 3 dimensions\n\n""""""\nMultidimensional scaling\n""""""\nmdsclust = treeCl.MultidimensionalScaling(dm)\npartition = mdsclust.cluster(3)\n\n# Alternatives: classical or metric MDS\nfrom treeCl.clustering import mds\npartition = mdsclust.cluster(3, algo=mds.CLASSICAL, method=methods.KMEANS)\npartition = mdsclust.cluster(3, algo=mds.METRIC, method=methods.GMM)\n\n# Getting transformed coordinates\nmdsclust.dm.embedding(3, \'cmds\')  # Classical MDS, 3 dimensions\nmdsclust.dm.embedding(2, \'mmds\')  # Metric MDS, 2 dimensions\n\n""""""\nScore the result via likelihood\n""""""\nraxml = treeCl.tasks.RaxmlTaskInterface()\nsc = treeCl.Scorer(c, cache_dir=\'scorer\', task_interface=raxml) \nsc.write_partition(partition)\nresults = sc.analyse_cache_dir(executable=\'raxmlHPC-PTHREADS-AVX\', threads=8)\n\n""""""\nGet the results\n""""""\n# Get concatenated sequence alignments for each group\nconcats = [c.concatenate(grp) for grp in partition.get_membership()]\nalignments = [conc.alignment for conc in concats]\n\n# Get a list of the loci in each group\nloci = sc.get_partition_members(partition)\n\n# Get trees for each group\ntrees = sc.get_partition_trees(partition)\n\n# Get full model parameters for each group\nfull_results = sc.get_partition_results(partition)  # same as returned by analyse_cache_dir\n\n'",Phylogenetic clustering package
https://github.com/regreg/regreg,"b'######\nRegReg\n######\n\nRegReg is a simple multi-algorithm Python framework for prototyping and solving\nregularized regression problems such as the LASSO. The goal is to enable\npractitioners to quickly and easily experiment with a variety of different\nmodels and choices of regularization.  In that spirit, the emphasis is on the\nflexibility of the framework instead of computational speed for any particular\nproblem, though the speed trade-off will generally not be too bad.\n\n****\nCode\n****\n\nSee https://github.com/regreg/regreg\n\nReleased under the BSD two-clause license - see the file ``LICENSE`` in the\nsource distribution.\n\nThe latest released version is at https://pypi.python.org/pypi/regreg\n\n*******\nSupport\n*******\n\nPlease put up issues on the `regreg issue tracker\n<https://github.com/regreg/regreg/issues>`_.\n'",A multi-algorithm Python framework for regularized regression
https://github.com/shiftshuffle/biomat,b'# biomat\nbiomat\n',biomat
https://github.com/devrandom/python-blockstack,"b'[![Build Status](https://travis-ci.org/devrandom/python-blockstack.svg?branch=master)](https://travis-ci.org/devrandom/python-blockstack)\n\nBlockstack API (https://blockstack.io/)\n\n## Examples\n\n\n```python\n    from blockstack.client import BlockstackClient\n    token = ""YOUR_API_TOKEN""\n    client = BlockstackClient(base_uri=\'https://YOURNAMEHERE.blockstack.io/api\', token=token)\n    alice = client.wallets.get(\'Blue\')\n    bob = client.wallets.get(\'Green\')\n    print(alice.__dict__.keys())\n    print(alice.currentAddress)\n    print(alice.currentHeight)\n```\n\n```\n    dict_keys([\'name\', \'allBalances\', \'currentAddress\', \'auth\', \'base_uri\', \'id\', \'transactions\', \'currentHeight\', \'parent\', \'assetAddress\', \'balances\', \'timeout\', \'netBalances\'])\n    ms9tDLTTqjQa7daCQHj39jjHdTU8AK4of3\n    653\n```\n\n\n```python\n    alice_txs = alice.transactions\n    bob_txs = bob.transactions\n    print([t.id for t in alice_txs.list()])\n    partial = alice_txs.propose(atomic=True, asset=\'TRY\', address=bob.assetAddress, amount=200)\n    complete = bob_txs.create(atomic=True, asset=\'RUB\', address=alice.assetAddress, amount=100, transaction=partial[\'transaction\'])\n    signed1 = client.oracles.get(\'Green\').transactions.sign(complete.id, complete.transaction)\n    committed = client.oracles.get(\'Blue\').transactions.broadcast(complete.id, signed1.transaction) # sign and broadcast\n```\n\n```\n    [\'1f5381760c56f89b0a35dd6ae5c4ee6a6fb0941f72a27c6fed786cf0829c31bb\', \'9d2fe8946d0d1575972f0c3181fbfdf22e5e411e1f96ab0e44302245ca510bac\', \'26b3af7a30f552e2af83d3052ea33a51664ccc3d7dd589d27d58daffadfab6ae\', \'820364378b7f75f26f77e640167aa1e5ccba9e46b92b977027f547bc6a116443\', \'ab99b8b7531592bd3585f2c6c0293e8fe1f952f69558efe44b1ad569329b7272\']\n```\n\n\n\n```python\n    tx = alice_txs.get(committed.id)\n    print(tx.id)\n    print(tx.changes)\n```\n\n```\n    41a5b967732826d37768c588336b5e3ab5a0c4814885a0fb5b924f4dcd9324e2\n    {\'RUB\': 100, \'TRY\': -200, \'Tokens\': -20287}\n```\n'",Python API for blockstack.io
https://github.com/demunger/time_series,"b'### Techniques and Applications of Time Series Analysis\n\nIncluded above are samples of notebooks demonstrating random process simulations, as well as implementations of time series analyses.\n\n* `bbm.ipynb`: a two-dimensional branching Brownian motion simulation, including functions to compute density autocorrelation and particle clustering\n* `fft.ipynb`: comparison of physical and Fourier space convolution methods on speed and accuracy\n* `pca.ipynb`: a one-dimensional diffusion simulation, with an application of principal component analysis\n\n*This repository draws on work from Spring 2017 Time Series Analysis and Stochastic Processes.*\n'",Techniques and Applications of Time Series Analysis
https://github.com/umer-rasheed/Lane-Detection,b'# Lane-Detection\n\nLane Detection for Self-Driving Cars\n\nThe repository aims to provide Canny Edge Detection based Lane Detection for for Self-Driving Cars.  <br />    The .ipynb and .html files are in Python-Files folder.   <br />                                                                  The input and output images and videos are Test-Images and Output-Images folder.     <br />                                      The input and output videos are in Videos folder.\n\n',Lane Detection for Self-Driving Cars
https://github.com/a-kravtsov/coolfunc_example,b'# coolfunc_example\nExample of how to use cooling functions tabulated by Gnedin &amp; Hollon (2012)\n',Example of how to use cooling functions tabulated by Gnedin & Hollon (2012)
https://github.com/karenlmasters/ComputationalPhysicsUnit,"b'# Computational Physics Unit\n\nContent developed/modified for Portsmouth University, 2nd year Computational Physics Unit. Six lectures (python). \n\nText Book: Newman, M., Computational Physics - Revised and Expanded, [2013] \nSome Chapters available to download from: http://www-personal.umich.edu/~mejn/computational-physics/\n\n* Tips for getting the most out of Computational Physics unit. Why people chose different languages. (e.g why we\xe2\x80\x99re doing Python). Introduction to Python programming in general, and specifically in Portsmouth. \n* More introduction to python - lists and arrays. User defined functions.   \n* Graphics and visualization with python. \n* Stochastic methods (in python)\n* Stochastic methods (in python)\n* Monte Carlo Simulations (in python; lab not done in 2017)\n\nFor those interested - a really nice Markdown Cheat Sheet can be found at https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet#code'",Jupyter Notebooks and Other Materials Used in Teaching Computational Physics (at Portsmouth Uni in 2017)
https://github.com/dmuralik/datascience,b'# datascience\nAll about data science!\n\n\nUseful resources:\n* http://slendermeans.org/pages/will-it-python.html - data analysis in R ported to python\n* http://greenteapress.com/thinkstats/thinkstats.pdf - probability and stats in python\n* http://www-bcf.usc.edu/~gareth/ISL/ - introduction to statistical learning\n* http://www.dataschool.io/\n* http://radimrehurek.com/data_science_python/\n* https://dnc1994.com/2016/05/rank-10-percent-in-first-kaggle-competition-en/ - Kaggle How to get to the top 10%\n* http://www.karsdorp.io/python-course/ - python course\n* https://www.pg-versus-ms.com/ - postgresql vs microsoft sql\n* https://pushpullfork.com/build-an-instant-twitter-dashboard-with-just-a-little-code/ - hwo to build a twitter dashboard\n* https://github.com/henripal/labnotebook - visualization for deep learning\n* http://serialmentor.com/dataviz/ - fundamentals of visualization in R\n* http://blog.minitab.com/blog/statistics-and-quality-data-analysis/what-is-a-t-test-and-why-is-it-like-telling-a-kid-to-clean-up-that-mess-in-the-kitchen - t statistics',All about data science!
https://github.com/livenb/LungCancerPrediction,"b'# LungCancerPrediction\nMy personal solution of data science bowl 2017\n### Data Preprocessing\nI have not precess the dicom image before, so I adapt some preprocess procedure from those 2 well written tutorial on kaggle [Tutorial 1](https://www.kaggle.com/sentdex/data-science-bowl-2017/first-pass-through-data-w-3d-convnet),\n[2](https://www.kaggle.com/gzuidhof/data-science-bowl-2017/full-preprocessing-tutorial)\n\nSince the image is scanned the in cylinder shape, the outside body part comes with a HU value -2000, which should set to 0.\n\nBefore             |  After\n:-------------------------:|:-------------------------:\n![](img/a1.png)  |  ![](img/a2.png)\n\n| Substance             | HU                                              |\n|-----------------------|-------------------------------------------------|\n| Air                   | \xe2\x88\x921000                                           |\n| Lung                  | \xe2\x88\x92500                                            |\n| Fat                   | \xe2\x88\x92100 to \xe2\x88\x9250                                     |\n| Water                 | 0                                               |\n| CSF                   | 15                                              |\n| Kidney                | 30                                              |\n| Blood                 | +30 to +45                                      |\n| Muscle                | +10 to +40                                      |\n| Grey matter           | +37 to +45                                      |\n| White matter          | +20 to +30                                      |\n| Liver                 | +40 to +60                                      |\n| Soft Tissue, Contrast | +100 to +300                                    |\n| Bone                  | +700 (cancellous bone) to +3000 (cortical bone) |\n'",My personal solution of data science bowl 2017
https://github.com/ChaseRichmond2/Data-Visualization-Project-,"b'1) Motivation\rCollege Basketball garners the interest of viewers across the country. This interest intensifies each year in March when the Champion is crowned in a 68-team bracket style tournament. This single elimination tournament is known as \xe2\x80\x9cMarch Madness\xe2\x80\x9d. Each year millions of Americans cast their predictions for how the bracket will play out. In an attempt to better understand this tournament and college basketball as a whole. I have analyzed College Basketball data. My analysis seeks to address three main questions: \r\xe2\x80\xa2\t1) Which in-game statistic is the best predictor of team wins?\ro\tDistributional Analyses, Correlational Analysis, and Linear Regression\r\xe2\x80\xa2\t2) Is there a group of influential points when predicting team wins?\ro\tOutlier Detection\r\xe2\x80\xa2\t3) Do certain clusters of teams that perform significantly better than other clusters? How many clusters are optimal?\ro\tClustering\r\xe2\x80\xa2\t4) What are the most important features for classifying a team as Qualified/Not Qualified? How accurately can we classify teams?\ro\tClassification (Random Forest)\r2) Data Source\r\tTo address these research questions, I used a dataset from Sports Reference: http://www.sports-reference.com/cbb/seasons/2017-school-stats.html. This dataset features season long statistics for 351 NCAA division 1 basketball teams. Upon import this data was in a .txt file with a numbered index, to streamline my analysis I reset the index as the name of the team. Additionally I dropped unnecessary columns that detailed a team\xe2\x80\x99s performance within their conference, because this information was already represented through a team\xe2\x80\x99s aggregate total. I also divided in-game statistics by the number of games that team had played to get the statistics on a per-game basis. After these transformations, the data included the following:\r\xe2\x80\xa2\tSchool: String (index)\t\t\xe2\x80\xa2 FT: numpy.float64\t\r\xe2\x80\xa2\tG: numpy.int64\t\t\t\xe2\x80\xa2FTA: numpy.float64\r\xe2\x80\xa2\tW: numpy.int64\t\t\t\xe2\x80\xa2FT%: numpy.float64\r\xe2\x80\xa2\tL: numpy.int64\t\t\t\xe2\x80\xa2ORB: numpy.float64\r\xe2\x80\xa2\tW-L%: numpy.float64\t\t\xe2\x80\xa2TRB: numpy.float64\r\xe2\x80\xa2\tSRS: numpy.float64\t\t\t\xe2\x80\xa2AST: numpy.float64\r\xe2\x80\xa2\tSOS: numpy.float64\t\t\t\xe2\x80\xa2STL: numpy.float64\r\xe2\x80\xa2\tTotal Points: numpy.float64\t\xe2\x80\xa2BLK: numpy.float64\r\xe2\x80\xa2\tOpponent Points: numpy.float64\t\xe2\x80\xa2TOV: numpy.float64\r\xe2\x80\xa2\tFG: numpy.float64\t\t\t\xe2\x80\xa2PF: numpy.float64\r\xe2\x80\xa2\tFGA: numpy.float64\t\t\t\xe2\x80\xa23P%: numpy.float64\r\xe2\x80\xa2\tFG%: numpy.float64\r'",My final project for a Data Visualization course working with NCAA basketball statistics
https://github.com/yuriyi/hackathon2017,"b""# hackathon2017\nHackathon 2017 project\n\n### An Agile Scientific and Total Organized Geoscience Hackathon\n\n### Project by ['Yuriy Ivanov', 'Anna Lim', 'Princy Ndong', 'Song Hou', 'Justin Gosses']\n\nJune 2017 Paris & virtually\n\n# 'Seismic Shotgather Interpreter'\nTeamname = 'Classy'\n\n1. Generate synthetic seismic data with labels (direct energy, reflection, multiples, coherent noise)\n2. Train a classifier using SVM\n3. Generate Tests dataset\n4. Draw interpreted lines\n5. Test model prediction\n\n\n![gui image](gui.png)\n\n### The final draft notebooks are below. \nThe UI notebook runs the functions notebook.\n- Project_hackathon_functions.ipynb\n- User_Interface_v6_Final""",Hackathon 2017 project
https://github.com/elisevmol/Machine-Learning,b'# Machine-Learning\nCourse Machine Learning AUC\n',Course Machine Learning AUC
https://github.com/slowmotionprojects/icestupa,b'# icestupa\nHosting code written by Slow Motion Projects in the context of its collaboration with the [Ice Stupa Project](http://icestupa.org).\n',Hosting code written by Slow Motion Projects in the context of its collaboration with the Ice Stupa Project (http://icestupa.org).
https://github.com/katanachan/AInetworks,"b'# AInetworks\n<br> Paper referred to : http://www.ncbi.nlm.nih.gov/pubmed/19499317\n<br> Self-sustained asynchronous irregular states and Up-Down states in thalamic, cortical and thalamocortical networks of nonlinear integrate-and-fire neurons.\n<br> Alain Destexhe et al. J. Computational Neuroscience: 2009\n\n<br>\nFor single neuron model:\n<br> 9 different types of results were recorded in the simulation:\n<br>1. For a hyperpolarising current:\n<br>a = 0.001 microSiemens, b = 0.04 nanoAmpere, we get fast spiking\n<br>a = 0.001, b = 0.005, we get bursting\n<br>a = 0.001, b = 0, we get fast spiking\n<br>If we consider a moderate value of a=0.02 and b=0, the model also displays spike frequency adaptation. However, this model also generates a rebound burst in response to hyperpolarising pulses. This behaviour is seen in cortical low-threshold spike.\n<br>A further increase in parameter a leads to more robust bursting activity and weaker spike-frequency adaptation and strong rebound bursts. This is observed in a = 0.04 and b = 0.\n<br> With larger values, a = 0.08 and b = 0.03, the model generated bursting activity in response to both depolarising and hyperpolarising stimuli. As seen in thalamus reticular neurons.\n\n<br>For network model:\n<br>To initiate activity, a number of randomly-chosen neurons (from 2% to 10% of the network) were stimulated by random excitatory inputs during the first 50 ms of the simulation. The mean frequency of this random activity was high enough (200\xe2\x80\x93400 Hz) to evoke random firing in the recipient neurons. In cases where selfsustained activity appeared to be unstable, different parameters of this initial stimulation were tested.  It is\nimportant to note that after this initial period of 50 ms, no input was given to the network and thus the activity states described here are self-sustained with no external input or added noise. The only source of noise was the random connectivity (also termed \xe2\x80\x9cquenched noise\xe2\x80\x9d).\n\n<br>The plots for a network of neurons of 20, 40, 60, 80 and 100 neurons in the network are present in the networks/ folder. The Poisson Input was given at 200 ms at a rate of 300 Hz.\n<br>Observations: As we increase the networks size, the mean firing rate increased and the irregulariy and asynchrony became higher.\n'",Asynchronous Irregular Networks in Cortical & Thalamic Regions
https://github.com/ThomasRoca/Lecture-Columbia-Science-Po-2017,"b'# The Data Driven Lecture & workshop Columbia-SIPA / Science-Po-2017:\n*Thomas Roca, Phd, Researcher and Data Officer @Agence Fran\xc3\xa7aise de D\xc3\xa9veloppement*\n\nStay in touch via [Twitter](https://twitter.com/Thomas_Roca) & [Github](https://github.com/ThomasRoca/)\n\n<br>\nPart I. June the 1st\n\n##  DataDriven development\n\nWhether they are massive - big data - or more traditional (census, household surveys, administrative data, etc.), we are the witness of the explosion of the use of data for decision and policy making; first within the private sector then in the administration and lately at the crossroads between sectors and public affairs (e.g. Cambridge analytics, project google election, Facebook monitoring fake news etc.)\n\nAlgorithms now ""make decisions"" based on real time data. But these algorithms are mostly black boxes, parsing and computing data that are not open data (which providers have never been so centralized - GAFA, etc.). This raises societal and democratic challenges. \n\nAre we entereing a Post-StatistiK world? What seems obvisous today is that data and statistics production are no longer a State monoply. What are the promises and challenges of the data revolution ?\n\n#### I. Data-driven lecture\n**Seminar organization**: 1h30: 3x 20 mins presentations + 10 mins QA<br>\n**Slide available in this folder:** Big_Data_Public_Policy_columbia_univ_SIPA.ipynb, [ + link to nbviewer](http://nbviewer.jupyter.org/github/ThomasRoca/Lecture-Columbia-Science-Po-2017/blob/master/Big_Data_Public_Policy_columbia_univ_SIPA.ipynb)\n\n#### II. Dataviz workshop\n**Slide available in this folder:** Dataviz_workshop_Columbia_Science_PO_SIPA.ipynb; [+ link to nbviewer](http://nbviewer.jupyter.org/github/ThomasRoca/Lecture-Columbia-Science-Po-2017/blob/master/Dataviz_workshop_Columbia_Science_PO_SIPA.ipynb)\n\n---\n\n### 1. Introduction: The Open movement, from accountability to efficiency\n\n- The open movement\n- Towards more accountable States\n- More efficient administration ?\n- Towards new data partenerships with in the private sector?\n- What about Development Assistance ?\n\nConclusion: data is about people\n\t- A new data ecostystem\n\t- A new power distribution\n\t- A new ethic\n### 2. Data, the raw material of the digital revolution:\n- ""Traditional"" sources of data:\n\t- Census and Survey data\n\t- Administrative data\n\t- Africa\'s Statisticall Tragedy\n- Big Data: when it\'s raining information:\n \t- Sensor data (Sat., IoT, Cell-phone)\n \t- Social network data\n   \t- Use case: \n\t    - vulnerability to flood using statellite imagery (use case: [Cloud to Street /AFD study](http://librairie.afd.fr/nt25-va-vunerability-flooding-senegal/))\n- Big Data: big difficulty to get access to it\n    - Privacy, security, business\n    - About Data ethics\n### 3. The Digital Humanitarian movement\n- The history of the digital humanitarian movement\n\t- Ha\xc3\xafti 2010\n\t- Crisis mapping ([Humanitarian OpenStreet Map](https://www.hotosm.org/))\n- Official statistics & Big Data in emergency context\n\t- When saving time saves life, real time data matters\n\t\t- Early assessment use case:\n\t\t\t\t    - Using cell phone to monitor displacement (use case: [Flowminder 2015 EarthQuake in Nepal](http://www.flowminder.org/case-studies/nepal-earthquake-2015))\t\t\n\t\t- Coordination of the emergency response ([UN-OCHA HDX](https://data.humdata.org/))\n\nConclusion: new tools, new skills and analytics strategy: AI everywhere ?\n\n## Further reading: \n- [Data Glossary - Data-Pop Alliance](https://github.com/ThomasRoca/Lecture-Columbia-Science-Po-2017/blob/master/Glossary.md)\n- [Mapping Data Sources for development](https://afdlab4dev.github.io/Wiki-DataExploration-in-AFD/)\n- Data-Pop Alliance, 2015, \xe2\x80\x9cBeyond Data Literacy: Reinventing Community Engagement and Empowerment in the Age of Data.\xe2\x80\x9d [Data-Pop Alliance White Paper Series](http://datapopalliance.org/item/beyond-data-literacy-reinventing-community-engagement-and-empowerment-in-the-age-of-data/). Bhargava, R & al.\n- Data-Pop Alliance, 2015, ""Leveraging Algorithms for Positive Disruption: On data, democracy, society and statistics"", [Data-Pop Alliance White Paper Series](http://datapopalliance.org/item/leveraging-algorithms-for-positive-disruption-on-data-democracy-society-and-statistics/), Letouz\xc3\xa9, E., Sangokoya, D.\n- Flowminder, 2016, Rapid and Near Real-Time Assessments of Population Displacement Using Mobile Phone Data Following Disasters: The 2015 Nepal Earthquake, Plos current disasters - [Link](http://currents.plos.org/disasters/article/rapid-and-near-real-time-assessments-of-population-displacement-using-mobile-phone-data-following-disasters-the-2015-nepal-earthquake/)\n- Lanier, J. 2013, Who owns the future ?\n- Meyer, P. 2015, [How Big Data is Changing the Face of Humanitarian Response](http://www.digital-humanitarians.com/)\n- O\'Neil, C. 2014, On Being a Data Skeptic, [ebook-pdf](http://www.oreilly.com/data/free/files/being-a-data-skeptic.pdf)\n- O\'Neil, C. 2016, Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy\n- Roca, T. 2015, [Toward data-driven development: Big improvement or big buzz?](https://www.devex.com/news/toward-data-driven-development-big-improvement-or-big-buzz-86192)\n- Roca, T. 2016, [Open algorithms: A new paradigm for using private data for social good](https://www.devex.com/news/open-algorithms-a-new-paradigm-for-using-private-data-for-social-good-88434)\n- Schartum, D.W., 2016, Law and algorithms in the public domain, Nordic Journal of Applied Ethics [n\xc2\xb01 2016](http://www.ntnu.no/ojs/index.php/etikk_i_praksis/article/view/1973/1989)\n- Schwarz, B. & al. 2017, Socio-Physical Vulnerability To Flooding In Senegal, AFD, Cloud to Street, [Link](http://librairie.afd.fr/nt25-va-vunerability-flooding-senegal/)\n- Schwarz, B., Roca, T. 2016, Data-driven preparedness for disaster, devex - [link](https://www.devex.com/news/opinion-data-driven-preparedness-for-disaster-88950)\n- Slatalla, M., Quittner, J. 1995, Master of Decepetion, the Gang that Ruled the Cyberspace, HarperPerennial\n- UN 2015, Data Revolution Report, [A WORLD THAT COUNTS](http://www.undatarevolution.org/report/)\n- UN-OCHA, 2012, humanitarianism in the network age, [Link](https://www.unocha.org/sites/unocha/files/HINA_0.pdf)\n- UN-OCHA, 2014, Humanitarian Innovation:The State of the Art [link](https://docs.unocha.org/sites/dms/documents/op9_understanding%20innovation_web.pdf)\n- World Bank, 2016 World Development Report: [Digital Dividend](http://www.worldbank.org/en/publication/wdr2016)\n\n--- \n\nPart II. June the 2nd\n\n## Workshop: Datavisulisation when statistics meets web & datascience (2h30)\nThis folder contains information and script for the dataviz workshop\n\nFor this workshop, we are going to use Python 3.4.3 and JavaScript.\nI recommend installing [Python](https://www.python.org/downloads/release/python-343) and a code editor (e.g. [Bracket](http://brackets.io/) or [Notepad++](https://notepad-plus-plus.org/fr/)) we are also going to use [https://jsfiddle.net/](https://jsfiddle.net/)\n\n**Here the thematic this workshop will cover:**\n\n**1. Basic instroduction: (1h30)**\n-   using packages with R and Python (15 min) \n-\tSIG using [Carto](https://carto.com/) and [leaflet](http://leafletjs.com/)\n-\tBasic introduction to [HTML](https://www.w3schools.com/html/default.asp), [CSS](https://www.w3schools.com/css/default.asp)\n- \tIntroduction to SVG\n-\tJavaScript & dataviz\n    + quick intro to [JavaScript](https://www.w3schools.com/js/default.asp)\n    + dataviz the example of [highchart](https://www.highcharts.com/) and [d3.js](https://d3js.org/)\n   \n**2. Getting real time data: the API revolution: (1h)**\n\n- Introduction: what is an [API](https://en.wikipedia.org/wiki/Application_programming_interface) ?\n    + Intro to [JSON](https://en.wikipedia.org/wiki/JSON)\n-  Example: Using [twitter API in python](http://nbviewer.jupyter.org/url/www.stats4dev.com/Ipython/Where_on_earth_is_Helen.ipynb)\n-  Use cases:\n    + An application leveraging [World Bank API](https://datahelpdesk.worldbank.org/knowledgebase/articles/889392-api-documentation)\n    + Buidling an [API based Velib\'](https://developer.jcdecaux.com/#/opendata/vls?page=getstarted) application \n    + Scrapping the web with [YQL](https://developer.yahoo.com/yql/)(and twitter again) \n\t- [example: wacky twitter race !](https://www.highcharts.com/blog/post/query-api-twitter-wacky-race-using-highcharts/) \n## Further reading:\n- Lupi, H., Posavec, S.,2016, ""Dear Data"", http://www.dear-data.com/\n- Roca, T. 2014, ""Web programming and dataviz with Stata"" - [Link](http://stats4dev.com/doc/Stata%20web%20programming.pdf)\n- Yau, N. 2011, visualize this: the flowingdata guide to design, visualization, and statistics\n- https://www.w3schools.com\n- https://www.highcharts.com/blog/\n'",This folder contains and information and script for the dataviz workshop
https://github.com/GregVial/clue_hackathon_code,"b'<p align=""center""><img src=""images/the_simptoms.png"" width=""400""></p>\n\nThis repository contains our contribution to the\n[Clue-WATTx hackathon](http://cluehackathon.wattx.io/)\n\n# 1. Usage\n## 1.1 Dependencies\nThe code was developed with python 3.5 and the following libraries and\nrespective versions :\n- pandas 0.19.2\n- keras 2.0.1\n- tensorflow-gpu 1.0.1 (It can also work with non gpu version)\n- numpy 1.12.0\n- joblib 0.10.3\n\n## 1.2 Training\n   First, you have to train the model using the train.py script.\n\n   While training, the weights are automatically stored each time the\n   validation loss decreases (in the /weights directory).\n   The parameters by default train a simple\n   LSTM model with  1 layer of 128 neurons over 15 epochs. It uses 100000 sequences\n   for training and 50000 for testing. The input and output size is 16\n   (i.e. 16 symptoms in input and 16 symptoms in output), \n   and sequences of 90 consecutive days are used for training and\n   predicting. The parameters can be tweaked from the command line\n   interface :\n\n\n```bash\n$ python train.py --help\nusage: train.py [-h] [-N_train N_TRAIN] [-N_test N_TEST] [-N_epochs N_EPOCHS]\n                [-batch_size BATCH_SIZE] [-input_size INPUT_SIZE]\n                [-output_size OUTPUT_SIZE] [-maxlen MAXLEN]\n                [-step_days STEP_DAYS] [-model {1,2}] [-weights WEIGHTS]\n                [-debug]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -N_train N_TRAIN      Number of sequences used for training. (default:\n                        100000)\n  -N_test N_TEST        Number of sequences used for testing. (default: 50000)\n  -N_epochs N_EPOCHS    Number of epochs (default: 15)\n  -batch_size BATCH_SIZE\n                        Batch size (default: 256)\n  -input_size INPUT_SIZE\n                        Input size (default: 16)\n  -output_size OUTPUT_SIZE\n                        Output size (default: 16)\n  -maxlen MAXLEN        Max length of the sequence (default: 90)\n  -step_days STEP_DAYS  STEP_DAYS (default: 3)\n  -model {1,2}          1 or 2 layers model (default: 1)\n  -weights WEIGHTS      Where to store the weights after training (default:\n                        None)\n  -debug                If True, use a reduced subset of the data. (default:\n                        False)\n\n\n```\n\n## 1.3 Prediction\n   After model training, the predictions are made with predict.py.\n\n   It automatically loads the pretrained weights assuming you use the\n   exact same parameters as during training. The parameters can be tweaked from the\n   command line interface :\n\n```bash\nusage: predict.py [-h] [-input_size INPUT_SIZE] [-output_size OUTPUT_SIZE]\n                  [-maxlen MAXLEN] [-model {1,2}] [-weights WEIGHTS]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -input_size INPUT_SIZE\n                        Input size (default: 16)\n  -output_size OUTPUT_SIZE\n                        Output size (default: 16)\n  -maxlen MAXLEN        Max length of the sequence (default: 90)\n  -model {1,2}          1 or 2 layers model (default: 1)\n  -weights WEIGHTS      Where to load pretrained weights (default: None)\n```\n\n## 1.4 Additional checks\n\n   Before starting the training or prediction phase, ensure all csv files are added\n   to the data/ directory. Due to privacy concerns these files have not been uploaded\n   with the source code.\n\n# 2. Preprocessing\n\nThe preprocessing steps are explained in detail on this notebook:\n[1.0-Prepare-training-data](https://github.com/felipeam86/clue_hackathon_code/blob/master/notebooks/1.0-Prepare-training-data.ipynb)\n\n# 3. Modeling\nOur solution leverages neural networks, more specifically recurrent neural networks (RNN) with long short term memory (LSTM). RNNs are well suited to deal with time series, which is why we chose this approach.\n\n## 3.1. Benefits of neural networks\nNeural networks offer the benefit of being end-to-end solutions, i.e. if well architectured they deal with feature\nengineering by themselves for the most part. For instance with a RNN there is no need to include whether the user was inactive on a specific day, or whether she was active but didn\'t experience a specific symptom. RNNs will determine this by themselves, if it is a useful feature for minimizing loss.\n\nThe other benefit is that neural networks often provide better performance than traditional machine learning techniques.\nThis has been observed for image recognition, image caption, speech recognition amongst others.\n\n## 3.2 Drawbacks of neural networks\nThe main drawback of neural networks is the time it takes to train them. Convergence to a minima can be very time consuming,\nin particular for RNNs which consist of many neural networks running in parallel, rapidly growing to millions of parameters to tune.\nThis has been a big challenge for this competition given the amount of data to process and the timeout set to 2 hours on the Statice platform.\n\nOur solution was designed so that the RNN can be trained locally and weights are reused without further training on the Statice platform.\nThis speeds up processing, but this also means that training is performed on synthetic data that doesn\'t necessarily match well the real data.\nAs a result we observed big discrepancies between local performance and on the Statice platform.\n\nAnother drawback of NNs is the difficulty to interpret them. With millions of parameters and little human feature engineering,\nunderstanding the logic of how the NN learns and predicts can be nearly impossible. With increasing concerns for transparency and new\nEU regulations soon to be effective, the need to explain clearly the automated decisions may prevent the use of NNs in certain situations.\nHowever, this is not an issue in this particular case, as the RNNs are not used to take clinical decisions, only as a suggestion engine of plausible\nexperienced symptoms throughout the cycle.\n\n## 3.3 Architecture\nWe chose to explore two main architectures: 1 LSTM layer with 128 cells and 2 LSTM layers with 256 cells.\n\n<p align=""center""><img src=""images/lstm.png"" width=""150""></p>\n<p align=""center"">Illustration of a multi input/single output lstm</p>\n\nOur RNNs are trained with a historical sequence of n days (by default 90) describing symptoms experienced by users (the input X), and the labels are\nthe symptoms experiences by the same users on the n+1 day (the output y)\n\nGiven the amount of users and length of the history, ideally we would like to train our network on all sequences of 90+1 days for every\nwoman, however this would generate too much data. Therefore we limit the number of training sequences to 100000 by default (but\nthis can be increased) and we chose to skip m (by default, STEP_DAYS = 3) step days of history for each woman. In other words, we look at the sequence of days\n1 to 90, then 4 to 93, then 7 to 96 etc.\n\nOther parameters include the number of symptoms to be predicted (by default 16 corresponding to the symptoms to be predicted in the hackathon, and we never modified that value), as\nwell as the number of symptoms to be used as input (by default the same 16 symptoms, but can be increase to\nthe full 81 symptoms). Our code also allows the user to include further data: day in cycle, whether user is experiencing her periods on a given day. We did not include these features for our tests due to training time constraints but we believe they may be useful.\n\nFinally the number of epochs corresponds to the number of times the RNN will see the full set of training sequences. Typically we observe\nthat for a number of epochs the training and validation loss both decrease, then we reach a point where validation loss stagnates, and\nfinally the validation loss increases while the training loss keeps decreasing. This last phase corresponds to overfitting, and at that\npoint it is better to stop the training. We setup the network so the RNN weights are saved only when the validation loss improves, so\ncontinuing training after reaching the overfit phase doesn\'t harm the model, but it is a pure waste of time.\n\n# 4 Performance\nOn local machines the performance of RNN looked very promising. We used a PC with 16Gb RAM and a GPU GTX 960M to pre-train the models.\nWith the default parameters, the RNN took 21 minutes to train and we achieved a log loss on hold out set (validation)\n of 0.0531 after 15 epochs, as shown in the graph below.\n <p align=""center""><img src=""images/lstm_1_layers_16_input_size_16_output_size_90_maxlen.png"" width=""400""></p>\nUsing the same weights on the Statice platform the obtained log loss is 0.0761\n\nTrained on the Statice platform with the same parameters, we obtain a log loss of 0.0748\n\nThe training phase can be tested in a more interactive manner using this notebook:\n[3.0-Train-LSTM-visualize-log-loss](https://github.com/felipeam86/clue_hackathon_code/blob/master/notebooks/3.0-Train-LSTM-visualize-log-loss.ipynb).\nThe training/validation loss and accuracy evolution over epochs are displayed at the end of the training.\n\nThere may be several reasons why the performance on the remotely trained model is not as good as the performance on the synthetic data.\nOne highly likely reason is that the 100,000 sequences we used to train on the static platform are not representative enough of the full user base. The power of neural nets lies in utilising large datasets, therefore the parameter N_train should be increased to train on more samples (if hardware allows).\n\nIt is also expected that increasing the sequence length from 90 to 120 days and reducing the step to 1 day intead of 3 will lead\nto better performances (if hardware allows).\n\n# 5. Next steps\n## 5.1 Add additional variables\nOur solution didn\'t take into account several variables made available to us, in particular specifics about the user such as\nage, weight, country etc. These information may be meaningful and could help improve performance.\n\nAlso, our intuition is that adding an additional variable ""last day of cycle"" would greatly help the RNN to improve prediction\non the first few days of the next cycle.\n\n## 5.2 Improve the RNN architecture\nThere are two obvious areas where the RNN can be improved.\n\nThe first one is linked to the regularization technique. We used simple dropout of 50%, but it is know that for RNNs dropout\nshould only be applied to non recurrent layers, as described in this [paper](https://arxiv.org/pdf/1409.2329.pdf)\n\nThe second one is connected to statefulness of RNNs. Our RNN is stateless, however we are processing sequences which are related\nto each other, therefore at training time we could use statefulness to improve network.\n\n## 5.3 Test the solution on a remote platform equipped with GPU\nMost of our attempts to train the RNN on the Statice platform failed due to the the 2 hours timeout, whereas they were\nexecuting successfully locally on a PC equipped with GPU. Having a remote environment running a GPU would allow remote training\nand would very likely lead to performances equivalent to those observed locally.\n\n# 6. Lessons learned\nThis competition was the first hackathon that all members of the team ever attended. It has been a lot of fun, a lot of effort\nand came with numerous teachings. Here are some of them\n\n## 6.1 Neural networks\nWe loved working with RNNs. They are state of the art and the way forward for many applications. Despite the lack of results on the\nStatice platform, the good results we obtained locally give us confidence that they are the right way to deal with the challenge\nproposed by clue.\n\nWe will keep learning about them and experimenting with them in future assignments.\n\n## 6.2 Statice platform\nWorking with the Statice platform was a good challenge. Given this was the first public test of the platform, there are lots of adjustments that can be made. Our main recommendations are the following:\n- enable GPU instances for those using neural networds\n- enable better tracking/logging of errors to avoid to many back and forth between the developer and the platform administrator\n- setup multiple platforms with variable amount of data to enable quicker iterative process during the development phase that requires\nintensive testing\n\nOverall we were proud to be \'early adoptors\' of the Statice platform, which will no doubt address important privacy concerns that might have held many companies from sharing their data in similar competitions.\n\n## 6.3 Clue data\nThis readme document referred a lot to the technical approach and little to the data itself. This is in part because the approach we chose required minimal feature engineering of the data itself, although extensive preprocessing was necessary to train/predict using a RNN.\n\nIt is worth mentioning that being purely a team of males, all of us learnt a lot from the initial brief and the data itself. The breadth of symptoms experienced and reported by women surprised us and definitely give us a better understanding of women around us.\n\nThank you Clue for collecting this data and using it for the benefit of all!\n'",Time series analysis and prediction with RNNs
https://github.com/dvu4/AIND-Recognizer,"b'# Artificial Intelligence Engineer Nanodegree\n## Probabilistic Models\n## Project: Sign Language Recognition System\n\n### Install\n\nThis project requires **Python 3** and the following Python libraries installed:\n\n- [NumPy](http://www.numpy.org/)\n- [SciPy](https://www.scipy.org/)\n- [scikit-learn](http://scikit-learn.org/0.17/install.html)\n- [pandas](http://pandas.pydata.org/)\n- [matplotlib](http://matplotlib.org/)\n- [jupyter](http://ipython.org/notebook.html)\n- [hmmlearn](http://hmmlearn.readthedocs.io/en/latest/)\n\nNotes: \n1. It is highly recommended that you install the [Anaconda](http://continuum.io/downloads) distribution of Python and load the environment included in the ""Your conda env for AI ND"" lesson.\n2. The most recent development version of hmmlearn, 0.2.1, contains a bugfix related to the log function, which is used in this project.  In order to install this version of hmmearn, install it directly from its repo with the following command from within your activated Anaconda environment:\n```sh\npip install git+https://github.com/hmmlearn/hmmlearn.git\n```\n\n### Code\n\nA template notebook is provided as `asl_recognizer.ipynb`. The notebook is a combination tutorial and submission document.  Some of the codebase and some of your implementation will be external to the notebook. For submission, complete the **Submission** sections of each part.  This will include running your implementations in code notebook cells, answering analysis questions, and passing provided unit tests provided in the codebase and called out in the notebook. \n\n### Run\n\nIn a terminal or command window, navigate to the top-level project directory `AIND_recognizer/` (that contains this README) and run one of the following command:\n\n`jupyter notebook asl_recognizer.ipynb`\n\nThis will open the Jupyter Notebook software and notebook in your browser. Follow the instructions in the notebook for completing the project.\n\n\n### Additional Information\n##### Provided Raw Data\n\nThe data in the `asl_recognizer/data/` directory was derived from \nthe [RWTH-BOSTON-104 Database](http://www-i6.informatik.rwth-aachen.de/~dreuw/database-rwth-boston-104.php). \nThe handpositions (`hand_condensed.csv`) are pulled directly from \nthe database [boston104.handpositions.rybach-forster-dreuw-2009-09-25.full.xml](boston104.handpositions.rybach-forster-dreuw-2009-09-25.full.xml). The three markers are:\n\n*   0  speaker\'s left hand\n*   1  speaker\'s right hand\n*   2  speaker\'s nose\n*   X and Y values of the video frame increase left to right and top to bottom.\n\nTake a look at the sample [ASL recognizer video](http://www-i6.informatik.rwth-aachen.de/~dreuw/download/021.avi)\nto see how the hand locations are tracked.\n\nThe videos are sentences with translations provided in the database.  \nFor purposes of this project, the sentences have been pre-segmented into words \nbased on slow motion examination of the files.  \nThese segments are provided in the `train_words.csv` and `test_words.csv` files\nin the form of start and end frames (inclusive).\n\nThe videos in the corpus include recordings from three different ASL speakers.\nThe mappings for the three speakers to video are included in the `speaker.csv` \nfile.\n'", Build word recognizer for American Sign Language video sequences with Probabilistic Model
https://github.com/bertrammueller/FBT-analyze,b'# FBT-analyze\n\nscraping trades from facebooktrader\n',Scrape trades from Facebook channel and execute on IG Markets
https://github.com/amueller/applied_ml_spring_2017,b'# Applied machine learning\nWebsite and material for the Sprint 2017 COMS W4995 course on Applied Machine Learning.\nYou can find the course website [here](https://amueller.github.io/applied_ml_spring_2017/).\n',Website and material for the FIXME course on Practical Machine Learning
https://github.com/kjayashankar/python-exercises,b'# python-exercises\nSample Applications in Python\n',Sample Applications in Python
https://github.com/dBeker/Faster-RCNN-TensorFlow-Python3,"b""# tf-faster-rcnn\nTensorflow Faster R-CNN for Windows and Linux by using Python 3\n\nThis is the branch to compile Faster R-CNN on Windows and Linux. It is heavily inspired by the great work done [here](https://github.com/smallcorgi/Faster-RCNN_TF) and [here](https://github.com/rbgirshick/py-faster-rcnn).\n\nCurrently, this repository supports Python 3.5, 3.6 and 3.7. Thanks to @morpheusthewhite\n\n### PLEASE BE AWARE: I don't have the time or intention to fix all the issues for this branch because I don't use it commercially. I created this branch just for fun. If you want to make any commitment, it is more than welcome. Tensorflow has already released an object detection API. Please refer to it. https://github.com/tensorflow/models/tree/master/research/object_detection\n\n# How To Use This Branch\n1. Install tensorflow, preferably GPU version. Follow [instructions]( https://www.tensorflow.org/install/install_windows). If you do not install GPU version, you need to comment out all the GPU calls inside code and replace them with relavent CPU ones.\n\n2. Checkout this branch\n\n3. Install python packages (cython, python-opencv, easydict) by running  \n`pip install -r requirements.txt`   \n(if you are using an environment manager system such as `conda` you should follow its instruction)\n\n4. Go to  ./data/coco/PythonAPI  \nRun `python setup.py build_ext --inplace`  \nRun `python setup.py build_ext install`  \nGo to ./lib/utils and run `python setup.py build_ext --inplace`\n\n5. Follow [these instructions](https://github.com/rbgirshick/py-faster-rcnn#beyond-the-demo-installation-for-training-and-testing-models) to download PyCoco database.\nI will be glad if you can contribute with a batch script to automatically download and fetch. The final structure has to look like  \n`data\\VOCDevkit2007\\VOC2007`  \n\n1. Download pre-trained VGG16 from [here](http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz) and place it as `data\\imagenet_weights\\vgg16.ckpt`.  \nFor rest of the models, please check [here](https://github.com/tensorflow/models/tree/master/research/slim#pre-trained-models)\n\n7. Run train.py\n\nNotify me if there is any issue found.\n\n""",Tensorflow Faster R-CNN for Windows/Linux and Python 3 (3.5/3.6/3.7)
https://github.com/pydata/parallel-tutorial,"b'# Parallel Python: Analyzing Large Datasets\n\n[![Join the chat at https://gitter.im/pydata/parallel-tutorial](https://badges.gitter.im/pydata/parallel-tutorial.svg)](https://gitter.im/pydata/parallel-tutorial?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\n\n\n## Student Goals\n\nStudents will walk away with a high-level understanding of both parallel\nproblems and how to reason about parallel computing frameworks.  They will also\nwalk away with hands-on experience using a variety of frameworks easily\naccessible from Python.\n\n\n## Student Level\n\nKnowledge of Python and general familiarity with the Jupyter notebook are\nassumed.  This is generally aimed at a beginning to intermediate audience.\n\n\n## Outline\n\nFor the first half we cover basic ideas and common patterns in parallel\ncomputing, including embarrassingly parallel map, unstructured asynchronous\nsubmit, and large collections.\n\nFor the second half we cover complications arising from distributed memory\ncomputing and exercise the lessons learned in the first section by running\ninformative examples on provided clusters.\n\n- Part one\n    - Parallel Map\n    - Asynchronous Futures\n    - High Level Datasets\n- Part two\n    -  Scaling cross validation parameter search\n    -  Tabular data with map/submit\n    -  Tabular data with dataframes\n\n\n## Installation\n\n1.  Download this repository:\n\n        git clone https://github.com/pydata/parallel-tutorial\n\n    or download as a [zip file](https://github.com/pydata/parallel-tutorial/archive/master.zip).\n\n2. Install [Anaconda](https://www.anaconda.com/downloads) (large) or [Miniconda](https://conda.io/miniconda.html) (small)\n3. Create a new conda environment:\n\n        conda env create -f environment.yml\n        source activate parallel  # Linux OS/X\n        activate parallel         # Windows\n\n4. If you want to use Spark (this is a large download):\n\n        conda install -c conda-forge pyspark\n\nTest your installation:\n\n    python -c ""import concurrent.futures, dask, jupyter""\n\n\n## Dataset Preparation\n\nWe will generate a dataset for use locally.  This will take up about 1GB of\nspace in a new local directory, `data/`.\n\n    python prep.py\n\n\n## Part 1: Local Notebooks\n\nPart one of this tutorial takes place on your laptop, using multiple cores.\nRun Jupyter Notebook locally and navigate to the `notebooks/` directory.\n\n    jupyter notebook\n\nThe notebooks are ordered 1, 2, 3, so you can start with `01-map.ipynb`\n\n\n## Part 2: Remote Clusters\n\nPart two of this tutorial takes place on a remote cluster.\n\nVisit the following page to start an eight-node cluster:\n[https://pydata-parallel.jovyan.org/](https://pydata-parallel.jovyan.org/)\n\nIf at any point your cluster fails you can always start a new one by\nre-visiting this page.\n\n*Warning: your cluster will be deleted when you close out.  If you want to save\nyour work you will need to *Download* your notebooks explicitly.*\n\n\n## Slides\n\nBrief, high level slides exist at\n[http://pydata.github.io/parallel-tutorial/](http://pydata.github.io/parallel-tutorial/).\n\n\n## Sponsored Cloud Provider\n\nWe thank Google for generously providing compute credits on\n[Google Compute Engine](https://cloud.google.com/compute/).\n'",Parallel computing in Python tutorial materials
https://github.com/haustre/eurobot-hauptsteuerung,b'Eurobot 2015 Hauptsteuerung\n===========================\n\nThis software has two parts. One runs on the BeagleBone and controls the robot. The other is running on the laptop.\n\nTo start the programm on the BeagleBone ::\n    Python3 `PATH TO PROJECT`/eurobot/hauptsteuerung_main.py\n\nTo start the programm on the laptop ::\n    Python3 `PATH TO PROJECT`/eurobot/laptop_main.py\n\nThe full documentation for this software is in /docs/_build/html.',Eurobot 2015 Hauptsteuerung
https://github.com/BuzzFeedNews/2017-04-fake-news-ad-trackers,"b'# Fake News Ad Trackers Analysis \xe2\x80\x94\xc2\xa0before Nov. 2016 vs. March 2017\n\nThis repository contains data, analytic code, and findings based on a collaboration between BuzzFeed News and [Liliana Bounegru](https://github.com/lilianabounegru), an author of the forthcoming [A Field Guide to Fake News](http://fakenews.publicdatalab.org/).\n\nThe findings support portions of the BuzzFeed News article, ""[Fake News, Real Ads](https://www.buzzfeed.com/craigsilverman/fake-news-real-ads),"" published April 4, 2017. Please read that article, which contains important context and details, before proceeding.\n\n## Data\n\nThis repository contains the following five CSV files:\n\n- [`data/observed-trackers.csv`](data/observed-trackers.csv): The data BuzzFeed News received from the researchers behind A Field Guide to Fake News, slightly restructured.\n\n- [`output/tracker-matrix.csv`](output/tracker-matrix.csv): For each website-and-tracker combination, whether the website had that tracker (a) before Nov. 2016, and/or (b) in March 2017. (Includes only ""comparable"" sites. See below for details.)\n\n- [`output/tracker-counts.csv`](output/tracker-counts.csv): Using the data in `output/tracker-matrix.csv`, counts the number of comparable sites containing each tracker during the two timeframes, and the net change.\n\n- [`output/tracker-statuses.csv`](output/tracker-statuses.csv): Using the data in `output/tracker-matrix.csv`, classifies each website-and-tracker combination into one of four categories: kept, removed, added or never had the tracker.\n\n- [`output/tracker-statuses-pivot.csv`](output/tracker-statuses-pivot.csv): Using the data in `output/tracker-statuses.csv`, counts the number of sites that kept, removed, added, or never had each tracker.\n\n### A note on ""comparable"" sites\n\nTo make the two time frames \xe2\x80\x94\xc2\xa0before Nov. 2016 and in March 2017 \xe2\x80\x94\xc2\xa0comparable, we removed two types of sites:\n\n- Sites with no observed trackers in the ""before"" period\n\n- Sites that had disappeared by the ""after"" period\n\nAfter doing so, we were left with 51 websites.\n\n## Code\n\nThe Python code that processes the data can be [found here](notebooks/analysis.ipynb).\n\n## Feedback / Questions?\n\nContact Jeremy Singer-Vine at jeremy.singer-vine@buzzfeed.com.\n\nLooking for more from BuzzFeed News? [Click here for a list of our open-sourced projects, data, and code](https://github.com/BuzzFeedNews/everything).\n'","Data and analysis supporting portions of the BuzzFeed News article, ""Fake News, Real Ads,"" published April 4, 2017."
https://github.com/Automating-GIS-processes/Lesson-7-Automating-Raster-Data-Processing,"b'# Automating Raster Data Processing:\n- **Lesson 7:** [Basics of raster data processing with Python and Gdal](Python-and-Gdal.ipynb)\n- **Exercise 7:** [Delineate forest areas from the Global Forest Change Dataset](https://classroom.github.com/assignment-invitations/e59dfd42577e7fe6d98d19d70ecccf53)\n\n### Online Resources:\n - [www.gdal.org/] (http://www.gdal.org/)\n - [Python GDAL/ORG Cookbook] (https://pcjericks.github.io/py-gdalogr-cookbook/)\n - Lawhead 2013, Chapters 6 & 8 [Available as an ebook] (http://site.ebrary.com/lib/helsinki/detail.action?docID=10790286)\n \n'",Basics of raster data processing with Python and Gdal
https://github.com/4OH4/datascience-python-snippets,b'# dataSciencePython\nExample scripts and code fragments for data analysis and management in Python\n',Example scripts and code fragments for data analysis and management in Python
https://github.com/fmacrae/AI-Learning,b'# AI-Learning\n# AI-Learning\n',Stuff I build while learning AI
https://github.com/vinyasmusic/Projects,"b""# Projects\nA collection of Projects undertaken while studying Data Science.\n\n * Indian Data Set : A few visualizations using Tableau for some data sets I found interesting to look into. (Data from data.gov.in)\n * Kaggle : A script used in Kaggle competition for the Analytics Edge course at edX by MITx. The aim was to predict the voting category for the given test set using a training set. \n * Panama Papers : A visualization in R of Indian addresses mentioned in the latest Panama Papers data set. The necessary data was filtered in R and the script used is also included. It is also accompanied by 2 more visualizations created with the help of BatchGeo and Google's Fusion Tables, source for which is the accompanying CSV file.\n * TextNook Assignment : 2 Assignments part of an Interview. One involves scraping data from a blog. The other was to create a Reddit Clone. \n * \n \n\nNotes : \n * Need to include Rmd for Kaggle detailing the thought process and explaining the code\n * Try out Scrapy for scraping the data.\n""",A collection of Projects undertaken while studying Data Science
https://github.com/msk007/approximate-square-root,b'',folder for square root HW
https://github.com/cal859/odsc_hackathon,b'# odsc_hackathon\niPython Notebook for Challenge 1 of ODSC Hackathon\n',iPython Notebook for Challenge 1 of ODSC Hackathon
https://github.com/whaley-group-berkeley/qspectra,"b'QSpectra: Nonlinear Spectroscopy for Molecular Aggregates\n=========================================================\n\nQSpectra is a Python package designed for fast and flexible calculations of non-\nlinear spectroscopy signals on molecular aggregates, such as photosynthetic\nlight-harvesting complexes. The focus is on solving approximate models of\nelectronic dynamics under known effective Hamiltonians as open quantum systems.\n\nThe core idea is a dynamical model interface which allows for flexible\ncomposition of different dynamical models with different spectroscopy methods,\nas long as the equation of motion is a linear function of the system\nHamiltonian.\n\nTo enable efficient calculations, all simulations are performed under the\nrotating wave approximation. Furthermore, because the effective Hamiltonians\nconserve the number of electronic excitations, each model (when practical) only\npropagates within the necessary fixed subspaces of Liouville subspace.\n\nAlthough the QSpectra framework is written in Python, we expect that eventually\nsubmodules for new dynamical models may be written in a compiled language such\nas Fortran or C when if necessary for satisfactory performance.\n\n**Note (November 12, 2021)**: qspectra is no longer maintained. If you find it\nuseful in your research, I encourage you to fork it on GitHub.\n\nInstall\n-------\n\nFirst, make sure you\'re running Python 3 and have recent versions of numpy\nand scipy installed.\n\nThen, clone the git repository and use `pip`:\n```\ngit clone https://github.com/whaley-group-berkeley/qspectra.git\npip install -e qspectra\n```\n\nI highly recommend using `-e` flag, which keeps the install directory in-place\nfor local development.\n\nTo view the example notebooks, you need ``jupyter``. To run the unit tests,\nyou need ``nosetests``.\n\nFeatures\n--------\n\n- Hamiltonians:\n    - Electronic systems (under effective Hamiltonians within the Heitler-London\n      approximation)\n    - Vibronic systems (electronic systems with explicit vibrational modes)\n- Dynamical models:\n    - Unitary\n    - Redfield theory (secular/non-secular)\n    - Zeroth order functional expansion (ZOFE)\n    - Hierarchical equations of motion (HEOM).\n    - Time non-local ME (special case of HEOM), *on the roadmap*\n- Spectroscopy/simulation:\n    - Free evolution (no field)\n    - Equations of motion including fields, such as:\n        + The density matrix following pump pulses\n        + Equation-of-motion phase-matched-approach (EOM-PMA), *in progress*\n    - Linear response based methods, such as:\n        + Absorption and emission spectra\n        + Impulsive probe pulses\n    - Third-order response functions (in particular, for 2D spectroscopy)\n\nExamples\n--------\n\nExample notebooks demonstrating the features of QSpectra are included in the\n""examples"" directory.\n'",Quantum simulations of nonlinear spectroscopy and dynamics for molecular aggregates
https://github.com/dataewan/dlnd-your-first-network,"b""# DLND your first network\n\nFirst udacity project, implementing a neural net in numpy.\nPredicting ride sharing usage,\nbased on actual data.\n\n## My Notebook\n\nThis builds on the homework lessons, where solutions were provided.\nCombining all of them together.\nThis was fiddly.\n\nOnce the neural net is coded, the main bit of work is picking the hyperparameters.\nA few observations there.\nYou can graphically see when the training rate is too high or too low by looking at the loss functions over epochs.\nIf it slopes down too slowly, increase the training rate.\nIf it jumps about too much, then the training rate is too high.\n\nI don't have an intuition for the number of nodes in the hidden layer yet, \nthis is something that I'll have to do more reading on.\nIt isn't just a case that putting more hidden nodes in is always a good thing.\n""",First neural net from the deep learning nanodegree
https://github.com/ub247/CrimesResearch,"b'# CrimesResearch\n\nI will be sharing my Crimes research related code here but without out sharing the data.\nIts for my own personal reference, it will be good if can be helpful for others as well, just basic data science stuff.\n'",Crimes Reserach Masters Thesis
https://github.com/smy5/Sandy---KimLab,"b""# Sandy---KimLab\nSandy's Jupyter Notebooks \nI love pizza\n""",Sandy's Jupyter Notebooks 
https://github.com/smallnamespace/fireplace-profiling,b'# fireplace-profiling\nProfile fireplace to look for speed gains\n',Profile fireplace to look for speed gains
