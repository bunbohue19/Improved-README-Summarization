repo_url,description,readme
https://github.com/JasonJWilliamsNY/biocoding_2015,Coursework for BioCoding 2015 Course,b'# biocoding_2015\nCoursework for BioCoding 2015 Course\n'
https://github.com/bollwyvl/es6-widget-example,an evolving approach for writing Jupyter/IPython widgets with ES6/ES2015,"b'# ES6 Widget Example\n\n> \n\nAn evolving approach to creating a  with .\n\n## Challenges\n-  have slightly different opinions, especially with respect to static properties\n\n## Process Tools\n- [x]  for minimal automation tasks\n- [x]  for live developing with continuous integration\n- [x]  for compilation\n  - [x]  for un-opinonated use of experimental features\n  - [x]  for -compatible modules\n- [x]  for documentation\n- [x]  for code style\n- [ ]  for tests and coverage\n- [ ]  for (inevitable) packaging of external dependencies distributed on \n'"
https://github.com/k-sokolov/ensemble,Identifying fraudulent transactions and banknotes as anomaly detection and classification tasks,b'isolation forest for credit card fraud detection '
https://github.com/adbeyer23/MTA-Analysis-Project,Analyzing NYC subway data,"b'# MTA-Analysis-Project\n\nFirst Project done at Metis! In groups, we looked at MTA subway data to determine the best times and stations to target people for a fictional fundraising gala. This involved a lot of cleaning data, visualizing and analyzing it in different ways, and making assumptions about which groups of people would be most willing to give email information to our canvassers. \n'"
https://github.com/chakpongchung/Spherical-Harmonics-Expansion,spherical-harmonics-expansion using boost and gsl,"b'spherical harmonics expansion and the inner product of rotational gradient operator\n========================================================================\n\n\nTUTORIAL\n--------\n\n\nhttps://nbviewer.jupyter.org/github/chakpongchung/Spherical-Harmonics-Expansion/blob/master/derivation.ipynb\n\n\n\n\n\n\n\n We used it in nematic polymer simulation to find out the parameters that can coverge to  periodic pattern.\n\n\n\nThe core algorithm is implemented with Boost C++ Libraries and GNU Scientific Library(GSL) in C/C++. It can also be used with SymPy in Python since SciPy 2015.  \nhttp://docs.sympy.org/dev/aboutus.html#sympy-development-team\n\n\nTo get started, you can run the following commands:\n\n    tests$ bash ./run.sh\n\n\n\nLIBRARIES DEPENDENCIES:\n-----------------------\nUpdates:  \nstarting from c++17, Associated Legendre Polynomials(hence the function for Spherical Harmonics) are supported in C++17  .\n\nBoost C++ Libraries \nwww.boost.org/\n\nGSL - GNU Scientific Library - GNU Project\nwww.gnu.org/software/gsl/\n\nSymPy\nhttp://www.sympy.org/\n\n\n'"
https://github.com/lila/spot_price_history,"IPython notebook that uses Boto, Pandas and MatPlotLib to show historical price data","b'# README for Spot Pricing project\n\n>This project provides the code basis for . \n\nfiles:\n  bin/ - scripts\n  \tdownload.sh - bash script to download (by default 1 days worth) pricing dataset as json\n  data/ - datasets\n  notebooks - the ipython notebooks\n  \n\n## to start:\n\n1. startup ipython\n\n> % ipython notebook\n\n2. using the web interface, interact with the notebooks.\n\n## managing the dependencies\n\nsince the notebooks require a bunch of libraries, (pandas, matlabplot, etc), you can\nrun the following command to get all the requirements installed:\n\n> % pip install -r requirements.txt\n\n## Example notebook\n\nGo to \n\n'"
https://github.com/securesystemslab/zippy,ZipPy is a Python3 interpreter on top of Truffle framework,"b""\r\n# ZipPy  #\r\n\r\n|                 | Standard JVM  | Graal JVM   |\r\n| :------------------: |:-------------:| :----------:|\r\n| Linux Ubuntu 14.04.5  |   |  |\r\n| Mac OSX 10.12 |   |  |\r\n\r\nZipPy is a fast and lightweight  implementation built using the  framework. ZipPy leverages the underlying Java JIT compiler and compiles Python programs to highly optimized machine code at runtime.\r\n\r\nZipPy is currently maintained by  at the \xe2\x80\x8b.\r\n\r\n### Short instructions (Using Standard JDK):\r\n\r\n##### Prerequisites:\r\n\r\n1. Install the most recent \r\n\r\n#### Getting ZipPy:\r\n\r\n1. Create a working directory ($ZIPPY_HOME)\r\n2. Clone mxtool:\r\n\r\n        $ cd $ZIPPY_HOME\r\n        $ git clone https://github.com/graalvm/mx.git\r\n\r\n3. Append the  build tool directory to your .\r\n\r\n        $ export PATH=$ZIPPY_HOME/mx:$PATH\r\n\r\n4. Clone ZipPy:\r\n\r\n        $ git clone https://github.com/securesystemslab/zippy.git\r\n\r\n5. Get all ZipPy's dependencies:\r\n\r\n        $ cd $ZIPPY_HOME/zippy\r\n        $ mx spull\r\n\r\n6. Create a file  and add JDK path\r\n\r\n        JAVA_HOME=/path/to/jdk8\r\n        DEFAULT_VM=server\r\n\r\n> For instructions on .\r\n\r\n> For more information please visit the .\r\n\r\n\r\n### Build:\r\n\r\n    $ cd $ZIPPY_HOME/zippy\r\n    $ mx build\r\n\r\n### Run:\r\n\r\n    $ cd $ZIPPY_HOME/zippy\r\n    $ mx python <file.py>\r\n\r\n### Test:\r\n\r\n    $ cd $ZIPPY_HOME/zippy\r\n    $ mx junit\r\n\r\nFor more details and instructions for downloading and building the system, please visit the .\r\n"""
https://github.com/Tiryoh/probrobo_note,Probabilistic Robotics Test Repository,b'# probrobo_note\nTest repository\n\n# References\n* \n* \n'
https://github.com/cben/ansible_jupyter_kernel,[PyCon IL 2017] WIP Jupyter kernel for executing Ansible plays,"b'ansible_jupyter_kernel\n======================\n\nWIP Jupyter kernel for executing Ansible plays\n\nUsage\n-----\n\n    $ git clone https://github.com/cben/ansible_jupyter_kernel\n    $ cd ansible_jupyter_kernel\n    $ pip3 install . [--user]\n\n(cant , not uploaded to PyPI yet)\n\nNote that ""develop mode""  does not copy  into the correct place.\nYou can manually create a directory , , or  and copy  there.\n\nLicense\n-------\n\nGPL v3 or later, same as Ansible.\n\nPrior Art\n---------\n\nSome people have been using Ansible inside Jupyter:\n\n- https://github.com/NII-cloud-operation/Literate-computing-Basics\n  - https://www.youtube.com/watch?v=xyfdufiibQk\n- http://enakai00.hatenablog.com/entry/2016/04/22/204125\n- https://chusiang.gitbooks.io/automate-with-ansible/content/07.how-to-practive-the-ansible-with-jupyter1.html\n- https://www.slideshare.net/irix_jp/osc2016-kyoto-heat-ansible-jupyter\n- https://chusiang.gitbooks.io/automate-with-ansible/content/07.how-to-practive-the-ansible-with-jupyter1.html,\n  https://chusiang.gitbooks.io/automate-with-ansible/content/08.how-to-practive-the-ansible-with-jupyter2.html\n\nAFAICT all these are running a Python kernel and shelling out using  syntax, or  followed by .\n\nThat might actually be more flexible (and less buggy) than a kernel that only runs ansible like Im doing here \xe2\x80\x94 and they clearly unlike me have lots of experience actually achieving things using Ansible inside Jupyter \xe2\x80\x94 but the purpose of this exercise was seeing what I can gain from writing a kernel...\n\n- But https://github.com/NII-cloud-operation also have some nbextensions\n'"
https://github.com/ezdatascience/ezpython,ez's python repository,"b'# python\n# used for saving python code\n# for now, it is ipython notebook\n'"
https://github.com/franpe1/starting_with_python_ver2,Excersices and Data for Starting with Python Ver2 book,b'# starting_with_python_ver2\nExcersices and Data for Starting with Python Ver2 book\n'
https://github.com/coolcalbeans/PEPredictor,"The project parsed through a data file containing 70 fundamental / momentum attributes (Returns, Volatility, RSI etc) for ~2200 US Equity Tickers and use KNN algorithm to find the nearest neighbor for a given stock and determines its Predicted fundamental ratios (P/E, P/B and P/S). Also generated a 95% confidence range for price using yearly volatility of the equity. Later improvements, as a part of the Data Science capstone project @ General Assembly, took advantage of the Pandas and Gradient Boosted Regression Techniques from Scikit learn. ","b'PEPredictor\n===========\n\nThe project parsed through a data file containing 70 fundamental / momentum attributes (Returns, Volatility, RSI etc) for ~2200 US Equity Tickers and use KNN algorithm to find the nearest neighbor for a given stock and determines its Predicted fundamental ratios (P/E, P/B and P/S). Also generated a 95% confidence range for price using yearly volatility of the equity. Later improvements, as a part of the Data Science capstone project @ General Assembly, took advantage of the Pandas and Gradient Boosted Regression Techniques from Scikit learn. \n'"
https://github.com/hfoffani/GAN_MNIST,A GAN that generates numbers from MNIST dataset,b'# Simple Number Generation with GAN\n\nA GAN that generates numbers from MNIST data.\n\n\n### Instructions\n\nI recommend Anaconda to run this notebook.\nSpecifically Miniconda https://conda.io/miniconda.html\nFollow the instrunctions in that page and\ncreate a new environment using the file \nthat is provided in this project.\n\n'
https://github.com/biocore/qiime-workshops,Materials for biocore organized workshops,b''
https://github.com/niharikabalachandra/Linear-regression-MiniProject,Linear Regression using Boston Housing data set. The Boston Housing data set contains information about the housing values in suburbs of Boston. This dataset was originally taken from the StatLib library which is maintained at Carnegie Mellon University and is now available on the UCI Machine Learning Repository.,b'# Linear-regression\nLinear Regression using Boston Housing data set. The Boston Housing data set contains information about the housing values in suburbs of Boston. This dataset was originally taken from the StatLib library which is maintained at Carnegie Mellon University and is now available on the UCI Machine Learning Repository.\n'
https://github.com/vishal-keshav/Conv-neural-network,An application of convolutional neural network algorithms with caffee library and python interface for image and speech detection,"b'# Conv-neural-network\nAn application of Tensorflow and caffee for text, image and speech recognition\n'"
https://github.com/RyosukeHonda/CarND-LaneLines-P1,Finding Lane Lines on the Road,"b'#Finding Lane Lines on the Road \n\n\nWhen we drive, we use our eyes to decide where to go.  The lines on the road that show us where the lanes are act as our constant reference for where to steer the vehicle.  Naturally, one of the first things we would like to do in developing a self-driving car is to automatically detect lane lines using an algorithm.\n\nIn this project you will detect lane lines in images using Python and OpenCV.  OpenCV means ""Open-Source Computer Vision"", which is a package that has many useful tools for analyzing images.  \n\nStep 1: Getting setup with Python\n\nTo do this project, you will need Python 3 along with the numpy, matplotlib, and OpenCV libraries, as well as Jupyter Notebook installed. \n\nWe recommend downloading and installing the Anaconda Python 3 distribution from Continuum Analytics because it comes prepackaged with many of the Python dependencies you will need for this and future projects, makes it easy to install OpenCV, and includes Jupyter Notebook.  Beyond that, it is one of the most common Python distributions used in data analytics and machine learning, so a great choice if youre getting started in the field.\n\nChoose the appropriate Python 3 Anaconda install package for your operating system here.   Download and install the package.\n\nIf you already have Anaconda for Python 2 installed, you can create a separate environment for Python 3 and all the appropriate dependencies with the following command:\n\n\n\n\n\nStep 2: Installing OpenCV\n\nOnce you have Anaconda installed, first double check you are in your Python 3 environment:\n\n    \n  \n  \n  \n   \n(Ctrl-d to exit Python)\n\nrun the following commands at the terminal prompt to get OpenCV:\n\n  \n\n\nthen to test if OpenCV is installed correctly:\n\n  \n  \n  \n(Ctrl-d to exit Python)\n\nStep 3: Installing moviepy  \n\nWe recommend the ""moviepy"" package for processing video in this project (though youre welcome to use other packages if you prefer).  \n\nTo install moviepy run:\n\n  \n\nand check that the install worked:\n\n  \n  \n  \n(Ctrl-d to exit Python)\n\nStep 4: Opening the code in a Jupyter Notebook\n\nYou will complete this project in a Jupyter notebook.  If you are unfamiliar with Jupyter Notebooks, check out Cyrille Rossants Basics of Jupyter Notebook and Python to get started.\n\nJupyter is an ipython notebook where you can run blocks of code and see results interactively.  All the code for this project is contained in a Jupyter notebook. To start Jupyter in your browser, run the following command at the terminal prompt (be sure youre in your Python 3 environment!):\n\n\n\nA browser window will appear showing the contents of the current directory.  Click on the file called ""P1.ipynb"".  Another browser window will appear displaying the notebook.  Follow the instructions in the notebook to complete the project.  \n'"
https://github.com/zeroviscosity/d3-js-step-by-step,http://zeroviscosity.com/category/d3-js-step-by-step,"b""## D3.js Step by Step\n\nCheck out the .\n\n#### NOTE: Examples now target D3 v4.\n\nHere's what changed in this repo from v3 to v4:\n\n*  became \n*  became \n*  needs to be explicitly defined so it is now  for the pie chart\n*  became \n"""
https://github.com/elhanarinc/deeplearning,Ceng 783 Deep Learning Assignments,b'# Ceng 783 Deep Learning\n\n* Check the website for information: [<marko.inline.RawText object at 0x000002CBAF38CB88>]'
https://github.com/patrickfuller/imolecule,An embeddable webGL molecule viewer and file format converter.,"b'imolecule\n=========\n\nAn embeddable webGL molecule viewer and file format converter.\nhttp://patrickfuller.github.io/imolecule/\n\nExamples\n========\n\n * \n * \n\nIPython\n=======\n\nThe IPython notebook is an open-source tool poised to replace MATLAB in many\napplications. As a scientist of sorts, Im all about it. Therefore, I made\nhandles to use imolecule with the notebook. Install through pip:\n\n\n\nOpen a new notebook and test the setup by typing:\n\n\n\ninto a notebook cell. This should convert, optimize and draw the specified\nSMILES structure (in this case, penicillin) into the notebook.\n\nNote that this requires Open Babel to function. If you do not have Open Babel,\nsee below for installation details.\n\nThe drawer can handle any format specified ,\nand can be set up to better handle different use cases. Check out the docstrings\nassociated with the IPython interface for more.\n\nServer\n======\n\nIf you want to run the file format converter on your own computer, install the library with:\n\n\n\nThen run from the command line with:\n\n\nwget https://raw.githubusercontent.com/patrickfuller/imolecule/master/imolecule/js/build/imolecule.min.js\njavascript\nimolecule.create(my-selector);\nimolecule.draw(myMolecule);\njavascript\noptions = {\n    drawingType: ""ball and stick"", // Can be ""ball and stick"", ""wireframe"", or ""space filling""\n    cameraType: ""perspective"", // Can be ""perspective"" or ""orthogonal""\n    shader: ""toon"" // three.js shader to use, can be ""toon"", ""basic"", ""phong"", or ""lambert""\n};\njavascript\n{\n    atoms: [\n        { element: ""C"", location: [ -0.762160, 1.168557, 0.022754 ] },\n        { element: ""C"", location: [ 0.631044, 1.242862, -0.013022 ] },\n        { element: ""C"", location: [ 1.391783, 0.076397, -0.012244 ] },\n        { element: ""C"", location: [ 0.762101, -1.168506, 0.026080 ] },\n        { element: ""C"", location: [ -0.631044, -1.242903, -0.011791 ] },\n        { element: ""C"", location: [ -1.391806, -0.076430, -0.014083 ] },\n    ],\n    bonds: [\n        { atoms: [ 0, 1 ], order: 2 },\n        { atoms: [ 1, 2 ], order: 1 },\n        { atoms: [ 2, 3 ], order: 2 },\n        { atoms: [ 3, 4 ], order: 1 },\n        { atoms: [ 4, 5 ], order: 2 },\n        { atoms: [ 0, 5 ], order: 1 }\n    ]\n}\n\nconda install -c conda-forge openbabel\n\ngit clone https://github.com/openbabel/openbabel\nmkdir build && cd build\ncmake ../openbabel -DPYTHON_BINDINGS=ON\nmake && make install\n```\n'"
https://github.com/nazmiasri95/Bidirectional-LSTM,Experimental on Bidirectional LSTM and comparison with LSTM,b'# Bidirectional-LSTM\nExperimental on Bidirectional LSTM and comparison with LSTM\n'
https://github.com/tphinkle/pore_stats,"Event detection, extraction, and analysis in micro and nano resistive pulse experiments.","b'\n\n\n## Contents\n1. Overview\n2. Event extraction\n3. Analysis\n\n## 1. Overview\npore_stats is a software library written in Python for analyzing  experimental data. The library consists of a GUI program written in PyQt for extracting pulses from the baseline, and modules for analyzing the extracted events.\n\n## 2. Extraction\n\n#### Feature highlights\n\n- Single events are detected and extracted automatically, even from signals with drifting or jagged baselines.\n\n\n\n- The program allows the user to change the parameters most relevant to the detection algorithm.\n\n- A low-pass filter can be used to reduce noise and find events that are buried in the baseline.\n\n\n\n#### Event validation\n\n- Detected events can be rejected after detection in one of three ways:\n\n1. Manual accept/reject decision making\n\t- Commands to scroll through events and accept/reject are bound to simple hot keys that make manual review of the events as fast as possible.\n\n2. Population slicing\n\t- A region-of-interest (ROI) square can be dragged on the amplitude-duration scatter plot to remove events from regions known to contain undesirable events (e.g., double events with amplitudes that are too large, spurious short-lived noise spikes that were detected as events, etc.)\n\n\n\n\n\n3. Machine learning\n\t- Whenever the event data is saved, the raw data and decision for each event is saved to a separate file. The saved data for all the events constitutes a data set for training a model to make future accept/reject binary decisions on new events. Currently the training data is saved automatically, but the model must be trained manually. After training a model in scikit-learn, it must be pickled and placed in the correct directory for the GUI program to locate it. This method is functional, but will require some hacking to work; unlike the other two methods, this method doesnt work out of the box (for now).\n\n## 3. Analysis\n\n- The pore_stats event analysis libraries can automatically determine the event amplitude, duration, local minima and maxima, and current levels for non-constant pulses.\n\n- Events are loaded in from the file produced by the event extraction program. An RP event is instantiated as an object of type RPEvent, a class that bundles the events data and methods for performing calculations and transformations on the data.\n\n#### Gallery\n\nHere are some plots of the data created by using the pore_stats analysis library.\n\n \n\n \n\n \n'"
https://github.com/davidaknowles/tf_net,A custom conv net for the DREAM ENCODE challenge,"b""## A custom CNN for the DREAM ENCODE challenge\n\nThis is a pretty standard convolutional neural net on genomic sequence with the following features added:\n* normalized per-base DNase I cuts for the + and - strand are concatenated onto the one hot encoding of sequence, to give a [sequence context] x 6 input matrix.\n* gene expression PCs are included as features to allow the model to interpolate between different cell types.\n* a three class ordinal likelihood is used for the Unbound/Ambiguous/Bound labels.\n* simultaneous analysis of the forward and reverse complement.\n* down-sampling of the negative set to speed up training (and accounting for by weighting the likelihood). \n\nFrom the round 2 leaderboard you can see performance is highly competitive for some TFs (e.g. MAX https://www.synapse.org/#!Synapse:syn6131484/wiki/402503) and less so for others (e.g. REST https://www.synapse.org/#!Synapse:syn6131484/wiki/402505).\n\nThe repo is intended to be fully self contained (save dependencies on synapseclient, pysam and pyDNase python packages), including programmatic download of challenge data, pre-processing, model fitting, prediction and submission.\n\n goes through the math for the ordinal likelihood, negative set downsampling and forward/RC model. \n\n### Installation\n\nYou'll need the following python packages: pysam, pyDNase, scikit-learn (for performance metrics), synapseclient (for downloading the data and submitting), numpy, scipy, theano. \n\n### Usage\n\nThe script  will in principle run all of these steps for you. Realistically you'll want to train each TF model (and probably do the DNase pre-processing) on a cluster since this is pretty time consuming (10ish hours). \n\n1. Set a data location, e.g. add something like the following to your ~/.bash_profile\n\n\n2. Download the challenge data using , but note you'll need to set your Synapse email/password in that script.\n\n3. [optional] Calculate gene expression PCs using . I included the output file, 'ge_pca.txt' so you don't strictly need to rerun this. If you do want to do this yourself you'll need the R packages irlba and foreach.\n\n4. Calculate DNase I cut counts using the  script. This converts the DNase I bams into an efficient numpy representation of cut counts saved in .npz files. The bam first need indexing (e.g. using samtools).  will do this for you. \n\n5. Train models for each TF using . This script includes outputting leaderboard and final submissions.\n\n6. Submit to Synapse using . Note you'll need to set up a folder in Synapse to use for this and set the id in the script. """
https://github.com/mmt/deeprl22,Study group for Deep Reinforcement Learning,"b""# Deep Reinforcement Learning Study Group\n\nThis repository is a clone of the Spring 2017 deep reinforcement\nlearning  at Berkeley.\nWe've cloned it for the purpose of having a collaborative study group\nwatching the lectures and working on the problem sets.\n\n# Resources.\n## Lecture Material\n\n- Google's deep learning Udacity  seems like a good introduction.\n\n- Professor Bertsekas's Dynamic Programming  seem like a good supplement to these lectures.\n\n## Papers\n\n- Yann Lecun's  discusses\n  tricks for normalization and initialization.\n\n## Blog Posts etc.\n\n- A  describing RNNs.\n- A  explaining LSTMs.\n- A pair of posts on \n  and .\n- A  giving general background on RL and diving into score function gradient estimators.\n\n## Books\n\n- .\n\n## Software\n\n### Tensorflow\n\n- \n\n-  seems like a good way to instrument training algorithms.\n\n- \n\n- \n\n-  might save time.\n\n### Other\n\n-  seems like a useful debug tool.\n\n## Other courses\n-  - another class more focused on convnets\n"""
https://github.com/Patrick-Woo/CarND-LaneLines-P1,This is my first update.,"b'#Finding Lane Lines on the Road \n\n---\nThe goals / steps of this project are the following:\n* Make a pipeline that finds lane lines on the road\n* Reflect on your work in a written report\n\n\n[//]: # (Image References)\n\n\n\n\n---\n\n### Reflection\n\n###1. Describe your pipeline. \nMy pipeline consisted of 5 steps:\n\n* Convert the images to grayscale\n* Perform Gausian smoothing and apply Canny edge detection\n* Select region of interest and mask other areas of the image\n* Apply Hough Transform to detect lane lines\n* Superimpose the lane lines on the original image\n\nIn order to draw a single line on the left and right lanes, I modified the draw_lines() function by:\n\n* Calculated slope and center of each line. \n* Then based on the slope, sort it into right or left lane line\n* Calculate the average slope and the center of right and left lane\n* Then using the Y coordinates, based on Region of Interest, figure out the X cordinates using the avg slope and center point of lane lines [equation: (y-y) = M (x-x)]\n\nThe explaination of this equation: (y-y) = M (x-x) is as follows:\ny is equal with ymax and y is equal with y_avg.\nAs (xmax,ymax) and (x_avg,y_avg) are two points that are located in the same line. \nAs a result, the line has the unique M and b.\n\ny=Mx+b,  b=y-Mx\ny=Mx+b,\tb=y-Mx\nb=b, y-Mx=y-Mx\n(y-y) = M (x-x)\n\nReplacing xmax,x_avg,y_max,y-avg to this equation, I get the result below:\nymax-y_avg=M(xmax-x_avg)\nxmax=x_avg-(y_avg-ymax)/slope_avg          \n\nThen put above xmax into draw_lines function and calculate the xmax and xmin cordinates for both right and left sides.\n \n\n\n\n###2. Identify potential shortcomings with your current pipeline\n\n\nPotential shortcomings of my approach:\n\n* The region of interest in Image masking is static, hence it can only work in specific scenarios\n\n* Slope conditions used for detecting right and left lanes do not work in case of a curve in the road\n\n\n###3. Suggest possible improvements to your pipeline\n\nPossible improvements to my approach:\n\n* Make the image mask selection dynamic, so that it could work in different scenarios\n\n* Modify the slope conditions, so that they work with curve in the road\n'"
https://github.com/balaganesh99/project,project,b'# project\nproject\n'
https://github.com/todddangerfarr/mlnd_p4_train_a_smartcab_to_drive,Udacity Machine Learning NanoDegree Project 4 - Reinforcement Learning: Train a Smartcab to Drive ,"b'# Train a Smartcab How to Drive\n\nUdacity Machine Learning - Reinforcement Learning Project\nThe goal of this project was to implement a Q-Learning algorithm so that a smartcab agent can learn to navigate to a destination within a predefined time in a grid-world.\n\nThe methodology for the implementation of this code is found in the report found in the main directory of this repository.  \n\n## Install\n\nThis project requires Python 2.7 with the pygame library installed:\n\nhttps://www.pygame.org/wiki/GettingStarted\n\n## Code\n\nOpen  and implement . Follow s for further instructions.\n\n## Run\n\nMake sure you are in the top-level project directory  (that contains this README). Then run:\n\n\n\nOR:\n\n\n\n## Automated Data Collection\n\nTo run the automated data collection uncomment the script according to single scenario or iterative.\n\n#### For iterative collection\n- Comment out the Single Scenario Function call\n- set the number of times for each iteration\n- choose ranges and steps for Gamma, Epsilon and Epsilon Decay\n\n\n\n- Run \n\n#### For Single Scenario\n- Comment out the Iterative Data Collection Function Call\n- Set the variables for Gamma, Epsilon, Epsilon Decay and the number of times\n\n\n\n- Run \n'"
https://github.com/rohitvarkey/Compose3D.jl,A library to try and extend Compose.jl to 3D.,"b""# Compose3D\n\nCompose3D is a Julia package written to try and extend  to 3-D. Currently, only WebGL backends using Three.JS are supported which allows you to draw 3D in a Jupyter notebook. The long term goal would be to integrate this package with the Compose package.\n\nPlease check the exp folder for some example IJulia notebooks.\n\n# Documentation\n\n### Contexts\n\nContexts are the things that you are able to draw. Contexts are created by specifying an origin point and the width, height and depth of the required context. \n\nYou can use the Context constructor to create a Context. \n\n* Context(x0,y0,z0,width,height,depth) - This will return a Context created which has it's coordinate system relative to (x0,y0,z0) and a width of 'width', height of 'height', and depth of 'depth'. \n\n### Geometries (Forms)\n\nGeometries look to provide the user with primitives for creation of 3-D shapes.\n\nCurrent primitives implemented are :\n\n   * Cubes\n   * Spheres\n   * Pyramids\n\nFunctions available for users to use to create such geometries are:\n\t\n* cube(x0,y0,z0,side) - Returns a cube centered at (x0,y0,z0) and of the specified \n* sphere(x0,y0,z0,radius) - Returns a sphere centered at (x0,y0,z0) with the specified radius.\n* pyramid(x0,y0,z0,baseLength, h) - Returns a square base pyramid of base length 'baseLength' with a corner at (x0,y0,z0) and the specified height 'h'.\n\n### Materials (Properties)\n\nMaterials are to add properties to the 3D objects like color and texture maps. \n\nFunctions available currently are:\n\n* mesh_color(color) -  can be a string or any Color in Color.jl that can be converted to RGB. All geometries that are children of this node and are part of the subtree at the same level will be coloured with this color. \n\n### Compositions\n\nDrawing things is done by composing a root Context with other Contexts or Geometries.   \n\nCompositions work exactly like in Compose except for :\n\n* Measures of the parent Context is resolved by adding 'w','h' and 'd' rather than just providing numbers in Compose. \n* There is no support for using the width, height and the depth of the root box as of now like 'w','h' and 'd' do in Compose.\n* The root Context has to have absolute values rather than relative values.\n\nThe function to be used by the user is the compose function. \n\n* compose(Context, Geometry) - Returns a new context after adding the geometry object to the context. The geometry's relative measures are converted to absolute measures based on the parent context.\n* compose(Context, Context) - Returns a new context after adding the context object to the parent context. The contexts relative measures are converted to absolute measures based on the parent context.\n\nCompose can also take inputs in S-tree formats to build a tree. This saves the user from having to call compose again and again.\n\n### Measures\n\nCompose3D makes use of a similar measure system to Compose. The basic unit is of 'mm'.\n\nAbsolute measure units can be used like :\n\n* cm - 10mm\n* inch - 25.4mm\n* pt - inch/72\n\nRelative measures can also be used where the units will be :\n\n* w - width\n* h - height\n* d - depth\n\nAbsolute and relative measures can be combined.\n  \n### Examples\n\n\n\nThe drawing of a Sierpenski Pyramid can be done with the following code:\n\n\n### Acknowledgements\n\n for supplying the initial JavaScript code and getting this project started.\n"""
https://github.com/catherinedevlin/ipython-sql,"%%sql magic for IPython, hopefully evolving into full SQL client","b'===========\nipython-sql\n===========\n\n:Author: Catherine Devlin, http://catherinedevlin.blogspot.com\n\nIntroduces a %sql (or %%sql) magic.\n\nLegacy project\n--------------\n\nIPython-SQLs functionality and maintenance have been eclipsed by JupySQL_, a fork maintained and developed by the Ploomber team.  Future work will be directed into JupySQL - please file issues there, as well!\n\nDescription\n-----------\n\nConnect to a database, using _ connect strings, then issue SQL\ncommands within IPython or IPython Notebook.\n\n.. image:: https://raw.github.com/catherinedevlin/ipython-sql/master/examples/writers.png\n   :width: 600px\n   :alt: screenshot of ipython-sql in the Notebook\n\nExamples\n--------\n\n.. code-block:: python\n\n    In [1]: %load_ext sql\n\n    In [2]: %%sql postgresql://will:longliveliz@localhost/shakes\n       ...: select * from character\n       ...: where abbrev = ALICE\n       ...:\n    Out[2]: [(uAlice, uAlice, uALICE, ua lady attending on Princess Katherine, 22)]\n\n    In [3]: result = \n\n    In [4]: print(result)\n    charid   charname   abbrev                description                 speechcount\n    =================================================================================\n    Alice    Alice      ALICE    a lady attending on Princess Katherine   22\n\n    In [4]: result.keys\n    Out[5]: [u)\n    charname   speechcount\n    ======================\n    Poet       733\n\nIf no connect string is supplied,  will provide a list of existing connections;\nhowever, if no connections have yet been made and the environment variable \nis available, that will be used.\n\nFor secure access, you may dynamically access your credentials (e.g. from your system environment or ) to avoid storing your password in the notebook itself. Use the  before any variable to access it in your  command.\n\n.. code-block:: python\n\n    In [11]: user = os.getenv(SOME_USER)\n       ....: password = os.getenv(SOME_PASSWORD)\n       ....: connection_string = ""postgresql://{user}:{password}@localhost/some_database"".format(user=user, password=password)\n       ....: %sql $connection_string\n    Out[11]: uConnected: some_user@some_database\n\nYou may use multiple SQL statements inside a single cell, but you will\nonly see any query results from the last of them, so this really only\nmakes sense for statements with no output\n\n.. code-block:: python\n\n    In [11]: %%sql sqlite://\n       ....: CREATE TABLE writer (first_name, last_name, year_of_death);\n       ....: INSERT INTO writer VALUES (William, Shakespeare, 1616);\n       ....: INSERT INTO writer VALUES (Bertold, Brecht, 1956);\n       ....:\n    Out[11]: []\n\n\nAs a convenience, dict-style access for result sets is supported, with the\nleftmost column serving as key, for unique values.\n\n.. code-block:: python\n\n    In [12]: result = %sql select * from work\n    43 rows affected.\n\n    In [13]: result[richard2]\n    Out[14]: (urichard2, uRichard II, uHistory of Richard II, 1595, uh, None, uMoby, 22411, 628)\n\nResults can also be retrieved as an iterator of dictionaries ()\nor a single dictionary with a tuple of scalar values per key ()\n\nVariable substitution \n---------------------\n\nBind variables (bind parameters) can be used in the ""named"" (:x) style.\nThe variable names used should be defined in the local namespace.\n\n.. code-block:: python\n\n    In [15]: name = Countess\n\n    In [16]: %sql select description from character where charname = :name\n    Out[16]: [(umother to Bertram,)]\n\n    In [17]: %sql select description from character where charname = {name} \n    Out[17]: [(umother to Bertram,)]\n\nAlternately,  or  can be \nused to inject variables from the local namespace into the SQL \nstatement before it is formed and passed to the SQL engine.\n(Using  and  together, as in , \nis not supported.)\n\nBind variables are passed through to the SQL engine and can only \nbe used to replace strings passed to SQL.   and  are \nsubstituted before passing to SQL and can be used to form SQL \nstatements dynamically.\n\nAssignment\n----------\n\nOrdinary IPython assignment works for single-line  queries:\n\n.. code-block:: python\n\n    In [18]: works = %sql SELECT title, year FROM work\n    43 rows affected.\n\nThe  operator captures query results in a local variable, and\ncan be used in multi-line :\n\n.. code-block:: python\n\n    In [19]: %%sql works << SELECT title, year\n        ...: FROM work\n        ...:\n    43 rows affected.\n    Returning data to local variable works\n\nConnecting\n----------\n\nConnection strings are _ standard.\n\nSome example connection strings::\n\n    mysql+pymysql://scott:tiger@localhost/foo\n    oracle://scott:tiger@127.0.0.1:1521/sidname\n    sqlite://\n    sqlite:///foo.db\n    mssql+pyodbc://username:password@host/database?driver=SQL+Server+Native+Client+11.0\n\n.. SQLAlchemy URL  for HiveServer2 requires disabling autocommit::\n\n    %config SqlMagic.autocommit=False\n    %sql impala://hserverhost:port/default?kerberos_service_name=hive&auth_mechanism=GSSAPI\n\n.. impyla: https://github.com/cloudera/impyla\n\nConnection arguments not whitelisted by SQLALchemy can be provided as\na flag with (-a|--connection_arguments)the connection string as a JSON string.\nSee .\n\n    | %sql --connection_arguments {""timeout"":10,""mode"":""ro""} sqlite:// SELECT * FROM work;\n    | %sql -a {""timeout"":10, ""mode"":""ro""} sqlite:// SELECT * from work;\n\n.. SQLAlchemy Args.\n\nPandas\n------\n\nIf you have installed , you can use a result sets\n method\n\n.. code-block:: python\n\n    In [3]: result = %sql SELECT * FROM character WHERE speechcount > 25\n\n    In [4]: dataframe = result.DataFrame()\n\n\nThe  argument, with the name of a \nDataFrame object in memory, \nwill create a table name\nin the database from the named DataFrame.  \nOr use  to add rows to an existing \ntable by that name.\n\n.. code-block:: python\n\n    In [5]: %sql --persist dataframe\n\n    In [6]: %sql SELECT * FROM dataframe;\n\n.. Pandas: http://pandas.pydata.org/\n\nGraphing\n--------\n\nIf you have installed  commands (, , etc.)\nare provided by .  Example:\n\n.. code-block:: python\n\n    In[9]: %sql \n- Mike Wilson for bind variable code\n- Thomas Kluyver and Steve Holden for debugging help\n- Berton Earnshaw for DSN connection syntax\n- Bruno Harbulot for DSN example \n- Andr\xc3\xa9s Celis for SQL Server bugfix\n- Michael Erasmus for DataFrame truth bugfix\n- Noam Finkelstein for README clarification\n- Xiaochuan Yu for  operator, syntax colorization\n- Amjith Ramanujam for PGSpecial and incorporating it here\n- Alexander Maznev for better arg parsing, connections accepting specified creator\n- Jonathan Larkin for configurable displaycon \n- Jared Moore for  support\n- Gilbert Brault for  \n- Lucas Zeer for multi-line bugfixes for var substitution,  \n- vkk800 for \n- Jens Albrecht for MySQL DatabaseError bugfix\n- meihkv for connection-closing bugfix\n- Abhinav C for SQLAlchemy 2.0 compatibility\n\n.. _Distribute: http://pypi.python.org/pypi/distribute\n.. _Buildout: http://www.buildout.org/\n.. _modern-package-template: http://pypi.python.org/pypi/modern-package-template\n.. _JupySQL: https://github.com/ploomber/jupysql\n'"
https://github.com/rmalliga/Roja-Malligarjunan,Random,b'# Roja-Malligarjunan\nRandom\n'
https://github.com/JuliaPackageMirrors/SpikingNetworks.jl,Julia package mirror.,b'see '
https://github.com/Ramprasad94/Yelp-Data-Analysis,Repository containing source code for performing data analysis on Yelp reviews and tips,"b'# Yelp-Data-Analysis\nRepository containing the code for performing data analysis on Yelp reviews and tips.\n\n##Task 1 : Predicting categories of a business from review and tip text\n\n-For each business, create a Lucene document containing the reviews and tip text.\n-Proceed to index the Lucene documents.\n-Create a query list consisting of all the categories from dataset, where each category will be a query term.\n-Compute the scores for each query term. (Score function will be tf-idf)\n-Set a threshold value to give out the top 3 category scores as belonging to the business.\n\n### Similarity Used:\n- Classic Similarity\n- BM25 Similarity\n- LMJM Similarity \n- LMD Similarity\n\n### Evaluation measure: \n- Accuracy with penalties for missed and incorrect predictions.\n\n\n## Task 2: User rating prediction from their review\n\n### Data preprocessing:\n- Clean data by removing stop words, punctuations, numbers\n- Convert all text data in lower case\n- Generate tf-idf matrix from the data\n- Normalize\n\n### Data visualization\n- Understand the distribution of words\n- Feature importance\n\n### Predictive models\n- Split train and test data\n\nWe have used following machine learning algorithms for this task:\n- Linear regression\n- Ridge regression\n- Lasso regression\n- Elasticnet regression\n- K-nearest neighbours\n- Decision trees\n- Extra trees\n- Random forest \n- Boosting (Adaboost, gradient boosting)\n\n### Tuning models\n- We used 20% of training data as validation set\n- Use different objective functions like mean absolute error, mean squared error, root mean squared error etc. to optimize models\n \n### Evaluation \n- Root mean squared error\n'"
https://github.com/cagdasyetkin/raspberryPi3,Raspberry Pi Projects,b'# raspberryPi3\nRaspberry Pi Projects\n\ndata_logging.py collects data from the sensor and creates data_log.csv\n\nand then we can visualize this data using SensorViz.ipynb\n\nblink.py will help you to understand how connections work. It will turn on/off the LED lights\n'
https://github.com/Shreyas3108/movielens-analytics-recomendation,Analytics of Movielens dataset (100k) along with recomendation based on the user preference,"b""# movielens-analytics-recomendation\nAnalytics of Movielens dataset (100k) along with recomendation based on the user preference\nEDA of the dataset along with basic visualization using plot function of pandas has been used. \n\nConsists of analysis of movielens dataset (100k) along with recomendation based on it using python. \nMovielens(100k) dataset consists of 100,000 ratings for movies from 943 users on 1682 movies \n\nThe link to this dataset https://grouplens.org/datasets/movielens/100k/ \n\nThe recomendation takes user's choices and creates a matrix and gives recomendation \n\n\n"""
https://github.com/daaltces/pydaal-getting-started,"Introduction and tutorials for using PyDAAL, i.e. the Python API of Intel Data Analytics Acceleration Library","b""\nThis repository consists of various materials introducing PyDAAL (Python API of ) that facilitates Python and Machine Learning practitioners to start off with PyDAAL concepts. \n\nAdditionally, helper functions and classes have been provided to aid frequently performed PyDAAL operations.\n\n# \n\nVolume 1, 2 and 3 in PyDAAL Gentle Introduction Series are available as . These volumes are designed to provide a quick introduction to essential features of PyDAAL.\nThese Jupyter Notebooks offer a collection of code examples that can be executed in the interactive command shell, and helper functions to automate common PyDAAL functionalities.\n\n## How to use?\n\nInstall  (IDP) through . IDP consists of a large set of commonly used mathematical and statistical Python packages that are optimized for Intel architectures. \n\n1. Install the latest version of .    \n- Choose the Python 3.5 version2. \n\n2. From the shell prompt (on Windows, use Anaconda Prompt), execute these  commands:\n\n\nIDP environment is installed with necessary packages and activated to run these notebooks.  \n  \nMore detailed instructions can be found from .\n\n# \n\nVarious stages of machine learning model building process are bundled together to constitute one helper function class. These classes are constructed using PyDAAL\xe2\x80\x99s data management and algorithm libraries to achieve a complete model deployment. \n\n### Stages supported by each helper function classes\n1. Training\n2. Prediction\n3. Model Evaluation and Quality Metrics\n4. Trained Model Storage and Portability\n\nMore details on all these stages are available in .\n\n### Currently, helper function classes are provided for\n1. \n2. \n3. \n4. \n3. \n4. \n2. \n3. \n\n\nFor practice, usage examples with sample datasets are also provided that utilize these helper function classes.\n\n# \n\nPyDAAL API's have been used to tailor Python modules that support common operations on DAAL's Data Management library.\n\nImport the  module and explore basic utilities provided for data retrieval and manipulation operations on DAAL's Data Management library\n\n1. getArrayFromNT() : Extracts a numpy array from numeric table\n2. getBlockOfNumericTable(): Slices a block of numeric table with specific range of rows and columns\n3. getBlockOfCols(): Extracts a block of numeric table within specific range of columns\n4. getNumericTableFromCSV(): Reads a CSV file into a numeric table\n5. serialize(): Serializes any input data and saves it into a local variable/disk\n6. deserialize(): Deserailizes serialized data from a local variable/disk\n\n# \n\nThese tutorials are spread across a collection of Jupyter notebooks comprising a theoritical explanation on algorithms and interactive command shell to execute using PyDDAL API.  \n\n### Tutorials Notebooks\n\n* \n\n* \n\n* \n\n* \n\n* \n\n* \n\nData files used in the tutorials are in the  folder. \nThese data files are downloaded from the .\n\n\n\n"""
https://github.com/kellino/Genetic_Composition,simple attempt at an algorithmic composition using a genetic algorithm,"b""Simple algorithmic composition for Computer Music UCl\n===========\n\nRequirements:\n    * Python3.5\n    * numpy\n    * pydub (available on pip)\n\nWhile the program is written in python3.5, it should be relatively easy to convert to python2.7\nA presentation is also available for the ipython / jupyter notebook. It can be viewed on \nthe  for this project.\n\nThe sample, for which I am not the copyright holder, is taken from a performance by Andreas Scholl of the\ntraditional English Ballad 'I will give my love an apple' \n"""
https://github.com/sofroniewn/2pRAM-paper,Notebooks accompanying the 2-photon random access mesoscope (2p-RAM) paper,"b'# Mesoscale two-photon imaging with the 2p-RAM\n\n\n\nNotebooks and data acquired with the two-photon random access mesoscope (2p-RAM), accompanying\n\nA large field of view two-photon mesoscope with subcellular resolution for in vivo imaging \n\nSofroniew, N. J. 1, *, Flickinger, D. 1, , King, J. 2, Svoboda, K. 1\n\n1 Janelia Research Campus, Ashburn VA 20147, USA\n2 Vidrio Technologies, Ashburn VA 20147, USA\n\nThese authors contributed equally to this work\n\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nRaw imaging data is hosted on AWS. Copies of the processed data and meta data are stored in this repository\n\nVideos derived from this data can be found \n'"
https://github.com/fukuta0614/chainer-SeqGAN,implementation of SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient,b'# chainer-SeqGAN\n \n- implementation of \n- Complete oracle test in this paper\n\n## requirements\n\n- Python > 3.4\n- Chainer > 1.5\n- Tensorflow (CPU-only)\n\n\n## Usage\n\n\n\n\nAny advice or suggestion is strongly welcomed in issues.\n'
https://github.com/RazerM/orbital,High level orbital mechanics package.,"b""## Orbital\n[![Build Status][bsi]][bsl] [![PyPI Version][ppi]][ppl] [![Python Version][pvi]][pvl] [![MIT License][mli]][mll]\n\n  [bsi]: http://img.shields.io/travis/RazerM/orbital.svg?style=flat-square\n  [bsl]: https://travis-ci.org/RazerM/orbital\n  [ppi]: http://img.shields.io/pypi/v/orbitalpy.svg?style=flat-square\n  [ppl]: https://pypi.python.org/pypi/orbitalpy/\n  [pvi]: https://img.shields.io/badge/python-2.7%2C%203-brightgreen.svg?style=flat-square\n  [pvl]: https://www.python.org/downloads/\n  [mli]: http://img.shields.io/badge/license-MIT-blue.svg?style=flat-square\n  [mll]: https://raw.githubusercontent.com/RazerM/orbital/master/LICENSE\n\nOrbital is a high level orbital mechanics package for Python.\n\n### Installation\n\n\n\n### Example\n\n\n\n\n\n### Documentation\n\nFor more information, view the [documentation online][doc].\n\n  [doc]: http://pythonhosted.org/OrbitalPy/"""
https://github.com/ekarademir/exercises,Exercises from various courses and tutorials,"b""# exercises\nExercises from various courses and tutorials\n\n# codewords-issueone-functionalprogramming\n[An introduction to functional programming][funcproginpython] by Mary Rose Cook\n# fisher-yates-test\nFew timing tests I'm running on the shuffling algorithm. I'm comparing imperative coding to declarative coding.\n\n[funcproginpython]:https://codewords.recurse.com/issues/one/an-introduction-to-functional-programming\n\n# dublin-airport-challenge\nExploratory codes for Dublin Airport Hackathon BUILDchallenge. . Can't share the dataset.\n\n# genetic_search\nWriting quotes from Shakespeare by bashing on the typewriter. Fittest basher wins.\n"""
https://github.com/istochkoj/coursera-MIPT-Project,Course 6. Final project/,b'# coursera-MIPT-Project\nCourse 6. Final project/\n'
https://github.com/jngaravitoc/Lecture_notes_UofA,Lectures Notes of courses taken at the University of Arizona,b'# Lecture_notes_UofA\nLectures Notes of courses taken at the University of Arizona\n'
https://github.com/sofiatti/stat_analysis_radiobiological_data,Stat 215A paper refereeing project,b'# stat_analysis_radiobiological_data\n'
https://github.com/angelsalazar95/M-todos-Num-ricos,Códigos clases métodos numéricos,b'# M-todos-Num-ricos\nC\xc3\xb3digos clases m\xc3\xa9todos num\xc3\xa9ricos\n'
https://github.com/modee00/DEP-Project,Consulting Project for the Department of Environmental Protection of NYC,"b'# DEP-Project\nConsulting Project for the Department of Environmental Protection of NYC\n\nThis project has two aims. One allows the water department to look up the consumption for a specific meter,\nand forecast consumption for that specific meter and the building(s) it serve(s).\n\nThe second aim allows the user to compare two building types side by side, study the average consumption for each,\nand examine the forecasted consumption for each building type.\n'"
https://github.com/suwangcompling/bayesianmodels,Bayesian Modeling,"b'# bayesianmodels\n\n* Baseline Models\n  * Model 1: Bimodal Multinomial Topic Model (Collapsed Gibbs, Steyvers & Griffiths 2004; Yao et al. 2009)\n    * A. Toy Data\n    * B. Brown\n    * C. BNC\n'"
https://github.com/djays/DeepLearning_Bike_Rental,Building a simple neural network to estimate bike rentals,b'# DeepLearning_Bike_Rental\n\nBuilding a simple neural network to estimate bike rentals !\n'
https://github.com/psygrammer/dprl,의사결정(DP) + 강화학습(RL) + 온라인광고(OA) + 파이썬웹(Pyweb),"b'# dprl\n* \xec\x9d\x98\xec\x82\xac\xea\xb2\xb0\xec\xa0\x95(DP) + \xea\xb0\x95\xed\x99\x94\xed\x95\x99\xec\x8a\xb5(RL) + \xec\x98\xa8\xeb\x9d\xbc\xec\x9d\xb8\xea\xb4\x91\xea\xb3\xa0(OA) + \xed\x8c\x8c\xec\x9d\xb4\xec\x8d\xac\xec\x9b\xb9(Pyweb) + \xec\x98\xa8\xeb\x9d\xbc\xec\x9d\xb8 \xec\x95\xa1\xec\x85\x98(\xea\xb4\x91\xea\xb3\xa0, \xed\x85\x8c\xec\x8a\xa4\xed\x8c\x85)\n\n# \xec\x8a\xa4\xed\x84\xb0\xeb\x94\x94 \xec\xa0\x95\xeb\xb3\xb4 \n* http://psygrammer.github.io/dprl\n'"
https://github.com/rj67/germVar2,TCGA germline variants,"b'germVar2\n=============\n\nSupplementary materials for \n#### Pan-cancer sequencing analysis reveals frequent germline mutations in cancer genes \nRuomu Jiang, William Lee, Nadeem Riaz, Chris Sander, Timothy J Mitchison^, Debora S Marks^\n\nInstall\n-----------\n    install.packages(""devtools"")\n    library(devtools)\n    install_github(""rj67/germVar2"")\n\nData objects\n-----------\nDataframes that can be loaded\n\n* list_goi -- candidate gene list\n\n|    |Gene  |Approved.Name                |Ensembl.Gene    |MDG  |CPG  |Class |\n|:---|:-----|:----------------------------|:---------------|:----|:----|:-----|\n|83  |ATM   |ATM serine/threonine kinase  |ENSG00000149311 |TRUE |TRUE |H-TSG |\n|135 |BRCA1 |breast cancer 1, early onset |ENSG00000012048 |TRUE |TRUE |H-TSG |\n\n* all_patients -- all the patient information\n\n|Patient |disease2 | age|       agez|EA   |race2 |gender |\n|:-------|:--------|---:|----------:|:----|:-----|:------|\n|P6-A5OG |ACC      |  45| -0.2248333|TRUE |WHITE |FEMALE |\n|OR-A5JY |ACC      |  68|  1.0679583|TRUE |WHITE |FEMALE |\n\n* LoF_vars -- variant information, each row is a variant and columns are various annotation\n\n|     |Gene  |uid              |EFF                | TAC2|   AN2|rare |AAChange               |Transcript      |\n|:----|:-----|:----------------|:------------------|----:|-----:|:----|:----------------------|:---------------|\n|3444 |BRCA1 |17-41199682-C-T  |stop_gained        |    1| 17630|TRUE |p.Trp711*/c.2133G>A    |ENST00000491747 |\n|3445 |BRCA1 |17-41201208-TG-T |frameshift_variant |    1| 17630|TRUE |p.Gln1732fs/c.5194delC |ENST00000493795 |\n\n* LoF_muts -- variant carrier information, each row corresponds to the carrier of a particular variant.\n\n|    |Gene  |uid              |Patient |disease2 |AAChange               | DP|        AB|N_hom | nA| nB|\n|:---|:-----|:----------------|:-------|:--------|:----------------------|--:|---------:|:-----|--:|--:|\n|8   |BRCA1 |17-41247941-T-G  |04-1336 |OV       |c.453A>C               | 39| 0.9411765|FALSE |  3|  0|\n|205 |BRCA1 |17-41201208-TG-T |09-2045 |OV       |p.Gln1732fs/c.5194delC | 96| 0.8541667|FALSE |  1|  0|\n\n*  nsSNP_vars -- variant information, each row is a variant and columns are various annotation\n\n|                |Gene  |uid             |EFF              | TAC2|   AN2|rare |AAChange               |Transcript      |dele  |RCVaccession              | cosm_scount|\n|:---------------|:-----|:---------------|:----------------|----:|-----:|:----|:----------------------|:---------------|:-----|:-------------------------|-----------:|\n|17-41201181-C-A |BRCA1 |17-41201181-C-A |missense_variant |    1| 17630|TRUE |p.Gly1788Val/c.5363G>T |ENST00000357654 |TRUE  |RCV000048961;RCV000031241 |          NA|\n|17-41215920-G-A |BRCA1 |17-41215920-G-A |missense_variant |    1| 17630|TRUE |p.Ala1708Val/c.5123C>T |ENST00000357654 |FALSE |NA                        |          NA|\n\n* nsSNP_muts -- variant carrier information, each row corresponds to the carrier of a particular variant.\n\n|   |Gene  |uid             |Patient |disease2 |AAChange              |  DP|        AB|N_hom | nA| nB|\n|:--|:-----|:---------------|:-------|:--------|:---------------------|---:|---------:|:-----|--:|--:|\n|3  |BRCA1 |17-41245027-G-A |02-0047 |GBM      |p.Arg841Trp/c.2521C>T | 330| 0.4575758|FALSE |  1|  1|\n|89 |BRCA1 |17-41245027-G-A |05-5425 |LUAD     |p.Arg841Trp/c.2521C>T | 195| 0.5179487|FALSE |  3|  1|\n\n\n\n\nJuptyer notebooks\n-----------\nReproduce most of the analysis/figures in the paper\n\n* project_overview -- Samples, candidate gene, all variants overview.\n* known_cancer_gene_variants -- Summary variants in known cancer genes.\n* loss_of_heterozygousity_analysis -- LOH of all germline variants\n* low_frequency_variants_association -- Assocation test of low frequency missense and truncation variants comparing to 1000G and ESP\n\nConvenience functions\n-----------\n\n* plotMutRNASeq -- Plot mutation RNASeq levels \n* plotDiseaseDistr -- Plot cancer type distribution given a list of mutations\n\nDependency\n-----------\n\n* plyr, dplyr, reshape2, ggplot2, magrittr, RColorbrewer, gridExtra\n'"
https://github.com/marcelbernic/visual-teach-and-repeat,Projet de session (GLO-4001),b'# visual-teach-and-repeat\nProjet de session (GLO-4001)\n\n# SIFT\n\n# OpenCV\n'
https://github.com/tperol/am207-NILM-project,Repo for am207 final project,"b'# AM207-NILM-project\nThis is the repository for AM 207 final project. It can be found at https://github.com/tperol/am207-NILM-project\n\n# Energy disaggregation from Non-Intrusive Load Monitoring\n\nYoutube video: https://www.youtube.com/watch?v=9a8dR9NEe6w\n\nFinal report can be found in the report folder: Report.pdf\n\nThe notebooks on the three implemented methods can be found in this repository: CO, FHMM, ConvNet.\n\nThe presented poster can be found in the poster folder.\n\nThe data can be downloaded freely at http://redd.csail.mit.edu\n\nContributors:\n\n* Karen Yu\n* Nick Vasios\n* Thibaut Perol\n'"
https://github.com/michelleful/ipynb-intro,iPython notebook intro for PyLadies Boston,b'ipynb-intro\n===========\n\niPython notebook intro for PyLadies Boston\n'
https://github.com/Himscipy/CODES,Collection of Various small CFD Codes ,"b'# CODES\nCollection of codes, scripts and routines developed by me for research and learning purpose.\n\n* HS_Chebpy :  \n  * IPython NoteBook \n  codes in for spectral Method from  \n  Lloyd N. Trefethen, Spectral Methods in MATLAB, SIAM, Philadelphia, 2000\n\n* HS_CFD_Course Projects:   \n  * Contains codes developed while learning Numerical analysis and CFD.   \n  * \n\n* High Performance Computing Example Problem:\n\n\n* Gerris Codes:\n\n\n* VisIT Scripts:\n\n\n* Shell Scripts:\n\n\n \n'"
https://github.com/tayden/titanic-death-decider,Predicts the survival of Titanic passengers using a SVM classifier.,"b'#titanic-death-decider\n\nPredict the survival of Titanic passengers using a SVM model.\n\n## About\nThe Titanic dataset (from Kaggle) contains information about the passengers aboard the Titanic, such as age, sex, fare cost, class and wether or not they survived the sinking of the ship.\nThis project implements an SVM classifier model to predict if passengers with unknown fates survived or perished the sinking event.\n\nThe model makes use of 3-fold cross-validation and achieves approximately ~78% accuracy on the test dataset. \n\n## Opening the project\nTo run the project, numpy, sklearn, and ipython notebook python packages must be installed. Refer to their respective documentation to do so.\nOnce these requirements are satisfied, you may run ""ipython notebook"" to start a notebook server where the titanic-death-decider.ipynb file can be opened.\n\n'"
https://github.com/jimmychou0704/Simulation,Monte Carol simulation of stochastic process,b'# Simulation\nMonte Carol simulation of stochastic process in Stochastic_process.ipynb.  \nMonte Carol simulation of option pricing in pricing.ipynb.\n'
https://github.com/mike-grayhat/quora_qp,Repository for quora question pairs competition,b'# quora_qp\nRepository for quora question pairs competition. This solution is based on an existed NN with decomposable attention model https://github.com/explosion/spaCy/tree/master/examples/keras_parikh_entailment and was adapted for low-mid end hardware.\n'
https://github.com/subimal/class-demos,Simulations for teaching undergraduates,"b'# class-demos\nSimulations for teaching undergraduates\n\nAuthor : Subimal Deb\n\ne-mail : subimal.deb@gmail.com \n\n### Quantum Mechanics\n\n| Topic |\tDescription |\n| ----- | ----------- |\n| Square well potential |\tIPython notebook. Tested with python 3. |\n\n### Solid State Physics\n| Topic |\tDescription |\n| ----- | ----------- |\n| Unit cells |\tAsymptote codes for unit cells of simple cubic, base-centered cubic, FCC, body-centered cubic, hexagonal close packed structures. |\n'"
https://github.com/FRESNA/openmod-atlite-de,Demonstration of using atlite to generate historical German 2012 wind and solar feed-in,b'# openmod-atlite-de\n\nDemonstration of using atlite to generate historical German 2012 wind and solar feed-in\n\nRefer to the notebook  and its HTML conversion (with figures) at http://fias.uni-frankfurt.de/~hoersch/openmod-atlite-de.html.\n\n'
https://github.com/L4brax/ml_antoine_hess,Machine Learning projects - Ynov,"b'# Explorer, apprendre\n\nLe document pandas.ipynb sert dintroduction \xc3\xa0 lensemble de donn\xc3\xa9es\ndans  et .  Lisez-le, faites\nles exercices.  Par contre, cest pour vous, pas de formalit\xc3\xa9 de\nsoumission.\n\n# \xc3\x80 faire pour le 16 d\xc3\xa9cembre\n\nVous \xc3\xaates nouvel embauche chez CoolCorp, une startup qui sp\xc3\xa9cialise\ndans lanalyse des d\xc3\xa9sastres maritimes.  Un nouveau client, White Star\nLines, se trouve face \xc3\xa0 un proc\xc3\xa8s \xc3\xa0 travers le temps, affaire d\xc3\xa9licat.\nVotre chef vous demande, en tant que seul data scientist chez\nCoolCorp, danalyser les donn\xc3\xa9es .  WSL a d\xc3\xa9j\xc3\xa0 engag\xc3\xa9 un expert\npour annoter une partie des donn\xc3\xa9es () avec la survie ou pas\ndes passagers.\n\nLe chef comprend que vous \xc3\xaates nouveau, cest pourquoi il vous laisse\nune semaine compl\xc3\xa8te pour cette analyse.\n\nCr\xc3\xa9ez un r\xc3\xa9pertoire qui sappelle ""P1"" dans votre repository github.\nDans ce repository, \xc3\xa9crivez des analyses des donn\xc3\xa9es.  Cr\xc3\xa9ez des\nipython notebooks afin de pouvoir documenter ce que vous faites,\npourquoi certaines choses sont int\xc3\xa9ressantes et dautres moins.\nD\xc3\xa9couvrez de que vous pouvez dans les donn\xc3\xa9es.\n\nEt puis, \xc3\xa0 la fin, \xc3\xa9crivez un ipython notebook avec votre analyse \xc3\xa0\npr\xc3\xa9senter \xc3\xa0 votre chef en pr\xc3\xa9sence du grand chef de White Star Lines.\nAppelez-le ""rapport_final.ipynb"".\n\nNoubliez pas de faire des commits et des push au fur et \xc3\xa0 mesure de\nvotre progr\xc3\xa8s.  \xc3\x87a vous prot\xc3\xa8ge en plus, car vous pouvez plus\nfacilement faire marche en arri\xc3\xa8re en cas de b\xc3\xaatise.\n\n# \xc3\x89valuation\n\n## Crit\xc3\xa8res d\xc3\xa9valuation\n\nVous serez \xc3\xa9valu\xc3\xa9 sur les axes suivants :\n* profondeur de votre analyse\n* clart\xc3\xa9 (en fran\xc3\xa7ais et technique) de votre pr\xc3\xa9sentation\n* attention au raisonnement\n\n## M\xc3\xa9thodes d\xc3\xa9valuation\n\nDeux de vos co-\xc3\xa9tudiants seront choisi au hasard de remarquer sur\nvotre rapport finale en utilisant les crit\xc3\xa8res ci-dessus.\n\n_Une question \xc3\xa0 discuter ensemble : comment communiquer ces analyse de\nrapport._\n\nLe prof \xc3\xa9valuera tout le monde.\n\nBien entendu, vous serez demand\xc3\xa9 \xc3\xa0 remarquer sur les rapports de deux\nautres \xc3\xa9tudiants.  Vous serez \xc3\xa9galement \xc3\xa9valu\xc3\xa9 (par le prof) sur votre\nanalyse de vos coll\xc3\xa8gues.\n'"
https://github.com/bje-/NEMO,National Electricity Market Optimiser,"b'# National Electricity Market Optimiser (NEMO)\n\n\n\n\n\n\nNEMO is a chronological production-cost and capacity expansion model\nfor testing and optimising different portfolios of renewable and\nfossil electricity generation technologies. It has been developed and\nimproved over the past decade and has a growing number of users.\n\n\n\nIt requires no proprietary software to run, making it particularly\naccessible to the governments of developing countries, academic\nresearchers and students. The model is available for others to inspect\nand, importantly, to validate the results.\n\n## Installation\n\n\n\n## Features\n\nFor a set of given (or default) generation or demand traces, users can:\n\n  1. Specify & simulate a custom resource mix, or;\n  2. ""Evolve"" a resource mix using pre-configured scenarios, or\n     configure their own scenario\n\n### Evolution strategy\n\nThe benefit of an evolutionary approach is that while NEMO is\nsearching for the least-cost solution, NEMO can also explore\n""near-optimal"" resource mixes.\n\nNEMO no longer uses genetic algorithms, but has adopted the better\nperforming  method.\n\n### Resource models\n\nNEMO has models for the following resources: wind (including\noffshore), photovoltaics, concentrating solar power (CSP), hydropower,\npumped storage hydro, biomass, black coal, open cycle gas turbines\n(OCGTs), combined cycle gas turbines (CCGTs), diesel generators, coal\nwith carbon capture and storage (CCS), CCGT with CCS, geothermal,\ndemand response, batteries, electrolysers, hydrogen fuelled gas\nturbines, and more.\n\n## Documentation\n\nDocumentation is progressively being added to a \nin the form of a Jupyter notebook.\n\n exists for\nthe  module. This is useful when building new tools that use the\nsimulation framework.\n\nThe model is described in an Energy Policy paper titled \nby Elliston, MacGill and Diesendorf (2013).\n\n## System requirements\n\nNEMO should run on any operating system where Python 3 is available\n(eg, Windows, Mac OS X, Linux). It utilises some add-on packages:\n\n- ,\n- ,\n- ,\n  , \n  and\n- .\n\n### Scaling up\n\nFor simple simulations or scripted sensitivity analyses, a laptop or\ndesktop PC will be adequate. However, for optimising larger systems, a\ncluster of compute nodes is desirable. The model is scalable and you\ncan devote as many locally available CPU cores to the model as you\nwish.\n\n> #### Note\n>\n> Due to a lack of active development, support for\n>  has been removed. It\n> will be soon replaced with something like .\n\n## Citation\n\nIf you use NEMO, please cite the following paper:\n\n> Ben Elliston, Mark Diesendorf, Iain MacGill, ,\n> Energy Policy, Volume 45, 2012, Pages 606-613, ISSN 0301-4215,\n> \n\n## Community\n\nThe  mailing\nlist is where users and developers can correspond.\n\n## Contributing\n\nEnhancements and bug fixes are very welcome. Please report bugs in the\n. Authors retain\ncopyright over their work.\n\n## License\n\nNEMO was first developed by  in 2011 at\nthe .\n\nNEMO is free software and the source code is licensed under the .\n\n## Useful references\n\nAustralian cost data are taken from the \n(2012, 2013), the  (2015)\nand the CSIRO \n(2021, 2022, 2023). The GenCost reports provide the basis of the input\ncost assumptions for the AEMO .\nCosts for other countries may be added in time.\n\nRenewable energy trace data covering the Australian National\nElectricity Market territory are taken from the AEMO 100% Renewables\nStudy. An accompanying\n\ndescribes the method of generating the traces.\n\n## Acknowledgements\n\nEarly development of NEMO was financially supported by the  (ARENA). Thanks to\nundergraduate and postgraduate student users at UNSW who have provided\nvaluable feedback on how to improve (and document!) the model.\n'"
https://github.com/AugustLONG/crawler,基于django和scrapy的采集系统,"b'# \xe5\xa4\xa7\xe6\x95\xb0\xe6\x8d\xae\xe6\x90\x9c\xe7\xb4\xa2\n\xe5\x9f\xba\xe4\xba\x8escrapy\xe7\x9a\x84\xe9\x87\x87\xe9\x9b\x86\xe7\xb3\xbb\xe7\xbb\x9f\xef\xbc\x8c\xe9\x87\x87\xe7\x94\xa8reids\xe3\x80\x81kafka\xe3\x80\x81celery\xe3\x80\x81rabbitmq\xe3\x80\x81elasticsearch\xe3\x80\x81mysql\xe3\x80\x81django\xe7\xad\x89\xe6\x8a\x80\xe6\x9c\xaf\n\xe6\x89\x93\xe9\x80\xa0\xe6\x96\xb0\xe7\x94\x9f\xe4\xbb\xa3\xe7\x9a\x84\xe6\x95\xb0\xe6\x8d\xae\xe6\x90\x9c\xe7\xb4\xa2\xe5\x92\x8c\xe6\x95\xb0\xe6\x8d\xae\xe5\x9f\xba\xe7\xa1\x80\xe5\xb9\xb3\xe5\x8f\xb0\n\n# Scrapy Cluster\n\nThis Scrapy project uses Redis and Kafka to create a distributed on demand scraping cluster.\n\nThe goal is to distribute seed URLs among many waiting spider instances, whose requests are coordinated via Redis. Any other crawls those trigger, as a result of frontier expansion or depth traversal, will also be distributed among all workers in the cluster.\n\nThe input to the system is a set of Kafka topics and the output is a set of Kafka topics. Raw HTML and assets are crawled interactively, spidered, and output to the log. For easy local development, you can also disable the Kafka portions and work with the spider entirely via Redis, although this is not recommended due to the serialization of the crawl requests.\n\n## Dependencies\n\nPlease see  for Pip package dependencies across the different sub projects.\n\nOther important components required to run the cluster\n\n- Python 2.7: https://www.python.org/downloads/\n- Redis: http://redis.io\n- Zookeeper: https://zookeeper.apache.org\n- Kafka: http://kafka.apache.org\n\n## Core Concepts\n\nThis project tries to bring together a bunch of new concepts to Scrapy and large scale distributed crawling in general. Some bullet points include:\n\n- The spiders are dynamic and on demand, meaning that they allow the arbitrary collection of any web page that is submitted to the scraping cluster\n- Scale Scrapy instances across a single machine or multiple machines\n- Coordinate and prioritize their scraping effort for desired sites\n- Persist across scraping jobs or have multiple scraping jobs going at the same time\n- Allows for unparalleled access into the information about your scraping job, what is upcoming, and how the sites are ranked\n- Allows you to arbitrarily add/remove/scale your scrapers from the pool without loss of data or downtime\n- Utilizes Apache Kafka as a data bus for any application to interact with the scraping cluster (submit jobs, get info, stop jobs, view results)\n\n## Documentation\n\nPlease check out our official  for more details on how everything works!\n\n\n## Redis Keys\nThe following keys within Redis are used by the Scrapy Cluster:\n\n- timeout::: - The timeout value of the crawl in the system, used by the Redis Monitor. The actual value of the key is the date in seconds since epoch that the crawl with that particular spiderid, appid, and crawlid will expire.\n- :queue - The queue that holds all of the url requests for a spider type. Within this sorted set is any other data associated with the request to be crawled, which is stored as a Json object that is Pickle encoded.\n- :dupefilter: - The duplication filter for the spider type and crawlid. This Redis Set stores a scrapy url hash of the urls the crawl request has already seen. This is useful for coordinating the ignoring of urls already seen by the current crawl request.\n- :blacklist - A permanent blacklist of all stopped and expired crawlid\xe2\x80\x98s . This is used by the Scrapy scheduler prevent crawls from continuing once they have been halted via a stop request or an expiring crawl. Any subsequent crawl requests with a crawlid in this list will not be crawled past the initial request url.'"
https://github.com/donallmc/CarND-Traffic-Sign-Classifier-Project,Udacity Self-driving car course project 2,"b'#Traffic Sign Recognition \n\nBuild a Traffic Sign Recognition Project\n\nThe goals / steps of this project are the following:\n* Load the data set (see below for links to the project data set)\n* Explore, summarize and visualize the data set\n* Design, train and test a model architecture\n* Use the model to make predictions on new images\n* Analyze the softmax probabilities of the new images\n* Summarize the results with a written report\n\n\n[//]: # (Image References)\n\n[test_freqs]: ./images/test_freqs.png ""frequencies""\n[training_freqs]: ./images/training_freqs.png ""frequencies""\n[validation_freqs]: ./images/validation_freqs.png ""frequencies""\n[final_freqs]: ./images/final_freqs.png ""frequencies""\n[no_passing]: ./images/no_passing.png ""no passing""\n[stop]: ./images/stop.png ""stop""\n[speed_80]: ./images/speed_80.png ""speed_80""\n[german1]: ./images/german1.jpg ""german1""\n[german2]: ./images/german2.jpg ""german2""\n[german3]: ./images/german3.jpg ""german3""\n[german4]: ./images/german4.jpg ""german4""\n[german5]: ./images/german5.jpg ""german5""\n[german6]: ./images/german6.jpg ""german6""\n[german7]: ./images/german7.jpg ""german7""\n[german8]: ./images/german8.jpg ""german8""\n\n\n[image2]: ./examples/grayscale.jpg ""Grayscaling""\n[image3]: ./examples/random_noise.jpg ""Random Noise""\n[image4]: ./examples/placeholder.png ""Traffic Sign 1""\n[image5]: ./examples/placeholder.png ""Traffic Sign 2""\n[image6]: ./examples/placeholder.png ""Traffic Sign 3""\n[image7]: ./examples/placeholder.png ""Traffic Sign 4""\n[image8]: ./examples/placeholder.png ""Traffic Sign 5""\n\n## Rubric Points\n###Here I will consider the  individually and describe how I addressed each point in my implementation.  \n\n---\n###Writeup / README\n\n####1. Provide a Writeup / README that includes all the rubric points and how you addressed each one. You can submit your writeup as markdown or pdf. You can use this template as a guide for writing the report. The submission includes the project code.\n\nYoure reading it! and here is a link to my \n\n###Data Set Summary & Exploration\n\n####1. Provide a basic summary of the data set and identify where in your code the summary was done. In the code, the analysis should be done using python, numpy and/or pandas methods rather than hardcoding results manually.\n\nThe code for this step is contained in the second code cell of the IPython notebook.  \n\nI used the pandas library to calculate summary statistics of the traffic\nsigns data set:\n\n* Number of training examples = 34,799\n* Number of testing examples = 12,630\n* Number of validation examples = 4,410\n* Image data shape = (32, 32, 3)\n* Number of unique classes/labels = 43\n\n####2. Include an exploratory visualization of the dataset and identify where the code is in your code file.\n\nThe code for this step is contained in the third code cell of the IPython notebook.  \n\nHere is an exploratory visualization of the data set. Here are image frequencies for test, training, and validation sets:\n\n\n![alt text][training_freqs]\n![alt text][validation_freqs]\n![alt text][test_freqs]\n\nHere are some examples of randomly chosen images for 3 classes:\n\n![alt text][speed_80]\n5: Speed limit (80km/h)\n\n![alt text][no_passing]\n9: No passing\n\n![alt text][stop]\n14: Stop\n\nSamples for the full 43 classes can be seen in the python notebook.\n\n###Design and Test a Model Architecture\n\n####1. Describe how, and identify where in your code, you preprocessed the image data. What tecniques were chosen and why did you choose these techniques? Consider including images showing the output of each preprocessing technique. Pre-processing refers to techniques such as converting to grayscale, normalization, etc.\n\nThe code for this step is contained in the fourth and fifth code cells of the IPython notebook.\n\nAs a first step, I decided to convert the images to grayscale to simplify things by ignoring colour and having smaller feature sets to process. I also applied a histogram normalisation to handle the pronounced differences in brightness in the sample dataset. However, after some testing it became apparent that this kind of normalisation was not performing better than the colour dataset so I dropped it.\n\nThe only normalisation that is applied to the colour images is a simple normalisation to constrain all values to  and I achieved satisfactory results with this approach. \n\nThere is also some data augmentation code in this cell, which I will describe in the next section.\n\n####2. Describe how, and identify where in your code, you set up training, validation and testing data. How much data was in each set? Explain what techniques were used to split the data into these sets. (OPTIONAL: As described in the ""Stand Out Suggestions"" part of the rubric, if you generated additional data for training, describe why you decided to generate additional data, how you generated the data, identify where in your code, and provide example images of the additional data)\n\nThe code for splitting the data into training and validation sets is contained in the first code cell of the IPython notebook along with a snarky comment about how the dataset changed and caused me to lose a lot of time! :)\n\nTo cross validate my model, I randomly split the training data into a training set and validation set. I did this by using the appropriate SKLearn function.\n\nThe fifth code cell of the IPython notebook contains the code for augmenting the data set. I decided to generate additional data because the provided data set is relatively small and this kind of deep learning benefits from much larger sets. In addition, the images provided vary significantly in terms of lighting conditions, image position, obscured components, etc. An augmented dataset would increase the number of examples seen of each permutation of these parameters, leading to a more robust classification. \n\nTo add more data to the the data set, I applied some random transformations to the image including adjusting the brightness, applying a random rotation, cropping, and translating the image. The original version of my code applied some of these transformations (as well as some normalisation) using the TensorFlow library. It was my intention to perform these transformations on-the-fly. However, even running on a GPU instance the time it took to process each image was unreasonably long and it had a detrimental effect on my iteration time. Instead I resolved to pre-process each image and generate additional examples before feeding data into the model. This has the obvious advantage that everything is done only once but it adds a pre-processing dependency that could theoretically lead to bugs. I did some Googling around pre-processing images in python for this dataset and actually came across another students solution. I used the functions he defined on the basis that they looked well put-together and I didnt think I could improve on them. I could have easily re-implemented them myself, but Id prefer to leave them as is and credit the source.\n\nWhile augmenting the images I also took the opportunity to correct the imbalance in the datasets. As shown in the charts above, some classes are significantly more common than others. To correct this, I implemented a (somewhat hacky) means of selectively generating more images for under-represented classes than for the commonly occurring ones. The final dataset included 450,000 examples. The final class frequency distributions looks like this:\n\n![alt text][final_freqs]\n\n\n####3. Describe, and identify where in your code, what your final model architecture looks like including model type, layers, layer sizes, connectivity, etc.) Consider including a diagram and/or table describing the final model.\n\nThe code for my final model is located in the sixth cell of the ipython notebook. \n\nThe model is based on the LeNet code supplied in the course with the addition of a third convolutional layer and some dropout layers to handle overfitting. I also did a lot of tinkering with the sizes of the convolutions and strides. I was interested in having the model examine smaller chunks of the image to potentially learn components better as the traffic sign dataset contains images that are distinguishable only by small pixel areas. I dont think that my final numbers are optimal but they are adequate. My final model consisted of the following layers:\n\n| Layer         \t\t|     Description\t        \t\t\t\t\t| \n|:---------------------:|:---------------------------------------------:| \n| Input         \t\t| 32x32x3 RGB image   \t\t\t\t\t\t\t| \n| Convolution 8x8     \t| 2x2 stride, same padding, outputs 14x14x20 \t|\n| RELU\t\t\t\t\t|\t\t\t\t\t\t\t\t\t\t\t\t|\n| Max pooling\t      \t| 2x2 stride,  outputs 7x7x20 \t\t\t\t|\n| Convolution 5x5     \t| 1x1 stride, same padding, outputs 5x5x400 \t|\n| RELU\t\t\t\t\t|\t\t\t\t\t\t\t\t\t\t\t\t|\n| Max pooling\t      \t| 2x2 stride,  outputs 2x2x400 \t\t\t\t|\n| Convolution 1x1     \t| 1x1 stride, same padding, outputs 4x4x20 \t|\n| RELU\t\t\t\t\t|\t\t\t\t\t\t\t\t\t\t\t\t|\n| Max pooling\t      \t| 1x1 stride,  outputs 4x4x400 \t\t\t\t|\n| Fully connected\t\t| output 120        \t\t\t\t\t\t\t\t\t|\n|\tRELU\t\t\t\t|\t\t\t\t\t\t\t\t\t\t\t\t|\n|\tDropout\t\t\t\t\t|\t50%\t\t\t\t\t\t\t\t\t\t\t|\n| Fully connected\t\t| output 84        \t\t\t\t\t\t\t\t\t|\n|\tRELU\t\t\t\t|\t\t\t\t\t\t\t\t\t\t\t\t|\n|\tDropout\t\t\t\t\t|\t50%\t\t\t\t\t\t\t\t\t\t\t|\n| Fully connected\t\t| output 43        \t\t\t\t\t\t\t\t\t|\n\n\n####4. Describe how, and identify where in your code, you trained your model. The discussion can include the type of optimizer, the batch size, number of epochs and any hyperparameters such as learning rate.\n\nThe code for training the model is located in the seventh cell of the ipython notebook. \n\nTo train the model, I initially used an AdaGrad optimizer as I had read that it performed well on sparse data but as I began augmenting the dataset I noticed that Adam had better performance so I switched to that. In particuar, Adagrad often seemed to hit an accuracy level lesser than the Adam optimizer and then regress to a very low accuracy.\n\nI experimented with the batch size and larger batches seemed to perform better than smaller ones, although the difference wasnt that great above a certain threshold. The submitted version of the chose a batch size of 882, chosen because it was a denominator of the validation set and it was useful for debugging reasons!\n\nThe number of epochs in the submitted notebook is 50. After about 30 epochs the rate of improvement decreases noticeably but theres still a significant amount of improvement to be had in the final 20 epochs. After 50, however, the improvements just seem to fluctuate around a maximum value.\n\nas for hyperparameters, the learning rate was modified by Adam. I did some experimenting with the initial value but got no improvement so I went with a recommended value of 0.001. I also did some tweaking of the initial data parameters, mu and sigma, but my changes didnt result in any desirable effects.\n\n####5. Describe the approach taken for finding a solution. Include in the discussion the results on the training, validation and test sets and where in the code these were calculated. Your approach may have been an iterative process, in which case, outline the steps you took to get to the final solution and why you chose those steps. Perhaps your solution involved an already well known implementation or architecture. In this case, discuss why you think the architecture is suitable for the current problem.\n\nThe code for calculating the accuracy of the model is located in the seventh cell of the Ipython notebook.\n\nMy final model results were:\n* validation set accuracy of 98.4%\n* test set accuracy of 90.8%\n\nWorth noting that I re-ran the test set accuracy a few times (after all model details were finalised!) as part of re-running the ipython notebook after clean-up. Due to the large size of my training set I ran into memory problems which I wasnt able to resolve (and which I believe were due to transient conditions on my development machine). As a result, the submitted version included 50% fewer examples than an earlier version which achieved a test set accuracy of 92.5%. This is a good indication that a larger training set would improve the accuracy of the model.\n\n\nAs stated previously, the architecture was initially based on LeNet with some modifications as described above. The iterative design process was heavily impacted by a sudden drop in accuracy which took 3 days to resolve; it turns out I had moved to a new machine and checked out the data from Udacity. The dataset had been substantially modified and I had received any notifications (I have since heard it was mentioned in slack, which I dont consider to be an adequate notification). This caused my accuracy to fluctuate from 80%-90%. I tore apart the model and rebuilt everything then started experimenting heavily with adding and removing both fully connected and convolutional layers. Nothing seemed to improve things and after several days I realised that the issue was the modified dataset. Having spent so much time and now concerned that I wont get all the projects submitted by the deadline, I ended up tweaking the model that I then had to get it to a reasonable state. While this was an iterative process, I dont think it was good development! If Udacity would like me to try again I would request an additional weekend of development time to do it right. I would also politely request that significant changes to the dataset should be communicated more thoroughly and that the course materials should be updated (I followed the LeNet video line by line with my code and thats how I discovered that something was fishy about the dataset because David got 96% accuracy and I got < 90% with the exact same code).\n \n\n###Test a Model on New Images\n\n####1. Choose five German traffic signs found on the web and provide them in the report. For each image, discuss what quality or qualities might be difficult to classify.\n\nHere are five German traffic signs that I found on the web:\n\n![alt text][german1] ![alt text][german2] ![alt text][german3] \n![alt text][german4] ![alt text][german5]\n\nHere are three additional German traffic signs that do not map to one of the classes in the training data:\n\n![alt text][german6] ![alt text][german7] ![alt text][german8] \n\nThe first and fifth images might be difficult to classify because they are speeding signs. All speeding signs look reasonably similar and the images are small enough that the numbers are blurred, making it difficult to distinguish them. In addition, the first sign is smaller (in terms of pixels) than the fifth one so the problems are exacerbated. \n\n####2. Discuss the models predictions on these new traffic signs and compare the results to predicting on the test set. Identify where in your code predictions were made. At a minimum, discuss what the predictions were, the accuracy on these new predictions, and compare the accuracy to the accuracy on the test set (OPTIONAL: Discuss the results in more detail as described in the ""Stand Out Suggestions"" part of the rubric).\n\nThe code for making predictions on my final model is located in the ninth cell of the Ipython notebook.\n\nHere are the results of the prediction:\n\n| Image\t\t\t        |     Prediction\t        \t\t\t\t\t|\n|:---------------------:|:---------------------------------------------:| \n| Speed 120      \t\t| Speed 50   \t\t\t\t\t\t\t\t\t| \n| No Vehicles     \t\t\t| No Vehicles \t\t\t\t\t\t\t\t\t\t|\n| Yield\t\t\t\t\t| Yield\t\t\t\t\t\t\t\t\t\t\t|\n| No Passing\t      \t\t| No Passing\t\t\t\t\t \t\t\t\t|\n| Speed 30\t\t\t| Speed 30      \t\t\t\t\t\t\t|\n\n\nThe model was able to correctly guess 4 of the 5 traffic signs, which gives an accuracy of 80%. The pass that I mentioned previously with the larger training dataset correctly classified all 5, supporting the conclusion that a larger training set improves performance.\n\n####3. Describe how certain the model is when predicting on each of the five new images by looking at the softmax probabilities for each prediction and identify where in your code softmax probabilities were outputted. Provide the top 5 softmax probabilities for each image along with the sign type of each probability. (OPTIONAL: as described in the ""Stand Out Suggestions"" part of the rubric, visualizations can also be provided such as bar charts)\n\nThe code for making predictions on my final model is located in the tenth cell of the Ipython notebook.\n\nFor the first image, the model incorrectly classifies it as ""speed limit (50km/h)"". As evidenced by the top 5 scores, the model has difficulties distinguishing between different speed limits signs, as expected. I believe this is solvable with more training data.\n\nImage 0. Correct value: Speed limit (120km/h)\n\n| Predicted Value | Confidence |\n|------------------|-------------------|\n|Speed limit (50km/h)| 0.487|\n|Speed limit (30km/h)| 0.274|\n|Speed limit (100km/h)| 0.071|\n|Speed limit (70km/h)| 0.054|\n|Speed limit (80km/h)| 0.024|\n\nThe other 4 signs are correctly classified. Interestingly, the ""no vehicles"", ""yield"" and ""no passing"" signs have extremely high confidence. This is presumably because they have distinctive features like a different shape, or a horizontal line through the sign, or an empty white circle. The 30 speed limit sign is correctly classified but not quite as confidently as the others, showing that the model does indeed struggle with speed limits.\n\nOverall, the 5 new signs have a classification accuracy of 80%, compared to the test set accuracy of 90.8%. The probabaility distribution for the incorrectly classified sign suggests there might be some overfitting, particularly around the various classes which include speed limits.\n\n\n|Image 1. Correct value: No vehicles\n\n| Predicted Value | Confidence |\n|------------------|-------------------|\n|No vehicles| 1.000|\n|Bumpy road| 0.000|\n|Bicycles crossing| 0.000|\n|Yield| 0.000|\n|Speed limit (60km/h)| 0.000|\n\nImage 2. Correct value: Yield\n\n| Predicted Value | Confidence |\n|------------------|-------------------|\n|Yield| 1.000|\n|Speed limit (20km/h)| 0.000|\n|Speed limit (30km/h)| 0.000|\n|Speed limit (50km/h)| 0.000|\n|Speed limit (60km/h)| 0.000|\n\nImage 3. Correct value: No passing\n\n| Predicted Value | Confidence |\n|------------------|-------------------|\n|No passing| 1.000|\n|Speed limit (20km/h)| 0.000|\n|Speed limit (30km/h)| 0.000|\n|Speed limit (50km/h)| 0.000|\n|Speed limit (60km/h)| 0.000|\n\nImage 4. Correct value: Speed limit (30km/h)\n\n| Predicted Value | Confidence |\n|------------------|-------------------|\n|Speed limit (30km/h)| 0.968|\n|Speed limit (50km/h)| 0.024|\n|Speed limit (70km/h)| 0.007|\n|Speed limit (100km/h)| 0.000|\n|No vehicles| 0.000|\n\nThe three ""unseen"" additions are interesting. The first one is a ""no parking"" with an arrow in the sign. 4 of the 5 suggested results have arrows somewhere on the sign. The third unseend image is also a ""no parking"" sign, but with no arrow. In this case, the model is very confused and guesses a stop sign. If I had to hazard a guess I would say its the angular straight lines that have caused the confusion. \n\nImage 5. Correct value: unseen\n\n| Predicted Value | Confidence |\n|------------------|-------------------|\n|Keep right| 0.567|\n|Turn left ahead| 0.145|\n|Ahead only| 0.085|\n|Right-of-way at the next intersection| 0.044|\n|Road work| 0.030|\n\nImage 7. Correct value: unseen\n\n| Predicted Value | Confidence |\n|------------------|-------------------|\n|Stop| 0.998|\n|Speed limit (30km/h)| 0.001|\n|No vehicles| 0.001|\n|Speed limit (50km/h)| 0.000|\n|No passing| 0.000|\n\nThe second ""unseen"" image is of an end of speed limit (100 km/h). The model is extremely confident that this is an end of speed limit but gets the number wrong. This is clearly caused by the lack of 100km/h ends of speed limits in the training set. It is gratifying to see the model performing so well, though!\n\nImage 6. Correct value: unseen\n\n| Predicted Value | Confidence |\n|------------------|-------------------|\n|End of speed limit (80km/h)| 1.000|\n|Dangerous curve to the right| 0.000|\n|Slippery road| 0.000|\n|Children crossing| 0.000|\n|Speed limit (60km/h)| 0.000|\n\n'"
https://github.com/rkraig/cabi-predict,Predict near-term Capital Bikeshare availability using a random forest and Poisson regression. Display current status and predictions with leaflet.js map visualization.,"b'# cabi-predict\n\n## Capital Bikeshare: Predictions of Near-Term Supply and Demand\n\nThis project is a demand and outage predictor for Capital Bikeshare. I take dock status and trip history, weather and calendar data, then fit a random forest regression model to estimate the customer demand for bikes and docks at each bikeshare station as a function of ten predictors:\n- Time of Day\n- Day of Week\n- Day of Year\n- Holiday (Y/N)\n- Year\n- Air Temperature\n- Relative Humidity\n- Wind Speed\n- Precipitation within the past hour\n- Snow depth\n\nI demonstrate this model with a customer-facing app that predicts CaBi demand and station outages in the immediate future. This app scrapes real-time dock status and weather data to form a feature vector, estimates customer demand for bikes and docks using the random forest, then computes outage probabilities with a Poisson model. Predictions are visualized on a map using leaflet.js. Result is published to a web app using Flask, hosted on heroku.\n\n## App\n\nhttps://cabi-predict.herokuapp.com/\n\n(Update: I see that the leaflet.js map of prediction output data has broken on heroku... I have not investigated or fixed that yet.)\n\n'"
https://github.com/woobe/h2o_sandbox,H2O prototyping and experimentation,b'# h2o_sandbox\nH2O prototyping and experimentation\n'
https://github.com/pfctdayelise/leafvis,Python framework for visualising layer data using leaflet.,"b'Leaflet based visualization framework\n=====================================\n\nThis package makes it possible to easily visualize gridded meteorological data, represented as numpy arrays, using the excellent javascript visualization library leaflet.\n\nReferences:\n-----------\n\n     Vladimir Agafonkin, http://leafletjs.com/index.html. \n\n\nMaintainers\n-----------\n\n   - Nathan Faggian\n\nTesting\n-------\n\n\n\nDependencies\n------------\n\nThe required dependencies to build the software are:\n\n  - python\n  - numpy\n  - scipy\n  - flask\n  - pyproj\n  - matplotlib\n  - pyresample\n  - py.test\n  - Cython\n  - Jinja2\n  - MarkupSafe\n  - Werkzeug\n  - configobj\n  - itsdangerous\n  - joblib\n  - numexpr\n  - requests\n  - tables\n  - wsgiref\n  - tornado\n  - pyzmq\n  - ipython!\n\nInstall\n-------\n\nThis packages uses distutils, which is the default way of installing python modules. To install in your home directory, use:\n\n    python setup.py install --home\n\nTo install for all users on Unix/Linux:\n\n    python setup.py build\n    sudo python setup.py install\n\nExamples\n--------\n\nRefer to .\n\nDevelopment\n-----------\n\nFollow: Fork + Pull Model:\n\n    http://help.github.com/send-pull-requests/'"
https://github.com/johnkorn/speaker_recognition,Speaker recognition and verification with deep learning,b'Speaker recognition and verification from arbitrary audio using deep learning.\n'
https://github.com/gavinmh/GADS12-NYC,General Assembly Data Science 12,"b""# General Assembly Data Science 12\n\n## Contact Information\n\n* Gavin Hackeling: \n* James Beveridge: \n* Shawn Oakley: \n\n\n## Questions and Discussions:\n\n* \n* \n\n\n## Classes\n\n### Lecture 1: Introduction to Data Science\n_Thursday, 2014/08/07_\n\n#### Class Materials\n\n* \n* \n* \n* \n\n\n### Lecture 2: Simple Linear Regression\n_Tuesday, 2014/08/12_\n\n#### Class Materials\n\n* \n* \n* \n* \n\n\n### Lecture 3: From Simple Linear Regression to Multiple Linear Regression & an Introduction to scikit-learn\n_Thursday, 2014/08/14_\n\n#### Class Materials\n\n* \n* \n* \n* \n\n\n### Lecture 4: Introduction to pandas & matplotlib, Multiple Linear Regression Review\n_Tuesday, 2014/08/19_\n\n#### Class Materials\n\n* \n* \n* \n* \n* \n* \n\n\n#### Project 1\n\n* \n\n\n### Lecture 5: From Multiple Linear Regression to Polynomial Regression & Regression Project Workshop\n_Tuesday, 2014/08/21_\n\n#### Class Materials\n\n* \n* \n* \n\n#### Extra Materials\n\n* \n\n\n### Lecture 6: From Multiple Linear Regression to Logistic Regression & Text Feature Extraction\n_Tuesday, 2014/08/26_\n\n#### Class Materials\n\n* \n* \n\n\n### Lecture 7: Classification with Logistic Regression and K-Nearest Neighbors\n_Thursday, 2014/08/28_\n\n#### Class Materials\n* \n* \n\n\n### Lecture 8: (optional class) Kaggle Competitions\n_Tuesday, 2014/09/02_\n\n#### Class Materials\n\n\n### Lecture 9: Non-linear Classification and Regression with Decision Trees and Ensemble Learning\n_Thursday, 2014/09/04_\n\n#### Class Materials\n\n* \n* \n\n\n### Lecture 10: Cluster Analysis with K-Means\n_Tuesday, 2014/09/09_\n\n#### Class Materials\n\n* \n* \n* \n\n\n### Lecture 11: Dimensionality Reduction with Principal Component Analysis\n_Thursday, 2014/09/11_\n\n#### Class Materials\n\n* \n* \n* \n\n\n### Lecture 12: Machine Learning Review\n_Thursday, 2014/09/16_\n\n#### Class Materials\n\n* \n\n\n### Lecture 13: Project Workday\n_Thursday, 2014/09/18_\n\n#### Class Materials\n\n* \n\n\n### Lecture 14: The Perceptron \n_Thursday, 2014/09/23_\n\n#### Class Materials\n\n* \n* \n\n### No class\n_Thursday, 2014/09/25_\n\n\n### Lecture 15: From the Perceptron to Support Vector Machines\n_Thursday, 2014/09/29_\n\n#### Class Materials\n\n* \n* \n\n\n### Lecture 16: From the Perceptron to Artificial Neural Networks\n_Thursday, 2014/10/02_\n\n#### Class Materials\n\n* \n* \n* \n\n\n### Lecture 17: Web Apps with scikit-learn\n_Thursday, 2014/10/07_\n\n#### Class Materials\n\n* \n* \n\n\n### Lecture 18: Recommendation Engines\n_Thursday, 2014/10/02_\n\n#### Class Materials\n\n* \n* \n\n\n### Lecture 19: Guest Lecture 1\n_Thursday, 2014/10/09_\n\n#### Class Materials\n\n\n### Lecture 20: Guest Lecture 2: Visualization with D3.js\n_Thursday, 2014/10/14_\n\n#### Class Materials\n\n\n### Lecture 21: Final Project Workday\n_Thursday, 2014/10/16_\n\n#### Class Materials\n\n\n### Lecture 22: Final Project Presentations\n_Thursday, 2014/10/21_\n\n#### Class Materials\n\n\n### Lecture 23: Final Project Presentations\n_Thursday, 2014/10/23_\n\n#### Class Materials\n\n\n## Git Workflow and Command Line Tips:\n\n* \n\nUsing a virtual machine\n----\n\nFor further help/troubleshooting, feel free to come by the office hours or contact us for further help.\n\nIn case you're running into issues setting the environment up on your local environment, you can download a machine image from the following link: \n\n https://www.dropbox.com/s/7nt0rt54m7jtxj5/GADS-InstalledEnv_1%28import%29.ova\n\nUse the following user info to login: \n\nUser: GADS\n\nPassword: gadspassword\n\nNOTE: VMWare Player appears to occassionaly throw errors when dealing with this image.  Therefore, it would probably be easier to use [VirtualBox]\n\nTo install the virtual machine on VirtualBox\n\n* Install VirtualBox\n* Select File>Import Appliance\n* When prompted, select 'GADS-InstalledEnv_1(import).ova' (or whatever you decided to name the downloaded image)\n* Click through the rest of the import process\n* In the main menu of VirtualBox, highlight the name of the machine (it should be whatever you named the ova file), and press 'Start'.\n\nThe image has Ubuntu 14.04 installed.  I found that the default RAM allocation (512 MB) was running a bit slow, so I adjusted the default allocation to 1024 MB.  If you're running into performance issues, you may need to adjust the memory allocation settings.\n\nThe image has the following libraries installed on it:\n\n* [scikit-learn] (v 0.15.1)\n* [numpy] (v 1.8.1)\n* [pandas] (v 0.13.1)\n* [scipy] (v 0.13.3)\n* [pip] (v 1.5.6)\n* [matplotlib] (v 1.3.1)\n* [git]\n* [nltk] (v 2.0.4)\n\n\n#####  Git Repository in VM image\n\nThe class's git repo has been cloned as the following directory:\n\n~/Desktop/courseGit/GADS12-NYC\n\nThere is a small bash script (~/Desktop/courseGit/GADS12-NYC/update) that allows you to type 'update-GADS' in order to clone the latest version from the repo.  The command is only soft-linked to the script, meaning that if 'update' is moved, the command won't work anymore.  Even if that happens though, you can just type 'git pull' in the repo's directory to update your local repo.\n\n\n[scikit-learn]:http://scikit-learn.org/stable/\n[numpy]:http://www.numpy.org/\n[pandas]:http://pandas.pydata.org/\n[scipy]:http://www.scipy.org/\n[pip]:https://pypi.python.org/pypi/pip\n[matplotlib]:http://matplotlib.org/\n[git]:http://git-scm.com/\n[nltk]:http://www.nltk.org/\n[VirtualBox]:https://www.virtualbox.org/\n"""
https://github.com/htwangtw/DimensionsOfExperience,Scripts for paper Dimensions of Experience: Exploring the Ontology of the Wandering Mind,b'# DimensionsOfExperience\nScripts for paper Dimensions of Experience: Exploring the Ontology of the Wandering Mind\n'
https://github.com/learntextvis/code-samples,draft code to communicate ideas ,"b'# code-samples repo and project notes\ndraft code to communicate ideas.\n\nPlease read the proposal doc for audience and intent: https://docs.google.com/document/d/1aLM0y56zCDUqUd6NRyiL0Nl9HPbKognBvK0aczcq3_4/edit?usp=sharing\n(this is a copy that can be commented on)\n\nWould recommend playing with Lexos if you havent.\n\n## Notes & ToDo List\n\nGoal: single doc analysis/vis but also comparative multi-document in as many cases as possible.\n\n### ""Analysis"" to offer\n\n* Tokenization: word, sentence\n* lemmatize and bigrams\n* Parts of Speech\n* Tf-idf\n* Hclust (hierarchical clustering)\n* (PCA? Need an experiment that doesnt suck and is interpretable, the R sample one isnt)\n* Word-list based sentiment analysis (https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html#lexicon)\n\n### Counting Things\n\n* characters (maybe - for punctuation in particular)\n* words / lemmas\n* sentences\n* POS\n* bigrams -- needed as part of the tokenization step\n\nStopWord Handling!\n\n* We should always display the stop words used in the UI -- because they are a choice and carry consequences.\n* Start simple with an inline variable, then file, then tool in browser to update them?\n\n### Vis using the analysis/counts above\n\n* structure of page/doc? -- is this interesting? might be a good newbie view.\n    * (does this make sense in multi-document cases?)\n    * option to highlight items ""inline""\n    * Examples:\n        * Ben Frys Darwin thing\n        * Art things on my pinterest\n        * My example in html_js/book_shape.html - which doesnt work for line preceding spaces yet (e.g., for tabs, indents, poetry, etc)\n\n* count totals, count as percentage of whole\n    * bars\n    * word clouds and variants\n    * tf-idf for multi-document cases\n        * see examples like html_js/shiffman_tfidf.html, wordclouds_with_shiffman_tfidf.html\n\n* Word networks\n\n* Word Clouds: Let\n    * Get rid of random coloring or words - color only as indicator\n    * Idea: little bar charts beside wordclouds to show distribution of counts?\n    * Idea: ordered words with counts (mocked up)\n    * Extremely useful in small-multiple/multi-doc situations, but design issues.\n        * Tf-idf sizing is most interesting in that case. See:\n        * Show dynamic side-by-side and merged, with difference. Examples:\n        * merged: maybe a network style? also, that NYT bubble thing with circles.\n        * need good design for the combination operation for overlapping words when counts differ between the overlaps\n    * Single doc cases can become small-multiple if we allow word-clouds of POS, chars, etc.\n    * Ideally perform bigram analysis first!\n    * stop words are iterative process with word cloud displays\n\n* Timeseries\n    * show location of word in doc over time (concordance view\n    * use windowsize (user-settable) and count, show over time\n        * examples are wordclouds per chapter in a book, in order\n        * who talks when in a debate / play\n        * the Tarantino obscenity chart in 538. We should be able to make that.\n        * Simple example in timeseries.html shows just words per chapter in order in Emma.\n    * Vis types over time - bar, line, even word cloud??\n\n* Clustering docs\n    * see hclust code - actually, this is pretty bad in python in that it requires a bunch of libs. Rs is easier. (See my notebook code in python dir using Pattern.  Its easier to use scikit-learn but harder to explain to newbies. However, outputting the data from Pattern is awkward. Maybe NLTK is the best approach, I think you can save out numpy arrays easily?)\n    * R examples: https://eight2late.wordpress.com/2015/07/22/a-gentle-introduction-to-cluster-analysis-using-r/, mine in: R/tm_clustering_example.R\n    * tree view for first version? see output from R networkD3 dendrogram in html_js/networkD3_hclust_output_from_R.html\n    * Ideally the tree clustering allows collapsible nodes for large trees, and customizable labels on the edges.\n\n## How about a Structure Like This for the Site...\n\n1) Getting Setup:\n    * Options for R, Python, Pure JS\n    * (Explain cons of just in-browser JS)\n2) Shapes of texts\n3) Concordance-views (simple search) -- keywords in context\n4) Tokenization, Simple Counts, stop words\n    [We explain fancy tokenization happens in the python/r scripts, simple space sep can be done in js.]\n    Word networks come here - and bigrams/n-grams.\n5) Parts of speech, lemmatization - show how counts change, what POS gets you.\n6) Word clouds -- this basically sets us up for doing tf-idf because simple counts are bad for comparative documents, but tf-idf is better\n    * includes a variety of word cloud types -- bubble/networks, regular, maybe my ordered count css version\n7) Time Series - breaking a text into sections, using multiple texts that have time ordering\n    * Maybe simple sentiment via polarity word lists I added to the repo too.\n8) Clustering (if we get to it, or I can add it later, if you help with the cluster-output-to-tree structure for js)\n\n## Languages for Discussion\n\nDiscuss: Scripts for pre-processing (python/r) and/or in-page analysis with js.\n\n###JS\n\n* RiTa for POS: http://www.ghostweather.com/files/image_replacement/; I havent experimented with all of its capabilities yet. Not sure I believe it can be as good as nltk/SpaCy and Rs tm/NLP/SnowballC etc.\n* shiffmans tf-idf in js (used in some of my examples)\n* wordcount.js\n* that Natural node/js lib had issues last time I used it, but Shiffman was issueing merge requests against it...\n* I included some dudes textAnalysisSuite in the html_js/js dir, but I dont understand its tfidf. The bigram thing looks interesting/useful.\n* I dont really think the js tools are as good yet or as complete; and for sizable projects that would be slow in browser, would we give node instructions?  (Argh)\n\n###Python\n\n* Note that python stuff generally needs downloads. SpaCy and NLTK both do.\n* I can make command-line scripts that prep data as json if we want that... My example script in preprocess_files.py was for another class, and i used it here to get POS for the word cloud experiments\n* Need to well-document and test the install&run instructions for someone newbie\n* Requires command line expertise\n* SpaCy might be a good lib to use, I have used it for word2vec related things but havent checked carefully the contrast with NLTK/pattern.  It wont do tf-idf for us.\n\n###R\n\n* Is this simplest actually? scripts that can be run from RStudio or command line?\n* Still requires certain packages\n* My R is rusty but Jims is good I bet\n\n\nDiscuss: Should we have some kind of consistent format like JSON for output from the scripts?  Or csv, which is easier for newbies.  Configuration settings could be in a simple JSON file separate from the data files.\n\nToDo: Compare the accuracy and quality of the results in the 3 languagues and multiple libraries in Python/R. There are a lot.  I can do that.)\n\n###Data Sets ToDos\n\n* Need to act a play and/or script\n* Poetry examples\n* Maybe recipes?  some very different genre...\n\n'"
https://github.com/athirara/GBM,Geometrical Brownian Motion,b'# GBM\nGeometrical Brownian Motion\n'
https://github.com/alexisperrier/upem-topic-modeling,Cours sur le topic modeling - UPEM - Master Méthode computationnelle et analyse de contenu,"b""# upem-topic-modeling\nCours sur le topic modeling - UPEM - Master M\xc3\xa9thode computationnelle et analyse de contenu\n\n## I: Topic Modeling\n\n* Nature et applications\n* Approche Deterministe: LSA\n* Approche Probabiliste: LDA\n* Quelques librairies en R et python\n\n## II: Le package STM en R\n\n* Parametres\n* M\xc3\xa9triques: exclusivit\xc3\xa9 et coh\xc3\xa9rence s\xc3\xa9mantique\n* Appliqu\xc3\xa9 a un corpus propre\n\n## LAB - R STM\n\n* Le corpus: r\xc3\xa9sum\xc3\xa9s d'articles tech, IEEE et Arstechnica\n* Le package STM en R\n* Comment determiner le nombre optimal de topics?\n* Comment interpreter les r\xc3\xa9sultats?\n* Jupyter Notebook et Script R\n\n## III: forum Alt-right sur Facebook\n\n* 500.000 commentaires provenant du forum alt-right God Trump Emperor\n* De la n\xc3\xa9cessit\xc3\xa9 de travailler le contenu\n* Filtrer le bruit avec\n    * Lemmatization, tokenization\n    * Part of Speech tagging\n    * Named entity recognition\n* Jupyter Notebook et Script R\n\n## IV: Application au Francais\n\n* Quelles sont les librairies pour:\n    * Part of Speech\n    * Tokenization\n    * Lemmatization\n\n## V: Resources\n\n* Articles et blogs\n\n"""
https://github.com/bryancshepherd/RunStatus,"Build a script running status tracker with R, Python and RPi",b'# RunStatus\n'
https://github.com/jagman88/HACTproject_Julia,Describes and solves some simple HACT models in Julia.  The notes and code is modified and translated from Benjamin Moll's notes and codes: http://www.princeton.edu/~moll/notes.htm and http://www.princeton.edu/~moll/HACTproject.htm).,"b'# HACTproject_Julia\n\nDescribes and solves some simple HACT models in Julia. The notes and codes are modified and translated from Ben Molls notes and matlab codes at http://www.princeton.edu/~moll/notes.htm and http://www.princeton.edu/~moll/HACTproject.htm \n\nThis project was carried out as part of the assessment for John Stachurskis Computational Economics course at NYU, 2016. \n\nPlease note that the Jupyter notebook files (with the "".ipynb"" extension) may not easily compile into PDFs due to the presence of Gadfly plots. To compile from the Jupyter Notebook Viewer, you may need to install the Inkscape and Pandocs packages (and even then, the Gadfly plots may not show up properly...). It may be easier to compile to PDF via Latex. For this reason, I have included Pythons Matplotlib plots in the PDF writeup file, ""HACT_HuggettEconomy_Julia.pdf"". \n\n'"
https://github.com/timothydmorton/gaia-explore,first look into GAIA data,b'# gaia-explore\nfirst look into GAIA data\n'
https://github.com/doorleyr/SFCrime,Visualising crime in San Fransisco duing summer 2014,"b'# SFCrime\nA brief analysis of crime in San Franscico during summer, 2014, completed as an assignment for the Coursera module ""Communicating Data Science Results"". \n\nThe ipython notebook file was created as a Jupyter notebook using the R Kernel.\n'"
https://github.com/RunZGit/Magicmail,Using Word2Vec techniques and K-Mean method to classifies Rose-Hulman sharepoint emails based on its content,b'# Magicmail\nUsing Word2Vec techniques and K-Mean method to classifies Rose-Hulman sharepoint emails based on its content\n'
https://github.com/ramon-oliveira/deepstats,"Repository for ""Known Unknowns: Uncertainty Quality in Bayesian Neural Networks"" paper.","b'# Known Unknowns: Uncertainty Quality in Bayesian Neural Networks\n\nThis repository holds the code for the paper ""Known Unknowns: Uncertainty Quality in Bayesian Neural Networks"" accepted to the Bayesian Deep Learning Workshop at NIPS 2016.\n\nArxiv link: https://arxiv.org/abs/1612.01251\n\nWe evaluate the uncertainty quality in neural networks using anomaly detection. We extract uncertainty measures (e.g. entropy) from the predictions of candidate models, use those measures as features for an anomaly detector, and gauge how well the detector differentiates known from unknown classes. We assign higher uncertainty quality to candidate models that lead to better detectors. We also propose a novel method for sampling a variational approximation of a Bayesian neural network, called One-Sample Bayesian Approximation (OSBA). We experiment on two datasets, MNIST and CIFAR10. We compare the following candidate neural network models: Maximum Likelihood, Bayesian Dropout, OSBA, and --- for MNIST --- the standard variational approximation. We show that Bayesian Dropout and OSBA provide better uncertainty information than Maximum Likelihood, and are essentially equivalent to the standard variational approximation, but much faster.\n\n## Reproducing results (Python 3.5)\n\n### Installing dependences\n\n\n### Running an experiment\n\n\n\nAvailable dataset options:\n* mnist\n* cifar10\n* svhn (experimental)\n\nAvailable model options:\n* mlp\n* mlp-dropout\n* mlp-poor-bayesian\n* mlp-bayesian\n* convolutional\n* convolutional-dropout\n* convolutional-poor-bayesian\n\n\n### Plotting results\n\n\n\n## ANOVA Results\n\n### MNIST\n\n\n\n\n\n\n### CIFAR10 (Updated Results)\n\n\n\n\n\n'"
https://github.com/Aniruddha-Tapas/Android_OpenCV_Scan2Excel,Android app to convert scanned tabular digit data to Excel using Tensorflow. [Incomplete],"b'\n\n# Android OpenCV demo with TensorFlow\n\n* Scans tabular images using OpenCV for Android\n* Recognizes digits using TensorFlow\n* Writes to Excel file [To be done!]\n\n\n\n\nMNIST For ML Beginners\nhttps://www.tensorflow.org/versions/r0.10/tutorials/mnist/beginners/index.html\n\nDeep MNIST for Experts\nhttps://www.tensorflow.org/versions/r0.10/tutorials/mnist/pros/index.html\n\n## How to train model.\nTraining scripts for neural network model are located at\n\nhttps://github.com/miyosuda/TensorFlowAndroidMNIST/tree/master/trainer-script\n\nTo create model by yourself, install Tensorflow and run python scripts like\n\n    $ python beginner.py\n\nor\n\n    $ python expert.py\n\nand locate exported .pb file to assets dir.\n\nTo export training model, I added some modification to original tutorial scripts.\n\nNow Tensorflow cannot export network graph and trained network weight Variable at the same time,\nso we need to create another graph to export and convert Variable into constants.\n\nAfter training is finished, converted trained Variable to numpy ndarray.\n\n    _W = W.eval(sess)\n    _b = b.eval(sess)\n\nand then convert them into constant and re-create graph for exporting.\n\n    W_2 = tf.constant(_W, name=""constant_W"")\n    b_2 = tf.constant(_b, name=""constant_b"")\n\nAnd then use tf.train.write_graph to export graph with trained weights.\n\n\n## How to build JNI codes\n\nNative .so files are already built in this project, but if you would like to\nbuild it by yourself, please install and setup NDK.\n\nFirst download, extract and place Android NDK.\n\nhttp://developer.android.com/intl/ja/ndk/downloads/index.html\n\nAnd then update your PATH environment variable. For example,\n\n    export NDK_HOME=""/Users/[your-username]/Development/android/android-ndk-r11b""\n    export PATH=$PATH:$NDK_HOME\n\nAnd build .so file in jni-build dir.\n\n    $ cd jni-build\n    $ make\n    \nand copy .so file into app/src/main/jniLibs/armeabi-v7a/ with\n\n    $ make install\n\n(Unlike original Android demo in Tensorflow, you dont need to install bazel to build this demo.\n\nTensorflow library files (.a files) and header files are extracted from original Tensorflow Android demo r0.10.\n\n'"
https://github.com/koaning/python_data_intro,A beginner notebook for people who want to get started with python and data. Joy ensues! ,b'# python_data_intro\n\nA beginner notebook for people who want to get started with python and data. Joy ensues! Feel free to use this notebook to teach people some basic pydata! \n'
https://github.com/profhuster/profhuster.github.io,Personal Web Site for Michael Huster (profhuster),b'# profhuster.github.io\nPersonal Web Site for Michael Huster (profhuster)\n'
https://github.com/bz866/Exploration-on-New-York-Crime-Open-Data-Based-on-PolyGamy-Thoughts,NYU Big Data CourseTerm Project,"b""# BigData2017Project\nBigData2017Project\n\n### File Explaination\nThe repo consists of three file:\n\n1.  This is used for generating validation check and output 24 files for each column.\n2.  spark script for summarizing how many Valid/Invalid/Null in each category.\n3.  code for using pysparkSQL to check how many distinct values in each category.\n4.  This is used for generating all precincts code.\n5.  This is used for generating proportions of all kinds of crime in all precincts. \n6.  This is used for generating the average time used to solve a case in different boroughs. This script must be run after .\n7.   This is used for generating required data to compute the average time used to solve a case in different boroughs. This script must be run before .\n8.  This is used for generating some of patterns we used in this project.\n9.  This is the jupyter notebook used for generating all plots in this project.\n\n### Steps for running jobs\nTo get the result required, i.e. check for base_type, semantic_type, validity, simply run the spark_job_script.py on your dumbo as follows,\n\n1. Upload  to your hpc storage with command:\n\n\n2. Upload  to hpc like above then upload it to hdfs with command:\n\n\n3. Since we need to use some python packages, first set up your python environment with below sentences\n\n\n\n4. Submit spark job\n\n\n5. Get output \n\n\n### Please note:\n1. We assume user won't change the name of the CSV file and keep it as NYPD_Complaint_Data_Historic.csv;\n2. Please make sure the previously output on your HPC is removed so it won't influence running the script.\n\n\n"""
https://github.com/nikitakit/nbtools,Tools for IPython notebooks,b'nbtools - Tools for IPython notebooks\n'
https://github.com/fperez/gh-activity,Tools to measure activity on github for a set of orgs and repos,b'# gh-activity\nTools to measure activity on github for a set of orgs and repos\n'
https://github.com/Anima-OS/Anima_Terminal,The default terminal. Based on gterm,"b'.. README:\n\nREADME\n==================================================================\n \n.. contents::\n\nIntroduction\n----------------------------------------------------------------------------------------------\n\n for teaching and demonstrations. The\n   GraphTerm server can be set up in the cloud and accessed by\n   multiple users using their laptop/mobile browsers, with Google\n   Authentication. The lab instructor can\n   \n   via a ""dashboard"", and users can collaborate with each other by\n   sharing terminals and notebooks.\n\n\n \nwhich implemented a terminal using the Mozilla framework and\n\nwhich is an AJAX/Python terminal implementation. (Other recent\nprojects along these lines include  \nand .)\n\nA GraphTerm terminal window is just a web page served from the\nGraphTerm server program. Multiple users can connect\nsimultaneously to the web server to share terminal sessions.\nMultiple hosts can also connect to the server (on a different port),\nallowing a single user to access all of them via the browser.\nThe GraphTerm server acts as a  for visual tracing of python programs\n - : Using Yahoo weather API to display weather\n\nImages of GraphTerm in action can be found in _ \nand in this .\nHere is a sample screenshot showing the output of the\n\ncommand, which embeds six smaller terminals within the main terminal, running\nsix different commands from the GraphTerm toolchain: (i) live twitter stream output using\n, (ii) weather info using ,\n(ii) slideshow from markdown file using  and reveal.js,\n(iv) word cloud using  and d3.js, (v) inline graphics using ,\nand (vi) notebook mode using the standard python interpreter.\n\n\n Screenshot 3: Embedding terminals within GraphTerm\n\n.. figure:: https://github.com/mitotic/graphterm/raw/master/doc-images/gt-metro.jpg\n   :align: center\n   :width: 90%\n   :figwidth: 100%\n\n.. installation:\n\nInstallation\n----------------------------------------------------------------------------------------------\n\nTo install , untar,\nand execute the following command in the  directory::\n\n   python setup.py install\n\nFor the manual install, you will also need to install the \nweb server, which can be downloaded from\n\n\nYou can also try out GraphTerm without installing it, by untarring the\nsource tarball (or checking out the source from .\n\nQuick Start\n----------------------------------------------------------------------------------------------\n\nTo start the  server, use the command::\n\n    gtermserver --terminal --auth_type=none\n\nThis will run the  server and open a GraphTerm terminal window\nusing the default browser. For multi-user computers,\nomit the  option\nwhen starting the server, and enter the authentication code stored in\nthe file  as needed. (The \ncommand can automatically enter this code for you.)\n\nYou can access the GraphTerm server using any browser that supports\nwebsockets. Google Chrome works best, but Firefox, Safari, or IE10\nare also supported. Start by entering the following URL::\n\n    http://localhost:8900\n\nIn the  browser page, select the GraphTerm host you\nwish to connect to and create a new terminal session. (Note: The GraphTerm\nhost is different from the network hostname for the server.)\nWithin a GraphTerm window, you can use terminal/new menu option, or\ntype the command , to create a new GraphTerm session \n\nYou can also open additional GraphTerm terminal windows using\nthe  command::\n\n    gterm --noauth [session_name]\n\nwhere the terminal session name argument is optional.\n\nOnce you have a terminal, try out the following commands::\n\n    gls \n    gvi \n\nThese are commands in the GraphTerm toolchain that imitate\nbasic features of the standard  and  commands.\n(Note: You need to execute the  command\nto be able to use the GraphTerm toolchain. Otherwise, you will\nencounter a  error.)\nSee \nfor more info on using GraphTerm. You can also\n\nusing GraphTerm.\n\nDocumentation and Support\n----------------------------------------------------------------------------------------------\n\nUsage info and other documentation can be found on the project home page,\n.\nSee the \npage for an overview of the documentation and the\n\npage for more advanced usage examples.\n\nYou can also use the following command::\n\n  greveal $GTERM_DIR/bin/landslide/graphterm-talk1.md | gframe -f\n\nto view a slideshow about GraphTerm within GraphTerm.\nClick on the red X in the top right corner to exit the slideshow.\n\nThere is a \nfor announcements of new releases, posting questions related to\nGraphTerm etc. You can also follow _ on Twitter for updates.\n\nTo report bugs and other issues, use the Github .\n\nCaveats and Limitations\n----------------------------------------------------------------------------------------------\n\n -  and\n. \nIt borrows many of the ideas from  and the client uses .\n\nThe , and graphical editing uses the\n_ as well as\n\n\nThe 3D perspective mode was inspired by Sean Slinsky.\n\nOther packaged open source components include:\n\n - _  Data driven documents\n\n - _ presentation\n   program\n\n - Online Python Tutorial from \n\n -  Javascript\n   Markdown converter\n\n - \n   menu plugin\n\n -  utility library\n\n\n\nLicense\n----------------------------------------------------------------------------------------------\n\n is distributed as open source under the _.\n\n'"
https://github.com/fanshi118/NLP_NMT_Project,Neural Machine Translation project for NLP Fall 2016,b'# NLP_NMT_Project\nNeural Machine Translation project for NLP Fall 2016\n\n## Steps to run\n\n(i.e.  for fr->en translation)\n\nTraining results are shown in .\n\nTesting results and findings are discussed in the paper.'
https://github.com/yannche/pyCommon,ensemble de fichiers a mettre dans l'image virtualbox,b'# pyCommon\n'
https://github.com/grantrosario/advanced_lane_finding,An advance implementation of detecting lane lines ,"b'\n#Advanced Lane Finding Project\n---\nThe goals / steps of this project are the following:\n\n* Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n* Apply a distortion correction to raw images.\n* Use color transforms, gradients, etc., to create a thresholded binary image.\n* Apply a perspective transform to rectify binary image (""birds-eye view"").\n* Detect lane pixels and fit to find the lane boundary.\n* Determine the curvature of the lane and vehicle position with respect to center.\n* Warp the detected lane boundaries back onto the original image.\n* Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.\n\n[//]: # (Image References)\n\n[image1]: output_images/calibration_img.png ""Undistorted""\n[image2]: output_images/undistort_street.png ""Road Transformed""\n[image3]: output_images/binary.png ""Road Binary""\n[image4]: output_images/warped.jpg ""Warped Lanes""\n[image5]: output_images/warped_binary.png ""Warped Binary Lanes""\n[image6]: output_images/histogram.png ""Histogram""\n[image7]: output_images/find_lanes.png ""Polynomial Lanes""\n[image8]: output_images/find_lanes_margin.png ""Margin Lanes""\n[image9]: output_images/final.png ""Output Image""\n[video1]: final_attempt.mp4 ""Video""\n\n\n### Camera Calibration\n\n#### 1. Description\n\nThe code for this step is contained in the second code cell of the IPython notebook located in ""advanced_lane_finding.ipynb"" (or in lines 44 through 71 of the file called ).  \n\nI start by preparing ""object points"", which will be the (x, y, z) coordinates of the chessboard corners in the world. Here I am assuming the chessboard is fixed on the (x, y) plane at z=0, such that the object points are the same for each calibration image.  Thus,  is just a replicated array of coordinates, and  will be appended with a copy of it every time I successfully detect all chessboard corners in a test image.   will be appended with the (x, y) pixel position of each of the corners in the image plane with each successful chessboard detection.  \n\nI then used the output  and  to compute the camera calibration and distortion coefficients using the  function.  I applied this distortion correction to the test image using the  function and obtained this result: \n\n![alt text][image1]\n\n### Pipeline (single images)\n\n#### 1. Undistort\n\nThe first step in the pipeline is to undistort the test image as seen below: \n\n![alt text][image2]\n\n#### 2. Create thresholded binary image\n\nI decided to avoid using gradients since they do not handle shadows well. Instead, I used a combination of color thresholds to generate a binary image (thresholding steps at lines 73 through 145 in ). Specifically, I utilized the rgb color set to focus on yellow lines and the L channel in the HLS color set to focus on white lines. Heres an example of my output for this step.\n\n![alt text][image3]\n\n#### 3. Perspective Transform\n\nThe code for my perspective transform includes a function called , which appears in lines 150 through 159 in the file  (or in the 4th code cell of the IPython notebook).  The  function takes as inputs an image (), as well as source () and destination () points.  I chose the hardcode the source and destination points in the following manner:\n\n\n\nThis resulted in the following source and destination points:\n\n| Source        | Destination   | \n|:-------------:|:-------------:| \n| 200, 720      | 320, 720        | \n| 453, 547      | 320, 590.40002441      |\n| 835, 547     | 960, 590.40002441      |\n| 1100, 720      | 960, 720       |\n\nI verified that my perspective transform was working as expected by drawing the  and  points onto a test image and its warped counterpart to verify that the lines appear parallel in the warped image. I then converted it to binary as well.\n\n![alt text][image4]\n![alt text][image5]\n\n#### 4. Lane-line Identification\n\nThis step can be seen in lines 152 through 320 of the code in . I calculated the following histogram for the image which shows active pixels along all columns on the lower half of the image.\n\n![alt text][image6] \n\nThe two tallest peaks on the left half and right half show where in the image the lane lines appear. \n \n  \nNext, I created 9 individual windows starting from the bottom and generating one by one and scanning their area for pixels. Using this, I was able to generate a fit for the left and right lane lines as seen below.\n\n![alt text][image7] \n\nOnce I was able to estimate this from scratch, I could use this estimation for future frames of the video instead of having to estimate the lines from scratch each time. I instead scanned around future areas using a margin of 100 starting from the point of the previous frame line. Below is a visualization of this margin mathod. \n\n![alt text][image8]\n\n#### 5. Radius of Curvature and Distance Calculation\n\nI did this in lines 323 through 371 in my code in \n\nThe radius of curvature was calculated based on the previously calculated second order polynomial.\n\nI first converted the pixels to meters in accordance with U.S. regulations that lane width be 3.7 meters and our images have a lane length of about 30 meters.\n\nOnce I do this I just fit the left and right lane to a polynomial and use the following formula to calculate the radius. \n \n\nIn order to calculate the distance from the center I just calculated the car position as the center of the image, subtracted the mean of the left and right intercepts from it and mutiplied that by the image.\n\n#### 6. Provide an example image of your result plotted back down onto the road such that the lane area is identified clearly.\n\nI implemented this step in lines 373 through 419 in my code in  in the functions  and .  Here is an example of my result on a test image:\n\n![alt text][image9]\n\n---\n\n### Pipeline (video)\n\n#### 1. Provide a link to your final video output.  Your pipeline should perform reasonably well on the entire project video (wobbly lines are ok but no catastrophic failures that would cause the car to drive off the road!).\n\nHeres a \n\n---\n\n### Discussion\n\n#### 1. Briefly discuss any problems / issues you faced in your implementation of this project.  Where will your pipeline likely fail?  What could you do to make it more robust?\n\nHere Ill talk about the approach I took, what techniques I used, what worked and why, where the pipeline might fail and how I might improve it if I were going to pursue this project further.\n\nThe biggest factors of this project were thresholding, skipping the sliding windows functionality if need be, and smoothing the fits. These were the 3 areas that took me the most time, had the largest effect, and I believe could be greatly improved upon.  \n  \nAs stated, I used the sliding window functionality in order to read the lane lines and skipped this funcationaly when I could see the line based on the previous frame. I would like to see how using convolution would compare to this for possible improvement.\n\nI also only used the RGB and L-channel of HLS for thresholding. This is the biggest weakness as the program does not fair well in shadows and noisy environemnts. I would definitely like to explore this in more detail and build a more robust threshold. One possibility would be to create a dynamic threshold by scanning the brightness of the image.\n  \nLastly, smoothing the fits took a lot of trial and error. I ended up using an  method in the Line() class to average the fits and find the best one. I think this may be able to be improved upon by avoiding the hard-coded values I used.\n'"
https://github.com/tuanavu/udacity-course,Udacity courses,b'Udacity\n---\n\nThis repo contains my lectures and assignments of all  courses.\n\n\n## License\n\nAll Solutions licensed under MIT License. See LICENSE for further details.\n'
https://github.com/mu529/airport_modes,Modal choice study for NYC airports,b''
https://github.com/rllabmcgill/rlcourse-march-10-hugobb,rlcourse-march-10-hugobb created by GitHub Classroom,"b'# Automatic Discovery of Subgoals Using Diverse Density\n\nThis repository contains an implementation of []\n\nThe code is tested on a two-room gridworld as in the paper. Only the subgoal discovery is implemented, the policy learning over the option hasnt been implemented.\n\nOpen notebook for explanation, and results.\n\n<a name=1> [1] \n'"
https://github.com/data-8/datascience,A Python library for introductory data science,"b'# datascience\n\nA Berkeley library for introductory data science.\n\n_written by Professor , Professor\n,\n, and _\n\nFor an example of usage, see the .\n\n\n\n\n\n## Installation\n\nUse :\n\n\n\nA log of all changes can be found in CHANGELOG.md.\n'"
https://github.com/betagouv/mes-aides-analytics,Analyse de données sur les usages de Mes Aides,"b""# Statistiques du site Aides-Jeunes\n\nD\xc3\xa9p\xc3\xb4ts d'exp\xc3\xa9rimentations autour des donn\xc3\xa9es de Aides-Jeunes et du moteur de calculs OpenFisca-France\n\n## Mise en production\n\nLa mis en production est faite automatiquement sur .\n"""
https://github.com/Gamrix/cs231n_proj,Deep View Morphing,b'# cs231n_proj\nDeep View Morphing\n'
https://github.com/mi41814/tf_test1,tf test 1,"b'# TensorFlow Models\n\nThis repository contains machine learning models implemented in\n. The models are maintained by their\nrespective authors. To propose a model for inclusion, please submit a pull\nrequest.\n\nCurrently, the models are compatible with TensorFlow 1.0 or later. If you are\nrunning TensorFlow 0.12 or earlier, please\n.\n\n\n## Models\n- : various autoencoders.\n- : compressing and decompressing images using a pre-trained Residual GRU network.\n- : privacy-preserving student models from multiple teachers.\n- : image-to-text neural network for image captioning.\n- : deep convolutional networks for computer vision.\n- :  a large-scale life-long memory module for use in deep learning.\n- : language modeling on the one billion word benchmark.\n- : recognize and generate names.\n- : highly parallel neural computer.\n- : neural network augmented with logic and mathematic operations.\n- : probabilistic future frame synthesis via cross convolutional networks.\n- : density estimation using real-valued non-volume preserving (real NVP) transformations.\n- : deep and wide residual networks.\n- : recurrent neural network sentence-to-vector encoder.\n- : image classification models in TF-Slim.\n- : identify the name of a street (in France) from an image using a Deep RNN.\n- : the Swivel algorithm for generating word embeddings.\n- : neural models of natural language syntax.\n- : sequence-to-sequence with attention model for text summarization.\n- : spatial transformer network, which allows the spatial manipulation of data within the network.\n- : models described in the .\n- : predicting future video frames with neural advection.\n'"
https://github.com/j3hempsey/dotfiles,Just some dotfiles,b'\nInstallation\n------------\n\n\n\nFonts for polybar provided by:\nhttps://github.com/stark/siji\n\n\n\n'
https://github.com/Gumil9/Ciscer,Программа для успешного написания тестов CCNA без заучивания ответов,"b'# Ciscer\n\n\xd0\x9f\xd1\x80\xd0\xbe\xd0\xb3\xd1\x80\xd0\xb0\xd0\xbc\xd0\xbc\xd0\xb0 \xd0\xb4\xd0\xbb\xd1\x8f \xd1\x83\xd1\x81\xd0\xbf\xd0\xb5\xd1\x88\xd0\xbd\xd0\xbe\xd0\xb3\xd0\xbe \xd0\xbd\xd0\xb0\xd0\xbf\xd0\xb8\xd1\x81\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x8f \xd1\x82\xd0\xb5\xd1\x81\xd1\x82\xd0\xbe\xd0\xb2 CCNA \xd0\xb1\xd0\xb5\xd0\xb7 \xd0\xb7\xd0\xb0\xd1\x83\xd1\x87\xd0\xb8\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x8f \xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd0\xbe\xd0\xb2.\n\n_\xd0\x97\xd0\xb0\xd0\xbc\xd0\xb5\xd1\x87\xd0\xb0\xd0\xbd\xd0\xb8\xd0\xb5. \xd0\x9a\xd0\xbe\xd0\xbc\xd0\xbf\xd1\x8c\xd1\x8e\xd1\x82\xd0\xb5\xd1\x80\xd0\xbd\xd1\x8b\xd0\xb5 \xd1\x81\xd0\xb5\xd1\x82\xd0\xb8 - \xd0\xb2\xd0\xb0\xd0\xb6\xd0\xbd\xd0\xb0\xd1\x8f \xd0\xb8 \xd0\xbf\xd0\xbe\xd0\xbb\xd0\xb5\xd0\xb7\xd0\xbd\xd0\xb0\xd1\x8f \xd0\xb4\xd0\xb8\xd1\x81\xd1\x86\xd0\xb8\xd0\xbf\xd0\xbb\xd0\xb8\xd0\xbd\xd0\xb0._\n\n## \xd0\xa3\xd1\x81\xd1\x82\xd0\xb0\xd0\xbd\xd0\xbe\xd0\xb2\xd0\xba\xd0\xb0\n\n1. \xd0\xa3\xd1\x81\xd1\x82\xd0\xb0\xd0\xbd\xd0\xbe\xd0\xb2\xd0\xb8\xd1\x82\xd0\xb5 \xd0\xbf\xd0\xbb\xd0\xb0\xd0\xb3\xd0\xb8\xd0\xbd  \xd0\xb4\xd0\xbb\xd1\x8f Firefox. \xd0\x9f\xd0\xbe\xd1\x81\xd0\xbb\xd0\xb5 \xd1\x83\xd1\x81\xd1\x82\xd0\xb0\xd0\xbd\xd0\xbe\xd0\xb2\xd0\xba\xd0\xb8 \xd1\x81\xd0\xbf\xd1\x80\xd0\xb0\xd0\xb2\xd0\xb0 \xd0\xbd\xd0\xb0 \xd0\xbf\xd0\xb0\xd0\xbd\xd0\xb5\xd0\xbb\xd0\xb8 \xd0\xb8\xd0\xbd\xd1\x81\xd1\x82\xd1\x80\xd1\x83\xd0\xbc\xd0\xb5\xd0\xbd\xd1\x82\xd0\xbe\xd0\xb2 \xd0\xb4\xd0\xbe\xd0\xbb\xd0\xb6\xd0\xbd\xd0\xb0 \xd0\xbf\xd0\xbe\xd1\x8f\xd0\xb2\xd0\xb8\xd1\x82\xd1\x8c\xd1\x81\xd1\x8f \xd0\xb8\xd0\xba\xd0\xbe\xd0\xbd\xd0\xba\xd0\xb0 \xd1\x81 \xd0\xb8\xd0\xb7\xd0\xbe\xd0\xb1\xd1\x80\xd0\xb0\xd0\xb6\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5\xd0\xbc \xd0\xb4\xd0\xbe\xd0\xb2\xd0\xbe\xd0\xbb\xd1\x8c\xd0\xbd\xd0\xbe\xd0\xb9 \xd0\xbe\xd0\xb1\xd0\xb5\xd0\xb7\xd1\x8c\xd1\x8f\xd0\xbd\xd0\xba\xd0\xb8.\n2. \xd0\x92 \xd0\xbc\xd0\xb5\xd0\xbd\xd1\x8e \xd0\xbf\xd0\xbb\xd0\xb0\xd0\xb3\xd0\xb8\xd0\xbd\xd0\xb0 \xd0\xb2\xd1\x8b\xd0\xb1\xd0\xb5\xd1\x80\xd0\xb8\xd1\x82\xd0\xb5 \xd0\xbf\xd1\x83\xd0\xbd\xd0\xba\xd1\x82 \xc2\xab\xd1\x81\xd0\xbe\xd0\xb7\xd0\xb4\xd0\xb0\xd1\x82\xd1\x8c \xd1\x81\xd0\xba\xd1\x80\xd0\xb8\xd0\xbf\xd1\x82\xc2\xbb. \xd0\x92 \xd0\xbf\xd0\xbe\xd1\x8f\xd0\xb2\xd0\xb8\xd0\xb2\xd1\x88\xd0\xb5\xd0\xbc\xd1\x81\xd1\x8f \xd0\xb4\xd0\xb8\xd0\xb0\xd0\xbb\xd0\xbe\xd0\xb3\xd0\xbe\xd0\xb2\xd0\xbe\xd0\xbc \xd0\xbe\xd0\xba\xd0\xbd\xd0\xb5 \xd0\xb2 \xd0\xbf\xd0\xbe\xd0\xbb\xd1\x8f \xc2\xab\xd0\xbd\xd0\xb0\xd0\xb7\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xb8\xd0\xb5\xc2\xbb \xd0\xb8 \xc2\xab\xd0\xbf\xd1\x80\xd0\xbe\xd1\x81\xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd1\x81\xd1\x82\xd0\xb2\xd0\xbe \xd0\xb8\xd0\xbc\xd1\x91\xd0\xbd\xc2\xbb \xd0\xb2\xd0\xb2\xd0\xb5\xd0\xb4\xd0\xb8\xd1\x82\xd0\xb5 , \xd0\xb2\xd1\x81\xd0\xb5 \xd0\xbe\xd1\x81\xd1\x82\xd0\xb0\xd0\xbb\xd1\x8c\xd0\xbd\xd1\x8b\xd0\xb5 \xd0\xbf\xd0\xbe\xd0\xbb\xd1\x8f \xd0\xbe\xd1\x81\xd1\x82\xd0\xb0\xd0\xb2\xd1\x8c\xd1\x82\xd0\xb5 \xd0\xbf\xd1\x83\xd1\x81\xd1\x82\xd1\x8b\xd0\xbc\xd0\xb8. \xd0\x9d\xd0\xb0\xd0\xb6\xd0\xbc\xd0\xb8\xd1\x82\xd0\xb5 \xc2\xabOK\xc2\xbb.\n3. \xd0\x92 \xd0\xbe\xd1\x82\xd0\xba\xd1\x80\xd1\x8b\xd0\xb2\xd1\x88\xd0\xb8\xd0\xb9\xd1\x81\xd1\x8f \xd1\x80\xd0\xb5\xd0\xb4\xd0\xb0\xd0\xba\xd1\x82\xd0\xbe\xd1\x80 \xd1\x81\xd0\xba\xd0\xbe\xd0\xbf\xd0\xb8\xd1\x80\xd1\x83\xd0\xb9\xd1\x82\xd0\xb5 \xd0\xba\xd0\xbe\xd0\xb4 \xd0\xb8\xd0\xb7 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb\xd0\xb0  (\xd0\xbe\xd0\xb1\xd1\x84\xd1\x83\xd1\x81\xd1\x86\xd0\xb8\xd1\x80\xd0\xbe\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xbd\xd0\xb0\xd1\x8f \xd0\xb2\xd0\xb5\xd1\x80\xd1\x81\xd0\xb8\xd1\x8f \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb\xd0\xb0 ) \xd0\xb8 \xd1\x81\xd0\xbe\xd1\x85\xd1\x80\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x82\xd0\xb5.\n4. \xd0\x9f\xd1\x80\xd0\xbe\xd1\x82\xd0\xb5\xd1\x81\xd1\x82\xd0\xb8\xd1\x80\xd1\x83\xd0\xb9\xd1\x82\xd0\xb5 \xd1\x81\xd0\xba\xd1\x80\xd0\xb8\xd0\xbf\xd1\x82: \xd0\xbe\xd1\x82\xd0\xba\xd1\x80\xd0\xbe\xd0\xb9\xd1\x82\xd0\xb5 \xd0\xba\xd0\xb0\xd0\xba\xd1\x83\xd1\x8e-\xd0\xbd\xd0\xb8\xd0\xb1\xd1\x83\xd0\xb4\xd1\x8c \xd1\x81\xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x86\xd1\x83, \xd0\xbd\xd0\xb0\xd0\xb6\xd0\xbc\xd0\xb8\xd1\x82\xd0\xb5  \xd0\xb8 \xd0\xb2\xd1\x8b\xd0\xb4\xd0\xb5\xd0\xbb\xd0\xb8\xd1\x82\xd0\xb5 \xd0\xbb\xd1\x8e\xd0\xb1\xd0\xbe\xd0\xb9 \xd1\x82\xd0\xb5\xd0\xba\xd1\x81\xd1\x82. \xd0\x95\xd1\x81\xd0\xbb\xd0\xb8 \xd0\xb2 \xd0\xbb\xd0\xb5\xd0\xb2\xd0\xbe\xd0\xbc \xd0\xbd\xd0\xb8\xd0\xb6\xd0\xbd\xd0\xb5\xd0\xbc \xd1\x83\xd0\xb3\xd0\xbb\xd1\x83 \xd1\x8d\xd0\xba\xd1\x80\xd0\xb0\xd0\xbd\xd0\xb0 \xd0\xbf\xd0\xbe\xd1\x8f\xd0\xb2\xd0\xb8\xd0\xbb\xd0\xb0\xd1\x81\xd1\x8c \xd0\xbd\xd0\xb0\xd0\xb4\xd0\xbf\xd0\xb8\xd1\x81\xd1\x8c, \xd0\xbd\xd0\xb0\xd0\xbf\xd1\x80\xd0\xb8\xd0\xbc\xd0\xb5\xd1\x80, , \xd1\x82\xd0\xbe \xd0\xb2\xd1\x81\xd1\x91 \xd1\x80\xd0\xb0\xd0\xb1\xd0\xbe\xd1\x82\xd0\xb0\xd0\xb5\xd1\x82.\n\n## \xd0\x98\xd1\x81\xd0\xbf\xd0\xbe\xd0\xbb\xd1\x8c\xd0\xb7\xd0\xbe\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xb8\xd0\xb5\n\n\xd0\x9f\xd0\xb5\xd1\x80\xd0\xb5\xd0\xb9\xd0\xb4\xd0\xb8\xd1\x82\xd0\xb5 \xd0\xbd\xd0\xb0 \xd1\x81\xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x86\xd1\x83 \xd1\x82\xd0\xb5\xd1\x81\xd1\x82\xd0\xb8\xd1\x80\xd0\xbe\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x8f \xd0\xb8 \xd0\xb0\xd0\xba\xd1\x82\xd0\xb8\xd0\xb2\xd0\xb8\xd1\x80\xd1\x83\xd0\xb9\xd1\x82\xd0\xb5 \xd1\x81\xd0\xba\xd1\x80\xd0\xb8\xd0\xbf\xd1\x82 \xd0\xba\xd0\xbb\xd0\xb0\xd0\xb2\xd0\xb8\xd1\x88\xd0\xb0\xd0\xbc\xd0\xb8 . \xd0\x98\xd0\xb7 \xd1\x80\xd0\xb5\xd0\xbf\xd0\xbe\xd0\xb7\xd0\xb8\xd1\x82\xd0\xbe\xd1\x80\xd0\xb8\xd1\x8f \xd0\xb7\xd0\xb0\xd0\xb3\xd1\x80\xd1\x83\xd0\xb7\xd1\x8f\xd1\x82\xd1\x81\xd1\x8f \xd0\xbc\xd0\xbe\xd0\xb4\xd1\x83\xd0\xbb\xd0\xb8 \xd1\x81 \xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd0\xb0\xd0\xbc\xd0\xb8 \xd0\xba \xd1\x82\xd0\xb5\xd1\x81\xd1\x82\xd1\x83.\n\xd0\x94\xd0\xb0\xd0\xbb\xd0\xb5\xd0\xb5 \xd0\xbc\xd1\x8b\xd1\x88\xd0\xba\xd0\xbe\xd0\xb9 \xd0\xb2\xd1\x8b\xd0\xb4\xd0\xb5\xd0\xbb\xd1\x8f\xd0\xb9\xd1\x82\xd0\xb5 \xd0\xb2\xd0\xbe\xd0\xbf\xd1\x80\xd0\xbe\xd1\x81\xd1\x8b \xd1\x82\xd0\xb5\xd1\x81\xd1\x82\xd0\xb0 (\xd0\xb8\xd0\xbb\xd0\xb8 \xd0\xb8\xd1\x85 \xd1\x87\xd0\xb0\xd1\x81\xd1\x82\xd0\xb8), \xd0\xb8 \xd0\xb2 \xd0\xbb\xd0\xb5\xd0\xb2\xd0\xbe\xd0\xbc \xd0\xbd\xd0\xb8\xd0\xb6\xd0\xbd\xd0\xb5\xd0\xbc \xd1\x83\xd0\xb3\xd0\xbb\xd1\x83 \xd1\x8d\xd0\xba\xd1\x80\xd0\xb0\xd0\xbd\xd0\xb0 \xd0\xb1\xd1\x83\xd0\xb4\xd1\x83\xd1\x82 \xd0\xbf\xd0\xbe\xd0\xba\xd0\xb0\xd0\xb7\xd1\x8b\xd0\xb2\xd0\xb0\xd1\x82\xd1\x8c\xd1\x81\xd1\x8f \xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd1\x8b. \xd0\x92 \xd0\xbf\xd1\x80\xd0\xbe\xd0\xb3\xd1\x80\xd0\xb0\xd0\xbc\xd0\xbc\xd0\xb5 \xd0\xb7\xd0\xb0\xd0\xb4\xd0\xb0\xd0\xbd\xd0\xbe \xd1\x81\xd0\xbe\xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd1\x81\xd1\x82\xd0\xb2\xd0\xb8\xd0\xb5 \xc2\xab\xd0\xb2\xd0\xbe\xd0\xbf\xd1\x80\xd0\xbe\xd1\x81 - \xd0\xbf\xd1\x80\xd0\xb0\xd0\xb2\xd0\xb8\xd0\xbb\xd1\x8c\xd0\xbd\xd1\x8b\xd0\xb5 \xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd1\x8b\xc2\xbb. \xd0\x9f\xd0\xbe\xd1\x81\xd0\xbb\xd0\xb5 \xd0\xb2\xd1\x8b\xd0\xb4\xd0\xb5\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xb8\xd1\x8f \xd0\xb2\xd0\xb0\xd0\xbc\xd0\xb8 \xd1\x82\xd0\xb5\xd0\xba\xd1\x81\xd1\x82\xd0\xb0 \xd1\x81\xd0\xba\xd1\x80\xd0\xb8\xd0\xbf\xd1\x82 \xd0\xb8\xd1\x89\xd0\xb5\xd1\x82 \xd0\xb2 \xd1\x81\xd0\xb2\xd0\xbe\xd0\xb5\xd0\xb9 \xd0\xb1\xd0\xb0\xd0\xb7\xd0\xb5 \xd0\xb2\xd0\xbe\xd0\xbf\xd1\x80\xd0\xbe\xd1\x81, \xd1\x81\xd0\xbe\xd0\xb4\xd0\xb5\xd1\x80\xd0\xb6\xd0\xb0\xd1\x89\xd0\xb8\xd0\xb9 \xd1\x8d\xd1\x82\xd0\xbe\xd1\x82 \xd1\x82\xd0\xb5\xd0\xba\xd1\x81\xd1\x82. \xd0\x95\xd1\x81\xd0\xbb\xd0\xb8 \xd0\xbd\xd0\xb5 \xd1\x83\xd0\xb4\xd0\xb0\xd1\x91\xd1\x82\xd1\x81\xd1\x8f \xd0\xbd\xd0\xb0\xd0\xb9\xd1\x82\xd0\xb8 \xd0\xb2\xd0\xbe\xd0\xbf\xd1\x80\xd0\xbe\xd1\x81 \xd0\xbf\xd0\xbe \xd0\xba\xd0\xb0\xd0\xba\xd0\xbe\xd0\xb9-\xd1\x82\xd0\xbe \xd0\xb5\xd0\xb3\xd0\xbe \xd1\x87\xd0\xb0\xd1\x81\xd1\x82\xd0\xb8, \xd0\xbf\xd0\xbe\xd0\xbf\xd1\x80\xd0\xbe\xd0\xb1\xd1\x83\xd0\xb9\xd1\x82\xd0\xb5 \xd0\xb2\xd1\x8b\xd0\xb4\xd0\xb5\xd0\xbb\xd0\xb8\xd1\x82\xd1\x8c \xd0\xb4\xd1\x80\xd1\x83\xd0\xb3\xd1\x83\xd1\x8e \xd1\x87\xd0\xb0\xd1\x81\xd1\x82\xd1\x8c. \xd0\x94\xd0\xbb\xd1\x8f \xd0\xbf\xd0\xbe\xd0\xb8\xd1\x81\xd0\xba\xd0\xb0 \xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd0\xb0 \xd0\xbd\xd0\xb5\xd0\xbe\xd0\xb1\xd1\x85\xd0\xbe\xd0\xb4\xd0\xb8\xd0\xbc\xd0\xbe, \xd1\x87\xd1\x82\xd0\xbe\xd0\xb1\xd1\x8b \xd0\xb4\xd0\xbb\xd0\xb8\xd0\xbd\xd0\xb0 \xd0\xb2\xd1\x8b\xd0\xb4\xd0\xb5\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xbd\xd0\xbe\xd0\xb3\xd0\xbe \xd1\x82\xd0\xb5\xd0\xba\xd1\x81\xd1\x82\xd0\xb0 \xd0\xb1\xd1\x8b\xd0\xbb\xd0\xb0 \xd0\xbd\xd0\xb5 \xd0\xbc\xd0\xb5\xd0\xbd\xd0\xb5\xd0\xb5 10 \xd1\x81\xd0\xb8\xd0\xbc\xd0\xb2\xd0\xbe\xd0\xbb\xd0\xbe\xd0\xb2.\n\n#### \xd0\x93\xd0\xbe\xd1\x80\xd1\x8f\xd1\x87\xd0\xb8\xd0\xb5 \xd0\xba\xd0\xbb\xd0\xb0\xd0\xb2\xd0\xb8\xd1\x88\xd0\xb8\n+  \xe2\x80\x94 \xd0\xb2\xd0\xba\xd0\xbb\xd1\x8e\xd1\x87\xd0\xb8\xd1\x82\xd1\x8c/\xd0\xb2\xd1\x8b\xd0\xba\xd0\xbb\xd1\x8e\xd1\x87\xd0\xb8\xd1\x82\xd1\x8c \xd0\xbd\xd0\xb5\xd0\xb7\xd0\xb0\xd0\xbc\xd0\xb5\xd1\x82\xd0\xbd\xd0\xbe\xd0\xb5 \xd0\xb2\xd1\x8b\xd0\xb4\xd0\xb5\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5 \xd1\x82\xd0\xb5\xd0\xba\xd1\x81\xd1\x82\xd0\xb0\n+  \xe2\x80\x94 \xd0\xbf\xd0\xb5\xd1\x80\xd0\xb5\xd0\xbc\xd0\xb5\xd1\x81\xd1\x82\xd0\xb8\xd1\x82\xd1\x8c \xd0\xbf\xd0\xbe\xd0\xb4\xd1\x81\xd0\xba\xd0\xb0\xd0\xb7\xd0\xba\xd1\x83 \xd0\xb2\xd0\xb2\xd0\xb5\xd1\x80\xd1\x85/\xd0\xb2\xd0\xbd\xd0\xb8\xd0\xb7\n+  \xe2\x80\x94 \xd1\x80\xd0\xb0\xd1\x81\xd1\x81\xd1\x82\xd0\xb0\xd0\xb2\xd0\xb8\xd1\x82\xd1\x8c \xd1\x82\xd0\xbe\xd1\x87\xd0\xba\xd0\xb8 \xd0\xbf\xd0\xb5\xd1\x80\xd0\xb5\xd0\xb4 \xd0\xbf\xd1\x80\xd0\xb0\xd0\xb2\xd0\xb8\xd0\xbb\xd1\x8c\xd0\xbd\xd1\x8b\xd0\xbc\xd0\xb8 \xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd0\xb0\xd0\xbc\xd0\xb8*\n+  \xd0\xb8\xd0\xbb\xd0\xb8 \xe2\x80\x94 \xd0\xbf\xd0\xbe\xd0\xba\xd0\xb0\xd0\xb7\xd0\xb0\xd1\x82\xd1\x8c \xd0\xbf\xd1\x80\xd0\xb5\xd0\xb4\xd1\x8b\xd0\xb4\xd1\x83\xd1\x89\xd1\x83\xd1\x8e/\xd1\x81\xd0\xbb\xd0\xb5\xd0\xb4\xd1\x83\xd1\x8e\xd1\x89\xd1\x83\xd1\x8e \xd0\xbf\xd0\xbe\xd0\xb4\xd1\x81\xd0\xba\xd0\xb0\xd0\xb7\xd0\xba\xd1\x83 \xd0\xba \xd0\xbb\xd0\xb0\xd0\xb1\xd0\xbe\xd1\x80\xd0\xb0\xd1\x82\xd0\xbe\xd1\x80\xd0\xbd\xd0\xbe\xd0\xb9 \xd1\x80\xd0\xb0\xd0\xb1\xd0\xbe\xd1\x82\xd0\xb5 \xd0\xb8\xd0\xbb\xd0\xb8 \xd0\xb2\xd0\xbe\xd0\xbf\xd1\x80\xd0\xbe\xd1\x81\xd1\x83 \xd0\xbd\xd0\xb0 \xd1\x81\xd0\xbe\xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd1\x81\xd1\x82\xd0\xb2\xd0\xb8\xd0\xb5\n\n\xd0\xa2\xd0\xbe\xd1\x87\xd0\xba\xd0\xb8 \xd0\xbf\xd0\xb5\xd1\x80\xd0\xb5\xd0\xb4 \xd0\xbf\xd1\x80\xd0\xb0\xd0\xb2\xd0\xb8\xd0\xbb\xd1\x8c\xd0\xbd\xd1\x8b\xd0\xbc\xd0\xb8 \xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd0\xb0\xd0\xbc\xd0\xb8 \xd0\xb2\xd1\x8b\xd0\xb3\xd0\xbb\xd1\x8f\xd0\xb4\xd1\x8f\xd1\x82 \xd0\xb2\xd0\xbe\xd1\x82 \xd1\x82\xd0\xb0\xd0\xba (\xd0\xbd\xd0\xb0 \xd0\xba\xd0\xb0\xd1\x80\xd1\x82\xd0\xb8\xd0\xbd\xd0\xba\xd0\xb5 \xd0\xbf\xd1\x80\xd0\xb0\xd0\xb2\xd0\xb8\xd0\xbb\xd1\x8c\xd0\xbd\xd1\x8b\xd0\xb9 \xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82 - \xd0\xbf\xd0\xbe\xd1\x81\xd0\xbb\xd0\xb5\xd0\xb4\xd0\xbd\xd0\xb8\xd0\xb9):\n\n\n\n*\xd0\xa2\xd0\xbe\xd1\x87\xd0\xba\xd0\xb8 \xd0\xbf\xd0\xb5\xd1\x80\xd0\xb5\xd0\xb4 \xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd0\xb0\xd0\xbc\xd0\xb8 \xd0\xbf\xd0\xbe\xd1\x8f\xd0\xb2\xd1\x8f\xd1\x82\xd1\x81\xd1\x8f \xd1\x82\xd0\xbe\xd0\xbb\xd1\x8c\xd0\xba\xd0\xbe \xd0\xb2 \xd1\x82\xd0\xb5\xd1\x85 \xd0\xb2\xd0\xbe\xd0\xbf\xd1\x80\xd0\xbe\xd1\x81\xd0\xb0\xd1\x85 \xd1\x82\xd0\xb5\xd1\x81\xd1\x82\xd0\xb0, \xd0\xba\xd0\xbe\xd1\x82\xd0\xbe\xd1\x80\xd1\x8b\xd0\xb5 \xd1\x83\xd0\xb6\xd0\xb5 \xd0\xb1\xd1\x8b\xd0\xbb\xd0\xb8 \xd0\xb2\xd0\xb0\xd0\xbc\xd0\xb8 \xd0\xbf\xd1\x80\xd0\xbe\xd1\x81\xd0\xbc\xd0\xbe\xd1\x82\xd1\x80\xd0\xb5\xd0\xbd\xd1\x8b (\xd1\x81\xd0\xb2\xd1\x8f\xd0\xb7\xd0\xb0\xd0\xbd\xd0\xbe \xd1\x8d\xd1\x82\xd0\xbe \xd1\x81 \xd1\x82\xd0\xb5\xd0\xbc, \xd1\x87\xd1\x82\xd0\xbe \xd0\xb2\xd0\xbe\xd0\xbf\xd1\x80\xd0\xbe\xd1\x81\xd1\x8b \xd0\xb7\xd0\xb0\xd0\xb3\xd1\x80\xd1\x83\xd0\xb6\xd0\xb0\xd1\x8e\xd1\x82\xd1\x81\xd1\x8f \xd0\xbd\xd0\xb0 \xd1\x81\xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x86\xd1\x83 \xd1\x82\xd0\xbe\xd0\xbb\xd1\x8c\xd0\xba\xd0\xbe \xd0\xbf\xd0\xbe \xd0\xbc\xd0\xb5\xd1\x80\xd0\xb5 \xd0\xb8\xd1\x85 \xd0\xbe\xd1\x82\xd0\xba\xd1\x80\xd1\x8b\xd1\x82\xd0\xb8\xd1\x8f).\n\n## \xd0\x94\xd0\xbe\xd0\xb1\xd0\xb0\xd0\xb2\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5 \xd1\x82\xd0\xb5\xd1\x81\xd1\x82\xd0\xbe\xd0\xb2\n\n\xd0\x92\xd0\xbe\xd0\xbf\xd1\x80\xd0\xbe\xd1\x81\xd1\x8b \xd0\xb8 \xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd1\x8b \xd0\xba \xd1\x82\xd0\xb5\xd1\x81\xd1\x82\xd1\x83 \xd0\xbf\xd1\x80\xd0\xbe\xd0\xb3\xd1\x80\xd0\xb0\xd0\xbc\xd0\xbc\xd0\xb0 \xd0\xb7\xd0\xb0\xd0\xb3\xd1\x80\xd1\x83\xd0\xb6\xd0\xb0\xd0\xb5\xd1\x82 \xd0\xb8\xd0\xb7 \xd1\x80\xd0\xb5\xd0\xbf\xd0\xbe\xd0\xb7\xd0\xb8\xd1\x82\xd0\xbe\xd1\x80\xd0\xb8\xd1\x8f \xd0\xb8\xd0\xb7 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb\xd0\xb0 . \xd0\xa1\xd0\xbe\xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd1\x81\xd1\x82\xd0\xb2\xd0\xb5\xd0\xbd\xd0\xbd\xd0\xbe, \xd0\xbf\xd0\xb5\xd1\x80\xd0\xb5\xd0\xb4 \xd0\xba\xd0\xb0\xd0\xb6\xd0\xb4\xd1\x8b\xd0\xbc \xd0\xb7\xd0\xb0\xd0\xbd\xd1\x8f\xd1\x82\xd0\xb8\xd0\xb5\xd0\xbc \xd0\xb2 \xd1\x8d\xd1\x82\xd0\xbe\xd0\xbc \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb\xd0\xb5 \xd0\xb4\xd0\xbe\xd0\xbb\xd0\xb6\xd0\xbd\xd1\x8b \xd0\xbd\xd0\xb0\xd1\x85\xd0\xbe\xd0\xb4\xd0\xb8\xd1\x82\xd1\x8c\xd1\x81\xd1\x8f \xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd1\x8b \xd0\xbd\xd0\xb0 \xd1\x82\xd0\xb5\xd0\xba\xd1\x83\xd1\x89\xd0\xb8\xd0\xb9 \xd1\x82\xd0\xb5\xd1\x81\xd1\x82. \xd0\xa1\xd1\x82\xd1\x80\xd1\x83\xd0\xba\xd1\x82\xd1\x83\xd1\x80\xd0\xb0 \xd0\xb5\xd0\xb3\xd0\xbe \xd1\x81\xd0\xbb\xd0\xb5\xd0\xb4\xd1\x83\xd1\x8e\xd1\x89\xd0\xb0\xd1\x8f.\n\n\xd0\x9f\xd0\xbe\xd1\x81\xd0\xbb\xd0\xb5 \xd1\x81\xd1\x82\xd1\x80\xd0\xbe\xd0\xba\xd0\xb8  \xd1\x83\xd0\xba\xd0\xb0\xd0\xb7\xd1\x8b\xd0\xb2\xd0\xb0\xd1\x8e\xd1\x82\xd1\x81\xd1\x8f \xd0\xb2\xd1\x81\xd0\xb5 \xd0\xb2\xd0\xbe\xd0\xbf\xd1\x80\xd0\xbe\xd1\x81\xd1\x8b \xd1\x81 \xd0\xb2\xd1\x8b\xd0\xb1\xd0\xbe\xd1\x80\xd0\xbe\xd0\xbc \xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd0\xb0 \xd0\xb8 \xd0\xb2\xd0\xbe\xd0\xbf\xd1\x80\xd0\xbe\xd1\x81\xd1\x8b \xd1\x81 \xd0\xbf\xd1\x80\xd0\xbe\xd0\xbf\xd1\x83\xd1\x81\xd0\xba\xd0\xbe\xd0\xbc. \xd0\xa4\xd0\xbe\xd1\x80\xd0\xbc\xd0\xb0\xd1\x82 \xd1\x82\xd0\xb0\xd0\xba\xd0\xbe\xd0\xb9:\n\n\xd0\x9f\xd0\xb5\xd1\x80\xd0\xb2\xd1\x8b\xd0\xb9 \xd0\xb0\xd1\x80\xd0\xb3\xd1\x83\xd0\xbc\xd0\xb5\xd0\xbd\xd1\x82 - \xd0\xb2\xd0\xbe\xd0\xbf\xd1\x80\xd0\xbe\xd1\x81, \xd0\xb7\xd0\xb0 \xd0\xbd\xd0\xb8\xd0\xbc - \xd0\xbf\xd1\x80\xd0\xb0\xd0\xb2\xd0\xb8\xd0\xbb\xd1\x8c\xd0\xbd\xd1\x8b\xd0\xb5 \xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd1\x8b.\n\n\xd0\x9f\xd0\xbe\xd1\x81\xd0\xbb\xd0\xb5 \xd1\x81\xd1\x82\xd1\x80\xd0\xbe\xd0\xba\xd0\xb8  \xd1\x83\xd0\xba\xd0\xb0\xd0\xb7\xd1\x8b\xd0\xb2\xd0\xb0\xd1\x8e\xd1\x82\xd1\x81\xd1\x8f \xd0\xbf\xd0\xbe\xd0\xb4\xd1\x81\xd0\xba\xd0\xb0\xd0\xb7\xd0\xba\xd0\xb8 \xd0\xba \xd0\xb2\xd0\xbe\xd0\xbf\xd1\x80\xd0\xbe\xd1\x81\xd0\xb0\xd0\xbc \xd0\xbd\xd0\xb0 \xd1\x81\xd0\xbe\xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd1\x81\xd1\x82\xd0\xb2\xd0\xb8\xd0\xb5\xd0\xb5 (grad-and-drop) \xd0\xb8 \xd0\xba \xd0\xbb\xd0\xb0\xd0\xb1\xd0\xbe\xd1\x80\xd0\xb0\xd1\x82\xd0\xbe\xd1\x80\xd0\xbd\xd0\xbe\xd0\xb9 \xd1\x80\xd0\xb0\xd0\xb1\xd0\xbe\xd1\x82\xd0\xb5 \xd0\xb2 \xd1\x84\xd0\xbe\xd1\x80\xd0\xbc\xd0\xb0\xd1\x82\xd0\xb5:\n\n\xd0\x9d\xd0\xb0\xd0\xbf\xd1\x80\xd0\xb8\xd0\xbc\xd0\xb5\xd1\x80,\n\n\n\xd0\x92\xd0\xbe\xd0\xbf\xd1\x80\xd0\xbe\xd1\x81\xd1\x8b \xd0\xb8 \xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd1\x8b \xd0\xba \xd1\x82\xd0\xb5\xd1\x81\xd1\x82\xd0\xb0\xd0\xbc \xd0\xbc\xd0\xbe\xd0\xb6\xd0\xbd\xd0\xbe \xd0\xbd\xd0\xb0\xd0\xb9\xd1\x82\xd0\xb8 \xd0\xbd\xd0\xb0 \xd1\x81\xd0\xb0\xd0\xb9\xd1\x82\xd0\xb5 ccna5.net.\n\n## \xd0\x92\xd0\xbd\xd0\xb5\xd1\x81\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5 \xd0\xb8\xd0\xb7\xd0\xbc\xd0\xb5\xd0\xbd\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb9 \xd0\xb2 \xd1\x80\xd0\xb5\xd0\xbf\xd0\xbe\xd0\xb7\xd0\xb8\xd1\x82\xd0\xbe\xd1\x80\xd0\xb8\xd0\xb9\n\n\xd0\x9f\xd0\xb5\xd1\x80\xd0\xb5\xd0\xb4 \xd0\xba\xd0\xb0\xd0\xb6\xd0\xb4\xd1\x8b\xd0\xbc \xd0\xb7\xd0\xb0\xd0\xbd\xd1\x8f\xd1\x82\xd0\xb8\xd0\xb5\xd0\xbc \xd0\xb2 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb\xd0\xb5  \xd1\x80\xd0\xb5\xd0\xbf\xd0\xbe\xd0\xb7\xd0\xb8\xd1\x82\xd0\xbe\xd1\x80\xd0\xb8\xd1\x8f \xd0\xb4\xd0\xbe\xd0\xbb\xd0\xb6\xd0\xbd\xd1\x8b \xd0\xb1\xd1\x8b\xd1\x82\xd1\x8c \xd0\xb2\xd0\xbe\xd0\xbf\xd1\x80\xd0\xbe\xd1\x81\xd1\x8b \xd0\xb8 \xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd1\x8b \xd0\xbd\xd0\xb0 \xd1\x82\xd0\xb5\xd0\xba\xd1\x83\xd1\x89\xd0\xb8\xd0\xb9 \xd1\x82\xd0\xb5\xd1\x81\xd1\x82. \xd0\x95\xd1\x81\xd0\xbb\xd0\xb8 \xd0\xb2\xd1\x8b \xd1\x85\xd0\xbe\xd1\x82\xd0\xb8\xd1\x82\xd0\xb5 \xd0\xb8\xd0\xb7\xd0\xbc\xd0\xb5\xd0\xbd\xd1\x8f\xd1\x82\xd1\x8c \xd1\x8d\xd1\x82\xd0\xbe\xd1\x82 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb, \xd0\xbd\xd0\xb0\xd0\xbf\xd0\xb8\xd1\x88\xd0\xb8\xd1\x82\xd0\xb5 , \xd0\xbf\xd1\x80\xd0\xb8\xd0\xba\xd1\x80\xd0\xb5\xd0\xbf\xd0\xb8\xd0\xb2 \xd1\x81\xd1\x81\xd1\x8b\xd0\xbb\xd0\xba\xd1\x83 \xd0\xbd\xd0\xb0 \xd0\xb2\xd0\xb0\xd1\x88 \xd0\xb0\xd0\xba\xd0\xba\xd0\xb0\xd1\x83\xd0\xbd\xd1\x82 GitHub, \xd0\xb8 \xd1\x8f \xd0\xb4\xd0\xbe\xd0\xb1\xd0\xb0\xd0\xb2\xd0\xbb\xd1\x8e \xd0\xb2\xd0\xb0\xd1\x81 \xd0\xb2 collaborators list.\n\n#### \xd0\x9f\xd1\x80\xd0\xb0\xd0\xb2\xd0\xb8\xd0\xbb\xd0\xb0 \xd1\x80\xd0\xb5\xd0\xb4\xd0\xb0\xd0\xba\xd1\x82\xd0\xb8\xd1\x80\xd0\xbe\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x8f \xd1\x80\xd0\xb5\xd0\xbf\xd0\xbe\xd0\xb7\xd0\xb8\xd1\x82\xd0\xbe\xd1\x80\xd0\xb8\xd1\x8f:\n\n1. \xd0\x98\xd0\xb7\xd0\xbc\xd0\xb5\xd0\xbd\xd1\x8f\xd1\x82\xd1\x8c \xd1\x80\xd0\xb0\xd0\xb7\xd1\x80\xd0\xb5\xd1\x88\xd0\xb0\xd0\xb5\xd1\x82\xd1\x81\xd1\x8f \xd1\x82\xd0\xbe\xd0\xbb\xd1\x8c\xd0\xba\xd0\xbe \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb\xd1\x8b  \xd0\xb8 .\n2. \xd0\x9f\xd0\xb5\xd1\x80\xd0\xb5\xd0\xb4 \xd1\x81\xd0\xbe\xd1\x85\xd1\x80\xd0\xb0\xd0\xbd\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5\xd0\xbc \xd0\xb8\xd0\xb7\xd0\xbc\xd0\xb5\xd0\xbd\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb9 \xd0\xb2  \xd1\x83\xd0\xb1\xd0\xb5\xd0\xb4\xd0\xb8\xd1\x82\xd0\xb5\xd1\x81\xd1\x8c \xd0\xb2 \xd0\xb8\xd1\x85 \xd0\xb2\xd0\xb0\xd0\xbb\xd0\xb8\xd0\xb4\xd0\xbd\xd0\xbe\xd1\x81\xd1\x82\xd0\xb8.\n3. \xd0\x92\xd0\xb0\xd1\x88\xd0\xb8 \xd0\xb8\xd0\xb7\xd0\xbc\xd0\xb5\xd0\xbd\xd0\xb5\xd0\xbd\xd0\xb8\xd1\x8f \xd0\xb2 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb\xd0\xb5  \xd0\xbd\xd0\xb5 \xd0\xb4\xd0\xbe\xd0\xbb\xd0\xb6\xd0\xbd\xd1\x8b \xd0\xbc\xd0\xb5\xd1\x88\xd0\xb0\xd1\x82\xd1\x8c \xd0\xb4\xd1\x80\xd1\x83\xd0\xb3\xd0\xb8\xd0\xbc \xd0\xb3\xd1\x80\xd1\x83\xd0\xbf\xd0\xbf\xd0\xb0\xd0\xbc: \xd0\xb5\xd1\x81\xd0\xbb\xd0\xb8 \xd0\xb2\xd1\x8b \xd0\xbf\xd0\xb8\xd1\x88\xd0\xb5\xd1\x82\xd0\xb5 \xd1\x82\xd0\xb5\xd1\x81\xd1\x82 , \xd0\xb0 \xd0\xb4\xd1\x80\xd1\x83\xd0\xb3\xd0\xb0\xd1\x8f \xd0\xb3\xd1\x80\xd1\x83\xd0\xbf\xd0\xbf\xd0\xb0 \xd0\xbf\xd0\xb8\xd1\x88\xd0\xb5\xd1\x82 \xd1\x82\xd0\xb5\xd1\x81\xd1\x82 , \xd1\x82\xd0\xbe \xd0\xbd\xd0\xb5 \xd1\x83\xd0\xb4\xd0\xb0\xd0\xbb\xd1\x8f\xd0\xb9\xd1\x82\xd0\xb5 \xd1\x82\xd0\xb5\xd1\x81\xd1\x82 .\n4. \xd0\x96\xd0\xb5\xd0\xbb\xd0\xb0\xd1\x82\xd0\xb5\xd0\xbb\xd1\x8c\xd0\xbd\xd0\xbe, \xd1\x87\xd1\x82\xd0\xbe\xd0\xb1\xd1\x8b \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb  \xd0\xbd\xd0\xb5 \xd1\x81\xd0\xbe\xd0\xb4\xd0\xb5\xd1\x80\xd0\xb6\xd0\xb0\xd0\xbb \xd0\xbb\xd0\xb8\xd1\x88\xd0\xbd\xd0\xb8\xd1\x85 \xd1\x82\xd0\xb5\xd1\x81\xd1\x82\xd0\xbe\xd0\xb2.\n5. \xd0\x9f\xd1\x80\xd0\xb8 \xd1\x83\xd0\xb4\xd0\xb0\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb8 \xd0\xbd\xd0\xb0\xd0\xbf\xd0\xb8\xd1\x81\xd0\xb0\xd0\xbd\xd0\xbd\xd0\xbe\xd0\xb3\xd0\xbe \xd1\x82\xd0\xb5\xd1\x81\xd1\x82\xd0\xb0 \xd0\xb8\xd0\xb7 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb\xd0\xb0  \xd0\xb4\xd0\xbe\xd0\xb1\xd0\xb0\xd0\xb2\xd0\xbb\xd1\x8f\xd0\xb9\xd1\x82\xd0\xb5 \xd1\x8d\xd1\x82\xd0\xbe\xd1\x82 \xd1\x82\xd0\xb5\xd1\x81\xd1\x82 \xd0\xb2 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb  - \xd0\xb7\xd0\xb0\xd0\xb1\xd0\xbe\xd1\x82\xd1\x8c\xd1\x82\xd0\xb5\xd1\x81\xd1\x8c \xd0\xbe \xd1\x81\xd0\xbb\xd0\xb5\xd0\xb4\xd1\x83\xd1\x8e\xd1\x89\xd0\xb8\xd1\x85 \xd0\xbf\xd0\xbe\xd0\xba\xd0\xbe\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xb8\xd1\x8f\xd1\x85 :).\n6. \xd0\x9f\xd1\x80\xd0\xb8\xd0\xbc\xd0\xb5\xd1\x80\xd1\x8b commit message: , .\n\n\xd0\x95\xd1\x81\xd0\xbb\xd0\xb8 \xd1\x87\xd1\x82\xd0\xbe-\xd1\x82\xd0\xbe \xd0\xbd\xd0\xb5 \xd1\x80\xd0\xb0\xd0\xb1\xd0\xbe\xd1\x82\xd0\xb0\xd0\xb5 \xd0\xb8\xd0\xbb\xd0\xb8 \xd1\x83 \xd0\xb2\xd0\xb0\xd1\x81 \xd0\xb5\xd1\x81\xd1\x82\xd1\x8c \xd0\xbf\xd1\x80\xd0\xb5\xd0\xb4\xd0\xbb\xd0\xbe\xd0\xb6\xd0\xb5\xd0\xbd\xd0\xb8\xd1\x8f \xd0\xbf\xd0\xbe \xd1\x83\xd0\xbb\xd1\x83\xd1\x87\xd1\x88\xd0\xb5\xd0\xbd\xd0\xb8\xd1\x8e \xd0\xbf\xd1\x80\xd0\xbe\xd0\xb3\xd1\x80\xd0\xb0\xd0\xbc\xd0\xbc\xd1\x8b - \xd0\xb4\xd0\xbe\xd0\xb1\xd1\x80\xd0\xbe \xd0\xbf\xd0\xbe\xd0\xb6\xd0\xb0\xd0\xbb\xd0\xbe\xd0\xb2\xd0\xb0\xd1\x82\xd1\x8c \xd0\xbd\xd0\xb0 \xd0\xb2\xd0\xba\xd0\xbb\xd0\xb0\xd0\xb4\xd0\xba\xd1\x83 Issues.\n\n\xd0\x95\xd1\x81\xd0\xbb\xd0\xb8 \xd0\xb2\xd1\x8b \xd0\xb7\xd0\xbd\xd0\xb0\xd0\xb5\xd1\x82\xd0\xb5 JavaScript, \xd0\xb8 \xd0\xb2\xd0\xb0\xd0\xbc \xd1\x81\xd0\xb0\xd0\xbc\xd0\xb8\xd0\xbc \xd1\x85\xd0\xbe\xd1\x87\xd0\xb5\xd1\x82\xd1\x81\xd1\x8f \xd1\x87\xd1\x82\xd0\xbe-\xd1\x82\xd0\xbe \xd0\xb8\xd0\xb7\xd0\xbc\xd0\xb5\xd0\xbd\xd0\xb8\xd1\x82\xd1\x8c \xd0\xb2 \xd0\xba\xd0\xbe\xd0\xb4\xd0\xb5 (, ), \xd1\x82\xd0\xbe \xd0\xb4\xd0\xb5\xd0\xbb\xd0\xb0\xd0\xb9\xd1\x82\xd0\xb5 fork \xd1\x80\xd0\xb5\xd0\xbf\xd0\xbe\xd0\xb7\xd0\xb8\xd1\x82\xd0\xbe\xd1\x80\xd0\xb8\xd1\x8f, \xd1\x80\xd0\xb5\xd0\xb4\xd0\xb0\xd0\xba\xd1\x82\xd0\xb8\xd1\x80\xd1\x83\xd0\xb9\xd1\x82\xd0\xb5 \xd0\xb8 \xd1\x81\xd0\xbe\xd0\xb7\xd0\xb4\xd0\xb0\xd0\xb2\xd0\xb0\xd0\xb9\xd1\x82\xd0\xb5 pull request.\n\n## FAQ\n\n1. \n'"
https://github.com/tksaha/con-s2v,Con-S2V: A Generic Framework for Incorporating Extra-Sentential Context into Sen2Vec,"b'# CON-S2V: A Generic Framework for Incorporating Extra-Sentential Context into Sen2Vec\nExtra-Sentential Context into Sen2Vec\nLatent Representation for the sentences.\n\n# Citation\nIf you are using the code, please consider citing the following papers:\n\n\n\n\n## Requirements\n* \n* \n\n## Python Environment setup and Update\n\n1. Copy the sen2vec_environment.yml file into anaconda/envs folder\n2. Get into anaconda/envs folder.\n3. Run the following command:\n\n\n\nNow, you have successfully installed sen2vec environment and now you can activate the environment using the following command. \n\n\n\n\nIf you have added more packages into the environment, you \ncan update the .yml file using the following command: \n\n\n\n## ROUGE Environment setup\nPlease go to the ROUGE directory and run the following command to check whether \nthe provided perl script will work or not:\n\n\nIf it shows the options for running the script, then you are fine. However, if it shows \nyou havent have XML::DOM installed then please type following command to install \nit: \n\n\nHere, CPAN stands for Comprehensive Perl Archive Network. \n\n## Database Creation and update \n\nIf you have already installed , then \nyou can create a table with the following command for the newsgroup [news] dataset: \n\n\n\nAfter creating the database, use pg_restore to create the schemas which is agnostic to \nthe dataset: \n\n\n\nor \n\n\nWe are assuming that either you are using  as the username or any other username\nwhich already has all the required privileges. To change the password for the  user,\nuse the following command-\n\n\n\nIf you have made any changes to the database, you can updated the dump \nfile using following command (schema only): \n\n[You may need to set peer authentication: ]\n\n\n\nTo dump the data of a particular table from the database: \n\n\n## Setting Environment Variables\n\nSet the dataset folder path and the connection string in the environment.sh file properly and \nthen run the following command-\n\n\n\n## Creating Executable for Word2Vec (Mikolovs Implementation)\nPlease go to the word2vec code directory inside the project and \ntype the following command for creating executable:\n\n\n\n## Installation of Theano for Skip-Thought\n\n\n## Installation of Keras (Sequential API)\n\n\nTo change the backend to  please change the default configuration \nin ~/.keras/keras.json\n\n\n\n## Downloading the {C-PHRASE} vectors:\nPlease download the C-Phrase vectors from [C-Phrase link] (http://clic.cimec.unitn.it/composes/cphrase-vectors.html) and \njoin the files using following commands:\n\n\n\n## Downloading GLove Pretrained Vectors for SDAE:\nPlease download the vectors from [Glove link] (http://nlp.stanford.edu/projects/glove/) and then append \na line in the first line using following command:\n\n\n\n\n## Running the Project \nRun sen2vec with -h argument to see all possible options:\n\n\n\nFor example, you can run for the news dataset using the following command-\n\n\n\n'"
https://github.com/fnielsen/afinn,AFINN sentiment analysis in Python,"b'afinn\n=====\n\nAFINN sentiment analysis in Python: Wordlist-based approach for sentiment analysis.\n\nHow to install\n--------\n    >>> git clone https://github.com/fnielsen/afinn\n    >>> cd afinn\n    >>> python setup.py install\n\n\n\nExamples\n--------\n\n    >>> from afinn import Afinn\n    >>> afinn = Afinn()\n    >>> afinn.score(This is utterly excellent!)\n    3.0\n\nIn Danish:\n\n    >>> afinn = Afinn(language=da)\n    >>> afinn.score(Hvis ikke det er det mest afskyelige flueknepperi...)\n    -6.0\n\nIn Finnish:\n\n\t>>> afinn = Afinn(language=fi)\n\t>>> afinn.score(Siell\xc3\xa4 on uusi hyv\xc3\xa4 juttu, katsokaa ja kuunnelkaa ihmeess\xc3\xa4.)\n\t3.0\n\nIn Swedish:\n\n\t>>> afinn = Afinn(language=sv)\n\t>>> afinn.score(det \xc3\xa4r inte bra)\n\t-2.0\n\nIn Turkish:\n\n\t>>> afinn = Afinn(language=tr)\n\t>>> from six import u\n\t>>> afinn.score(u(iyi deu011Fil))\n\t-2.0\n\n\t>>> afinn = Afinn(language=tr)\n\t>>> afinn.score(iyi de\xc4\x9fil)\n\t-2.0\n\nWith emoticons:\n\n    >>> afinn = Afinn(emoticons=True)\n    >>> afinn.score(I saw that yesterday :))\n    2.0\n\nWith multiple sentences (here with data from an Austen novel available in Gutenberg):\n\n    >>> from afinn import Afinn\n    >>> from nltk.corpus import gutenberg\n    >>> import textwrap\n    >>> afinn = Afinn()\n    >>> sentences = ("" "".join(wordlist) for wordlist in gutenberg.sents(austen-sense.txt))\n    >>> scored_sentences = ((afinn.score(sent), sent) for sent in sentences)\n    >>> sorted_sentences = sorted(scored_sentences)\n    >>> print(""n"".join(textwrap.wrap(sorted_sentences[0][1], 70)))\n    To attach myself to your sister , therefore , was not a thing to be\n    thought of ;-- and with a meanness , selfishness , cruelty -- which no\n    indignant , no contemptuous look , even of yours , Miss Dashwood , can\n    ever reprobate too much -- I was acting in this manner , trying to\n    engage her regard , without a thought of returning it .-- But one\n    thing may be said for me : even in that horrid state of selfish vanity\n    , I did not know the extent of the injury I meditated , because I did\n    not THEN know what it was to love .\n\nCitation\n--------\nIf you as a scientist use the wordlist or the code please cite this one:\n\n* Finn \xc3\x85rup Nielsen, ""A new ANEW: evaluation of a word list for sentiment analysis in microblogs"", Proceedings of the ESWC2011 Workshop on Making Sense of Microposts: Big things come in small packages. Volume 718 in CEUR Workshop Proceedings: 93-98. 2011 May. Matthew Rowe, Milan Stankovic, Aba-Sah Dadzie, Mariann Hardey (editors)\n\nPaper with supplement: http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/6006/pdf/imm6006.pdf\n\nSee also\n--------\n* http://neuro.compute.dtu.dk/wiki/AFINN - Brede Wiki entry on AFINN with pointers to many scientific papers\n* https://github.com/darenr/afinn - Sentiment analysis in Javascript with AFINN word list\n* https://github.com/prograils/afinn - Sentiment analysis in Ruby with AFINN word list\n\n\nTravis et al.\n-------------\n\n.. image:: https://travis-ci.com/fnielsen/afinn.svg?branch=master\n    :target: https://travis-ci.com/fnielsen/afinn\n\n.. image:: https://coveralls.io/repos/fnielsen/afinn/badge.svg?branch=master :target: https://coveralls.io/github/fnielsen/afinn?branch=master\n\n.. image:: https://img.shields.io/pypi/dm/afinn.svg?style=flat\n   :target: https://pypi.python.org/pypi/afinn\n   :alt: Downloads\n\n.. image:: https://www.openhub.net/p/afinn/widgets/project_thin_badge.gif\n   :target: https://www.openhub.net/p/afinn\n   :alt: Open Hub\n'"
https://github.com/vck/geophy,geophysics homework,b'# geophy\ngeophysics homework\n'
https://github.com/blairhudson/learningml,Learning ML ~ A practical guide to understanding and applying machine learning algorithms in the quest to become a 🦄 by Blair Hudson,"b'\n\n\n\n# Learning ML\n\nA practical guide to understanding and applying machine learning algorithms in the quest to become a \xf0\x9f\xa6\x84.\n\nby [<marko.inline.RawText object at 0x000002CBAF3B1948>]\n\nThis book is a work in progress. Follow \n\n\n## Table of Contents\n---\n\n### \n* \n* \n\n### \n* \n* \n* \n* \n* Logistic Regression\n* Support Vector Machines\n* Elastic Net\n* Stochastic Gradient Descent\n* RuleFit\n* Ensembles\n* Neural Networks\n\n### \n* Linear Regression\n* Support Vector Regression\n\n### \n* k-means\n* t-SNE\n* Apriori\n* PCA\n* LDA\n\n### \n\n* Keras and TensorFlow\n* Deep Neural Networks: Classification and Regression\n* DNN Problems and Architectures\n\n### \n\n* Spark MLLib\n\n### \n* \n* \n* \n\n---\n\n / This work is licensed under .\n\n\n  (function(i,s,o,g,r,a,m){i[GoogleAnalyticsObject]=r;i[r]=i[r]||function(){\n  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\n  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\n  })(window,document,script,https://www.google-analytics.com/analytics.js,ga);\n\n  ga(create, UA-96446446-1, auto);\n  ga(send, pageview);\n\n\n'"
https://github.com/johnnygreco/hugs-graveyard,old hugs project repository,b'## A graveyard for HUGs \n'
https://github.com/jam1245/Visualizations,Collection of various visualizations,b'# Visualizations\nCollection of various visualizations\n'
https://github.com/ericwayman/nflData,Some models and analysis to investigate efficiency of the NFL gambling market,"b'# nflData\n#Script to implement a Logistic regression model of the form\n#Y^_i = b_0+ b_1HWP_i + b_2AWP_i + b_3HL4_i + b_4AL4_i b_5FAV_i + e_i\n#i is the index of the game number\n#Y^_i is our prediction for Y_i the indicator of the home team beating the spread in game i\n#{b_0,b_1,b_2,b_3,b_4,b_5} the parameters to train\n#HWP_i: overall win percentage (relative to the spread) of home team\n#AWP_i: overall win percentage (relative to the spread) of away team\n#HL4_i: number of times home team has beaten the spread in the last 4 games played (this season)\n#HL4_i: number of times away team has beaten the spread in the last 4 games played (this season)\n#FAV_i: indicator the home team is the favorite \n'"
https://github.com/rgdn-info-community/piramidy,База пирамид,"b'\xd0\x94\xd0\xb0\xd0\xbd\xd0\xbd\xd1\x8b\xd0\xb9 \xd1\x80\xd0\xb5\xd0\xbf\xd0\xbe\xd0\xb7\xd0\xb8\xd1\x82\xd0\xbe\xd1\x80\xd0\xb8\xd0\xb9 \xd1\x81\xd0\xbe\xd0\xb7\xd0\xb4\xd0\xb0\xd0\xbd \xd0\xb4\xd0\xbb\xd1\x8f \xd0\xb1\xd0\xbe\xd0\xbb\xd0\xb5\xd0\xb5 \xd1\x83\xd0\xb4\xd0\xbe\xd0\xb1\xd0\xbd\xd0\xbe\xd0\xb9 \xd1\x80\xd0\xb0\xd0\xb1\xd0\xbe\xd1\x82\xd1\x8b \xd0\xbf\xd0\xbe \xd1\x81\xd0\xbe\xd0\xb7\xd0\xb4\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x8e \xd0\xbe\xd0\xb1\xd1\x89\xd0\xb5\xd0\xb9 \xd0\xb1\xd0\xb0\xd0\xb7\xd1\x8b \xd0\xbf\xd0\xb8\xd1\x80\xd0\xb0\xd0\xbc\xd0\xb8\xd0\xb4 (\xd0\xb8 \xd0\xbf\xd0\xbe\xd0\xba\xd0\xb0 \xd0\xbd\xd0\xb0\xd1\x85\xd0\xbe\xd0\xb4\xd0\xb8\xd1\x82\xd1\x81\xd1\x8f \xd0\xb2 \xd0\xbf\xd1\x80\xd0\xbe\xd1\x86\xd0\xb5\xd1\x81\xd1\x81\xd0\xb5 \xd0\xb4\xd0\xbe\xd1\x80\xd0\xb0\xd0\xb1\xd0\xbe\xd1\x82\xd0\xba\xd0\xb8).\n\n# \xd0\x9d\xd0\xbe\xd0\xb2\xd0\xbe\xd1\x81\xd1\x82\xd0\xb8\n\n## \xd0\xa7\xd1\x82\xd0\xbe \xd1\x83\xd0\xb6\xd0\xb5 \xd0\xb4\xd0\xbe\xd0\xb1\xd0\xb0\xd0\xb2\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xbe\n- \xd0\x94\xd0\xbe\xd0\xb1\xd0\xb0\xd0\xb2\xd0\xbb\xd0\xb5\xd0\xbd\xd1\x8b \xd0\xbc\xd0\xb5\xd1\x82\xd0\xba\xd0\xb8 \xd0\xbe\xd1\x82 Lada\n- \xd0\x9f\xd0\xb8\xd1\x80\xd0\xb0\xd0\xbc\xd0\xb8\xd0\xb4\xd1\x8b \xd0\xb2 \xd0\x90\xd0\xbd\xd1\x82\xd0\xb0\xd1\x80\xd0\xba\xd1\x82\xd0\xb8\xd0\xb4\xd0\xb5\n  - 2 \xd1\x88\xd1\x82\xd1\x83\xd0\xba\xd0\xb8 \xd1\x80\xd1\x8f\xd0\xb4\xd0\xbe\xd0\xbc \xd1\x81 \n  \n    - \xd0\xbf\xd0\xbb\xd0\xbe\xd1\x85\xd0\xbe \xd0\xb2\xd0\xb8\xd0\xb4\xd0\xbd\xd1\x8b \xd0\xbd\xd0\xb0 \xd1\x81\xd0\xbd\xd0\xb8\xd0\xbc\xd0\xba\xd0\xbe\xd0\xb2 \xd1\x81 \xd0\xba\xd0\xbe\xd1\x81\xd0\xbc\xd0\xbe\xd1\x81\xd0\xb0, \xd0\xbd\xd0\xb0 \xd0\xbd\xd0\xb0 \xd1\x84\xd0\xbe\xd1\x82\xd0\xbe \xd1\x81\xd0\xbe \xd1\x81\xd1\x82\xd0\xb0\xd0\xbd\xd1\x86\xd0\xb8\xd0\xb8 \xd0\xb2\xd0\xb8\xd0\xb4\xd0\xbd\xd1\x8b \xd1\x85\xd0\xbe\xd1\x80\xd0\xbe\xd1\x88\xd0\xbe\n    - \n      - . \xd0\x9d\xd0\xb0\xd0\xb4\xd0\xbe \xd0\xb1\xd1\x8b \xd0\xbd\xd0\xb0\xd0\xb9\xd1\x82\xd0\xb8 \xd0\xb5\xd0\xb3\xd0\xbe \xd0\xbc\xd0\xb5\xd1\x82\xd0\xba\xd0\xb8...\n- \xd0\x9c\xd0\xb5\xd1\x82\xd0\xba\xd0\xb8 \xd0\xbe\xd1\x82 saha \xd0\xb8 austria\n- Geolines.ru\n  - \xd0\xb2\xd1\x81\xd0\xb5 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb\xd1\x8b \xd0\xbe\xd0\xb1\xd1\x8a\xd0\xb5\xd0\xb4\xd0\xb8\xd0\xbd\xd0\xb5\xd0\xbd\xd1\x8b \xd0\xb2 \xd0\xbe\xd0\xb4\xd0\xb8\xd0\xbd \xd0\xb8 \xd0\xbd\xd0\xb5\xd0\xbc\xd0\xbd\xd0\xbe\xd0\xb3\xd0\xbe \xd0\xbe\xd0\xb1\xd1\x80\xd0\xb0\xd0\xb1\xd0\xbe\xd1\x82\xd0\xb0\xd0\xbd\xd1\x8b (\xd1\x83\xd0\xb1\xd1\x80\xd0\xb0\xd0\xbd\xd1\x8b \xd0\xbb\xd0\xb8\xd1\x88\xd0\xbd\xd0\xb8\xd0\xb5 \xd0\xbf\xd0\xb0\xd0\xbf\xd0\xba\xd0\xb8, \xd0\xbd\xd0\xb0\xd0\xb7\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x8f \xd0\xbf\xd0\xb5\xd1\x80\xd0\xb5\xd0\xb2\xd0\xb5\xd0\xb4\xd0\xb5\xd0\xbd\xd1\x8b \xd0\xbc\xd0\xb5\xd1\x81\xd1\x82\xd0\xb0\xd0\xbc\xd0\xb8 \xd0\xb8 \xd1\x82.\xd0\xb4.)\n  - \xd1\x81\xd0\xbe\xd0\xb7\xd0\xb4\xd0\xb0\xd0\xbd \xd0\xbe\xd1\x82\xd0\xb4\xd0\xb5\xd0\xbb\xd1\x8c\xd0\xbd\xd1\x8b\xd0\xb9 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb \xd1\x81 \xd0\xbf\xd0\xb8\xd1\x80\xd0\xb0\xd0\xbc\xd0\xb8\xd0\xb4\xd0\xb0\xd0\xbc\xd0\xb8:  (\xd0\xbc\xd0\xbe\xd0\xb6\xd0\xbd\xd0\xbe \xd0\xba\xd0\xbb\xd0\xb8\xd0\xba\xd0\xbd\xd1\x83\xd1\x82\xd1\x8c \xe2\x80\x93 \xd1\x81\xd0\xbc. \xd0\xba\xd0\xb0\xd0\xba \xd1\x81\xd0\xba\xd0\xb0\xd1\x87\xd0\xb0\xd1\x82\xd1\x8c \xd0\xbd\xd0\xb8\xd0\xb6\xd0\xb5)\n  - \xd0\xbd\xd0\xb0\xd0\xb4\xd0\xbe \xd0\xb1\xd1\x83\xd0\xb4\xd0\xb5\xd1\x82 \xd0\xb5\xd1\x89\xd1\x91 \xd1\x80\xd0\xb0\xd0\xb7 \xd0\xbf\xd0\xb5\xd1\x80\xd0\xb5\xd1\x81\xd0\xbc\xd0\xbe\xd1\x82\xd1\x80\xd0\xb5\xd1\x82\xd1\x8c \xd0\xbf\xd0\xbe\xd0\xbb\xd0\xbd\xd1\x8b\xd0\xb9 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb \xe2\x80\x93 , \xd1\x82.\xd0\xba. \xd1\x8f \xd0\xb5\xd0\xb3\xd0\xbe \xd0\xbb\xd0\xb8\xd1\x88\xd1\x8c \xd0\xbf\xd0\xbe\xd0\xb2\xd0\xb5\xd1\x80\xd1\x85\xd0\xbd\xd0\xbe\xd1\x81\xd1\x82\xd0\xbd\xd0\xbe \xd0\xbf\xd1\x80\xd0\xbe\xd1\x81\xd0\xbc\xd0\xbe\xd1\x82\xd1\x80\xd0\xb5\xd0\xbb, \xd0\xbc\xd0\xbe\xd0\xb6\xd0\xb5\xd1\x82 \xd1\x87\xd1\x82\xd0\xbe-\xd1\x82\xd0\xbe \xd1\x83\xd0\xbf\xd1\x83\xd1\x81\xd1\x82\xd0\xb8\xd0\xbb, \xd0\xbd\xd0\xb0\xd0\xb4\xd0\xbe \xd0\xbf\xd0\xb5\xd1\x80\xd0\xb5\xd1\x81\xd0\xbc\xd0\xbe\xd1\x82\xd1\x80\xd0\xb5\xd1\x82\xd1\x8c \xd0\xb8 \xd1\x83\xd0\xb4\xd0\xb0\xd0\xbb\xd0\xb8\xd1\x82\xd1\x8c \xd0\xbd\xd0\xb5 \xd0\xbf\xd0\xb8\xd1\x80\xd0\xb0\xd0\xbc\xd0\xb8\xd0\xb4\xd1\x8b\n  - \xd1\x82\xd0\xb0\xd0\xba\xd0\xb6\xd0\xb5, \xd0\xbd\xd0\xb5 \xd0\xb7\xd0\xbd\xd0\xb0\xd1\x8e \xd1\x87\xd1\x82\xd0\xbe \xd0\xb4\xd0\xb5\xd0\xbb\xd0\xb0\xd1\x82\xd1\x8c \xd1\x81 \xd0\xbe\xd0\xb1\xd1\x8a\xd0\xb5\xd0\xba\xd1\x82\xd0\xb0\xd0\xbc\xd0\xb8 \xd0\x9c\xd0\xb0\xd0\xb9\xd1\x8f (\xd0\xb8 \xd0\xb4\xd1\x80\xd1\x83\xd0\xb3\xd0\xb8\xd1\x85 \xd0\xba\xd0\xbe\xd1\x80\xd0\xb5\xd0\xbd\xd0\xbd\xd1\x8b\xd1\x85 \xd0\xb6\xd0\xb8\xd1\x82\xd0\xb5\xd0\xbb\xd0\xb5\xd0\xb9 \xd0\xae\xd0\xb6\xd0\xbd\xd0\xbe\xd0\xb9/\xd0\xa1\xd0\xb5\xd0\xb2\xd0\xb5\xd1\x80\xd0\xbd\xd0\xbe\xd0\xb9 \xd0\x90\xd0\xbc\xd0\xb5\xd1\x80\xd0\xb8\xd0\xba\xd0\xb8) \xe2\x80\x93 \xd1\x82\xd0\xbe\xd0\xb6\xd0\xb5 \xd0\xbd\xd0\xb0\xd0\xb4\xd0\xbe \xd0\xb3\xd0\xbb\xd1\x8f\xd0\xbd\xd1\x83\xd1\x82\xd1\x8c \xd0\xb8 \xd0\xb4\xd0\xbe\xd0\xb1\xd0\xb0\xd0\xb2\xd0\xb8\xd1\x82\xd1\x8c \xd1\x82\xd0\xb5, \xd0\xba\xd0\xbe\xd1\x82\xd0\xbe\xd1\x80\xd1\x8b\xd0\xb5 \xd0\xbf\xd0\xbe\xd1\x85\xd0\xbe\xd0\xb6\xd0\xb8 \xd0\xbd\xd0\xb0 \xd0\xbf\xd0\xb8\xd1\x80\xd0\xb0\xd0\xbc\xd0\xb8\xd0\xb4\xd1\x8b\n\n## \xd0\x92 \xd0\xbf\xd0\xbb\xd0\xb0\xd0\xbd\xd0\xb0\xd1\x85\n- \xd0\xa1\xd1\x82\xd0\xb0\xd1\x82\xd1\x8c\xd1\x8f \xd0\xbf\xd1\x80\xd0\xbe \xd0\xbf\xd0\xb8\xd1\x80\xd0\xb0\xd0\xbc\xd0\xb8\xd0\xb4\xd1\x8b \xd0\x95\xd0\xb3\xd0\xb8\xd0\xbf\xd1\x82\xd0\xb0 \xe2\x80\x93 \xd0\xb2\xd1\x81\xd1\x91 \xd1\x81\xd0\xbe\xd1\x85\xd1\x80\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x82\xd1\x8c \xd0\xb2 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb\n  - \xd0\x98 \xd1\x83\xd0\xb4\xd0\xb0\xd0\xbb\xd0\xb8\xd1\x82\xd1\x8c \xd0\xbf\xd0\xbe\xd0\xb2\xd1\x82\xd0\xbe\xd1\x80\xd1\x8b\n- \xd0\x9f\xd1\x80\xd0\xbe\xd0\xb2\xd0\xb5\xd1\x80\xd0\xb8\xd1\x82\xd1\x8c \xd1\x81\xd0\xb0\xd0\xb9\xd1\x82\xd1\x8b\n  - \n  - \n    -  \xd0\xb8 \xd0\xb4\xd0\xb0\xd0\xbb\xd0\xb5\xd0\xb5 \xd0\xbf\xd0\xbe \xd1\x81\xd1\x81\xd1\x8b\xd0\xbb\xd0\xba\xd0\xb0\xd0\xbc\n  - http://sibved.livejournal.com/tag/\xd0\x9f\xd0\xb8\xd1\x80\xd0\xb0\xd0\xbc\xd0\xb8\xd0\xb4\xd1\x8b\n  - http://www.tart-aria.info/frantsiya-piramida-le-pertus-pod-sloem-gliny/\n  - http://www.tart-aria.info/piramidalnye-holmy-terrikony/\n  - https://ru.wikipedia.org/wiki/%D0%A1%D0%BF%D0%B8%D1%81%D0%BE%D0%BA_%D0%B5%D0%B3%D0%B8%D0%BF%D0%B5%D1%82%D1%81%D0%BA%D0%B8%D1%85_%D0%BF%D0%B8%D1%80%D0%B0%D0%BC%D0%B8%D0%B4\n  - http://xn--e1adcaacuhnujm.xn--p1ai/drevnie-piramidy-mira-sistema-ili-sovpadenie.html\n  - http://sirderya.blogspot.com/2013/12/ilim-cinde-de-olsa-gidip-alnz.html\n    - http://www.onemforum.com/dunya-sehirleri-resim/375784-beyaz-piramitlerin-google-earth-goruntusu.html\n  - http://www.kazan-newage.ru/sila/81-egi769petskie-pirami769d.html\n  - http://www.exomapia.ru/places/281713\n  - http://piramidainfo.net/map.php\n  - http://www.world-pyramids.com/\n  - http://hghltd.yandex.net/yandbtm?fmode=inject&url=http%3A%2F%2Fwww.mir.h19.ru%2Fxronologiya%2F16-WhiteGods%2Fimg%2Ffoto.php%3Fpgnum%3D49&tld=ru&lang=ru&la=1492602112&tm=1493745767&text=url%3Amir.h19.ru%2Fxronologiya%2F16-WhiteGods%2Fimg%2Ffoto.php%3Fpgnum%3D49%20%7C%20url%3Awww.mir.h19.ru%2Fxronologiya%2F16-WhiteGods%2Fimg%2Ffoto.php%3Fpgnum%3D49&l10n=ru&mime=html&type=touch&sign=629dd6c200aafda637770bd5c163dbca&keyno=0\n  - http://lah.flybb.ru/topic455.html\n  - http://lah.flybb.ru/topic1785.html\n  - https://www.google.com/maps/d/viewer?mid=19FJI3bP6wwTP5Hcvk5DKkFCbOyk\n  - http://lifeglobe.net/entry/1582\n  - http://turbina.ru/q/advice/69771/\n  - https://slavmaps.ru/spisok\n  - http://doublepyramid.org/zagadka-drevnih-piramid-na-kolskom-poluostrove/\n  - http://taboo.su/istoriya/zapretnaya-arkheologiya/74-kompleksy/233-teotiuakan-kompleks-piramid-v-meksike.html\n  - http://taboo.su/istoriya/zapretnaya-arkheologiya/74-kompleksy/193-ninchurt.html\n  - http://www.kosmm.ru/arl4.html\n\n# \xd0\xa7\xd0\xb0\xd0\x92\xd0\xbe? (\xd0\xa7\xd0\x90\xd1\x81\xd1\x82\xd0\xbe \xd0\xb7\xd0\xb0\xd0\xb4\xd0\xb0\xd0\xb2\xd0\xb0\xd0\xb5\xd0\xbc\xd1\x8b\xd0\xb5 \xd0\x92\xd0\x9e\xd0\xbf\xd1\x80\xd0\xbe\xd1\x81\xd1\x8b)\n## \xd0\x9a\xd0\xb0\xd0\xba \xd1\x81\xd0\xba\xd0\xb0\xd1\x87\xd0\xb0\xd1\x82\xd1\x8c...\n### \xd0\xb2\xd1\x81\xd0\xb5 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb\xd1\x8b?\n\xd0\xa7\xd0\xb5\xd1\x80\xd0\xb5\xd0\xb7 \xd0\xba\xd0\xbd\xd0\xbe\xd0\xbf\xd0\xba\xd1\x83 \n- \xd0\xb2\xd0\xb0\xd0\xbc \xd0\xbd\xd1\x83\xd0\xb6\xd0\xbd\xd0\xb0 \xd0\xbf\xd0\xb0\xd0\xbf\xd0\xba\xd0\xb0 , \xd0\xb3\xd0\xb4\xd0\xb5 \xd0\xbb\xd0\xb5\xd0\xb6\xd0\xb0\xd1\x82 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb\xd1\x8b \xd1\x81 \xd1\x80\xd0\xb0\xd1\x81\xd1\x88\xd0\xb8\xd1\x80\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5\xd0\xbc \n- \xd0\xbe\xd1\x81\xd1\x82\xd0\xb0\xd0\xbb\xd1\x8c\xd0\xbd\xd1\x8b\xd0\xb5 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb\xd1\x8b/\xd0\xbf\xd0\xb0\xd0\xbf\xd0\xba\xd0\xb8 \xd0\xbd\xd1\x83\xd0\xb6\xd0\xbd\xd1\x8b \xd0\xb4\xd0\xbb\xd1\x8f \xd1\x81\xd0\xb0\xd0\xb9\xd1\x82\xd0\xb0 \xd0\xb8 \xd0\xb4\xd0\xbb\xd1\x8f \xd1\x81\xd0\xb0\xd0\xbc\xd0\xbe\xd0\xb3\xd0\xbe \xd1\x80\xd0\xb5\xd0\xbf\xd0\xbe\xd0\xb7\xd0\xb8\xd1\x82\xd0\xbe\xd1\x80\xd0\xb8\xd1\x8f \xd0\xb3\xd0\xb8\xd1\x82\xd1\x85\xd0\xb0\xd0\xb1\xd0\xb0\n\n### \xd0\xbe\xd0\xb4\xd0\xb8\xd0\xbd \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb?\n- \xd0\xbf\xd0\xb5\xd1\x80\xd0\xb5\xd0\xb9\xd0\xb4\xd0\xb8\xd1\x82\xd0\xb5 \xd0\xb2 \xd0\xbf\xd0\xb0\xd0\xbf\xd0\xba\xd1\x83 \n- \xd0\xbd\xd0\xb0\xd0\xb9\xd0\xb4\xd0\xb8\xd1\x82\xd0\xb5 \xd0\xbd\xd1\x83\xd0\xb6\xd0\xbd\xd1\x8b\xd0\xb9 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb\n- \xd0\xbd\xd0\xb0 \xd1\x81\xd0\xbb\xd0\xb5\xd0\xb4\xd1\x83\xd1\x8e\xd1\x89\xd0\xb5\xd0\xb9 \xd1\x81\xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x86\xd0\xb5 \xd0\xbe\xd1\x82\xd0\xba\xd1\x80\xd0\xbe\xd0\xb9\xd1\x82\xd0\xb5 \xd0\xba\xd0\xbe\xd0\xbd\xd1\x82\xd0\xb5\xd0\xba\xd1\x81\xd1\x82\xd0\xbd\xd0\xbe\xd0\xb5 \xd0\xbc\xd0\xb5\xd0\xbd\xd1\x8e (\xd0\xbb\xd0\xb5\xd0\xb2\xd0\xb0\xd1\x8f \xd0\xba\xd0\xbd\xd0\xbe\xd0\xbf\xd0\xba\xd0\xb0 \xd0\xbc\xd1\x8b\xd1\x88\xd0\xb8/\xd1\x82\xd1\x80\xd1\x8d\xd0\xba\xd0\xbf\xd0\xb0\xd0\xb4\xd0\xb0/\xd0\xbf\xd1\x80.) \xd1\x83 \xd0\xba\xd0\xbd\xd0\xbe\xd0\xbf\xd0\xba\xd0\xb8  \xd0\xb8 \xd0\xb2\xd1\x8b\xd0\xb1\xd0\xb5\xd1\x80\xd0\xb5\xd1\x82\xd0\xb5 \xd0\xbf\xd1\x83\xd0\xbd\xd0\xba\xd1\x82  (\xd0\xbd\xd0\xb0\xd0\xb7\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xb8\xd0\xb5 \xd0\xb7\xd0\xb0\xd0\xb2\xd0\xb8\xd1\x81\xd0\xb8\xd1\x82 \xd0\xbe\xd1\x82 \xd0\xb1\xd1\x80\xd0\xb0\xd1\x83\xd0\xb7\xd0\xb5\xd1\x80\xd0\xb0/\xd1\x8f\xd0\xb7\xd1\x8b\xd0\xba\xd0\xb0)\n\n\n- \xd1\x83\xd0\xba\xd0\xb0\xd0\xb6\xd0\xb8\xd1\x82\xd0\xb5 \xd0\xba\xd1\x83\xd0\xb4\xd0\xb0 \xd1\x81\xd0\xbe\xd1\x85\xd1\x80\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x82\xd1\x8c\n\n## \xd0\x9a\xd0\xb0\xd0\xba \xd0\xb7\xd0\xb0\xd1\x80\xd0\xb5\xd0\xb3\xd0\xb8\xd1\x81\xd1\x82\xd1\x80\xd0\xb8\xd1\x80\xd0\xbe\xd0\xb2\xd0\xb0\xd1\x82\xd1\x8c\xd1\x81\xd1\x8f \xd0\xb8 \xd0\xbd\xd0\xb0\xd1\x87\xd0\xb0\xd1\x82\xd1\x8c \xd1\x80\xd0\xb0\xd0\xb1\xd0\xbe\xd1\x82\xd1\x83?\n1. \xd1\x81\xd0\xbe\xd0\xb7\xd0\xb4\xd0\xb0\xd1\x91\xd1\x82\xd0\xb5 \xd0\xb0\xd0\xba\xd0\xba\xd0\xb0\xd1\x83\xd0\xbd\xd1\x82 (sign up) \xd0\xbd\xd0\xb0 \xd1\x81\xd0\xb0\xd0\xb9\xd1\x82\xd0\xb5 \n  \n\n  - \xd0\xb2\xd1\x8b\xd0\xb1\xd0\xb8\xd1\x80\xd0\xb0\xd0\xb5\xd1\x82\xd0\xb5 \xd0\xbd\xd0\xb0\xd0\xb7\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xb8\xd0\xb5 \xd0\xb0\xd0\xba\xd0\xba\xd0\xb0\xd1\x83\xd0\xbd\xd1\x82\xd0\xb0 ()\n  - \xd0\xb2\xd0\xb2\xd0\xbe\xd0\xb4\xd0\xb8\xd1\x82\xd0\xb5 \xd1\x81\xd0\xb2\xd0\xbe\xd1\x8e \xd0\xbf\xd0\xbe\xd1\x87\xd1\x82\xd1\x83 \xd0\xb8 \xd0\xbf\xd0\xb0\xd1\x80\xd0\xbe\xd0\xbb\xd1\x8c\n\n2. \xd0\xbf\xd0\xbe\xd1\x81\xd0\xbb\xd0\xb5 \xd1\x80\xd0\xb5\xd0\xb3\xd0\xb8\xd1\x81\xd1\x82\xd1\x80\xd0\xb0\xd1\x86\xd0\xb8\xd0\xb8:\n\n  - \xd0\xbf\xd0\xb5\xd1\x80\xd0\xb5\xd1\x85\xd0\xbe\xd0\xb4\xd0\xb8\xd1\x82\xd0\xb5 \xd0\xbd\xd0\xb0 \xd1\x81\xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x86\xd1\x83 \n  - \xd0\xb8 \xd0\xbd\xd0\xb0\xd0\xb6\xd0\xb8\xd0\xbc\xd0\xb0\xd0\xb5\xd1\x82\xd0\xb5 \xd0\xbd\xd0\xb0 \xd0\xba\xd0\xbd\xd0\xbe\xd0\xbf\xd0\xba\xd1\x83    \n  - \xd1\x82\xd0\xb5\xd0\xbc \xd1\x81\xd0\xb0\xd0\xbc\xd1\x8b\xd0\xbc \xd0\xb2\xd1\x8b \xd0\xb5\xd1\x89\xd1\x91 \xd0\xb8 \xd0\xb1\xd1\x83\xd0\xb4\xd0\xb5\xd1\x82\xd0\xb5 \xd0\xbf\xd0\xbe\xd0\xbb\xd1\x83\xd1\x87\xd0\xb0\xd1\x82\xd1\x8c \xd1\x83\xd0\xb2\xd0\xb5\xd0\xb4\xd0\xbe\xd0\xbc\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xb8\xd1\x8f \xd0\xbe \xd1\x80\xd0\xb0\xd0\xb1\xd0\xbe\xd1\x82\xd0\xb5 \xd0\xbd\xd0\xb0\xd0\xb4 \xd1\x80\xd0\xb5\xd0\xbf\xd0\xbe\xd0\xb7\xd0\xb8\xd1\x82\xd0\xbe\xd1\x80\xd0\xb8\xd0\xb5\xd0\xbc \xd0\xbd\xd0\xb0 \xd0\xb3\xd0\xbb\xd0\xb0\xd0\xb2\xd0\xbd\xd0\xbe\xd0\xb9 \xd1\x81\xd1\x82\xd1\x80\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x86\xd0\xb5 \xd1\x81\xd0\xb2\xd0\xbe\xd0\xb5\xd0\xb3\xd0\xbe \xd0\xb3\xd0\xb8\xd1\x82\xd1\x85\xd0\xb0\xd0\xb1-\xd0\xb0\xd0\xba\xd0\xba\xd0\xb0\xd1\x83\xd0\xbd\xd1\x82\xd0\xb0\n\n3. \xd1\x80\xd0\xb0\xd0\xb1\xd0\xbe\xd1\x82\xd0\xb0 \xd1\x81 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb\xd0\xb0\xd0\xbc\xd0\xb8\n\n  - \xd0\xbf\xd0\xbe\xd0\xba\xd0\xb0 \xd1\x8f \xd0\xb2\xd0\xb0\xd1\x81 \xd0\xb5\xd1\x89\xd1\x91 \xd0\xbd\xd0\xb5 \xd0\xb4\xd0\xbe\xd0\xb1\xd0\xb0\xd0\xb2\xd0\xb8\xd0\xbb (\xd0\xb2 \xd0\xbe\xd1\x80\xd0\xb3\xd0\xb0\xd0\xbd\xd0\xb8\xd0\xb7\xd0\xb0\xd1\x86\xd0\xb8\xd1\x8e ), \xd0\xb2\xd1\x8b \xd0\xbd\xd0\xb5 \xd0\xbc\xd0\xbe\xd0\xb6\xd0\xb5\xd1\x82\xd0\xb5 \xd0\xb4\xd0\xbe\xd0\xb1\xd0\xb0\xd0\xb2\xd0\xbb\xd1\x8f\xd1\x82\xd1\x8c/\xd1\x80\xd0\xb5\xd0\xb4\xd0\xb0\xd0\xba\xd1\x82\xd0\xb8\xd1\x80\xd0\xbe\xd0\xb2\xd0\xb0\xd1\x82\xd1\x8c \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb\xd1\x8b, \xd0\xbd\xd0\xbe \xd0\xb2\xd1\x8b \xd0\xbc\xd0\xbe\xd0\xb6\xd0\xb5\xd1\x82\xd0\xb5 \xd1\x81\xd0\xba\xd0\xb0\xd1\x87\xd0\xb0\xd1\x82\xd1\x8c \xd0\xb8\xd1\x85 \xd0\xb8 \xd0\xbd\xd0\xb0\xd1\x87\xd0\xb0\xd1\x82\xd1\x8c \xd1\x80\xd0\xb0\xd0\xb1\xd0\xbe\xd1\x82\xd0\xb0\xd1\x82\xd1\x8c \xd1\x81 \xd0\xbd\xd0\xb8\xd0\xbc\xd0\xb8\n  - \xd0\xba\xd0\xbe\xd0\xb3\xd0\xb4\xd0\xb0 \xd1\x8f \xd0\xb2\xd0\xb0\xd1\x81 \xd1\x83\xd0\xb6\xd0\xb5 \xd0\xb4\xd0\xbe\xd0\xb1\xd0\xb0\xd0\xb2\xd0\xb8\xd0\xbb, \xd0\xbf\xd0\xb5\xd1\x80\xd0\xb5\xd1\x85\xd0\xbe\xd0\xb4\xd0\xb8\xd1\x82\xd0\xb5 \xd0\xba \xd1\x81\xd0\xbb\xd0\xb5\xd0\xb4\xd1\x83\xd1\x8e\xd1\x89\xd0\xb8\xd0\xbc \xd0\xbf\xd1\x83\xd0\xbd\xd0\xba\xd1\x82\xd0\xb0\xd0\xbc\n\n*\xd0\x9f\xd1\x80\xd0\xb8\xd0\xbc\xd0\xb5\xd1\x87\xd0\xb0\xd0\xbd\xd0\xb8\xd0\xb5: \xd0\xb5\xd1\x81\xd0\xbb\xd0\xb8 \xd0\xb2\xd1\x8b \xd0\xb7\xd0\xbd\xd0\xb0\xd0\xba\xd0\xbe\xd0\xbc\xd1\x8b \xd1\x81 git, \xd0\xbc\xd0\xbe\xd0\xb6\xd0\xb5\xd1\x82\xd0\xb5 \xd1\x84\xd0\xbe\xd1\x80\xd0\xba\xd0\xbd\xd1\x83\xd1\x82\xd1\x8c \xd1\x80\xd0\xb5\xd0\xbf\xd0\xbe\xd0\xb7\xd0\xb8\xd1\x82\xd0\xbe\xd1\x80\xd0\xb8\xd0\xb9 \xd0\xb8 \xd0\xb2\xd1\x80\xd0\xb5\xd0\xbc\xd1\x8f \xd0\xbe\xd1\x82 \xd0\xb2\xd1\x80\xd0\xb5\xd0\xbc\xd0\xb5\xd0\xbd\xd0\xb8 \xd0\xbf\xd1\x80\xd0\xb8\xd1\x81\xd1\x8b\xd0\xbb\xd0\xb0\xd1\x82\xd1\x8c \xd0\xb7\xd0\xb0\xd0\xbf\xd1\x80\xd0\xbe\xd1\x81\xd1\x8b \xd0\xbd\xd0\xb0 \xd1\x81\xd0\xbb\xd0\xb8\xd1\x8f\xd0\xbd\xd0\xb8\xd0\xb5. \xd0\x90 \xd0\xbc\xd0\xbe\xd0\xb6\xd0\xb5\xd1\x82 \xd0\xbb\xd1\x83\xd1\x87\xd1\x88\xd0\xb5 \xd0\xbf\xd1\x80\xd0\xbe\xd1\x81\xd1\x82\xd0\xbe \xd0\xb2\xd0\xb5\xd1\x82\xd0\xba\xd1\x83 \xd1\x81\xd0\xb4\xd0\xb5\xd0\xbb\xd0\xb0\xd1\x82\xd1\x8c? \xd0\x98\xd0\xbb\xd0\xb8 \xd0\xb2\xd0\xbe\xd0\xbe\xd0\xb1\xd1\x89\xd0\xb5 \xd1\x82\xd0\xbe\xd0\xbb\xd1\x8c\xd0\xba\xd0\xbe \xd0\xbd\xd0\xb0 \xd0\xbc\xd0\xb0\xd1\x81\xd1\x82\xd0\xb5\xd1\x80\xd0\xb5 \xd0\xb2\xd1\x81\xd0\xb5\xd0\xbc \xd1\x80\xd0\xb0\xd0\xb1\xd0\xbe\xd1\x82\xd0\xb0\xd1\x82\xd1\x8c? \xd0\x95\xd1\x81\xd0\xbb\xd0\xb8 \xd0\xba\xd1\x82\xd0\xbe \xd0\xb7\xd0\xbd\xd0\xb0\xd0\xb5\xd1\x82 \xe2\x80\x93 \xd0\xbf\xd0\xbe\xd0\xb4\xd1\x81\xd0\xba\xd0\xb0\xd0\xb6\xd0\xb8\xd1\x82\xd0\xb5, \xd0\xbf\xd0\xbe\xd0\xb6\xd0\xb0\xd0\xbb\xd1\x83\xd0\xb9\xd1\x81\xd1\x82\xd0\xb0, \xd0\xba\xd0\xb0\xd0\xba \xd0\xbb\xd1\x83\xd1\x87\xd1\x88\xd0\xb5.*\n\n## \xd0\x9a\xd0\xb0\xd0\xba \xd0\xb4\xd0\xbe\xd0\xb1\xd0\xb0\xd0\xb2\xd0\xbb\xd1\x8f\xd1\x82\xd1\x8c \xd1\x81\xd0\xb2\xd0\xbe\xd0\xb8 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb\xd1\x8b?\n\xd0\x97\xd0\xb0\xd1\x85\xd0\xbe\xd0\xb4\xd0\xb8\xd1\x82\xd0\xb5 \xd0\xb2 \xd0\xbf\xd0\xb0\xd0\xbf\xd0\xba\xd1\x83  \xd0\xb8 \xd0\xbf\xd1\x80\xd0\xbe\xd1\x81\xd1\x82\xd0\xbe \xd0\xbf\xd0\xb5\xd1\x80\xd0\xb5\xd1\x82\xd0\xb0\xd1\x81\xd0\xba\xd0\xb8\xd0\xb2\xd0\xb0\xd0\xb5\xd1\x82\xd0\xb5 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb \xd0\xb2 \xd0\xbe\xd0\xba\xd0\xbd\xd0\xbe \xd0\xb1\xd1\x80\xd0\xb0\xd1\x83\xd0\xb7\xd0\xb5\xd1\x80\xd0\xb0, \xd0\xbf\xd1\x80\xd0\xb8 \xd1\x8d\xd1\x82\xd0\xbe\xd0\xbc \xd0\xbe\xd0\xba\xd0\xbe\xd1\x88\xd0\xba\xd0\xbe \xd0\xb4\xd0\xbe\xd0\xbb\xd0\xb6\xd0\xbd\xd0\xbe \xd0\xb8\xd0\xb7\xd0\xbc\xd0\xb5\xd0\xbd\xd0\xb8\xd1\x82\xd1\x8c\xd1\x81\xd1\x8f. \xd0\x95\xd1\x81\xd0\xbb\xd0\xb8 \xd0\xbd\xd0\xb5\xd1\x82, \xd1\x82\xd0\xbe \xd1\x83 \xd0\xb2\xd1\x81\xd0\xb5 \xd0\xb2\xd1\x81\xd1\x91 \xd0\xb5\xd1\x89\xd1\x91 \xd0\xbd\xd0\xb5\xd1\x82 \xd0\xb4\xd0\xbe\xd1\x81\xd1\x82\xd1\x83\xd0\xbf\xd0\xb0 \xd0\xba \xd1\x80\xd0\xb5\xd0\xb4\xd0\xb0\xd0\xba\xd1\x82\xd0\xb8\xd1\x80\xd0\xbe\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x8e.  \n\n**\xd0\x96\xd0\xb5\xd0\xbb\xd0\xb0\xd1\x82\xd0\xb5\xd0\xbb\xd1\x8c\xd0\xbd\xd0\xbe**, \xd1\x87\xd1\x82\xd0\xbe\xd0\xb1\xd1\x8b \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb \xd0\xb1\xd1\x8b\xd0\xbb \xd1\x81 \xd1\x80\xd0\xb0\xd1\x81\xd1\x88\xd0\xb8\xd1\x80\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5\xd0\xbc  \xe2\x80\x93 \xd1\x8d\xd1\x82\xd0\xbe \xd0\xbd\xd0\xb5\xd0\xb7\xd0\xb0\xd0\xb0\xd1\x80\xd1\x85\xd0\xb8\xd0\xb2\xd0\xb8\xd1\x80\xd0\xbe\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xbd\xd1\x8b\xd0\xb9 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb \xd1\x81 \xd0\xb2\xd0\xb0\xd1\x88\xd0\xb8\xd0\xbc\xd0\xb8 \xd0\xbe\xd1\x82\xd0\xbc\xd0\xb5\xd1\x82\xd0\xba\xd0\xb0\xd0\xbc\xd0\xb8 \xd0\xbf\xd0\xb8\xd1\x80\xd0\xb0\xd0\xbc\xd0\xb8\xd0\xb4 ( \xe2\x80\x93 \xd1\x8d\xd1\x82\xd0\xbe\xd1\x82 \xd1\x82\xd0\xbe\xd1\x82 \xd0\xb6\xd0\xb5  \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb, \xd1\x82\xd0\xbe\xd0\xbb\xd1\x8c\xd0\xba\xd0\xbe \xd0\xb2 \xd0\xb0\xd1\x80\xd1\x85\xd0\xb8\xd0\xb2\xd0\xb5).\n\n## \xd0\x9a\xd0\xb0\xd0\xba \xd0\xbe\xd0\xb1\xd0\xbd\xd0\xbe\xd0\xb2\xd0\xbb\xd1\x8f\xd1\x82\xd1\x8c \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb \xd1\x81 \xd1\x82\xd0\xb5\xd0\xba\xd1\x83\xd1\x89\xd0\xb5\xd0\xb9 \xd0\xb1\xd0\xb0\xd0\xb7\xd0\xbe\xd0\xb9?\n\xd0\x94\xd0\xbe \xd1\x82\xd0\xbe\xd0\xb3\xd0\xbe, \xd0\xba\xd0\xb0\xd0\xba \xd1\x81\xd0\xbe\xd0\xb7\xd0\xb4\xd0\xb0\xd0\xb2\xd0\xb0\xd1\x82\xd1\x8c \xd0\xb5\xd0\xb4\xd0\xb8\xd0\xbd\xd1\x83\xd1\x8e \xd0\xb1\xd0\xb0\xd0\xb7\xd1\x83, \xd0\xbd\xd0\xb0\xd0\xb4\xd0\xbe \xd0\xbf\xd1\x80\xd0\xbe\xd0\xb4\xd1\x83\xd0\xbc\xd0\xb0\xd1\x82\xd1\x8c, \xd0\xba\xd0\xb0\xd0\xba\xd0\xb8\xd0\xbc \xd0\xb4\xd0\xbe\xd0\xbb\xd0\xb6\xd0\xb5\xd0\xbd \xd0\xb1\xd1\x8b\xd1\x82\xd1\x8c \xd0\xbd\xd0\xb0\xd0\xb7\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xb8\xd0\xb5, \xd1\x82.\xd0\xb5. \xd0\xb8\xd0\xb4\xd0\xb5\xd0\xbd\xd1\x82\xd0\xb8\xd1\x84\xd0\xb8\xd0\xba\xd0\xb0\xd1\x82\xd0\xbe\xd1\x80 \xd0\xbf\xd0\xb8\xd1\x80\xd0\xb0\xd0\xbc\xd0\xb8\xd0\xb4. \xd0\xad\xd1\x82\xd0\xbe \xd0\xbf\xd0\xbe\xd0\xbc\xd0\xbe\xd0\xb6\xd0\xb5\xd1\x82 \xd0\xb8\xd0\xb7\xd0\xb1\xd0\xb5\xd0\xb6\xd0\xb0\xd1\x82\xd1\x8c \xd0\xbf\xd0\xbe\xd0\xb2\xd1\x82\xd0\xbe\xd1\x80\xd0\xbe\xd0\xb2 \xd0\xb8 \xd1\x81\xd0\xb4\xd0\xb5\xd0\xbb\xd0\xb0\xd0\xb5\xd1\x82 \xd0\xb1\xd0\xb0\xd0\xb7\xd1\x83 \xd0\xb1\xd0\xbe\xd0\xbb\xd0\xb5\xd0\xb5 \xd0\xbe\xd1\x80\xd0\xb3\xd0\xb0\xd0\xbd\xd0\xb8\xd0\xb7\xd0\xbe\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xbd\xd0\xbe\xd0\xb9. \xd0\x9f\xd1\x80\xd0\xb5\xd0\xb4\xd0\xbb\xd0\xb0\xd0\xb3\xd0\xb0\xd1\x8e:\n\n- \xd0\xb2 \xd0\xba\xd0\xb0\xd1\x87\xd0\xb5\xd1\x81\xd1\x82\xd0\xb2\xd0\xb5 \xd0\xbd\xd0\xb0\xd0\xb7\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xb8\xd1\x8f \xd0\xb8\xd1\x81\xd0\xbf\xd0\xbe\xd0\xbb\xd1\x8c\xd0\xb7\xd0\xbe\xd0\xb2\xd0\xb0\xd1\x82\xd1\x8c \xd0\xbf\xd0\xb5\xd1\x80\xd0\xb2\xd1\x8b\xd0\xb5 \xd1\x82\xd1\x80\xd0\xb8 \xd1\x86\xd0\xb8\xd1\x84\xd1\x80\xd1\x8b \xd0\xba\xd0\xbe\xd0\xbe\xd1\x80\xd0\xb4\xd0\xb8\xd0\xbd\xd0\xb0\xd1\x82 (\xd0\xbd\xd0\xb0\xd0\xb4\xd0\xbe \xd0\xb2\xd1\x8b\xd0\xb1\xd1\x80\xd0\xb0\xd1\x82\xd1\x8c \xd0\xbc\xd0\xb5\xd0\xb6\xd0\xb4\xd1\x83 \xd0\xb2\xd0\xb0\xd1\x80\xd0\xb8\xd0\xb0\xd0\xbd\xd1\x82\xd0\xbe\xd0\xbc \xd1\x81 \xd0\xb3\xd1\x80\xd0\xb0\xd0\xb4\xd1\x83\xd1\x81\xd0\xb0\xd0\xbc\xd0\xb8 \xd0\xb8\xd0\xbb\xd0\xb8 \xd0\xb1\xd0\xb5\xd0\xb7)\n- \xd0\xb5\xd1\x81\xd0\xbb\xd0\xb8 \xd1\x83 \xd0\xbf\xd0\xb8\xd1\x80\xd0\xb0\xd0\xbc\xd0\xb8\xd0\xb4\xd1\x8b \xd0\xb5\xd1\x81\xd1\x82\xd1\x8c \xd0\xbd\xd0\xb0\xd0\xb7\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xb8\xd0\xb5, \xd0\xb5\xd0\xb3\xd0\xbe \xd0\xbc\xd0\xbe\xd0\xb6\xd0\xbd\xd0\xbe \xd1\x83\xd0\xba\xd0\xb0\xd0\xb7\xd0\xb0\xd1\x82\xd1\x8c \xd0\xbf\xd0\xbe\xd1\x81\xd0\xbb\xd0\xb5 \xd0\xb8\xd0\xb4\xd0\xb5\xd0\xbd\xd1\x82\xd0\xb8\xd1\x84\xd0\xb8\xd0\xba\xd0\xb0\xd1\x82\xd0\xbe\xd1\x80\xd0\xb0\n\n\xd0\x9b\xd0\xb5\xd0\xb3\xd1\x87\xd0\xb5 \xd0\xb2\xd1\x81\xd0\xb5\xd0\xb3\xd0\xbe \xd1\x8d\xd1\x82\xd0\xbe \xd1\x81\xd0\xb4\xd0\xb5\xd0\xbb\xd0\xb0\xd1\x82\xd1\x8c \xd0\xb2 \xd0\xbf\xd1\x80\xd0\xbe\xd0\xb3\xd1\x80\xd0\xb0\xd0\xbc\xd0\xbc\xd0\xb5 Google Earth \xe2\x80\x93 \xd1\x82\xd0\xb0\xd0\xbc \xd0\xbc\xd0\xbe\xd0\xb6\xd0\xbd\xd0\xbe \xd1\x81\xd0\xbe\xd0\xb7\xd0\xb4\xd0\xb0\xd0\xb2\xd0\xb0\xd1\x82\xd1\x8c \xd0\xbf\xd0\xb0\xd0\xbf\xd0\xba\xd0\xb8 \xd1\x81 \xd0\xb2\xd0\xb0\xd1\x88\xd0\xb8\xd0\xbc\xd0\xb8 \xd0\xb7\xd0\xb0\xd0\xbc\xd0\xb5\xd1\x82\xd0\xba\xd0\xb0\xd0\xbc\xd0\xb8, \xd0\xba\xd0\xbe\xd0\xbf\xd0\xb8\xd1\x80\xd0\xbe\xd0\xb2\xd0\xb0\xd1\x82\xd1\x8c, \xd0\xb2\xd1\x8b\xd1\x80\xd0\xb5\xd0\xb7\xd0\xb0\xd1\x82\xd1\x8c \xd0\xb8 \xd0\xb2\xd1\x81\xd1\x82\xd0\xb0\xd0\xb2\xd0\xbb\xd1\x8f\xd1\x82\xd1\x8c \xd0\xb8\xd1\x85. \xd0\x9e\xd1\x82\xd0\xba\xd1\x80\xd1\x8b\xd0\xb2\xd0\xb0\xd0\xb5\xd1\x82\xd0\xb5 \xd0\xb2\xd0\xb0\xd1\x88 \xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb \xd0\xb8 \xd1\x82\xd0\xb5\xd0\xba\xd1\x83\xd1\x89\xd1\x83\xd1\x8e \xd0\xb1\xd0\xb0\xd0\xb7\xd1\x83 \xd0\xbf\xd0\xb8\xd1\x80\xd0\xb0\xd0\xbc\xd0\xb8\xd0\xb4, \xd0\xb8 ,  ( \xd0\xb4\xd0\xbb\xd1\x8f \xd0\xbc\xd0\xb0\xd0\xba\xd0\xb8\xd0\xbd\xd1\x82\xd0\xbe\xd1\x88).\n\n\xd0\x9f\xd0\xbe\xd0\xba\xd0\xb0 \xd1\x8d\xd1\x82\xd0\xbe \xd0\xb1\xd1\x83\xd0\xb4\xd1\x83 \xd0\xb4\xd0\xb5\xd0\xbb\xd0\xb0\xd1\x82\xd1\x8c \xd1\x8f, \xd0\xb0 \xd0\xb5\xd1\x81\xd0\xbb\xd0\xb8 \xd0\xbb\xd1\x8e\xd0\xb4\xd0\xb5\xd0\xb9 \xd0\xb1\xd1\x83\xd0\xb4\xd0\xb5\xd1\x82 \xd0\xbc\xd0\xbd\xd0\xbe\xd0\xb3\xd0\xbe, \xd1\x82\xd0\xbe \xd1\x8f \xd0\xbe\xd0\xbf\xd0\xb8\xd1\x88\xd1\x83 \xd0\xba\xd0\xb0\xd0\xba \xd0\xb4\xd0\xb5\xd0\xbb\xd0\xb0\xd1\x82\xd1\x8c \xd1\x81 \xd0\xbf\xd0\xbe\xd0\xbc\xd0\xbe\xd1\x89\xd1\x8c \xd0\xbc\xd0\xb5\xd1\x85\xd0\xb0\xd0\xbd\xd0\xb8\xd0\xb7\xd0\xbc\xd0\xbe\xd0\xb2 \xd0\xb3\xd0\xb8\xd1\x82\xd0\xb0 (\xd1\x81 \xd0\xbf\xd0\xbe\xd0\xbc\xd0\xbe\xd1\x89\xd1\x8c\xd1\x8e \xd0\xb2\xd0\xb5\xd1\x82\xd0\xbe\xd0\xba (branches) \xd0\xb8\xd0\xbb\xd0\xb8 \xd0\xbe\xd1\x82\xd0\xb2\xd0\xb5\xd1\x82\xd0\xb2\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb9 (forks)).\n\n*\xd0\x9f\xd1\x80\xd0\xb8\xd0\xbc\xd0\xb5\xd1\x87\xd0\xb0\xd0\xbd\xd0\xb8\xd0\xb5: \xd0\x95\xd1\x81\xd0\xbb\xd0\xb8 \xd0\xb2\xd1\x8b \xd0\xb7\xd0\xbd\xd0\xb0\xd0\xb5\xd1\x82\xd0\xb5, \xd1\x87\xd1\x82\xd0\xbe  \xd1\x8d\xd1\x82\xd0\xbe \xd0\xbe\xd0\xb1\xd1\x8b\xd1\x87\xd0\xbd\xd1\x8b\xd0\xb9 -\xd1\x84\xd0\xb0\xd0\xb9\xd0\xbb, \xd1\x82\xd0\xbe \xd0\xbc\xd0\xbe\xd0\xb6\xd0\xb5\xd1\x82\xd0\xb5 \xd0\xb5\xd0\xb3\xd0\xbe \xd1\x80\xd0\xb5\xd0\xb4\xd0\xb0\xd0\xba\xd1\x82\xd0\xb8\xd1\x80\xd0\xbe\xd0\xb2\xd0\xb0\xd1\x82\xd1\x8c \xd0\xb2 \xd0\xbb\xd1\x8e\xd0\xb1\xd0\xbe\xd0\xbc \xd1\x82\xd0\xb5\xd0\xba\xd1\x81\xd1\x82\xd0\xbe\xd0\xb2\xd0\xbe\xd0\xbc \xd1\x80\xd0\xb5\xd0\xb4\xd0\xb0\xd0\xba\xd1\x82\xd0\xbe\xd1\x80\xd0\xb5.*\n\n## \xd0\x95\xd1\x81\xd1\x82\xd1\x8c \xd0\xbf\xd1\x80\xd0\xb5\xd0\xb4\xd0\xbb\xd0\xbe\xd0\xb6\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5? \xd0\xa7\xd1\x82\xd0\xbe-\xd1\x82\xd0\xbe \xd0\xbd\xd0\xb5 \xd1\x80\xd0\xb0\xd0\xb1\xd0\xbe\xd1\x82\xd0\xb0\xd0\xb5\xd1\x82?\n\xd0\x94\xd0\xbb\xd1\x8f \xd1\x82\xd0\xbe\xd0\xb3\xd0\xbe, \xd1\x87\xd1\x82\xd0\xbe\xd0\xb1\xd1\x8b \xd0\xb7\xd0\xb0\xd0\xb4\xd0\xb0\xd1\x82\xd1\x8c \xd0\xb2\xd0\xbe\xd0\xbf\xd1\x80\xd0\xbe\xd1\x81, \xd0\xbd\xd0\xb0\xd0\xb4\xd0\xbe \xd1\x81\xd0\xb4\xd0\xb5\xd0\xbb\xd0\xb0\xd1\x82\xd1\x8c \xd1\x81\xd0\xbb\xd0\xb5\xd0\xb4\xd1\x83\xd1\x8e\xd1\x89\xd0\xb5\xd0\xb5:\n- \xd0\xbd\xd0\xb0\xd0\xb6\xd0\xb0\xd1\x82\xd1\x8c \xd0\xbd\xd0\xb0 \xd0\xb7\xd0\xb0\xd0\xba\xd0\xbb\xd0\xb0\xd0\xb4\xd0\xba\xd1\x83 , \xd0\xb0 \xd0\xb7\xd0\xb0\xd1\x82\xd0\xb5\xd0\xbc \xd0\xbd\xd0\xb0 \xd0\xba\xd0\xbd\xd0\xbe\xd0\xbf\xd0\xba\xd1\x83 \n- \xd0\xbd\xd0\xb0\xd0\xb1\xd1\x80\xd0\xb0\xd1\x82\xd1\x8c \xd0\xbd\xd0\xb0\xd0\xb7\xd0\xb2\xd0\xb0\xd0\xbd\xd0\xb8\xd0\xb5 \xd0\xb8 \xd1\x82\xd0\xb5\xd0\xba\xd1\x81\xd1\x82 \xd1\x81 \xd0\xbe\xd0\xbf\xd0\xb8\xd1\x81\xd0\xb0\xd0\xbd\xd0\xb8\xd0\xb5\xd0\xbc \xd0\xbf\xd1\x80\xd0\xbe\xd0\xb1\xd0\xbb\xd0\xb5\xd0\xbc\xd1\x8b\n- \xd0\xbd\xd0\xb0\xd0\xb6\xd0\xb0\xd1\x82\xd1\x8c \xd0\xbd\xd0\xb0 \xd0\xba\xd0\xbd\xd0\xbe\xd0\xbf\xd0\xba\xd1\x83 \n\n# \xd0\x90 \xd0\xb2\xd1\x8b \xd0\xb7\xd0\xbd\xd0\xb0\xd0\xb5\xd1\x82\xd0\xb5, \xd1\x87\xd1\x82\xd0\xbe...\n- \xd0\x92 \xd0\xbf\xd1\x80\xd0\xb8\xd0\xbb\xd0\xbe\xd0\xb6\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb8 Google Earth \xd0\xbc\xd0\xbe\xd0\xb6\xd0\xbd\xd0\xbe \xd1\x83\xd0\xb2\xd0\xb8\xd0\xb4\xd0\xb5\xd1\x82\xd1\x8c \xd0\xba\xd0\xb0\xd1\x80\xd1\x82\xd1\x8b \xd0\xb7\xd0\xb0 \xd1\x80\xd0\xb0\xd0\xb7\xd0\xbd\xd1\x8b\xd0\xb5 \xd0\xbc\xd0\xbe\xd0\xbc\xd0\xb5\xd0\xbd\xd1\x82\xd1\x8b \xd0\xb2\xd1\x80\xd0\xb5\xd0\xbc\xd0\xb5\xd0\xbd\xd0\xb8? \xd0\x9d\xd0\xb0\xd0\xb4\xd0\xbe \xd0\xbb\xd0\xb8\xd1\x88\xd1\x8c \xd0\xbd\xd0\xb0\xd0\xb6\xd0\xb0\xd1\x82\xd1\x8c \xd0\xbd\xd0\xb0 \xd0\xba\xd0\xbd\xd0\xbe\xd0\xbf\xd0\xba\xd1\x83 \n- \xd0\x92 Google Earth \xd0\xbc\xd0\xbe\xd0\xb6\xd0\xbd\xd0\xbe \xd0\xb8 \xd0\xbd\xd0\xb0 \xd0\xb7\xd0\xb2\xd1\x91\xd0\xb7\xd0\xb4\xd1\x8b \xd0\xbf\xd0\xbe\xd1\x81\xd0\xbc\xd0\xbe\xd1\x82\xd1\x80\xd0\xb5\xd1\x82\xd1\x8c, \xd0\xbf\xd1\x80\xd0\xb8\xd1\x87\xd1\x91\xd0\xbc \xd1\x81 \xd0\xba\xd0\xb0\xd1\x80\xd1\x82\xd0\xb8\xd0\xbd\xd0\xba\xd0\xb0\xd0\xbc\xd0\xb8 \xd0\xbe\xd1\x82 \xd0\x9d\xd0\x90\xd0\xa1\xd0\x90 \xd0\xb8 \xd0\xb4\xd1\x80. \xd1\x82\xd0\xbe\xd0\xb2\xd0\xb0\xd1\x80\xd0\xb8\xd1\x89\xd0\xb5\xd0\xb9 \xd0\xb2\xd0\xbc\xd0\xb5\xd1\x81\xd1\x82\xd0\xb5 \xd1\x81 \xd0\xb7\xd0\xb0\xd0\xbc\xd0\xb5\xd1\x82\xd0\xba\xd0\xb0\xd0\xbc\xd0\xb8. \xd0\x9d\xd0\xb0\xd0\xb4\xd0\xbe \xd0\xbb\xd0\xb8\xd1\x88\xd1\x8c \xd0\xbd\xd0\xb0\xd0\xb6\xd0\xb0\xd1\x82\xd1\x8c \xd0\xbd\xd0\xb0 \xd1\x81\xd0\xb8\xd0\xbc\xd0\xb2\xd0\xbe\xd0\xbb \xd0\xbf\xd0\xbb\xd0\xb0\xd0\xbd\xd0\xb5\xd1\x82\xd1\x8b  \xd0\xb8 \xd0\xbf\xd0\xbe\xd1\x8f\xd0\xb2\xd0\xb8\xd1\x82\xd1\x81\xd1\x8f \xd1\x81\xd0\xbf\xd0\xb8\xd1\x81\xd0\xbe\xd0\xba:  \n\n\n\n# \xd0\x9f\xd0\xbb\xd0\xb0\xd0\xbd \xd1\x80\xd0\xb0\xd0\xb1\xd0\xbe\xd1\x82\xd1\x8b \xd0\xbf\xd0\xbe \xd1\x81\xd0\xb0\xd0\xb9\xd1\x82\xd1\x83\n- \xd0\xb3\xd0\xbe\xd1\x82\xd0\xbe\xd0\xb2 \xd0\xba \xd0\xb2\xd0\xb0\xd1\x88\xd0\xb8\xd0\xbc \xd0\xbf\xd1\x80\xd0\xb5\xd0\xb4\xd0\xbb\xd0\xbe\xd0\xb6\xd0\xb5\xd0\xbd\xd0\xb8\xd1\x8f\xd0\xbc, \xd0\xb4\xd0\xbe\xd1\x80\xd0\xbe\xd0\xb3\xd0\xb8\xd0\xb5 \xd0\xb4\xd1\x80\xd1\x83\xd0\xb7\xd1\x8c\xd1\x8f\n- \xd0\xb4\xd0\xbe\xd0\xb1\xd0\xb0\xd0\xb2\xd0\xbb\xd0\xb5\xd0\xbd\xd0\xb8\xd0\xb5 \xd0\xba\xd0\xb0\xd1\x80\xd1\x82\xd1\x8b \xd1\x81 Google.Maps \xd0\xb4\xd0\xbb\xd1\x8f \xd0\xb2\xd0\xb8\xd0\xb7\xd1\x83\xd0\xb0\xd0\xbb\xd0\xb8\xd0\xb7\xd0\xb0\xd1\x86\xd0\xb8\xd0\xb8 \xd0\xbf\xd1\x80\xd0\xbe\xd0\xb4\xd0\xb5\xd0\xbb\xd0\xb0\xd0\xbd\xd0\xbd\xd0\xbe\xd0\xb9 \xd1\x80\xd0\xb0\xd0\xb1\xd0\xbe\xd1\x82\xd1\x8b \xd1\x8f \xd0\xbf\xd0\xbe\xd0\xba\xd0\xb0 \xd0\xbe\xd1\x82\xd0\xbb\xd0\xbe\xd0\xb6\xd0\xb8\xd0\xbb \xd0\xb2 \xd0\xb4\xd0\xbe\xd0\xbb\xd0\xb3\xd0\xb8\xd0\xb9 \xd1\x8f\xd1\x89\xd0\xb8\xd0\xba \xe2\x80\x93 \xd0\xb5\xd1\x81\xd0\xbb\xd0\xb8 \xd0\xba\xd1\x82\xd0\xbe-\xd1\x82\xd0\xbe \xd1\x81\xd0\xbc\xd0\xbe\xd0\xb6\xd0\xb5\xd1\x82 \xd0\xbf\xd0\xbe\xd0\xbc\xd0\xbe\xd1\x87\xd1\x8c \xd1\x81 \xd0\xbe\xd1\x80\xd0\xb3\xd0\xb0\xd0\xbd\xd0\xb8\xd0\xb7\xd0\xb0\xd1\x86\xd0\xb8\xd0\xb5\xd0\xb9 \xd1\x81\xd0\xb5\xd0\xb3\xd0\xbe \xe2\x80\x93 \xd0\xbd\xd0\xb0\xd0\xbf\xd0\xb8\xd1\x88\xd0\xb8\xd1\x82\xd0\xb5 \xd0\xbc\xd0\xbd\xd0\xb5, \xd0\xbf\xd0\xbe\xd0\xb6\xd0\xb0\xd0\xbb\xd1\x83\xd0\xb9\xd1\x81\xd1\x82\xd0\xb0\n'"
https://github.com/mikeengland/Twitter-MongoDB-Python-Integration,Example adapted from Python for Data Analysis,"b""# Twitter-MongoDB-Python-Integration\nExample adapted from Python for Data Analysis by Wes McKinney (the example in the book used the old Twitter Search API)\n\nThis example shows how to connect to MongoDB using Python, query and store tweets in MongoDB relating to the keyword of 'python', load these JSON results into a data frame and display the data frame.\n"""
https://github.com/TueVJ/Python-Introduction,Introduces basic Python concepts for those used to other programming languages. Ends with writing optimization problems in Gurobi for Python.,"b'# Introduction to Python (and Gurobi)\n\nAn introduction to basic Python concepts, and most of what is needed to have the full work flow in OR research in Python.\nThe course is not intended to give an in-depth treatment of all aspects of Python, but should provide a solid starting point for self-learning.\nThe course level assumes that the user has experience in some other programming language (e.g. MATLAB, Java), and is looking to learn Python.\n\nThis workshop is intended for 2 sessions of 2 hours each, with the first 2 hours (notebooks 1 & 2) focusing on python basics, and the latter 2 on using standard python modules.\nNotebook 4 is directly intended for OR researchers; if your group has a different focus, e.g. machine learning, this session can be adapted to that purpose.\n\n## Needed tools\n\nTo run these tutorials on Windows or Macintosh, I recommend an installation of . \nOn Linux, use the version of iPython/Jupyter from your package manager.\n\nThe tutorial is written for Python 3.X, and is organized as ipython notebooks. These will work in  as well.\n\nTo run session 4, you will need an installation of , along with a license.  are available for academic users.\n\n## Authors\n\n- Tue Vissing Jensen: Initial version, maintainer\n- Esteban Morales Bondy: Update of sessions 1-3 for Python 3, css prettiness\n'"
https://github.com/PierreGe/RL-movie-recommender,The purpose of our research is to study reinforcement learning approaches to building a movie recommender system. We formulate the problem of interactive recommendation as a contextual multi-armed bandit.,"b'# RL-movie-recommender\n\n\n### Abstract\n\nThe purpose of our research is to study reinforcement learning\napproaches to building a movie recommender system.\nWe formulate the problem of interactive recommendation as\na contextual multi-armed bandit, learning user preferences\nrecommending new movies and receiving their ratings. We\nshow that using reinforcement learning solves the problem of\nexploitation-exploration trade-off and the cold-start problem.\nWe integrate the novelty of movies to the model. We explore\na content based approach as well as a collaborative filtering\napproach and both yield viable recommendation results.\n'"
https://github.com/QuantEcon/NUS_workshop_2016,Computational Economics with Python and Julia,"b'#  Computational Economics with Python and Julia\n\n##  National University of Singapore, October 2016\n\n\nLecturer: \n\nSupport: \n\nEconomists are increasingly adopting modern open source computing environments\nsuch as  and \nfor research and policy analysis.\n\nThis workshop will cover the basics of these two languages and their use in\ncomputational economics.  We assume that participants \nhave basic programming skills but do not know either Python or Julia.  We\nwill provide a comparison and overview of these languanges, including set up,\ninstallation and some useful libraries.\n\n\n### Instructions\n\n\n*  Before coming, please install  on your laptop\n\n    * Python + the main scientific libraries\n    * Free from http://continuum.io/downloads\n    * Choose the Python 3.5 version\n    * Make it your default Python distribution\n\n* Install \n\n    * Follow instructions for your OS\n    * Some extra info  if you need it\n\nPlease be sure to bring your laptop to the workshop.  We will have a trouble shooting session at the start.\n\nPlease make sure that you have wireless access.\n\n### Notebooks\n\n* \n* \n\n### Resources\n\n* \n* \n* \n\n\n\n### Get the Files\n\nTo access the files in this repositiory, either \n\n* , if you know what that is, or\n\n* download the zip file (see top right of page)\n\n\n### Other\n\nAlso consider installing \n\n* a modern browser (Chrome / Firefox / Vivaldi) \n\n* a good text editor such as \n\n* \n\n'"
https://github.com/bcongdon/sgdq-2017-schedule-analysis,📊 Visualizations of the SGDQ 2017 Marathon Schedule,b'# sgdq-2017-schedule-analysis\n\xf0\x9f\x93\x8a Visualizations of the SGDQ 2017 Marathon Schedule\n'
https://github.com/mnagaku/naumanni-exercises,ナウマンを改象,"b'{\n ""cells"": [\n  {\n   ""cell_type"": ""markdown"",\n   ""metadata"": {},\n   ""source"": [\n    ""# \xe3\x83\x8a\xe3\x82\xa6\xe3\x83\x9e\xe3\x83\xb3\xe3\x82\x92\xe6\x94\xb9\xe8\xb1\xa1n"",\n    ""n"",\n    ""Windows\xe3\x81\xa7\xe3\x82\x82Mac\xe3\x81\xa7\xe3\x82\x82Linux\xe3\x81\xa7\xe3\x82\x82\xe3\x80\x81Docker\xe3\x82\x92\xe3\x82\xa4\xe3\x83\xb3\xe3\x82\xb9\xe3\x83\x88\xe3\x83\xbc\xe3\x83\xab\xe3\x81\x99\xe3\x82\x8c\xe3\x81\xb0\xe3\x80\x81\xe4\xbb\xa5\xe4\xb8\x8b\xe3\x81\xab\xe8\xaa\xac\xe6\x98\x8e\xe3\x81\x99\xe3\x82\x8b\xe6\x89\x8b\xe9\xa0\x86\xe3\x81\xa7\xe3\x80\x81\xe4\xb8\xbb\xe3\x81\xabweb\xe3\x83\x96\xe3\x83\xa9\xe3\x82\xa6\xe3\x82\xb6\xe7\xb5\x8c\xe7\x94\xb1\xe3\x81\xa7\xe3\x80\x81naumanni\xe3\x82\x92\xe3\x83\x93\xe3\x83\xab\xe3\x83\x89\xe3\x81\x97\xe3\x81\x9f\xe3\x82\x8a\xe3\x80\x81\xe5\xae\x9f\xe8\xa1\x8c\xe3\x81\x97\xe3\x81\x9f\xe3\x82\x8a\xe3\x80\x81\xe6\x94\xb9\xe9\x80\xa0\xe3\x81\x97\xe3\x81\x9f\xe3\x82\x8a\xe3\x81\xa7\xe3\x81\x8d\xe3\x82\x8b\xe3\x80\x82n"",\n    ""n"",\n    ""# \xe4\xbd\xbf\xe3\x81\x84\xe6\x96\xb9n"",\n    ""n"",\n    ""## JupyterLab\xe8\xb5\xb7\xe5\x8b\x95n"",\n    ""n"",\n    ""docker\xe3\x82\xb3\xe3\x83\x9e\xe3\x83\xb3\xe3\x83\x89\xe3\x81\x8c\xe4\xbd\xbf\xe3\x81\x88\xe3\x82\x8b\xe3\x81\xa8\xe3\x81\x93\xe3\x81\xa7\xe3\x80\x81n"",\n    ""n"",\n    ""n"",\n    ""n"",\n    ""\xe3\x82\x92\xe5\xae\x9f\xe8\xa1\x8c\xe3\x81\x99\xe3\x82\x8b\xe3\x81\xa8JupyterLab\xe3\x81\x8c8888\xe7\x95\xaa\xe3\x83\x9d\xe3\x83\xbc\xe3\x83\x88\xe3\x81\xa7\xe3\x82\xa2\xe3\x82\xaf\xe3\x82\xbb\xe3\x82\xb9\xe3\x81\xa7\xe3\x81\x8d\xe3\x82\x8b\xe7\x8a\xb6\xe6\x85\x8b\xe3\x81\xa7\xe8\xb5\xb7\xe5\x8b\x95\xe3\x81\x99\xe3\x82\x8b\xe3\x80\x82n"",\n    ""n"",\n    ""## JupyterLab\xe6\x8e\xa5\xe7\xb6\x9an"",\n    ""n"",\n    ""web\xe3\x83\x96\xe3\x83\xa9\xe3\x82\xa6\xe3\x82\xb6\xe3\x81\x8b\xe3\x82\x89\xe3\x80\x81\xe8\xb5\xb7\xe5\x8b\x95\xe3\x81\x97\xe3\x81\x9fJupyterLab\xe3\x81\xab\xe3\x82\xa2\xe3\x82\xaf\xe3\x82\xbb\xe3\x82\xb9\xe3\x81\x99\xe3\x82\x8b\xe3\x80\x82\xe4\xbe\x8b\xe3\x81\x88\xe3\x81\xb0\xe3\x80\x81\xe6\x93\x8d\xe4\xbd\x9c\xe3\x81\x97\xe3\x81\xa6\xe3\x81\x84\xe3\x82\x8bPC\xe4\xb8\x8a\xe3\x81\xab\xe8\xb5\xb7\xe5\x8b\x95\xe3\x81\x97\xe3\x81\x9f\xe5\xa0\xb4\xe5\x90\x88\xe3\x81\xaf\xe3\x80\x81localhost\xe3\x81\xae8888\xe7\x95\xaa\xe3\x83\x9d\xe3\x83\xbc\xe3\x83\x88\xe3\x81\xab\xe3\x82\xa2\xe3\x82\xaf\xe3\x82\xbb\xe3\x82\xb9\xe3\x81\x99\xe3\x82\x8c\xe3\x81\xb0\xe3\x82\x88\xe3\x81\x84\xe3\x81\xae\xe3\x81\xa7\xe3\x80\x81n"",\n    ""n"",\n    ""n"",\n    ""n"",\n    ""\xe3\x81\xa7\xe3\x82\xa2\xe3\x82\xaf\xe3\x82\xbb\xe3\x82\xb9\xe3\x81\xa7\xe3\x81\x8d\xe3\x82\x8b\xe3\x80\x82n"",\n    ""n"",\n    ""## readme.ipynb\xe3\x81\xaeUploadn"",\n    ""n"",\n    ""web\xe3\x83\x96\xe3\x83\xa9\xe3\x82\xa6\xe3\x82\xb6\xe3\x81\xa7\xe3\x80\x81JupyterLab\xe3\x81\xaeHome\xe7\x94\xbb\xe9\x9d\xa2\xe3\x81\x8c\xe5\x87\xba\xe3\x81\x9f\xe3\x82\x89\xe3\x80\x81\xe5\xb7\xa6\xe4\xb8\x8a\xe3\x81\xab\xe3\x81\x82\xe3\x82\x8b\xe3\x80\x8c\xe4\xb8\x8b\xe7\xb7\x9a\xe3\x81\xab\xe4\xb8\x8a\xe5\x90\x91\xe3\x81\x8d\xe7\x9f\xa2\xe5\x8d\xb0\xe3\x80\x8d\xe3\x83\x9c\xe3\x82\xbf\xe3\x83\xb3\xe3\x81\x8b\xe3\x82\x89\xe3\x80\x81\xe3\x81\x93\xe3\x81\xae\xe6\x96\x87\xe6\x9b\xb8\xe3\x80\x8creadme.ipynb\xe3\x80\x8d\xe3\x82\x92\xe3\x82\xa2\xe3\x83\x83\xe3\x83\x97\xe3\x83\xad\xe3\x83\xbc\xe3\x83\x89\xe3\x81\x99\xe3\x82\x8b\xe3\x80\x82\xe3\x82\xa2\xe3\x83\x83\xe3\x83\x97\xe3\x83\xad\xe3\x83\xbc\xe3\x83\x89\xe3\x81\x8c\xe5\xae\x8c\xe4\xba\x86\xe3\x81\x97\xe3\x81\xa6\xe3\x80\x81Home\xe7\x94\xbb\xe9\x9d\xa2\xe3\x81\xae\xe4\xb8\x80\xe8\xa6\xa7\xe3\x81\xab\xe3\x80\x8creadme.ipynb\xe3\x80\x8d\xe3\x81\x8c\xe8\xa1\xa8\xe7\xa4\xba\xe3\x81\x95\xe3\x82\x8c\xe3\x81\x9f\xe3\x82\x89\xe3\x80\x81\xe3\x82\xaf\xe3\x83\xaa\xe3\x83\x83\xe3\x82\xaf\xe3\x81\x97\xe3\x81\xa6\xe3\x80\x8creadme.ipynb\xe3\x80\x8d\xe3\x81\xa7\xe3\x81\xae\xe6\x93\x8d\xe4\xbd\x9c\xe3\x81\xab\xe7\xa7\xbb\xe3\x82\x8b\xe3\x80\x82n"",\n    ""n"",\n    ""## JupyterLab\xe3\x82\xb3\xe3\x83\xb3\xe3\x83\x86\xe3\x83\x8a\xe5\x86\x85\xe3\x81\x8b\xe3\x82\x89docker\xe3\x82\xb3\xe3\x83\x9e\xe3\x83\xb3\xe3\x83\x89\xe3\x82\x92\xe4\xbd\xbf\xe3\x81\x88\xe3\x82\x8b\xe3\x82\x88\xe3\x81\x86\xe3\x81\xab\xe3\x81\x99\xe3\x82\x8bn"",\n    ""n"",\n    ""\xe5\xbf\xb5\xe3\x81\xae\xe3\x81\x9f\xe3\x82\x81\xe3\x80\x81\xe3\x82\xb3\xe3\x83\xb3\xe3\x83\x86\xe3\x83\x8a\xe5\x86\x85\xe3\x81\xae\xe3\x83\xa2\xe3\x82\xb8\xe3\x83\xa5\xe3\x83\xbc\xe3\x83\xab\xe3\x82\x92update\xe3\x81\x97\xe3\x81\xa6\xe3\x81\x8b\xe3\x82\x89\xe3\x80\x81docker\xe3\x82\xb3\xe3\x83\x9e\xe3\x83\xb3\xe3\x83\x89\xe3\x82\x92\xe3\x82\xa4\xe3\x83\xb3\xe3\x82\xb9\xe3\x83\x88\xe3\x83\xbc\xe3\x83\xab\xe3\x81\x99\xe3\x82\x8b\xe3\x80\x82n""\n   ]\n  },\n  {\n   ""cell_type"": ""code"",\n   ""execution_count"": null,\n   ""metadata"": {},\n   ""outputs"": [],\n   ""source"": [\n    ""%%bashn"",\n    ""sudo -in"",\n    ""apt-get updaten"",\n    ""apt-get upgrade -yn"",\n    ""apt-get install -y curln"",\n    ""curl -fsSL https://get.docker.com/builds/Linux/x86_64/docker-17.05.0-ce.tgz | tar -xzC /usr/local/bin --strip=1 docker/dockern"",\n    ""docker --version""\n   ]\n  },\n  {\n   ""cell_type"": ""markdown"",\n   ""metadata"": {},\n   ""source"": [\n    ""## naumanni\xe3\x81\xae\xe3\x83\x93\xe3\x83\xab\xe3\x83\x89\xe7\x92\xb0\xe5\xa2\x83\xe3\x82\x92\xe6\x95\xb4\xe3\x81\x88\xe3\x82\x8bn"",\n    ""n"",\n    ""\xe3\x81\xbe\xe3\x81\x9a\xe3\x81\xafnode.js\xe3\x81\x8b\xe3\x82\x89\xe3\x80\x82n"",\n    ""node >= v7.5.0 \xe3\x81\xa7\xe3\x81\x82\xe3\x82\x8b\xe3\x81\x93\xe3\x81\xa8\xe3\x82\x92\xe7\xa2\xba\xe8\xaa\x8d\xe3\x81\x99\xe3\x82\x8b\xe3\x80\x82n""\n   ]\n  },\n  {\n   ""cell_type"": ""code"",\n   ""execution_count"": null,\n   ""metadata"": {},\n   ""outputs"": [],\n   ""source"": [\n    ""%%bashn"",\n    ""sudo -in"",\n    ""curl -sL https://deb.nodesource.com/setup_7.x | sudo -E bash -n"",\n    ""apt-get install -y nodejsn"",\n    ""nodejs -v""\n   ]\n  },\n  {\n   ""cell_type"": ""markdown"",\n   ""metadata"": {},\n   ""source"": [\n    ""\xe6\xac\xa1\xe3\x81\xabyarn\xe3\x80\x82n"",\n    ""yarn >= 0.23.4 \xe3\x81\xa7\xe3\x81\x82\xe3\x82\x8b\xe3\x81\x93\xe3\x81\xa8\xe3\x82\x92\xe7\xa2\xba\xe8\xaa\x8d\xe3\x81\x99\xe3\x82\x8b\xe3\x80\x82n""\n   ]\n  },\n  {\n   ""cell_type"": ""code"",\n   ""execution_count"": null,\n   ""metadata"": {},\n   ""outputs"": [],\n   ""source"": [\n    ""%%bashn"",\n    ""sudo -in"",\n    ""curl -sS https://dl.yarnpkg.com/debian/pubkey.gpg | apt-key add -n"",\n    ""echo ""deb https://dl.yarnpkg.com/debian/ stable main"" | tee /etc/apt/sources.list.d/yarn.listn"",\n    ""apt-get updaten"",\n    ""apt-get install -y yarnn"",\n    ""yarn""\n   ]\n  },\n  {\n   ""cell_type"": ""markdown"",\n   ""metadata"": {},\n   ""source"": [\n    ""## naumanni\xe3\x81\xae\xe3\x82\xbd\xe3\x83\xbc\xe3\x82\xb9\xe3\x82\x92\xe3\x82\xaf\xe3\x83\xad\xe3\x83\xbc\xe3\x83\xb3\xe3\x81\x99\xe3\x82\x8bn""\n   ]\n  },\n  {\n   ""cell_type"": ""code"",\n   ""execution_count"": null,\n   ""metadata"": {},\n   ""outputs"": [],\n   ""source"": [\n    ""%%bashn"",\n    ""rm -rf naumannin"",\n    ""git clone --depth 1 https://github.com/naumanni/naumanni.git""\n   ]\n  },\n  {\n   ""cell_type"": ""markdown"",\n   ""metadata"": {},\n   ""source"": [\n    ""## naumanni\xe3\x82\x92\xe3\x83\x93\xe3\x83\xab\xe3\x83\x89\xe3\x81\x99\xe3\x82\x8bn""\n   ]\n  },\n  {\n   ""cell_type"": ""code"",\n   ""execution_count"": null,\n   ""metadata"": {},\n   ""outputs"": [],\n   ""source"": [\n    ""%%bashn"",\n    ""cd naumannin"",\n    ""yarnn"",\n    ""yarn run build""\n   ]\n  },\n  {\n   ""cell_type"": ""markdown"",\n   ""metadata"": {},\n   ""source"": [\n    ""## naumanni\xe3\x81\xae\xe3\x82\xb3\xe3\x83\xb3\xe3\x83\x86\xe3\x83\x8a\xe3\x82\x92\xe3\x83\x93\xe3\x83\xab\xe3\x83\x89\xe3\x81\x99\xe3\x82\x8bn""\n   ]\n  },\n  {\n   ""cell_type"": ""code"",\n   ""execution_count"": null,\n   ""metadata"": {},\n   ""outputs"": [],\n   ""source"": [\n    ""%%bashn"",\n    ""sudo docker rm -f noran"",\n    ""sudo docker rmi nora/naumannin"",\n    ""cd naumannin"",\n    ""sudo docker build -t nora/naumanni .n"",\n    ""sudo docker images""\n   ]\n  },\n  {\n   ""cell_type"": ""markdown"",\n   ""metadata"": {},\n   ""source"": [\n    ""## naumanni\xe3\x82\x92\xe5\xae\x9f\xe8\xa1\x8c\xe3\x81\x99\xe3\x82\x8bn"",\n    ""n"",\n    ""n"",\n    ""n"",\n    ""\xe3\x81\xa7\xe3\x82\xa2\xe3\x82\xaf\xe3\x82\xbb\xe3\x82\xb9\xe3\x81\xa7\xe3\x81\x8d\xe3\x82\x8b\xe3\x80\x82n""\n   ]\n  },\n  {\n   ""cell_type"": ""code"",\n   ""execution_count"": null,\n   ""metadata"": {},\n   ""outputs"": [],\n   ""source"": [\n    ""%%bashn"",\n    ""sudo docker run -d -p 80:80 --name nora nora/naumanni""\n   ]\n  },\n  {\n   ""cell_type"": ""markdown"",\n   ""metadata"": {},\n   ""source"": [\n    ""## \xe6\x94\xb9\xe8\xb1\xa1n"",\n    ""n"",\n    ""\xe5\xb7\xa6\xe3\x81\xab\xe8\xa1\xa8\xe7\xa4\xba\xe3\x81\x95\xe3\x82\x8c\xe3\x81\xa6\xe3\x81\x84\xe3\x82\x8b\xe3\x80\x8cFiles\xe3\x80\x8d\xe3\x81\x8b\xe3\x82\x89\xe3\x80\x81n"",\n    ""n"",\n    ""naumanni > src > js > utils > html.es6n"",\n    ""n"",\n    ""\xe3\x81\xa8\xe8\xbe\xbf\xe3\x81\xa3\xe3\x81\xa6\xe3\x80\x81html.es6\xe3\x82\x92\xe3\x83\x80\xe3\x83\x96\xe3\x83\xab\xe3\x82\xaf\xe3\x83\xaa\xe3\x83\x83\xe3\x82\xaf\xe3\x81\x99\xe3\x82\x8b\xe3\x81\xa8\xe3\x80\x81html.es6\xe3\x82\x92\xe7\xb7\xa8\xe9\x9b\x86\xe3\x81\x99\xe3\x82\x8b\xe3\x82\xbf\xe3\x83\x96\xe3\x81\x8c\xe9\x96\x8b\xe3\x81\x8f\xe3\x80\x82n"",\n    ""n"",\n    ""_expandMastodonStatus()\xe3\x81\xa8\xe3\x82\x86\xe3\x83\xbc\xe3\x80\x81\xe3\x82\xb5\xe3\x83\xbc\xe3\x83\x90\xe3\x81\x8b\xe3\x82\x89\xe5\x8f\x96\xe3\x81\xa3\xe3\x81\xa6\xe3\x81\x8d\xe3\x81\x9f\xe5\x91\x9f\xe3\x81\x8d\xe3\x82\x92\xe6\xad\xa3\xe8\xa6\x8f\xe5\x8c\x96\xe3\x81\x99\xe3\x82\x8b\xe5\x87\xa6\xe7\x90\x86\xe3\x81\x8c\xe3\x81\x82\xe3\x82\x8b\xe3\x81\xae\xe3\x81\xa7\xe3\x80\x81n"",\n    ""n"",\n    ""n"",\n    ""n"",\n    ""\xe3\x81\xa81\xe8\xa1\x8c\xe8\xb6\xb3\xe3\x81\x99\xe3\x81\xa8\xe3\x80\x81\xe3\x81\xbf\xe3\x82\x93\xe3\x81\xaa\xe3\x81\xae\xe5\x91\x9f\xe3\x81\x8d\xe3\x82\x92\xe5\xbc\xb7\xe5\x88\xb6\xe7\x9a\x84\xe3\x81\xab\xe6\x9b\xb8\xe3\x81\x8d\xe6\x8f\x9b\xe3\x81\x88\xe3\x82\x8b\xe3\x81\x93\xe3\x81\xa8\xe3\x81\x8c\xe3\x81\xa7\xe3\x81\x8d\xe3\x82\x8b\xe3\x80\x82n"",\n    ""n"",\n    ""\xe5\xa4\x89\xe6\x9b\xb4\xe3\x82\x92\xe4\xbf\x9d\xe5\xad\x98\xe3\x81\x97\xe3\x81\x9f\xe5\xbe\x8c\xe3\x80\x81\xe3\x83\x93\xe3\x83\xab\xe3\x83\x89\xe3\x83\xbb\xe3\x82\xb3\xe3\x83\xb3\xe3\x83\x86\xe3\x83\x8a\xe3\x81\xae\xe3\x83\x93\xe3\x83\xab\xe3\x83\x89\xe3\x83\xbb\xe5\xae\x9f\xe8\xa1\x8c\xe3\x81\xa7\xe3\x80\x81\xe5\x8b\x95\xe4\xbd\x9c\xe3\x82\x92\xe7\xa2\xba\xe8\xaa\x8d\xe3\x81\xa7\xe3\x81\x8d\xe3\x82\x8b\xe3\x80\x82n""\n   ]\n  }\n ],\n ""metadata"": {\n  ""kernelspec"": {\n   ""display_name"": ""Python 3"",\n   ""language"": ""python"",\n   ""name"": ""python3""\n  },\n  ""language_info"": {\n   ""codemirror_mode"": {\n    ""name"": ""ipython"",\n    ""version"": 3\n   },\n   ""file_extension"": "".py"",\n   ""mimetype"": ""text/x-python"",\n   ""name"": ""python"",\n   ""nbconvert_exporter"": ""python"",\n   ""pygments_lexer"": ""ipython3"",\n   ""version"": ""3.5.2""\n  }\n },\n ""nbformat"": 4,\n ""nbformat_minor"": 2\n}\n'"
https://github.com/solvcon/solvcon,A software framework of conservation-law solvers that use the space-time Conservation Element and Solution Element (CESE) method.,"b'SOLVCON implements conservation-law solvers that use the space-time\n__.\n\n|rtd_status|\n\n.. |rtd_status| image:: https://readthedocs.org/projects/solvcon/badge/?version=latest\n  :target: http://doc.solvcon.net/en/latest/\n  :alt: Documentation Status\n\nInstall\n=======\n\nClone from https://github.com/solvcon/solvcon::\n\n  $ git clone https://github.com/solvcon/solvcon\n\nSOLVCON needs the following packages: A C/C++ compiler supporting C++11, _ 3.7+, \nGit master,  3.6+, _ 0.16+, _ 1.5+,\n,  4+, _ 6.0+, _ 1.0+, _ 1.14+, _ 2.29.1+, and \n3+.  Support for  is to be enabled for conda\nenvironment.\n\nTo install the dependency, run the scripts  and\n (they use __).\n\nThe development version of SOLVCON only supports local build::\n\n  $ make; python setup.py build_ext --inplace\n\nTo build SOLVCON from source code and install it to your system::\n\n  $ make; make install\n\nTest the build::\n\n  $ nosetests --with-doctest\n  $ nosetests ftests/gasplus/*\n\nBuilding document requires _ 1.3.1+, _ 0.3.4+, and _ 2.28+.  Use the following command::\n\n  $ make -C doc html\n\nThe document will be available at .\n'"
https://github.com/pdfliberation/pdf_table_extraction,experimenting with pdf2text and python pdf-table-extract,"b'experiment_table-extraction\n===========================\n\nusing poppler, pdf-table-extract and numpy to extract tabular data\n\n# visuals\n\nThe ""visuals"" folder contains the output of   with additional css and js code to visualize the locations of all detected text.\n\n    pdftoppm -r 150 -png -f 1 -l 1 pdf_documents/DHS.TSA.2013.pdf visuals/DHS.TSA.2013.png\n\n    pdftotext -r 150 -bbox -f 1 -l 1 pdf_documents/DHS.TSA.2013.pdf visuals/DHS.TSA.2013.html\n\n# table-extract\n\nThe ""table-extract"" folder looks at the performance of the pdf-table-extract library on various source documents. Also some rough code for generating pandas dataframes from the output.\n'"
https://github.com/nvasilius/PyBootCamp,Files for the uDemy Complete Python Bootcamp,b'# PyBootCamp\nFiles for the uDemy Complete Python Bootcamp\n'
https://github.com/vishal8266/Tensorflow-Project---Bank-Note-Classification,Tensorflow Project - Bank Note Classification,"b'# Tensorflow-Project---Bank-Note-Classification\nTensorflow Project - Bank Note Classification\n\nUCI - banknote authentication Data Set\n\nData Set Information:\n\nData were extracted from images that were taken from genuine and forged banknote-like specimens. For digitization, \nan industrial camera usually used for print inspection was used. The final images have 400x 400 pixels. \nDue to the object lens and distance to the investigated object gray-scale pictures with a resolution of about 660 dpi were gained. \nWavelet Transform tool were used to extract features from images. \n'"
https://github.com/brakmic/Julia-Articles,Code examples from my articles on the Julia programming language,b'# Julia-Articles\n\nCode examples from my articles on the Julia programming language\n\n# License\n\nMIT\n'
https://github.com/qmcs/qmcs.github.io,EECS Society. Queen Mary University of London,"b""EECS society at Queen Mary\n==========================\n\n\nThis is the content of the . You are\nwelcome to contribute articles on any more or less technical topic.\n\nSharing your knowledge is cool. You can always mention in your CV that you\ncontribute to a blog, know git, familiar with peer reviews, and able to read\ndocumentation.\n\nYou can also directly point to your work. Your next employer will definitely\nlike it.\n\nWriting an article\n------------------\n\nThe content of the blog is hosted on  in the  folder.\n\n3. Name the file as , where\n\n   *  is a number of the article. Articles are numbered sequentially, pick\n     the next integer from the the _ directory.\n\n   *  is an informative but short identifier of the article.\n\n   *  is the file format we are using. It allows to define basic\n     formatting, such as headers, links or references. Refer to the\n     _ for examples.\n\n4. Apart form the article, the file should contain some meta information, such\n   as the name of the author, the date the article was written, its tags and\n   category. Here is an example, copy, paste and modify it:\n\n   .. code-block:: rst\n\n       ==========================================\n       An example of an article in the rst format\n       ==========================================\n\n       .. Some metadata\n\n       :date: 2014-04-02 15:00\n       :tags: technology, example\n       :category: blog\n       :author: Dmitrijs Milajevs\n\n       The first paragraph should introduce and possibly summarize the article.\n       It should be relatively short: 2 - 3 sentences.\n\n       .. Explicitly mark the end of the summary/introduction\n\n       -- PELICAN_END_SUMMARY --\n\n       .. Here goes the rest of the article.\n\n       This is a header\n       ================\n\n       This is the second paragraph. You can mark some text as bold or .\n\n5. Write the article!\n\n6. Save the file by clicking a light green button in the bottom right corner of the page.\n\n7. Now you are ready to create a !\n\n   a) Go to ,\na \nor  to get a general idea.\n\n\n1. Before writing an article, clone the repo:\n\n.. code-block:: bash\n\n    git clone git@github.com:username/qmcs.github.io\n    cd qmcs.github.io\n\n2. Write an article!\n\n3. Commit and push your changes:\n\n.. code-block:: bash\n\n    git st  # see what files have been changed\n    git diff  # see the actual changes\n    git add RELATED_FILES  # probably, somethig like content/articles/001-intro.rst\n    git ci -m'An article describing the enterprise (R) power of Java.'\n    git push  # send you changes to github\n\nCreate a .\n\nPersonal page\n~~~~~~~~~~~~~\n\nYou can add information about yourself, such as a brief description of who you\nare, your interests, your homepage and contact information, and, most\nimportantly, a picture.\n\nAuthor bibliographies are stored in  enjoys programming since he was a\n    teenager. He is interested in natural language processing.\n\n    -- PELICAN_END_SUMMARY --\n\n    He spends working days in his office surrounded by monitors and pile of\n    scientific papers. On a weekend, he escapes the office and spends most of the\n    day in a coffee shop somewhere in Central London. To compensate time spent\n    sitting, he does Modern Pentathlon.\n\n    You can find him (re)tweeting as \n    and showing off his  on Linkedin.\n\n\n    __ https://www.linkedin.com/in/dmitrijsmilajevs\n\nThe first paragraph should be short and clear. Note usage of\n to mark the end of the summary.\n\nMetadata field prefixed with  will appear as icons to the listed\nwebsites. Use names of the services that are available in __.\n\nThe cover image is a 461x461 picture of you or an avatar and should be located\nin .\n\nPut you CV to  and add the  metadata field.\n\nArticle with Cover Photo (alternative)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nYou can, instead of using the standard article template, use a template which\nincludes a cover photo \xe2\x80\x93 much inspired from medium.com. Here is how to use\nit. Beware, the procedure looks different in Markdown and RST.\n\nMeta header in Markdown:\n\n.. code-block:: md\n\n    Title: Space Kittens\n    Date: 2014-04-15\n    Tags: markdown, markup, languages\n    Category: languages\n    Author: Henrik O. Skogmo\n    Template: article_cover\n    Cover: space-cat.png\n\nMeta header in RST:\n\n.. code-block:: rst\n\n    =============\n    Space Kittens\n    =============\n\n    :date: 2014-04-15\n    :tags: markdown, markup, languages\n    :category: languages\n    :author: Henrik O. Skogmo\n    :template: article_cover\n    :cover: space-cat.png\n\n1. Write your article as you would otherwise, following the  guide in this readme.\n\n2. Change the meta  line to look like this\n instead of . This will trigger\nthe system to use the alternative template.\n\n3. Add the line  underneath. Here I use the photo named\n which I added in the folder .\nThis is how we tell the system which photo to use as a cover photo.\n\nNow all you have to do is commit and push your article, together with your \ndesired photo.\n\nPeer review\n-----------\n\nEvery article should be reviewed by two people. You are welcome to go trough any\nopen pull request and comment on the things you like or dislike. If you find the\nchanges to be merged, write a comment::\n\n :+1:\n\nIt's completely fine to comment about anything, but it's important to be polite,\nprecise and constructive.\n\nTo speed up the process assign someone from the team to do peer review. If your\narticle got comments from someone else, please fix them in a timely manner. The\nsooner you fix all the issues, the sooner the article appears on the website.\n\nGenerating the blog locally\n---------------------------\n\nWe use _ to deploy\nneeded software. A typical biuldout deployment consists of two steps:\nbootstrapping and building out.\n\nBootstraping is simple::\n\n    python bootstrap.py\n\nIn case you get an error about setuptools, you can install them:\n\n.. code-block:: bash\n\n    # Only if you get an error in the previus step!\n    python ez_setup.py --user\n    python bootstrap.py\n\nNow you are ready to ::\n\n    bin/buildout\n\nAn easy way to see rendered article files\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nYou can use _ to see rendered\n  or _ for  files in\nyour browser. For example:\n\n.. code-block:: bash\n\n    bin/restview content/articles/001-intro.rst  # to see the intro article\n    bin/meow content/articles/009-markdown.md  # to see the Markdown article\n\nThere are rumors, that you can feed a directory to restview and then select\nfiles in the browser::\n\n    bin/restview content\n\nGenerating the HTML version of a blog locally\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nNow, you can get a local version of the blog:\n\n.. code-block:: bash\n\n    make devserver\n    open http://localhost:8000  # gnome-open on Linux\n    # make stopserver is a logical way to stop the server\n\n\nDeveloping the theme and plugins\n--------------------------------\n\nOur blog uses a custom theme and plugins. The theme and the plugins are external\nprojects and don't belong to this git repository! However, during the\n step they are cloned to the  folder, thanks to . Here are the external\nprojects we depend on:\n\n.. code-block:: bash\n\n    tree -L 1 src/\n    src/\n    \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 pelican-plugins  # Extenal plugins. Don't bother about it.\n    \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 pelican_extended_authors # Our plugin that provided authors' metadata.\n    \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 pelicanium  # The theme we use.\n\nNote that, by default .\n\n.. code-block:: bash\n\n    rm src/ -rf  # Again, be vary careful!\n\n4. Run .\n\nChange remote urls in git repo\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIn case you want to push to github after you run , you need to\nchange remote urls by yourself, for example:\n\n.. code-block:: bash\n\n    cd src/pelicanium\n    git remote set-url origin git@github.com:YOUR_USERNAME/qmcs.github.io\n\nUpdate dependencies\n~~~~~~~~~~~~~~~~~~~\n\nIf you want to update the dependencies, run::\n\n    bin/develop up\n\nAdd a remote\n~~~~~~~~~~~~\n\nIn case you want to refer not only to your repo, but to others, you need to add\nanother remote:\n\n.. code-block:: bash\n\n    git remote add upstream git@github.com:qmcs/qmcs.github.io\n\nNow you can merge with the recent  branch:\n\n.. code-block:: bash\n\n    git checkout pelican\n    git fetch upstream\n    git merge upstream/pelican\n\nYou can also checkout feature branches:\n\n.. code-block:: bash\n\n    git checkout -b theme upstream/theme  # Get the theme branch from upstream\n    git push -u theme origin/theme  # Push it to your fork and set it as the default push destination\n\nUpdating the web site\n---------------------\n\nIn case you are lucky and have write access to the main repo you can upload the\ngenerated HTML version of the site, however you need to clone\n.\n\nTo upload the HTML just run::\n\n    make github\n\nLicense\n-------\n\n.. image:: http://i.creativecommons.org/l/by/4.0/80x15.png\n\nThis work is licensed under a _.\n"""
https://github.com/brunoperry/Udacity,Deep Learning Nanodegree Foundation,b'# Udacity DeepLearning Foundation\n'
https://github.com/hkaushalya/CMSPubWordFreqAnalysis,CDF/CMS Publications' WORD analysis.,"b""CMSPubWordFreqAnalysis\n======================\n\nCDF/CMS Publications' WORD analysis.\n"""
https://github.com/anubhav-reddy/Fun-Projects,Practice and Fun Project Code,b'This repository contains some fun little code and projects that I made for learning.\n'
https://github.com/natalia2q/Misc_Python,a mix of Python code that I've been working.,"b""# Misc_Python\na mix of Python code that I've been working.\n"""
https://github.com/davebiagioni/pylighter,"Simple, score-based text highlighting for Python","b""# pylighter\nSimple, score-based text highlighting for Python\n\nExample usage:\n\n\n\nRenders:\n\n\n\nSee  for additional examples.\n"""
https://github.com/llathrop/AIND-Recognizer,Udacity AIND Recognizer project,"b'# Artificial Intelligence Engineer Nanodegree\n## Probabilistic Models\n## Project: Sign Language Recognition System\n\n### Install\n\nThis project requires Python 3 and the following Python libraries installed:\n\n- \n- \n- \n- \n- \n- \n- \n\nNotes: \n1. It is highly recommended that you install the  distribution of Python and load the environment included in the ""Your conda env for AI ND"" lesson.\n2. The most recent development version of hmmlearn, 0.2.1, contains a bugfix related to the log function, which is used in this project.  In order to install this version of hmmearn, install it directly from its repo with the following command from within your activated Anaconda environment:\n\n\n### Code\n\nA template notebook is provided as . The notebook is a combination tutorial and submission document.  Some of the codebase and some of your implementation will be external to the notebook. For submission, complete the Submission sections of each part.  This will include running your implementations in code notebook cells, answering analysis questions, and passing provided unit tests provided in the codebase and called out in the notebook. \n\n### Run\n\nIn a terminal or command window, navigate to the top-level project directory  (that contains this README) and run one of the following command:\n\n\n\nThis will open the Jupyter Notebook software and notebook in your browser which is where you will directly edit and run your code. Follow the instructions in the notebook for completing the project.\n\n\n### Additional Information\n##### Provided Raw Data\n\nThe data in the  directory was derived from \nthe . \nThe handpositions () are pulled directly from \nthe database . The three markers are:\n\n*   0  speakers left hand\n*   1  speakers right hand\n*   2  speakers nose\n*   X and Y values of the video frame increase left to right and top to bottom.\n\nTake a look at the sample \nto see how the hand locations are tracked.\n\nThe videos are sentences with translations provided in the database.  \nFor purposes of this project, the sentences have been pre-segmented into words \nbased on slow motion examination of the files.  \nThese segments are provided in the  and  files\nin the form of start and end frames (inclusive).\n\nThe videos in the corpus include recordings from three different ASL speakers.\nThe mappings for the three speakers to video are included in the  \nfile.\n'"
https://github.com/LiXiling/ml_insurancePred,Insurance Claim Prediction using Machine Learning - Udacity Nanodegree Capstone Project,"b'# ml_insurancePred\nInsurance Claim Prediction using Machine Learning \n------------\nThis is my ""Capstone Project"" for the Udacity Machine Learning Engineer Nanodegree (https://www.udacity.com/course/machine-learning-engineer-nanodegree--nd009)\n\nIt uses the data provided by a competition hosted on kaggle.com by Allstate (https://www.kaggle.com/c/allstate-claims-severity).\n\n## Contents:\n* /docu/Capstone_proposal.pdf - the proposal file for this Capstone Project\n* /docu/Capstone_report.pdf - the final report for this Capstone Project\n* /insurance_prediction.ipynb - the developed code, as a IPython Notebook\n* /docu/insurance_prediction.html - the IPython Notebook exported as HTML\n* /data/train.csv - the dataset used in this project\n\n## Requirements\n* Python 2.7\n* scikit-learn 0.17\n'"
https://github.com/eisproject/levin,Lancaster Energy Visualization Interface,b'# levin\nLancaster Energy Visualization Interface\n'
https://github.com/shead-custom-design/pipecat,"Elegant, flexible data logging in Python for connected sensors and instruments.","b'# Welcome!\n\n\n\nWelcome to Pipecat ... elegant, flexible data logging in Python for\nconnected sensors and instruments.  Use Pipecat to log data from\nbattery chargers, GPS, automobiles, gyros, weather, and more!\n\nHere are some devices supported by Pipecat and examples of how to log their data:\n\n* .\n*  that generate NMEA data.\n* Vehicles that generate OBD-II data.\n* Motion (accelerometer) data from iOS devices.\n* METAR (aviation weather) data from the National Weather Service.\n* Any device that communicate over a serial port.\n* Any device that can handle HTTP GET requests.\n* Any device that can write to a socket using UDP.\n* Any device that can generate XML data.\n\nYou can see the full Pipecat documentation with tutorials at\nhttps://pipecat.readthedocs.io ... for questions, comments, or suggestions, get\nin touch with our team at https://gitter.im/shead-custom-design/pipecat.\n\nLicense\n=======\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see .\n'"
https://github.com/bdhammel/machine-learning-models,Tensorflow scripts of different machine learning algorithms ,"b'# Basic machine learning models for reference\n\nAlgorithms benchmarked to the MNIST dataset\n\n## Principle Component Analysis (PCA)\n\n  \nNaive Bayes accuracy: 56%\n\n## Artificial Neural Network (ANN)\n\nSimple fully-connected network\n\nMNIST accuracy: 96%\n\n## Convolutional Neural Network (CNN)\n\nSimple convolutional neural network\n\nMNIST accuracy: 98%\n\nNotes: Slow, only ran for 2 epoch. \n\n## Convolutional Neural Network (VGG16)\n\nImplementation of the VGG16 architecture with pretrained MNIST data\n\n  \n cite: https://www.cs.toronto.edu/~frossard/post/vgg16/ \n\nNote: Because MNIST is 28x28, first 10 layers omitted and layer depth made a factor of 3 smaller (as MNIST has only 1 color layer)\n\nMNIST accuracy: \n\n## Stacked Autoencoder (X-wing AE)\n\nA symmetric autoencoder for unsupervised learning \n\nNotes: Success is largely dependent on layer sizes, seems to have a small window of convergence\n\n\n\n## Convolutional Autoencoder (CAE)\n\nA symmetric convolutional autoencoder\n\n\n\n## Variational Autoencoder (VAE)\n\n\n\n## Generative Adversarial Network (DCGAN)\n\nA Deep Convolutional GAN\n\n - \n\n\n\n## Recurrent Neural Network (RNN)\n\nTODO: Add in additional hidden layers\n\n### Basic RNN cell\nMNIST accuracy: 98%   \nepochs: 100\n\n### LSTM cell\nMNIST accuracy: 95%   \nepochs: 10\n'"
https://github.com/rjleveque/amath584a2016,"UW Applied Math 584, Autumn 2016","b'# amath584a2016\n\nUW Applied Math 584, Autumn 2016\n\nSee \nfor a rendered version of the files in the  directory.\n\nSee\n\nfor rendered versions of the Jupyter notebooks.\n\nTo run the notebooks on the cloud you can use \nby clicking on the lauch icon below:\n\n\n\n'"
https://github.com/arve0/microscopestitching,Automatic merge/stitching of regular spaced images,"b""# microscopestitching\n\n[![build-status-image]][travis]\n[![pypi-version]][pypi]\n[![wheel]][pypi]\n\n## Overview\n\nThis software aims to be a reliable way to stitch your microscope images. To\nget good results, a couple of assumptions should be true about your dataset:\n\n- images are regular spaced\n- images are of same size\n- side by side images have translation only in one dimension\n  - (if not, check your scanning mirror rotation)\n- scale in edge of images are constant\n\n## Installation\n\nInstall using ...\n\n\n\n## Example\n\n\nSee also .\n\n## API reference\n\nAPI reference is at http://microscopestitching.rtfd.org.\n\n## Development\nInstall dependencies and link development version of microscopestitching to pip:\n\n\n### Testing\n\n\n### Build documentation locally\nTo build the documentation:\n\n\n\n\n[build-status-image]: https://secure.travis-ci.org/arve0/microscopestitching.png?branch=master\n[travis]: http://travis-ci.org/arve0/microscopestitching?branch=master\n[pypi-version]: https://img.shields.io/pypi/v/microscopestitching.svg\n[pypi]: https://pypi.python.org/pypi/microscopestitching\n[wheel]: https://img.shields.io/pypi/wheel/microscopestitching.svg\n"""
https://github.com/marisarivera/Arrivals,Análisis Preliminar,b'# Arrivals\nAn\xc3\xa1lisis Preliminar\n'
https://github.com/ericinlinux/BDiSN_Assignment_6_old,Deprecated.,"b'# Glascow research on smoking habits in teenagers\n\nAll the data was taken from .\n\nThis folder contains the data and the Python code for the data set provided at http://tinyurl.com/hunndyg\n\n## The matrix of friendships\n\nWe have three matrices with the question about the relations in 3 different years.\n\nThe data are valued; code 1 stands for ""best friend"", code 2 for ""just a friend"", and code 0 for ""no friend"".\nCode 10 indicates structural absence of the tie, i.e., at least one of the involved students was not yet part of the\nschool cohort, or had already left the school cohort at the given time point.\n\nFor this work, we will make the following changes:\n\n* code 1 = 0.9\n* code 2 = 0.5\n* code 10 = 0\n* code 0 = 0.1\n\n## The information about smoking habits\n\nTobacco use has the scores 1 (non), 2 (occasional) and 3 (regular, i.e. more than once per week). So for this pattern\nthe values are initialized with values 0.1, 0.5 and 0.9 respectively.\n\nThe idea of this code is to simulate the opinion change over time according to the initial values and the network\nprovided by the data set.\n\n## The assignment\n\nThe document for the assignment is .\n'"
https://github.com/kho226/nueralNetwork,A nueral network built with the purpose of recognizing handwritten letters,"b'# NeuralNetwork\n## A neural network built with the purpose of recognizing handwritten characters.\n###### A brief overview...\nThe concepts that underpin neural Networks are both mathematical and cognitive in nature. Mathematical concepts include elements of linear algebra such as matrix multiplication and the transpose of a matrix, elements of calculus such as the derivative and gradient descent, elements of statistics such as the normal distribution and the standard deviation, and elements of algebra such as slope-intercept form and cognitive concepts such as a basic understanding of the structure of a neuron.\n\nComputers are effectively extremely fast calculators, which makes them perfect for carrying out calculations.\nHumans brains are technically slower than calculators, yet we are capable of doing things that computers cannot; such as recognizing handwritten characters or even faces.\n\nNeuralNetworks attempt to bridge the gap between the human brain and computers; allowing us to imbue computers with almost human-like cognitive abilities. Such as the ability to effectively teach itself.\n\nThe neuron is the most basic building block of intelligent life on earth. Neurons consist of dendrites, axons and terminals. A dendrite of one neuron will connect to the terminal of another neuron, and within each neuron axons connect dendrites and terminals. Neurons are effectively the circuitry of our brains. They take an electrical signal as input and output another electrical signal. The output of a neuron does not share a linear relationship with its input. Instead, a neuron will suppress its output until the input crosses a threshold. An activation function is a function that takes an input signal and generates and output signal, but takes into account some threshold. I will be using the Sigmoid function or logistic function but there are many other candidates to use for the activation function.\n\nWe can model the neurons of a human brain with a matrix of nodes, where each node in the matrix represents a neuron. Take for example a 2 x 2 matrix of nodes. There are a total of 4 nodes. Each node in the first column has a link to every node in the second column. Each of these links have an associated weight / probability as well as an activation function. The activation functions and link weights determine how information travels through the matrix.\n\nInformation can propagate forward and propagate backward through the matrix of nodes.\n\nPropagating information forward is carried out by applying the activation function to the dot product of the input matrix and the matrix representing the link weights between the input layer and the hidden layer. Repeating this algorithm for every matrix of link weights in the network. The resulting matrix we will refer to as the output Matrix\n\nPropagating information backwards is carried out in a similar fashion as propagating information forward. Take the dot product of the transpose of the matrix of respective link weights and the error matrix. The error matrix is the difference between the output Matrix and the matrix of expected values.\n\n[<marko.inline.RawText object at 0x000002CBAF3B9108>]\n\nA neural network learns by adjusting the weights associated with the links. Link weights are adjusted using gradient descent.\n\n#TODO...\n- [x] write skeleton of class\n- [x] wrote display function for testing purposes\n- [x] add numpy to package\n- [x] updated initializer to create matrices of link weights\n- [x] updated initializer to randomly select link weights based off of the normal distribution with mean = 0 and standard deviation = (# of incoming links to a node)^(-1/2)\n- [x] add scipy to the package\n- [x] write query method\n- [x] write train method\n- [x] updated GUI\n- [x] added main function\n- [x] clean up code in main()\n- [x] format README.md\n- []  build a basic website around the network\n\n\n\n# \n> moved development to Jupyter / IPython since the Python IDLE does not support matplotlib interactive updates\n'"
https://github.com/paulovn/docker-dl-gpu,Build a Docker image for Deep Learning processes running on GPUs,b'# docker-dl-gpu\nBuild a Docker image for Deep Learning processes running on GPUs\n'
https://github.com/priyaranjan1202/Machine-Learning,This repository contains  Machine learning projects I have been working on.Currently it contains projects from Udacity Nanodegree.,"b'## Machine-Learning\nThis repository contains  Machine learning projects I have been working on.Currently it contains projects from Udacity Nanodegree.\n\n## Requirments and Installation\n* All the code is written using python 2.\n* Each project cotains Related Data, IPYTHON notebook solution and supporting files.\n* Anaconda distribution server is recommened.\n* Open COMMAND PROMPT or TERMINAL and type  to start the server.\n* Navigate to required project and open corresponding .ipynb notebook. \n* Run each indivual cell to understand the project.\n\n\n\n   \n   \n'"
https://github.com/selvakumar-sss/FirstGitProject,Loading working jupyter notebooks,b'# FirstGitProject\nLoading working jupyter notebooks\n'
https://github.com/miguel-faria/robot-serving,Master Thesis on Human-Robot Collaboration,b'# robot-serving\r\nThis is the repo for my master thesis project on social robotics.\r\n'
https://github.com/RaoOfPhysics/phd-notebooks,R Notebooks for my PhD research,"b'# My PhD notebooks\n\n\n\n\n\nA collection of my PhD-related Jupyter notebooks, containing mostly WIP material, which serves as a scratchpad for my work.\n\n## Setup information\n\nI work mostly with , and thats what Ive used in these notebooks.\n\nPlease note that efforts to run the code in the  files involving the  data frame will fail, since I cannot make the underlying research data public yet.\nIn particular, when the following line is called in the code, the file it refers to () can only be found in my private storage space on CERNs internal servers:\n\n\n\nHowever, if you want to download the notebooks and poke around, you will need to install:\n\n- Jupyter, for opening the notebooks themselves: \n- IRkernel, for running R code in the notebooks: \n\n## Licence\n\nMy PhD notebooks by Achintya Rao is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.\n'"
https://github.com/chrisjsewell/pandas3js,a pandas dataframe interface for traitlets and pythreejs,"b'NOTE: this is deprecated, in favour of https://github.com/chrisjsewell/ase-notebook\n\n\n\n\n\n# pandas3js: 3D Graphics UIs in the Jupyter Notebook\n\nAn extension for  and  that:\n\n1. Provides a 2-way  dataframe interface for trait objects.\n2. Provides simple, high level (renderer agnostic) geometries, with default json specified mappings to pythreejs primitives.\n3. Creates bespoke 3D Graphics GUIs in the Jupyter Notebook with only a few lines of code.\n    \n## Examples\n\n\n\n\n\nFor more information, all functions contain docstrings with tested examples.\n\n## Installation\n\n    $ pip install pandas3js\n    $ jupyter nbextension enable --py --sys-prefix pythreejs\n\t\n is integration tested against python versions 2.7, 3.4, 3.5 and 3.6\n\n## Technical Details\n\nEmploying a meta Model/View design; Unique geometry objects are stored in a  model object, \nwhich can be viewed as (and modified by) a , containing objects (by row) and traits/object_type (by column). \nThe  (and its objects) can then be directionally synced to a  (and s) view, via a json mapping specification.\n'"
https://github.com/pprzetacznik/IElixir,Jupyter's kernel for Elixir programming language,"b'IElixir\n=======\n\nJupyters kernel for Elixir\n\n\n\n\n\n\n\n\nHex: https://hex.pm/packages/ielixir.\n\nPlease see generated documentation for implementation details: http://hexdocs.pm/ielixir/.\n\n## Getting Started\n\n### Table of contents\n\n* \n* \n* \n* \n* \n* \n* \n* \n* \n* \n* \n* \n\n### Configure Jupyter\n\nI recommend you to use  and  for this project to isolate dependencies between this and other projects however you may also work without this if you dont like this.\n\nNow you need to load  script into your current environment. I recommend you to add this like as well to the  script to have this script loaded every time you open fresh bash.\n\n\nNow using our new tools we can easily create isolated virtual environment for jupyter installation.\n\n\n### Configure IElixir\n\nClone IElixir repository and prepare the project\n\n\nIf youre meeting problem with missing zeromq header files then you should install it. .\n\nRunning all tests, including longer ones that requires more time for evaluation:\n\n\nThere may be also need to install rebar before IElixir installation, you can do this with command:\n\nAfter this you may need to add  to your  variable if you dont have  visible yet outside  directory.\n\n### Install Kernel\n\nSimply run installation script to create file  file in  directory and bind it to the jupyter:\n\n\n### Use IElixir\n\nRun Jupyter console with following line:\n\n\nTo quit IElixir type .\n\nRun Jupyter Notebook with following line:\n\n\nGo to  site (by default) in your browser and pick IElixir kernel:\n\n\n\nEvaluate some commands in your new notebook:\n\n\n\n### Magic commands\n\nYou can also use  variable to access output of previous cell. Moreover, if you want to access any cell which you can do it using its number by calling  map, eg. .\n\n### Package management with Boyle\n\nYou can manage your packages in runtime with Boyle. Name of the package honours remarkable chemist, Robert Boyle. This package allows you to manage your Elixir virtual enviromnent without need of restarting erlang virtual machine. Boyle installs environment into  directory and creates new mix project there with requested dependencies. It keeps takes care of fetching, compiling and loading/unloading modules from dependencies list of that environment.\n\nYou can also use this environment as a separate mix project and run it interactively with  from the environment directory.\n\n\nCreating new Elixir virtual environment\n\n\nList available virtual environments\n\n\nActivate virtual environment\n\n\nInstall new package in virtual environment and use new package\n\n\nDeactivate virtual environment and unload packages installed within that virtual environment\n\n\nAdditional resources:\n* \n* \n* \n\n### Developement mode\n\nIf you want to see requests passing logs please use  environment to see what is happening in the background.\n\n\n\n### Generate documentation\n\nRun following command and see  directory for generated documentation in HTML:\n\n\n### Docker\n\nYou can find docker image at .\n\nRunning jupyter notebook:\n\n\nDocker image is based on following images:\n*  - this is image use as a base for ielixir image,\n*  - some installation parts were taken from dockerfile used for this image,\n*  - this image resolves all dependencies for jupyter and elixir so only IElixir installation is left.\n\nIf you would like to make some changes to the images you can find dockerfiles in:\n* docker/ielixir - for dockerfile source of pprzetacznik/ielixir image,\n* docker/ielixir-requirements - for dockerfile source of pprzetacznik/ielixir-requirements image.\n\n#### Other docker images worth seeing\n\n* \n\n### Some issues\n\n#### ZeroMQ header files missing\n\n\n\nInstall ZeroMQ development package for you operating system.\n\n* On RHEL/Centos/Fedora \n* On OSX \n* On Ubuntu \n* On Alpine Linux \n\n#### Erlang configuration\n\nThere may be need to run IElixir kernel with specific erlang attribute which can be turned on by setting variable:\n\nThis option has been included inside  and  scripts.\n\n### Contribution\n\n* Try to write some description of the feature or bug fix youre working in pull requests description and concise description of new modules or functions in moduledoc annotations,\n* Please follow Elixir style guides to keep style clear, consider Elixir and Phoenix source code as the style ground truth,\n* Keep as little comments as you can, comments tend to expire so try to use doctests instead to show how your code works,\n* Write some unit tests for your code but dont try to test private functions, class tests are bad and units tests are good - https://blog.arkency.com/2014/09/unit-tests-vs-class-tests/\n\n### References\n\nSome useful articles:\n\n* \n* \n* \n* \n* \n\nI was inspired by following codes and articles:\n\n* \n* \n* \n* \n* \n\n### License\n\nCopyright 2015 Piotr Przetacznik.\nIElixir source code is released under Apache 2 License.\nCheck  and  files for more information.\n'"
https://github.com/Pavle992/Cash-me-outside,How about that!?,b'# Data Mining and Text Mining Course Project\n\n### Team: Cash-me-outside\n\n### Description\nThe repo contains [Python notebook][2] created as a result of [Data Mining and Text mining Course Project][1] at Politecnico di Milano in academic year 2016/2017.\n\n### Documentation\n+ [Project Presentation][4]\n+ [Description of our work][3]\n\n[1]: ./CourseProjectDescription.pdf \n[2]: ./CashMeOutside%20-%20Data%20Mining%20Project/Data%20Mining%20and%20Text%20Mining%20Course%20Project.ipynb\n[3]: ./CashMeOutside%20-%20Data%20Mining%20Project/DataMiningandTextMiningCourseProject.pdf\n[4]: ./CashMeOutside%20-%20Data%20Mining%20Project/Project%20Presentation.pptx.pptx'
https://github.com/mahkuss/inflammation,inflammation project,b'# Inflammation Project\n\nThis is the inflammation project\n'
https://github.com/ilevcovitz/econ_tutoring_sign_in,Sign In for the Econ Tutoring Center,"b""##Read Me\n\nThis is a repository for the web-based tutoring sign-in system for the Economics Tutoring center. The software provides a framework for students to sign in when visiting the tutoring center. \n\nAccurate student attendance information is necessary in order to research the effectiveness of the tutoring center and learn how to improve it. \n\nThe software addionally allows instructors to view which of their course's students visited the tutoring center. This real-time attendance information gives instructors the possibility to provide incentives for students to visit tutoring. \n\nAnnonymouse attendance information is also displayed in an online ipython notebook which auto-updates daily.\n\nThe software runs under a Python Flask framework and mySQL database.\n\n###Guides\n\nInformation on how to setup and use this software is provided in this repo in the documents: \n\nINITIAL_SETUP_GUIDE.md (for initial setup)\n\nDATABASE_SETUP.md (for a guide on the mysql database structure)\n"""
https://github.com/brent-lemieux/p2-baseball,An investigation into whether or not the same factors that are correlated with scoring runs are also correlated with player salaries,"b""# Baseball: Salaries and Wins\nAn investigation into whether or not the same factors that are correlated with players contributions to winning games are also correlated with salaries.  The goal is to identify any player types (or individuals) that are not valued properly by the MLB.\n\nI began by trimming the data to exclude years before 1980 because I am primarily concerned with the modern game.  I also created new metrics that are popular among stats minded teams and fans.  Read more here: https://en.wikipedia.org/wiki/Sabermetrics.\n\n### What factors are important to winning games?\n\nFirst I looked at the distribution of team wins for the data from 1980 on.  I expected the plot to be normal, but it is fairly negatively skewed.  It turns out this was mainly due to the player strike in 1994 that led to the season being shortened.\n\n\n\n\nHere is what the distribution looks like with 1994 removed:\n\n\n\nThe difference is subtle, but you can easily tell that there are far fewer teams with less than 60 wins.\n\nFor each team metric, I wanted to see what variables were most related to winning percentage.  I ran a simple linear regression on each metric with win percentage as the dependent variable.  It's worth noting that each variable is standardized.  As you can see, Runs and Runs Against have the highest and lowest coefficients, respectively.  This makes intuitive, because the more runs you score and the less runs you allow on average, the higher you'd expect your win percentage to be.  It's also worth noting that On Base Percentage (OBP), Slugging Percentage (SP), and OPS (On Base Plus Slugging) are all near the top.  These are the metrics I created using other metrics that already existed in the data.  It makes sense why these are so popular among stats gurus across MLB front offices.\n\n\n\nI also ran a Random Forest Regressor on all of the metrics at once and charted the feature importance or the information gain from each metric.  Once again, Runs and Runs Against are the most important features, with Earned Run Average (ERA) coming in third.  ERA is essentially the rate at which a pitcher allows runs that are explicitly that pitchers fault, so runs allowed due to fielding errors are not factored in.  Since most runs allowed are indeed the pitchers fault, it makes sense that this comes in third for feature importance.\n\n\n\n\n### What factors are important to scoring runs?\nBecause scoring runs is such an important part of winning games, we will now look at what factors are important to scoring runs at the team level.  After we have a good idea of what metrics correlate with scoring, we will be able to look at individual player data to see how those important factors lineup with salaries.\n\n# Work in progress...\n"""
https://github.com/grigi/pyza14-docs,PyConZA 2014 Documentation presentation.,b'# How Python helps writing documentation less painful\nPyConZA 2014 Documentation presentation.\n\n## Slides:\n \n\n'
https://github.com/kamidox/pandas_tutor,Tutorial for Pandas,b'## Pandas \xe6\x95\x99\xe7\xa8\x8b\xe7\xa4\xba\xe4\xbe\x8b\xe4\xbb\xa3\xe7\xa0\x81\n\nThis is the sample code for Tutorial for Pandas hosted in http://www.maiziedu.com/\n\n## \xe8\xaf\xbe\xe7\xa8\x8b\xe6\x80\x9d\xe7\xbb\xb4\xe5\xaf\xbc\xe5\x9b\xbe\n\nhttp://naotu.baidu.com/file/5eba96c2d922e30b7a4bf6b74c638dd0?token=440f2f0ff8c8b88f\n\n\xe6\x8f\x90\xe5\x8f\x96\xe5\xaf\x86\xe7\xa0\x81\xef\xbc\x9a706V\n\n\n'
https://github.com/youralien/sign_follower,base repo for scaffolded computer vision project for comprobo17,"b'# Sign Follower\n\nBy the end of the Traffic Sign Follower project, you will have a robot that will look like this!\n\nVideo Demo: \n\n## System\n![alt text][system-overview]\n\n[system-overview]: images/vision-nav-system-overview.png ""Three stages of the vision and navigation system: 1) waypoint navigation 2) sign recognition, and 3) sign obeyance via changing the next waypoint""\n\nNavigation and mapping is handled by the built-in ROS package  .  Mapping the location of the robot in the environment was handled by , a package that provides laser-based SLAM (simultaneous localization and mapping).  Navigation was handled by the  package;   our program published waypoints to the  topic while the internals of path planning and obstacle avoidance were abstracted away.\n\nYou will put your comprobo-chops to the test by developing a sign detection node which publishes to a topic, , once it is confident about recognizing the traffic sign in front of it.\n\n### Running and Developing the street_sign_recognizer node\n\nWeve provided some rosbags that will get you going.\n\n\n\n\n\nStart by replaying these rosbags on loop:\n\n\n\nThey have  channel recorded. In order to republish the compressed image as a raw image,\n\n\n\nTo run the node,\n\n\n\nIf you ran on the steps above correctly, a video window should appear visualizing the Neato moving towards a traffic sign.\n\n### Detecting Signs in Scene Images\nReliable detection of traffic signs and creating accurate bounding box crops is an important preprocessing step for further steps in the data pipeline.\n\nYou will implement code that will find the bounding box around the traffic sign in a scene. Weve outlined a suggested data processing pipeline for this task.\n\n1. colorspace conversion to hue, saturation, value ( seen in the top left window).\n2. a filter is applied that selects only for objects in the yellow color spectrum. The range of this spectrum can be found using hand tuning ( seen in the bottom window).\n3. a bounding box is drawn around the most dense, yellow regions of the image.\n\n![][yellow_sign_detector]\n[yellow_sign_detector]: images/yellow-sign-detector.gif ""Bounding box generated around the yellow parts of the image.  The video is converted to HSV colorspace, an inRange operation is performed to filter out any non yellow objects, and finally a bounding box is generated.""\n\nYou will be writing all your image processing pipeline within the  callback function. Here is what the starter code looks like so far.\n\n\n\nThe goal of localizing the signs in the scene is to determine  and  points that define the upper lefthand corner and lower righthand corner of a bounding box around the sign. You can do most of your work in the instance method .\n\n\n\nWhether you follow along with the suggested steps for creating a sign recognizer or have ideas of your own, revisit these questions often when designing your image processing pipeline:\n\n* What are some distinguishing visual features about the sign?  Is there similarities in color and/or geometry?\n* Since we are interested in generating a bounding box to be used in cropping out the sign from the original frame, what are different methods of generating candidate boxes?\n* What defines a good bounding box crop?  It depends a lot on how robust the sign recognizer you have designed.\n\nFinally, if you think that working with individual images, outside of the  class would be helpful -- I often like to prototype the computer vision algorithms I am developing in a jupyter notebook -- feel free to use some of the image frames in the  folder.  In addition, you can save your own images from the video feed by using OpenCVs .\n\n#### Red-Green-Blue to Hue-Saturation-Value Images\n\nThere are different ways to represent the information in an image. A gray-scale image has . An rgb image has shape  since it has three channels: red, green, and blue (note: as you saw in class, and it is the case with the given starter code, that OpenCV uses the channel ordering blue, green, red instead).\n\nColor images are also represented in different ways too.  Aside from the default RGB colorspace, there exists alot of others. Well be focused on using : Hue, Saturation, and Value/Luminosity. Like RGB, a HSV image has three channels and is shape . The hue channel is well suited for color detection tasks, because we can filter by color on a single dimension of measurement, and it is a measure that is invariant to lighting conditions.\n\n.\n\nA good first step would be convert  into an HSV image and visualize it. Like any good roboticist, visualize everything to make sure it meets your expectations.  Note: if you are using OpenCV 3.1 (which is the case for anyone on Kinetic and Ubuntu 16.04), make sure to never called cv2.imshow from one of your sensor callback threads.  You should only ever call it from the main thread.\n\n#### Filtering the image for only yellow\n\nSince the set of signs we are recognizing are all yellow, by design, we can handtune a filter that will only select the certain shade of yellow in our image.\n\nHeres a callback that will help to display the RGB value when hovering over the image window with a mouse (Note: you get this behavior for free with OpenCV 3.1).\n\n\n\nIn the  method, connect this callback by adding the following line:\n\n\n\nAnd add the following lines to your run loop:\n\n\n\nNow, if you hover over a certain part of the image, it will tell you what R, G, B value you are hovering over. Once you have created an HSV image, you can edit this function to also display the Hue, Saturation, and Value numbers.\n\nOpenCV windows can be pretty powerful when setting up interactive sliders to change parameters.  As stated in class for Neato soccer, if you want to learn dynamic_reconfigure, you can use that instead of OpenCVs trackbars.\n\nIn the  method, copy the following lines which\n\n\nThen, add the following callback methods to the class definition that respond to changes in the trackbar sliders\n\n\n\nThe sliders will help set the hsv lower and upper bound limits ( and ), which you can then use as limits for filtering certain parts of the HSV spectrum. Check out the OpenCV  for more details on how to threshold an image for a range of a particular color.\n\nBy the end of this step, you should have a binary image mask where all the pixels that are white represent the color range that was specified in the thresholding operation.\n\n#### Generating a bounding box\n\nYou can develop an algorithm that operates on the binary image mask that you developed in the step above.\n\nOne method that could be fruitful would be dividing the image in a grid.  You might want to write a method that divides the image into a binary grid of ; if tile in the grid contains a large enough percentage of white pixels, the tile will be turned on.\n\nSince the images are stored as 2D arrays, you can use NumPy-like syntax to slice the images in order to obtain these grid cells. Weve provided an example in  which Ill show here:\n\n\n\nThe task now is to decide which grid cells contain the region of interest. You can write another function that takes this binary grid and determines the bounding box that will include all the grid cells that were turned on.\n\n![][grid]\n[grid]: images/grid.png\n\nOpenCV has a method called  which seems promising too.  I found a nice example, albeit in C++, that  like you have.  It seems like it will depend on your thresholding operation to be pretty clean (i.e. no spurious white points, the only object that should be unmasked is the sign of interest).\n\n![][boundingRectStars]\n[boundingRectStars]: images/boundingRectStars.png\n\nThe goal is to produce  and  that define the top left and bottom right corners of the bounding box.\n\n## Recognition\n\nRecognizing the signs involves determining how well the cropped image if the sign matches the template image for each type of sign. To do this, we will find keypoints in the template image and in the input image, then see how well we can align the keypoints, and finally see how similar the aligned images are.\n\n### testing\n\nWe have template images as well as static test images for the template matcher in the repository, so we can run the code with the correct template images and try to match them to the static test images we have.\n\nTo do this, first we need to initialize the template matcher with the template images:\n\nYou can put this at the bottom of the file, and this if statement will mean that this part wont run when template_matcher is imported by other files.\n\nNext, we can run  on our test scenes using this:\n\nThis reads the test images, runs  on each image, and prints the file name followed by the prediction. Given just the starter code, this should be the output:\n\n\n### finding keypoints\n\nWe are finding keypoints using open cvs implementation of the , then filtering the keypoints ourselves to find the points that match between the input image and each template.\n\nFor the template images, we can calculate the keypoints in the initialization function because the images wont chage. To find those keypoints, we can cycle through the input dictionary of template images, read the image files as grayscale images and compute the keypoints using openCVs SIFT implementation:\n\nThe  method is the ""main"" method of TemplateMatcher. It begins by finding the keypoints in the input image as the first step to matching it with at template. At this point, the template images are initialized and the predict method should run, so you can run the program and it should return predictions for each template image with a zero confidence value. Next,  calls  to find how well the input image matches each template image and stores the predictions in a dictionary.\n\n### aligning keypoints\n\nIn , the first step to aligning the images is to find which keypoints from the two images match. The simplest method for that is to say keypoints whcih are close to eachother match. (note, later steps will correct for ""matched"" keypionts from mismatched images)\n\nBased on the matched keypoints, the following lines find how to transform the input image so the keypoints align with the template images keypoints (using a homography matrix), then transorms the input image using that matrix, so it should align with the template.\n\nOnce you add these lines, you should change  in the line  to .\n\n### comparing images\n\nAt this point, we have two images which, if they are of the same sign, should be aligned with each other. If they are of different signs, the matched keypoints were likely not well aligned, and the homography matrix probably skewed the image into an unrecognizable blob, but the computer cant tell what is a reasonable image and what is an unrecognizable blob, so now we have to determine how similar the two images are.\n\nThe  function at the bottom of the file is used to find how similar two images are to each other. This one is left up to you, but here are a few hints:\n\nFirst, there is one thing we have yet to account for while comparing images: lighting. If you have tried to do blob detection and then tried again when the sun went down, you know that lighting wreaks havoc on computer vision. Since we are using grayscale images, and we have cropped them so both images are of the same thing, we can eliminate most of the effects of lighting by normalizing the image. Mathematically this can be done by taking . Images are stored as numpy arrays, so you can use some nice numpy functions to make this math easier.\n\nFor finding the difference between the images, remember that, in code, an image is just an array of numbers. You can do arithmatic with the images to find how close they are to the same.\n\n### converting to useful output\n\nBack in the  method, the output of  passes through  and is saved in the dictionary , which maps the keys associated with the template images to the calulated difference between that template image and the input image.\n\nThe final step for  is to convert these differences into a scaled confidence value representing how well the input image matches each of the templates. This step is really just algebra: you need to make large numbers small and small numbers large, but you also want to scale your output so that a value near 1 always represents a high confidence.\n\n### conclusion\n\nThats all, this class now outputs how well an input image matches each of a given set of templates. One nice part about this approach is that no single step needs to be perfectly tuned: finding slightly too many keypoints initially is quickly corrected when you find matches, and incorrect matches are generally eliminated in the homography matrix transformation, so by the time you get to the numerical comparison of images, you are usually looking at either a reasonable match or two extremely different images. This system is therefore relatively robust, and has a low rate of false positives; however, it is suseptible to false negatives.\n\n## Navigating\n'"
https://github.com/ihmeuw/fungi,"Command-line interface to search provenance data generated by Provda, stored in ElasticSearch","b""Elasticsearch for storage of and queries on provenance data.\n============================================================\n\nThis project provides storage and query of provenance\ninformation for data and processes, using the format\nunderlying the _.\n\nThis project's motivation is provision of command-line tools\nfor interacting with provenance information. It provides abstractions\nand functionality for storing and querying provenance information for\ndata and processes, leveraging elasticsearch-py and elasticsearch-dsl-py."""
https://github.com/maxalbert/auto-exec-notebook,Illustrates how to programmatically generate and execute an IPython notebook,"b'Example how to programmatically create and execute an IPython notebook,\nin response to .\n\nClick  to view the example on .\n\nClick  to see the result of executing the automatically generated notebook.\n'"
https://github.com/ziyanfeng/udacity-deep-learning,Assignments for Udacity's Deep Learning with TensorFlow course,"b'Assignments for Udacity Deep Learning class with TensorFlow\n===========================================================\n\nCourse information can be found at https://www.udacity.com/course/deep-learning--ud730\n\nRunning the Docker container from the Google Cloud repository\n-------------------------------------------------------------\n\n    docker run -p 8888:8888 -it b.gcr.io/tensorflow-udacity/assignments:0.5.0\n\nAccessing the Notebooks\n-----------------------\n\nOn linux, go to: http://127.0.0.1:8888\n\nOn mac, find the virtual machines IP using:\n\n    docker-machine ip default\n\nThen go to: http://IP:8888 (likely http://192.168.99.100:8888)\n\nSaving Your Progress\n--------------------\n\nBecause of the  flag above, stopping the docker container removes it, so any changes youve made will disappear. One way around this is to remove the  flag, and name the container for easy restarting:\n\n\nFAQ\n---\n\n* I\n\nIf youre using a Mac, Docker works by running a VM locally (which\nis controlled by ). Its quite likely that youll\nneed to bump up the amount of RAM allocated to the VM beyond the\ndefault (which is 1G).\n\nhas two good suggestions; we recommend using 8G.\n\nIn addition, you may need to pass  as an extra argument to\n.\n\n* I want to create a new virtual machine instead of the default one.\n\n is a tool to provision and manage docker hosts, it supports multiple platform (ex. aws, gce, azure, virtualbox, ...). To create a new virtual machine locally with built-in docker engine, you can use\n\n    docker-machine create -d virtualbox --virtualbox-memory 8196 tensorflow\n    \n means the driver for the cloud platform, supported drivers listed . Here we use virtualbox to create a new virtual machine locally.  means the name of the virtual machine, feel free to use whatever you like. You can use\n\n    docker-machine ip tensorflow\n    \nto get the ip of the new virtual machine. To switch from default virtual machine to a new one (here we use tensorflow), type\n\n    eval $(docker-machine env tensorflow)\n    \nNote that  outputs some environment variables such like . Then your docker client is now connected to the docker host in virtual machine \n\n\nNotes for anyone needing to build their own containers (mostly instructors)\n===========================================================================\n\nBuilding a local Docker container\n---------------------------------\n\n    cd tensorflow/examples/udacity\n    docker build --pull -t $USER/assignments .\n\nRunning the local container\n---------------------------\n\nTo run a disposable container:\n\n    docker run -p 8888:8888 -it --rm $USER/assignments\n\nNote the above command will create an ephemeral container and all data stored in the container will be lost when the container stops.\n\nTo avoid losing work between sessions in the container, it is recommended that you mount the  directory into the container:\n\n    docker run -p 8888:8888 -v </path/to/tensorflow/examples/udacity>:/notebooks -it --rm $USER/assignments\n\nThis will allow you to save work and have access to generated files on the host filesystem.\n\nPushing a Google Cloud release\n------------------------------\n\n    V=0.5.0\n    docker tag $USER/assignments b.gcr.io/tensorflow-udacity/assignments:$V\n    gcloud docker push b.gcr.io/tensorflow-udacity/assignments\n    docker tag -f $USER/assignments b.gcr.io/tensorflow-udacity/assignments:latest\n    gcloud docker push b.gcr.io/tensorflow-udacity/assignments\n\nHistory\n-------\n\n* 0.1.0: Initial release.\n* 0.2.0: Many fixes, including lower memory footprint and support for Python 3.\n* 0.3.0: Use 0.7.1 release.\n* 0.4.0: Move notMMNIST data for Google Cloud.\n* 0.5.0: Actually use 0.7.1 release.\n'"
https://github.com/hplgit/bumpy,Quick tutorial of scientific computing with Python using a real physics application.,"b'# bumpy\n\nQuick appetizer and first tutorial on scientific computing with Python\nusing examples from basic physics.\n\n## Contents\n\n * Part I introduces the fundamental Python syntax for\n   variables, loops, if-tests, arrays, plotting, files, and classes,\n   using a simple physics formula as example.\n * Part II is a real physics application involving analysis of mechanical\n   vibrations. Besides showing how typical scientific Matlab-style scripts\n   look in Python, this example also introduces more advanced concepts\n   like flexible storage of objects in lists and files, downloading\n   data from web sites, user input via the command line, unit testing,\n   symbolic mathematics, and modules.\n\n## Goal\n\nThe goal of the tutorials is to bring the reader quickly up to speed\nwith how Matlab-style programming (with functions of one variable)\ncan be done in Python. With such basic background, it is easier to\nfollow scientific courses and teaching material that apply Python as\nprogramming language. The tutorials are application-driven and brief.\nFor further and more detailed information on scientific computing\nwith Python we have compiled\na .\n'"
https://github.com/tarlen5/cbb_analytics,College Basketball analytics software for analyzing trends in 5 man units and rosters,"b""cbb_analytics\n=============\n\nCollege Basketball analytics software for analyzing trends in 5 man units and rosters. Originally written to analyze the 2014 - 2015 UCLA basketball season.\n\n* Stage 1: . Load games by webscraping the base url, following the urls for individual game play by play information, then store data frames containing event play by play as an sqlite3 database table, with rosters for both sides for each game. Each raw play by play table and each side's roster gets its own table in the database, tagged with the date in format: yyyy-mm-dd.\n\n* Stage 2: (currently in the notebook  but will be made into its own script when finished). Process each game by loading each game's  and roster table into a  DataFrame with the time stamps of each player on the court and the running total score for each side. This will be used to compute the strength of each lineup, accounting for factors such as: strength of opponent, leverage of the situation, strength of opponent's lineup, and any other factors deemed important.\n\n### Requirements\n\nInstalling this package requires the following \n\n* \n* \n* \n\n"""
https://github.com/anoblega/Spectral-Clustering-,Trabajo de Spectral Clustering para la matéria de Machine Learning,b'# Spectral-Clustering-\n'
https://github.com/shounakG/2012-US-presidential-Elections-Lobbying-Analysis,Presidential Elections 2012,"b'# 2012 US presidential Elections: Lobbying Analysis\n\n\nThis project studied the trend of lobbying in the 2012 Presidential elections intending to find out useful statistical results for lobbying analysis.\n\nTechnologies used: IBM Bluemix, Apache Spark data and analytics service, IPython Notebook, [Languages: Python]\n'"
https://github.com/jklymak/PelicanSite,My Peilican Website,b'# PelicanSite\nMy Pelican Website\n\nhttp://web.uvic.ca/~jklymak/\n\nOnly trick from all the other versions online is a special version of  that makes the publication list.\n\nTo run:\n\n\nIf we need to use the python environment then run\n\n\nThe other trick was to use  to include figures.  Much better than raw markdown.  \n'
https://github.com/lapis-zero09/deep_learning_book,Deep Learning Book勉強会の資料,b'# deep_learning_book\n\nDeep Learning Book\n\nChapter 7.1-7.4\n\nslide \n'
https://github.com/HanchenXiong/Python_Essential,a notebook for collecting pieces of Python programming at different corners,b'# Python_Essential\n'
https://github.com/lschollmeyer/gis_visualizations,Introductory Python visualizations for GIS data,"b'# gis_visualizations\n\nVery rudimentary notebooks for a hour-long talk to show some basic GIS data visualizations using matplotlib (extended via Pandas and GeoPandas) and Folium. Obviously, not intended to be anything other than being an introduction to GIS concepts and using Python libraries to perform them. \n'"
https://github.com/bamtak/titanic_survival_exploration,Titanic survival exploration Machine Learning project,"b'# Machine Learning Engineer Nanodegree\n## Introduction and Foundations\n## Project: Titanic Survival Exploration\n\n### Install\n\nThis project requires Python 2.7 and the following Python libraries installed:\n\n- \n- \n- \n- \n\nYou will also need to have software installed to run and execute a \n\nIf you do not have Python installed yet, it is highly recommended that you install the  distribution of Python, which already has the above packages and more included. Make sure that you select the Python 2.7 installer and not the Python 3.x installer.\n\n### Code\n\nTemplate code is provided in the notebook  notebook file. Additional supporting code can be found in . While some code has already been implemented to get you started, you will need to implement additional functionality when requested to successfully complete the project. Note that the code included in  is meant to be used out-of-the-box and not intended for students to manipulate. If you are interested in how the visualizations are created in the notebook, please feel free to explore this Python file.\n\n### Run\n\nIn a terminal or command window, navigate to the top-level project directory  (that contains this README) and run one of the following commands:\n\n\nor\n\n\nThis will open the Jupyter Notebook software and project file in your web browser.\n\n### Data\n\nThe dataset used in this project is included as . This dataset is provided by Udacity and contains the following attributes:\n\nFeatures\n-  : Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n-  : Name\n-  : Sex\n-  : Age\n-  : Number of Siblings/Spouses Aboard\n-  : Number of Parents/Children Aboard\n-  : Ticket Number\n-  : Passenger Fare\n-  : Cabin\n-  : Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n\nTarget Variable\n-  : Survival (0 = No; 1 = Yes)'"
https://github.com/evizitei/NFIRS-property-loss-prediction,Capstone project for Udacity Machine Learning Engineer nanodegree,"b'# NFIRS property loss prediction\n\nSee project_report.md for project overview\n\n### Requirements\nmake sure to pip install these all before running anything\n\nsklearn (http://scikit-learn.org/)\ndbfread (https://dbfread.readthedocs.io)\ndataset (https://dataset.readthedocs.io/en/latest/)\n\n### Exploring Data files\n\nyou can download the data for this project from\n\nhttp://www.fema.gov/media-library-data/20130726-2126-31471-8394/nfirs_2011_120612.zip\n\nAnd then to use any of the following commands\nyou can unzip that file into the ""data"" folder\nin this project directory (which is gitignored)\n\nAll the data files are in .dbf format, so\nyou need dbfread in order to consume them.\n\n\n\nthe basicincident.dbf file is the most interesting one, and its\nrecords look like this:\n\n\n\nThey use numerical codes a lot here so the data is going to need\nsome cleaning. Some of the codes can be found in ""codelookup.DBF"".\n\n\n\nIts useful to have this data in csv format for easy reference, and\nits only 6.6k lines or so. Theres a script in the bin folder\nthat can do this for you, and it runs in less than a second:\n\n\n\n\n\n Additionally, its hard to explore\nin dbf format, so it seems worth shoving the relevant files\n into sqlite tables with indexes for easier querying.\nTheres a useful helper script for doing this\n(make sure your sqlite folder exists):\n\n\n\nit will take some time to run, since there are a few\nmillion basic incidents and 600k fire incidents,\nand its processing them iteratively.  This takes\non the order of 3 hours, and so the final dataset\nwill be provided as a google drive file here:\n\n*include file link at some point*\n\nexploring the data in a sql database is a little easier:\n\n\n\nAlthough there are 2.3 million incidents in the basic incident file,\nmany of them are non-fire incidents, and for the purposes of\nthis process we really only care about the ones that are in the\nfire incidents file.  This next transformation reads in the sqlite\ndatabase that has all the incidents and produces a single joined table\nthat has only entries for fire incidents.\n\n\n\nThere are > 670k records to build here, and it can do\nabout 50/second, so this operation can take about 4 hours.\n\nThis data is now in one-record-per-fire, which is easier\nto explore for feature exploration and extraction.\n\n\n\nIn the FeatureExploration notebook you can find some analysis of which features\nwere selected for learning and why.  Then there is another script\nthat reads in the joined fire records and produces a ""useful"" dataset\nthat has only records with property losses > 0 and only the features\nI believe to be useful.  There are about 215k records to scan through that\nmeet this criteria (though some are skipped for other reasons).  The script\niterates through about 500 records per second, so thats about a 7-8 minute runtime.\n\n\n\nThis produces a much smaller and manageable set of data, 8.9MB for the\nfinal sqlite file.  Here we can count the rows in it:\n\n\n\nSummary statistics can be extracted for a given column like so:\n\n\n\nPROP_LOSS\ncount  196574.000000\nmean     8914.580748\nstd     12933.652034\nmin         1.000000\n25%      1000.000000\n50%      3000.000000\n75%     10000.000000\nmax     60000.000000\n\nThe next step is to clean up the data so it can be analyzed reliably.  This\nmeans removing additional outliers from numeric fields and estimating missing\nvalues.\n\nThis script does about 500 records per second over 200k, so a bit less than 7\nminutes to run it:\n\n\n\nThis results in a remaining dataset of 192,405 records.  At this point\nwe take the data and make categorical fields one-hot encoded vectors,\nand feature-scale numeric fields to be from 0 to 1. We also use\nthe log of the prop loss rather than its normal value to unskew it\nbefore trying to get a good regression.  Once again\nthere is a script for this:\n\n\n\nThis is still 192k records, but its only able to process about 200 records per\nsecond.  This means its closer to 16 minutes to execute.\n\nA cleaned and normalized record looks like this:\n\n\n\nOrderedDict([(id, 1),\n             (PROP_LOSS, 3.30102999566), # log of 2000\n             (STATE_EXPENSE_0, 0),\n             (STATE_EXPENSE_1, 0),\n             (STATE_EXPENSE_2, 1),\n             (INC_TYPE_0, 0),\n             (INC_TYPE_1, 0),\n             (INC_TYPE_2, 0),\n             (INC_TYPE_3, 1),\n             (INC_TYPE_4, 0),\n             (INC_TYPE_5, 0),\n             (INC_TYPE_6, 0),\n             (AID_0, 1),\n             (AID_1, 0),\n             (AID_2, 0),\n             (AID_3, 0),\n             (HOUR_GROUP_0, 1),\n             (HOUR_GROUP_1, 0),\n             (HOUR_GROUP_2, 0),\n             (HOUR_GROUP_3, 0),\n             (CONTROLLED_TIME, 0.09042553191489362),\n             (CLEAR_TIME, 0.02976190476190476),\n             (SUPPRESSION_APPARATUS, 0.0),\n             (SUPPRESSION_PERSONNEL, 0.014925373134328358),\n             (PROP_VALUE, 0.00021062965584656352),\n             (DETECTOR_FLAG, 0),\n             (HAZMAT_RELEASE_0, 1),\n             (HAZMAT_RELEASE_1, 0),\n             (HAZMAT_RELEASE_2, 0),\n             (HAZMAT_RELEASE_3, 0),\n             (HAZMAT_RELEASE_4, 0),\n             (MIXED_USE_0, 1),\n             (MIXED_USE_1, 0),\n             (MIXED_USE_2, 0),\n             (MIXED_USE_3, 0),\n             (MIXED_USE_4, 0),\n             (HEAT_SOURCE, 0),\n             (IGNITION_0, 0),\n             (IGNITION_1, 1),\n             (IGNITION_2, 0),\n             (IGNITION_3, 0),\n             (FIRE_SPREAD_0, 1),\n             (FIRE_SPREAD_1, 0),\n             (FIRE_SPREAD_2, 0),\n             (FIRE_SPREAD_3, 0),\n             (FIRE_SPREAD_4, 0),\n             (FIRE_SPREAD_5, 0),\n             (STRUCTURE_TYPE_0, 1),\n             (STRUCTURE_TYPE_1, 0),\n             (STRUCTURE_TYPE_2, 0),\n             (STRUCTURE_TYPE_3, 0),\n             (STRUCTURE_TYPE_4, 0),\n             (STRUCTURE_TYPE_5, 0),\n             (STRUCTURE_STATUS_0, 1),\n             (STRUCTURE_STATUS_1, 0),\n             (STRUCTURE_STATUS_2, 0),\n             (STRUCTURE_STATUS_3, 0),\n             (STRUCTURE_STATUS_4, 0),\n             (SQUARE_FEET, 0.0007094674556213017),\n             (AES_SYSTEM_0, 1),\n             (AES_SYSTEM_1, 0),\n             (AES_SYSTEM_2, 0),\n             (AES_SYSTEM_3, 0)])\n\n## training/validation split\n\nBefore we start analyzing, Im going to take some data (about 30,000 records)\nand split them off for a validation set.  Out of the 192k records, that means\nwell still have around 160k for training/crossvalidation.  Heres the splitter\nscript:\n\n\n\nOn my machine this is processing about 200 rows per second, so for 192k records\nto get sorted thats about 16 minutes.   Afterwards you can check\nthe data split sizes like this:\n\nA cleaned and normalized record looks like this:\n\n\n\nAlgorithm selection is explored in\na jupyter notebook at\nAlgorithmExploration.ipynb, which\ncan be started and explored with:\n\n\n\nHyperparameter tuning is done with the tune_params script in the bin\nfolder; it has the actual analysis step commented out, so you can uncomment\nthe oen you want to run; running all of them at once would take some time.\nThe comments in that file contain the resulting best found parameters for\neach learner.\n'"
https://github.com/datosgobar/presentacion-type-checking-python,"Una nueva forma de tener type checking en python (PEP 484), usando una notación vieja (PEP 3107) - PyConAR 2016.","b""# presentacion-type-checking-python\n\nUna nueva forma de tener type checking en python (PEP 484), usando una notaci\xc3\xb3n vieja (PEP 3107) - PyConAR 2016. \n\n<!-- START doctoc generated TOC please keep comment here to allow auto update -->\n<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->\n## Indice\n\n- \n- \n- \n- \n- \n\n\n\n## Outline\n\n1. Function Annotations - PEP 3107 (2006)\n2. Type Hints - PEP 484 (2014)\n3. Librer\xc3\xadas de terceros para hacer type checking\n    *  en Python 3\n    *  en Python 2\n    *  en Python 3 (con Jupyter)\n4. Tipos m\xc3\xa1s complejos\n\n## Ejemplos\n\n* foo_py2.py: Ejemplo de type checking para correr con  en Python 2 ()\n* foo_py3.py: Ejemplo de type checking para correr con  en Python 3 ()\n* Funciones anotadas (python 3).ipynb: Ejemplo para correr en jupyter con .\n\n## Referencias\n\nNota: varios ejemplos de la presentaci\xc3\xb3n fueron tomados de las referencias.\n\n* : Esta PEP introduce una sintaxis para a\xc3\xb1adir anotaciones de metadata arbitrarias a funciones de Python.\n* : Esta PEP introduce un m\xc3\xb3dulo provisional que define est\xc3\xa1ndares y herramientas para implementar el chequeo de tipado en funciones de python usando la sintaxis propuesta en PEP 3107 para anotar funciones.\n\n## Herramientas usadas en la presentaci\xc3\xb3n\n\n* . Se escribe en Markdown y renderiza en html.\n\n## Duraci\xc3\xb3n recomendada\n\n5 minutos\n"""
https://github.com/khwilson/pydata2014,Materials for PyData2014,"b""PyData2014 Materials\n====================\n\nThese are supplementary materials for my talk on Determining Skill Levels\nat PyData2NYC 2014.\n\nFootball\n--------\n\nThe football package parses and manipulates\n* cfb2013lines.csv\n* conferences.csv\n\n is a csv of the winners and losers of all college\nfootball games in 2013.\n\n is just a mapping from team to an abbreviation of their\nconference name (sorry for any inconsistencies; 2013 data is hard to get at\nstrangely).\n\nThe package itself has three main modules:\n*  for manipulating the data\n*  for doing Elo's method on this data\n*  for producing a win/lose directed dot graph, which you should\n  compile with .\n\nTo use , simply run .\n\nTo use , see the example in .\n\nIRT\n---\n\nThe IRT package contains a simple implementation of 1PL IRT with normal priors.\nIt contains a  function that allows you build data sets, and then\na  function which you can use with  to\nlearn the latent parameters.\n\nFor an example of usage, see .\n\nTools\n-----\n\nI've also included some tools for doing finite-difference tests of jacobians\nand hessians. Doing finite-difference tests is critical for testing whether\nor not your fast implementations of these functions actually match your\nobjective. If they do not, then you're going to quickly get in trouble with\nline search algorithms.\n\nExercises\n---------\n\nSince this was an intermediate talk, I guess I can assign people who come here\nexercises:\n\n1. Here's a simple one. Fork this repo and add the gradients and\n   hessians of the elo and irt objectives to the packages. No credit if you do\n   not test these functions, which you can do with the finite-difference tests\n   available in the  package.\n\n2. Then play around with the tools. Find other data sets or simulate larger\n   data sets. When do the tools break? What assumptions might break down?\n\n3. Be the BCS! Add priors to Elo's method to get your favorite team to the top.\n   Some simple ones you could try are:\n   - Put a normal prior centered at 1.0 on your favorite team's skill and a\n     prior centered at 0.0 for everyone else's. What happens (especially, what\n     happens to their conference)?\n   - Put a prior on a whole conference. What happens?\n   - Put a nondiagonal prior on the teams. For instance, try putting a prior\n     on every conference that the skills across the conference will be\n     normally distributed. How would you do that? Why might you think that's\n     a reasonable prior?\n\nLicense\n-------\n\nEverything in this repository is licensed under the Apache 2.0 license.\n"""
https://github.com/ricardodeazambuja/IJCNN2017,Short-Term Plasticity in a Liquid State Machine Biomimetic Robot Arm Controller,"b""# Experiments used for the paper ~~accepted for~~ presented at \n# Short-Term Plasticity in a Liquid State Machine Biomimetic Robot Arm Controller\n\n## Abstract:\nBiological neural networks are able to control limbs in different scenarios, with high precision and robustness. As neural networks in living beings communicate through spikes, modern neuromorphic systems try to mimic them making use of spike-based neuron models. Liquid State Machines (LSM), a special type of Reservoir Computing system made of spiking units, when it was first introduced, had plasticity on an external layer and also through Short-Term Plasticity (STP) within the reservoir itself. However, most neuromorphic hardware currently available does not implement both Short-Term Depression and Facilitation and some of them don't support STP at all. In this work we test the impact of STP in an experimental way using a 2 degrees of freedom simulated robotic arm controlled by an LSM. Four trajectories are learned and their reproduction analysed with Dynamic Time Warping accumulated cost as the benchmark. The results from two different set-ups showed the use of STP in the reservoir was not computationally cost-effective for this particular robotic task.\n\n\n1) \n\n2) \n\n3) \n\n4) \n\n5) Testing:\n- \n- \n- \n- \n\n\n## OBS:  \n- \n- \n- \n- \n- \n- \n\n## Preprint version:  \n- \n\n## Bibtex citation:\nhttps://github.com/ricardodeazambuja/IJCNN2017/blob/master/de_azambuja_stp_2017.bib\n\n## Final IEEE Xplore version:  \nhttp://ieeexplore.ieee.org/document/7966283/\n\n## Related works:\n- \n- \n- \n- \n\n\n\n"""
https://github.com/cvetakg/Traffic_Sign_Classifier_Jelena,Traffic_Sign_Classifier,b'# Traffic_Sign_Classifier_Jelena\n\nThis project is part of SDCND program - Term1\n'
https://github.com/eduDorus/udacity_dl_project_4,Language Translation,b'# udacity_dl_project_4\nLanguage Translation\n'
https://github.com/lorenzomartino86/titanic-survival,A sample machine learning predictor for Kaggle competitions startup,b'# titanic-survival\nA sample machine learning predictor for Kaggle competitions startup\n'
https://github.com/jbrambleDC/datachallenge,This is our data challenge. Used to asses how potential interns technical skills are.,"b'# Data Challenge\nThis is Cava Grills data challenge. Used to asses how potential interns technical skills are.\nIf you so choose, please complete the excersies and email us if you are interested in the position. \n\n## Data Interns\nIf you are applying for the Data Intern position, please complete the first part. Please submit all code, or direct us to the github repo you used to produce the results.\n\n## Business Intelligence\nIf you are applying for the BI position, please use SQL,MYSQL or any RDMS to query the first set. You are not required to do any plotting or further analysis. Simple summary statistics (counts,sums,averages) are required, and please demonstrate some advanced SQL.\n\n## Data Scientist\nIf you are applying for the DS position, please complete the entire excercise.\n'"
https://github.com/nishalad95/Langevin-Monte-Carlo,Statistical simulation of Brownian Motion using the Langevin Monte Carlo Markov Chain algorithm for a particle under the influence of a drift function. ,b'# Langevin-Monte-Carlo\nStatistical simulation of Brownian Motion using the Langevin Monte Carlo Markov Chain algorithm for a particle under the influence of a drift function. \n\nCurrently in progress!\n'
https://github.com/avtomato/netology-homework,:snake: Netology python course homeworks ,b' \n'
https://github.com/petethemeat/quora-competition,Repository for our final Data Science project,b'# quora-competition\nRepository for our final Data Science project\n'
https://github.com/tomlouden/SPIDERMAN-paper,The first spiderman paper,b'to sync overleaf version with master: git push overleaf master\nto sync master version with overleaf: git pull overleaf master\n\nsimple!\n'
https://github.com/wesleybeckner/wesleybeckner.github.io,wesley's github website,"b'## Phantom for Jekyll\n\nA minimalist, responsive portfolio theme for  with Bootstrap.\n\n\n\n.\n\n## Fancy using it for your own site?\n\nHere are some steps to get you started:\n\n1. Clone this repo and cd into the directory:\n\n  \n\n2. Run:\n\n  \n\n  You may need to append your commands with  if youre getting a permissions error.\n\n  _Dont have Jekyll yet? [Get http://127.0.0.1:4000_config.ymlnav_itemnav_enable_config.yml/_includes/contact-modal.html_config.yml`. You can add the pagination to other layouts with:\n\n\n\nRead more about the .\n\n## Credit\n\n* Bootstrap, http://getbootstrap.com/, (C) 2011 - 2016 Twitter, Inc., \n\n* Wow, https://github.com/matthieua/WOW, (C) 2014 - 2016 Matthieu Aussaguel\n, \n\n* Animate.css, https://github.com/daneden/animate.css, (C) 2016 Daniel Eden, \n'"
https://github.com/dustin-anthc/CCHRC,A repository of notebooks that we use for data analysis,b'# CCHRC\nA repository of notebooks that we use for data analysis\n'
https://github.com/pattrickmiller/udacity,Udacity Projects,"b'""# udacity"" \n'"
https://github.com/badphysics/velocity_selector,velocity selector for university physics II at RIT,b'# velocity_selector\nvelocity selector for university physics II at RIT\n\nThis is to supplement the Velocity Selector activity in University Physics II at Rochester Insititue of Technology\n'
https://github.com/llSourcell/The_evolution_of_gradient_descent,"This is the code for ""The Evolution of Gradient Descent"" by Siraj Raval on Youtube","b'# The_evolution_of_gradient_descent\n\nThis is the code for ""The Evolution of Gradient Descent"" by Siraj Raval on Youtube\n\n## Coding Challenge - Due Date, Thursday June 8th at 12 PM PST\n\nThis weeks coding challenge is to write out the  optimization strategy from scratch. In the process youll learn about all the other gradient descent variants and why Adam works so well. Bonus points if you add a visual element to it by plotting it in a Jupyter notebook. Good luck!\n\n## Overview\n\nThis is the code for  videon on Youtube by Siraj Raval. In the video, we go over the different optimizer options that Tensorflow gives us. Under the hood, they are all variants of gradient descent.\n\n## Dependencies\n\n* matplotlib\n* pyplot\n* numpy\n\ninstall missing dependencies with \n\n## Usage\n\nRun  to see the code that compares gradient descent to stochastic gradient descent run in the browser. Ive also got 2 seperate python files, one for adadelta and one for the nesterov method. Run those straight from terminal with the python command. \n\n## Credits\n\nThe credits for this code go to  and . Ive merely created a wrapper to get people started. \n\n'"
https://github.com/Davin-IBM/Proof-of-Technology,Proofs of Technology (PoT) are to determine feasible solutions to technical problems.,"b""# Proof-of-Technology\n\nProofs of Technology (PoT) are to determine feasible solutions to technical problems.\n\n## Contents:\n\n1. \n"""
https://github.com/ledrui/Advanced-Lane-Lines-detection,Detecting road lane using advanced Computer Vision techniques ,"b'## Advanced Lane Finding\n\n\n\nIn this project, your goal is to write a software pipeline to identify the lane boundaries in a video, but the main output or product we want you to create is a detailed writeup of the project.  Check out the  for this project and use it as a starting point for creating your own writeup.  \n\nCreating a great writeup:\n---\nA great writeup should include the rubric points as well as your description of how you addressed each point.  You should include a detailed description of the code used in each step (with line-number references and code snippets where necessary), and links to other supporting documents or external references.  You should include images in your writeup to demonstrate how your code works with examples.  \n\nAll that said, please be concise!  Were not looking for you to write a book here, just a brief description of how you passed each rubric point, and references to the relevant code :). \n\nYoure not required to use markdown for your writeup.  If you use another method please just submit a pdf of your writeup.\n\nThe Project\n---\n\nThe goals / steps of this project are the following:\n\n* Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n* Apply a distortion correction to raw images.\n* Use color transforms, gradients, etc., to create a thresholded binary image.\n* Apply a perspective transform to rectify binary image (""birds-eye view"").\n* Detect lane pixels and fit to find the lane boundary.\n* Determine the curvature of the lane and vehicle position with respect to center.\n* Warp the detected lane boundaries back onto the original image.\n* Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.\n\nThe images for camera calibration are stored in the folder called .  The images in  are for testing your pipeline on single frames.  If you want to extract more test images from the videos, you can simply use an image writing method like , i.e., you can read the video in frame by frame as usual, and for frames you want to save for later you can write to an image file.  \n\nTo help the reviewer examine your work, please save examples of the output from each stage of your pipeline in the folder called , and include a description in your writeup for the project of what each image shows.    The video called  is the video your pipeline should work well on.  \n\nThe  video is an extra (and optional) challenge for you if you want to test your pipeline under somewhat trickier conditions.  The  video is another optional challenge and is brutal!\n\nIf youre feeling ambitious (again, totally optional though), dont stop there!  We encourage you to go out and take video of your own, calibrate your camera and show us how you would implement this project from scratch!\n'"
https://github.com/ishantiw/customer_insights,Customer Insights,"b'\n     ,-----.,--.                  ,--. ,---.   ,--.,------.  ,------.\n      .--./|  | ,---. ,--.,--. ,-|  || o     |  ||  .-.   |  .---\n    |  |    |  || .-. ||  ||  | .-. |--, \n      --|  | -      ---.\n     -- ----  --    ------- socket.ioexpressserver.js` and start the app by clicking on the ""Run"" button in the top menu.\n\n2) Alternatively you can launch the app from the Terminal:\n\n    $ node server.js\n\nOnce the server is running, open the project in the shape of https://projectname-username.c9users.io/. As you enter your name, watch the Users list (on the left) update. Once you press Enter or Send, the message is shared with all connected clients.\n'"
https://github.com/gacafe/vala_tc,Introduction to Python for Librarians,b'# vala_tc\n\n\n'
https://github.com/Yurlungur/spectral-methods-demo,A Demonstration of Spectral Methods in Python,b'spectral-methods-demo\n=====================\nAuthor: Jonah Miller \n\nA Demonstration of Spectral Methods in Python\n'
https://github.com/jfnavarro21/Python,"Work in numpy and pandas, plus simple coding algorithms",b'Use to run ipython notebook from anywhere \n- /c/Python27/Scripts/./ipython notebook\n\n'
https://github.com/russodanielp/sbv,sbv Challenge,b'# svb IMPROVER SysTox Challenge\n\n'
https://github.com/sophienchu/tutorials,trying to learn python,b'# tutorials\ntrying to learn python\n\ndoing several tutorials to learn python\n'
https://github.com/lexieheinle/jour407homework,Agate-analyzed data for data journalism class,"b'#JOUR 407 Homework\n\nThis repository contains my homework for JOUR 407 Data Journalism class taught by Matt Waite at the University of Nebraska-Lincoln. The majority of the files involve data analysis by agate and documentation via Jupyter.\n\n##Contents\n###\nThis folder contains a dirty data set of [leaking underground storage tanks] (http://www.deq.state.ne.us/lustsurf.nsf/pages/sssi ""NDEQ lust database"") from the Nebraska Department of Environmental Quality. I used [OpenRefine] (http://openrefine.org) to normalize the data before finding the top 20 owners using Agate.\n\n###\nThis folder looked at data from the University of Nebraska-Lincoln Police Department. By using Agates group_by and count functions, inconsistent locations and buildings were revealed. Additionally, outliers in stolen and damaged amounts were found using agate-stats.\n\n###\nFrom Nebraska school data, I calculated the percent change in free &amp; reduced lunch and enrollment from 2013 to 2015. This assignment required joining agate tables and creating custom functions to round decimals.\n\n###\nUsing  a Python data visualization based on , I constructed three different chart styles of the top 10 Nebraska counties for mountain lion sightings.\n\n###\nFrom the political ads database, I attempted to split the events by subjects to see which ads/candidates are most likely to use certain ones. However, this quest wasnt successful due to program limitations.\n\n###\nUsing  from the Nebraska Secretary of State, I analyzed the changes in the states political party distribution from 2000 to 2014. From my analysis, I found that if the current growth trend of the independent party continues, it would overtake the Democratic party by 2020.\n\nTo find the partys distributions, I had to write custom functions to calculate the correct percents. The percent change was calculated using agate functions. Estimating the future growth was based on the instructions in  and required defining new functions.\n\n###\nTo gather the data for this story, I had to request records from the E911 departments in both  and . I looked at each states E911 fund, which is funded by surcharges on cellphones. In the article, I explored how emergency services adapted to cover cellphones and the future plans.\n\n###\nUsing irrigated acres data from the  in United States Department of Agriculture and registered groundwater well data from , I built an Agate custom computation to find whether a well was active during a given year. Next, I found the number of active wells for each county and also listed the number of irrigated acres. Currently, the complied information is saved in the .\n\n###\nI grabbed different climate-related statistics from the  for use in an in-depth story on Nebraska climate. Those statistics include the frost season lengths, mean temperatures, and snowfall sums. I also had to build a  to grab the monthly Lincoln temperatures from the  and how they compare to 30-year normals.\n\n###\nUsing the US Census  from 2012 and 2007, I looked at how the distribution of businesses that are equally owned and operated by both spouses has changed. Several industries including utilities have significantly dropped perhaps due to the Great Recession effects.\n\n###\nFrom the , I grabbed all events from January 1950 to 2016. To grab only Nebraska events from the plethora of files, I wrote a quick Python  and looked into how each storm event has increased/decreased over the years.\n\n###\nFrom a  that used the , I used QGIS to join those scores to a United States map. The  shows the lower scoring counties in dark brown or lighter yellow, and the higher ranking in either dark teal or a lighter one.\n'"
https://github.com/jkroso/HTTP.jl,An HTTP client and server for Julia,b'# HTTP.jl\n\nA client and server side implementation of HTTP for Julia.\n\nTo use it you will need the  module system.\nTo test it you will need Docker and \n'
https://github.com/thedanschmidt/unsupervised-stock-features,"My final project for Professor Gu's Mathematics of Big Data class, Fall 2016 Harvey Mudd College.","b'unsupervised-stock-features\n==============================\n\nUnsupervised learning of stock maret price features. Dan Schmidts final project, Big Data 2016 HMC Gu.\n\nProject Organization\n------------\n\n    \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 LICENSE\n    \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 Makefile           <- Makefile with commands like  or \n    \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 README.md          <- The top-level README for developers using this project.\n    \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 data\n    \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 external       <- Data from third party sources.\n    \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 interim        <- Intermediate data that has been transformed.\n    \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 processed      <- The final, canonical data sets for modeling.\n    \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 raw            <- The original, immutable data dump.\n    \xe2\x94\x82\n    \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 docs               <- A default Sphinx project; see sphinx-doc.org for details\n    \xe2\x94\x82\n    \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 models             <- Trained and serialized models, model predictions, or model summaries\n    \xe2\x94\x82\n    \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),\n    \xe2\x94\x82                         the creators initials, and a short  delimited description, e.g.\n    \xe2\x94\x82                         .\n    \xe2\x94\x82\n    \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 references         <- Data dictionaries, manuals, and all other explanatory materials.\n    \xe2\x94\x82\n    \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 reports            <- Generated analysis as HTML, PDF, LaTeX, etc.\n    \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 figures        <- Generated graphics and figures to be used in reporting\n    \xe2\x94\x82\n    \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.\n    \xe2\x94\x82                         generated with \n    \xe2\x94\x82\n    \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 src                <- Source code for use in this project.\n    \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 init.py    <- Makes src a Python module\n    \xe2\x94\x82   \xe2\x94\x82\n    \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 data           <- Scripts to download or generate data\n    \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 make_dataset.py\n    \xe2\x94\x82   \xe2\x94\x82\n    \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 features       <- Scripts to turn raw data into features for modeling\n    \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 build_features.py\n    \xe2\x94\x82   \xe2\x94\x82\n    \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 models         <- Scripts to train models and then use trained models to make\n    \xe2\x94\x82   \xe2\x94\x82   \xe2\x94\x82                 predictions\n    \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x9c\xe2\x94\x80\xe2\x94\x80 predict_model.py\n    \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 train_model.py\n    \xe2\x94\x82   \xe2\x94\x82\n    \xe2\x94\x82\xc2\xa0\xc2\xa0 \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 visualization  <- Scripts to create exploratory and results oriented visualizations\n    \xe2\x94\x82\xc2\xa0\xc2\xa0     \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 visualize.py\n    \xe2\x94\x82\n    \xe2\x94\x94\xe2\x94\x80\xe2\x94\x80 tox.ini            <- tox file with settings for running tox; see tox.testrun.org\n\n\n--------\n\nProject based on the cookiecutter data science project template. #cookiecutterdatascience\n'"
https://github.com/streety/djangodistrict-geoviews-bokeh-intro,Lightning talk given at the django district January 2017 new tech event on geoviews and bokeh,b'# djangodistrict-geoviews-bokeh-intro\n\n## Lightning talk given at the django district January 2017 new tech event on geoviews and bokeh\n\n'
https://github.com/anxingle/ctc_example,CTC tutorial of OCR recognition(tensorflow0.9),b'# ctc_example\ncopy from  \n## one layer LSTM to classify mnist data Set      \nhere is the network struct:      \n         \n               \n## we can use CTC to recognition OCR \n'
https://github.com/cessor/showmanship,A small presentation framework. You don't need PowerPoint. ,"b""# showmanship\nA small presentation framework. You don't need PowerPoint. \n"""
https://github.com/rpicatoste/carnd-term1-project2-traffic-sign-classifier,"Self-Driving Car Nanodegree Project 2, a traffic sign classifier","b'## Project: Build a Traffic Sign Recognition Program\n\nThe project repo has:\n\n* The jupyter notebook, that can be run cell by cell to obtain the results.\n* An html version of the notebook with the code run already and the results obtained.\n* The required writeup as markdown file, explaining how the different rubric points were addresses.\n* The python files with the code for the whole project.\n* A folder with images required for the writeup.\n* The csv file with the sign names, required to run the code.\n\nNOTE: The pickled data given for the project is not included and can be downloaded from .'"
https://github.com/LukasMosser/Jupetro,A collection of oil&gas related Jupyter notebooks,b'# Jupetro\nA collection of oil&amp;gas related Jupyter notebooks\n\nTo start an interactive session of this repository click here: \n\n## License\nAll notebooks are published under the GPL3 license.\n\nCreated by Lukas Mosser 2016.'
https://github.com/oist/TQM-demostrations,things that I might want to share with others in TQM,b'# TQM-demostrations\nthings that I might want to share with others in TQM\n'
https://github.com/abhshkdz/neural-vqa-attention,:question: Attention-based Visual Question Answering in Torch,"b""# neural-vqa-attention\n\nTorch implementation of an attention-based visual question answering model ([Stacked Attention Networks for Image Question Answering, Yang et al., CVPR16][1]).\n\n\n\n1. \n    1. \n    2. \n    3. \n2. \n    1. \n    2. \n3. \n\nIntuitively, the model looks at an image, reads a question, and comes up with an answer to the question and a heatmap of where it looked in the image to answer it.\n\nThe model/code also supports referring back to the image multiple times ([Stacked Attention][1]) before producing the answer. This is supported via a  parameter in the code (default = 1).\n\nNOTE: This is NOT a state-of-the-art model. Refer to [MCB][7], [MLB][8] or [HieCoAtt][9] for that.\nThis is a simple, somewhat interpretable model that gets decent accuracies and produces .\nThe code was written about ~1 year ago as part of [VQA-HAT][12], and I'd meant to release it earlier, but couldn't get around to cleaning things up.\n\nIf you just want to run the model on your own images, download links to pretrained models are given below.\n\n## Train your own network\n\n### Preprocess VQA dataset\n\nPass  as  to train on  and evaluate on , and  to train on + and evaluate on .\n\n\n\n\n### Extract image features\n\nSince we don't finetune the CNN, training is significantly faster if image features are pre-extracted. We use image features from VGG-19. The model can be downloaded and features extracted using:\n\n\n\n### Training\n\n\n\n## Use a pretrained model\n\n### Pretrained models and data files\n\nAll files available for download [here][10].\n\n- : model pretrained on + with 1 attention layer (SAN-1)\n- : model pretrained on + with 2 attention layers (SAN-2)\n- : vocabulary file for training on , evaluating on \n- : vocabulary file for training on +, evaluating on \n- : QA features for training on , evaluating on \n- : QA features for training on +, evaluating on \n-  & : image features for training on , evaluating on \n-  & : image features for training on +, evaluating on \n\n### Running evaluation\n\n\n\nThis will generate a JSON file containing question ids and predicted answers. To compute accuracy on , use [VQA Evaluation Tools][13]. For , submit to [VQA evaluation server on EvalAI][14].\n\n## Results\n\nFormat: sets of 3 columns, col 1 shows original image, 2 shows 'attention' heatmap of where the model looks, 3 shows image overlaid with attention. Input question and answer predicted by model are shown below examples.\n\n\nMore results available [here][3].\n\n### Quantitative Results\n\nTrained on  for  accuracies, and trained on + for  accuracies.\n\n#### VQA v2.0\n\n| Method                | val     | test    |\n| ------                | ---     | ----    |\n| SAN-1                 | 53.15   | 55.28   |\n| SAN-2                 | 52.82   | -       |\n| [d-LSTM + n-I][4]     | 51.62   | 54.22   |\n| [HieCoAtt][9]         | 54.57   | -       |\n| [MCB][7]              | 59.14   | -       |\n\n#### VQA v1.0\n\n| Method                | test-std    |\n| ------                | --------    |\n| SAN-1                 | 59.87       |\n| SAN-2                 | 59.59       |\n| [d-LSTM + n-I][4]     | 58.16       |\n| [HieCoAtt][9]         | 62.10       |\n| [MCB][7]              | 65.40       |\n\n## References\n\n- [Stacked Attention Networks for Image Question Answering][1], Yang et al., CVPR16\n- [Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering][11], Goyal and Khot et al., CVPR17\n- [VQA: Visual Question Answering][6], Antol et al., ICCV15\n\n\n## Acknowledgements\n\n- Data preprocessing script borrowed from [VT-vision-lab/VQA_LSTM_CNN][4]\n\n## License\n\n[MIT][2]\n\n\n[1]: https://arxiv.org/abs/1511.02274\n[2]: https://abhshkdz.mit-license.org/\n[3]: https://computing.ece.vt.edu/~abhshkdz/neural-vqa-attention/figures/\n[4]: https://github.com/VT-vision-lab/VQA_LSTM_CNN\n[5]: http://visualqa.org/download.html\n[6]: http://arxiv.org/abs/1505.00468\n[7]: https://github.com/akirafukui/vqa-mcb\n[8]: https://github.com/jnhwkim/MulLowBiVQA\n[9]: https://github.com/jiasenlu/HieCoAttenVQA\n[10]: https://computing.ece.vt.edu/~abhshkdz/neural-vqa-attention/pretrained/\n[11]: https://arxiv.org/abs/1612.00837\n[12]: https://computing.ece.vt.edu/~abhshkdz/vqa-hat/\n[13]: https://github.com/VT-vision-lab/VQA\n[14]: https://evalai.cloudcv.org/featured-challenges/1/overview\n"""
https://github.com/mutaihillary/Mytodo_app,"Todo App built using Flask, SQLAlchemy and intergrate with login API","b'# Flask TODOapp\n\nThe example TODOapp developed using Flask, Vertabelo, SQLAlchemy and html ,css. \nThe initial version is deployed to Heroku: http://flask-todoapp.herokuapp.com/.\n\n\n'"
https://github.com/codrut3/Kaggle-notebooks,"A collection of my Jupyter notebooks, created while analyzing data during Kaggle competitions.","b'# Kaggle-notebooks\nA collection of my Jupyter notebooks, created while analyzing data during Kaggle competitions.\n'"
https://github.com/DJMedhaug/Boggler,Dana and Kevin's Boggler project.,"b'\n\n\n   Boggle solver\n\nPurpose\n\nPurpose:  This project introduces Python modules as a way of breaking more \ncomplex Python programs into smaller, simpler and more reusable parts. game_dict.py\nis a Python module that you must complete.  \nThe boggler project also \nprovides more practice with recursive depth-first search. \n\n\nIn addition, this project gives a small introduction to object-oriented programming. \nboggle_board.py is a module that you \ndont have to modify, but you do need to read and understand. The boggle_board module\ncreates a BoggleBoard class.  Your boggler program will create and use a single\nBoggleBoard object.  In the following project, you will create classes and objects \nyourself. \n\n\n\nPair Assignment\n  Work together with one classmate.\n  Make sure you both have a solid understanding of the core\n  recursive algorithms in this program, including the way we\n  manage marking tiles that are already in use on the current\n  path.  (This is very typical of depth-first search algorithms.)\n  \n\n\n\n\nBoggle solver\n  Boggle is a popular word game in which players search for\n  words that can be formed by the letters on a set of tiles\n  in a 4x4 grid.  <a\n  href=""http://en.wikipedia.org/wiki/Boggle"">Wikipedia has a brief\n  description.\n  \n\n  This program reads a boggle board\n  and a dictionary of possible solutions, and prints\n  all the dictionary words that can be formed for the board, following\n  standard Boggle rules.  Words can be formed starting from any\n  position, continuing horizontally, vertically, or diagonally,\n  without using any tile twice in the same word. The list of found\n  words is printed in alphabetical order. \n  \n\n  Requirements\n  The behavior of your program in a shell (command line) should be exactly like this.\n  There is also a graphical display, which is not shown here, and is not graded.\n  \n$ python3 boggler.py ""ammaglxxxxpaxxxh"" shortdict.txt \nalpha 2\ngamma 2\nTotal score:  4\nPress enter to end\n\n\n$ python3 boggler.py ""ammaglxxxxpaxxxh"" dict.txt\nalp 1\nalpha 2\ngal 1\ngamma 2\nhap 1\nlag 1\nlam 1\nmag 1\nmax 1\nTotal score:  11\nPress enter to end\n\n\n  Starting materials\n  \n    graphics.py (dont change this)\n    boggle_board.py (dont change this)\n    test_harness.py (for testing)    \n    grid.py (for display, dont change this)  \n    game_dict.py You must complete this and turn it in  \n    boggler.py You must complete this and turn it in    \n    shortdict.txt A short list of words, \n        useful for testing\n    dict.txt A more typical list of \n    approximately 40,000 common English words.  This is usually enough to beat \n    human boggle players. Serious players of Scrabble and crossword puzzles \n    use larger lists that include less common words.\n    \n    \n    \n    How to get started\n    Start with the game dictionary.  I have included a pretty good test suite so that \n    you can test it separately from the rest of the boggler program. Notice the funny\n    code at the end of the file for testing whether the game_dict module is being \n    called as a main program.  This is so that you can test it like this: \n    \n\n$ python3 game_dict.py shortdict.txt \n[<marko.inline.RawText object at 0x000002CBAF39ADC8>]  First word in dictionar (alpha)  Expected: | 1 | but got | 0 |\n[<marko.inline.RawText object at 0x000002CBAF39AF88>]  Last word in dictionary (omega)  Expected: | 1 | but got | 0 |\n[<marko.inline.RawText object at 0x000002CBAF395188>]  Within dictionary (beta)  Expected: | 1 | but got | 0 |\n[<marko.inline.RawText object at 0x000002CBAF395348>]  Within dictionary (delta)  Expected: | 1 | but got | 0 |\n[<marko.inline.RawText object at 0x000002CBAF395508>]  Within dictionary (gamma)  Expected: | 1 | but got | 0 |\n[<marko.inline.RawText object at 0x000002CBAF395708>]  Prefix of first word (al)  Expected: | 2 | but got | 0 |\n[<marko.inline.RawText object at 0x000002CBAF395888>]  Prefix of last word (om)  Expected: | 2 | but got | 0 |\n[<marko.inline.RawText object at 0x000002CBAF395948>]  Prefix of interior word (bet)  Expected: | 2 | but got | 0 |\n[<marko.inline.RawText object at 0x000002CBAF395C08>]  Prefix of interior word (gam)  Expected: | 2 | but got | 0 |\n[<marko.inline.RawText object at 0x000002CBAF395CC8>]  Prefix of interior word (del)  Expected: | 2 | but got | 0 |\n   Passed --  Before any word (aardvark)  result:  0\n   Passed --  After all words (zephyr)  result:  0\n   Passed --  Interior non-word (axe)  result:  0\n   Passed --  Interior non-word (carrot)  result:  0\n   Passed --  Interior non-word (hagiography)  result:  0\n[<marko.inline.RawText object at 0x000002CBAF395FC8>]  First word in dictionar (alpha)  Expected: | 1 | but got | 0 |\n[<marko.inline.RawText object at 0x000002CBAF3C0188>]  Last word in dictionary (omega)  Expected: | 1 | but got | 0 |\n   Passed --  Short word omitted (beta)  result:  0\n \n You will know you probably have a working game dictionary when you can pass these\n tests.  (Writing executable test cases before writing the code that passes them is part \n of a practice called \n &ldquo;test-driven development,&rdquo; and is common in the so-called &ldquo;agile&rdquo; software\n development methodologies that are popular today.) \n \n I suggest writing the search function as a linear search first: Check each word \n in order, using the Python built-in method startswith to check for a \n prefix match. When you have a linear search function working, you can move on to \n the rest of the program and come back later to make the search function faster\n with binary search.\n When you have your game_dict module working, move on to boggler.py.  Use the \n game board module to represent the board.  (Duplicating any of the functionality of board.py \n within your boggler.py source file will be considered an error.) The places you will \n need to work on boggler.py are marked with FIXME comments.  The main one is the \n recursive find_words function.  I have included several FIXME comments \n to guide you.\n boggler.py includes a dedup function for putting a results list into \n sorted order, without duplicate words.  Instead of keeping the original results \n (with duplicates) in a list, you could use the Python set data structure.  If you do,\n you should put the words into the set from the find_words function, and \n then dedup should be replaced with a function that extracts a sorted \n list from the set.\n Optional: When your boggler.py program is working satisfactorily is a good time to return to\n game_dict.py and change the linear search to a binary search.  This requires some careful\n thought:  If the binary search does not find an exact match, which dictionary word\n should you check to see if the word you were looking for could be a prefix of a word\n in the dictionary?  The included test suite will help you check whether you got it right. \n It is better to turn in a working boggler program that uses linear search, than a \n broken boggler program that uses binary search (especially if the binary search is \n what is broken). \n \n    \n\n\n  Grading rubric\n  \n    \n      Functional correctness\n      &nbsp;\n      &nbsp;\n      &nbsp;\n      40\n    \n    \n      &nbsp;\n      Exactly meets input/output spec        \n      10\n      5 = minor discrepancy, 0 = ignored spec\n      &nbsp;\n      &nbsp;\n    \n    \n      &nbsp;\n      Correct results: Word finding\n      25\n      25 = works for all cases (finds all words, and nothing\n\telse), 18 = works for almost all cases, 12 = works for most\n\tcases (e.g., not when one word is a prefix of another), 5 = works for some cases\n      &nbsp;\n      &nbsp;\n    \n    \n      &nbsp;\n      Correct results: Scoring\n      5\n      5 points if all words of three or more letters are correctly\n      scored, and the correct total is shown\n      &nbsp;\n      &nbsp;\n    \n    \n      Other requirements\n      &nbsp;\n      &nbsp;\n      &nbsp;\n      40\n    \n    \n      &nbsp;\n      Header docstring        \n      10\n      10 = as specified, 7 = minor issue, 5 = as #comment, 0 = missing\n      &nbsp;\n      &nbsp;\n    \n    \n      &nbsp;\n      Function header docstrings\n      8\n      8 = good docstrings in all functions, 6 = minor problems, 4 = incorrect or multiple missing, 0 = docstrings not provided\n      &nbsp;\n      &nbsp;\n    \n    \n      &nbsp;\n      Correct use of modules\n      12\n      12 = modules used correctly, no violation of abstraction,\n\t8 = decomposition has some issues, such as misuse of global\n\tvariables, use of magic numbers instead of symbolic\n\tconstants,  0 = failure to use modules\n      &nbsp;\n      &nbsp;\n    \n    \n      &nbsp;\n      Program style and readability\n      10\n      10 Good variable names, indentation, etc --- very readable code, 8 = minor issues, such as inconsistent indentation, 5 = major issues that interfere with readability of code, 0 = unreadable mess \n      &nbsp;\n      &nbsp;\n    \n    \n      Total\n      &nbsp;\n      &nbsp;\n      &nbsp;\n      80\n    \n  \n  I cant anticipate all issues that may be encountered in grading,\n  so points may be deducted for other issues not listed in the\n  rubric.    A program that does not compile and run (e.g., because of a syntax error) starts with 0 points for functional correctness, but I may award some partial credit. \n  &nbsp;\n  \n'"
https://github.com/kacerchio/CS505-final-project,This is a repository for analysis on the success of movie releases based upon IMDB movie data. ,"b'# CS505-final-project\n### by Kristel Tan and Nisa Gurung\n\nThe main goal of this project was to identify trends in movie attributes that correlate with a movie\xe2\x80\x99s success in respect to the genre that it belongs in. Movie attributes analyzed include the duration of a movie, its allocated budget, average actor similarity, title length, director hits, and plot keywords. For the purposes of this project, we defined a film\xe2\x80\x99s success by how much gross profit it made in the box office, independent of critic reviews and ratings. By analyzing this data, we expect to be able to make recommendations for such movie attributes and predictions of whether or not a given set of attributes will result in a highly profitable movie launch. \n\n#Datasets \n\nThe dataset to support this project was initially retrieved from \xe2\x80\x98kaggle.com\xe2\x80\x99. It was a CSV file of 5,000 movie attributes scraped from IMDB and \xe2\x80\x98the-numbers.com\xe2\x80\x99. We cleaned this dataset of entries that were missing relevant attributes. Because this step eliminated a significant amount of data, we manually scraped and cleaned more data from IMDB\xe2\x80\x99s top box office lists for the past 12 years. We then merged this data back with the original dataset, totaling roughly 6,000 that had all of the attributes we were looking to analyze. We removed movies that were missing column entries because we wanted to ensure that every movie could contribute to every analysis equally. \n\n#Techniques \n\nFor this project, we planned to use two different techniques to help identify movie attributes related to its success. These methods include multiple linear regression and k-means clustering. Multiple linear regression was a natural choice because it gives us an idea of whether or not certain independent variables are significant to any given genre and how strongly they correlate with its gross profit. In particular, we looked at the duration, title length, budget, number of hits the director has produced, and average actor similarity in relation to the gross profit it made. Applying an OLS linear regression function to this data provided us with coefficients and confidence intervals to identify correlations. \n\nFurthermore, we used k-means clustering to analyze a different kind of attribute, which were plot keywords. We chose to focus on evaluating a movie\xe2\x80\x99s plot keywords for the top \n1,000 profitable movies and least 1,000 profitable movies of our overall dataset. In other words, we clustered these movies by the most similar plot keywords that are associated with them, provided from IMDB\xe2\x80\x99s website. Clusters formed based upon words of similar themes and meaning. Individuals points stayed closer to the center of the cluster or strayed away from it depending upon how similar it was to the other plot keywords in that cluster. At a high level, this gave us an idea of what plot keywords formed the largest clusters, which also indicated what storylines movie critics and audiences found most interesting.\n\n#Scripts \nThe scripts for this project are data-collection.ipynb which is in the data folder and Regression.ipynb and Kmeans.ipynb. \n\nWe retrieved our movie_master_dataset.csv from data-collection.ipynb. Regression.ipynb runs multiple linear regression on five popular genres: action, thriller, drama, romance and comedies and also on three unpopular genres: documentary, history and musical. Kmeans.ipynb runs k-means clustering on the plot keywords of the top 1000 profitable movies and also the 1000 least profitable movies. '"
https://github.com/bird-house/twitcher,Security Proxy for Web Processing Services (WPS),"b'============================\nTwitcher: OWS Security Proxy\n============================\n\n.. image:: https://img.shields.io/badge/docs-latest-brightgreen.svg\n   :target: http://twitcher.readthedocs.io/en/latest/?badge=latest\n   :alt: Documentation Status\n   \n.. image:: https://zenodo.org/badge/DOI/10.5281/zenodo.3736114.svg\n   :target: https://doi.org/10.5281/zenodo.3736114\n   :alt: Zenodo DOI\n\n.. image:: https://github.com/bird-house/twitcher/actions/workflows/tests.yml/badge.svg?branch=master\n   :target: https://github.com/bird-house/twitcher/actions/workflows/tests.yml\n   :alt: GitHub Actions Status\n\n.. image:: https://img.shields.io/github/license/bird-house/twitcher.svg\n   :target: https://github.com/bird-house/twitcher/blob/master/LICENSE.txt\n   :alt: GitHub license\n\n.. image:: https://badges.gitter.im/bird-house/birdhouse.svg\n   :target: https://gitter.im/bird-house/birdhouse?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge\n   :alt: Join the chat at https://gitter.im/bird-house/birdhouse\n\n\nTwitcher (the bird-watcher)\n  a birdwatcher mainly interested in catching sight of rare birds. ().\n\nTwitcher is a security proxy for OWS services like Web Processing Services (WPS).\nThe proxy service uses OAuth2 access tokens to protect the OWS service access.\nIn addition one can also use X.509 certificates for client authentication.\n\nThe implementation is not restricted to WPS services.\nIt will be extended to more OWS services like WMS (Web Map Service)\nand might also be used for Thredds catalog services.\n\nTwitcher extensions:\n\n*  project.\n* _  middleware by CRIM_. A reimplementation of an old \n  for workflow execution and a Swagger RESTful interface for Web Processing Services.\n\nTwitcher is implemented with the Python  web framework.\n\nYou can try Twitcher online using Binder, or view the notebooks on NBViewer.\n\n.. image:: https://mybinder.org/badge_logo.svg\n   :target: https://mybinder.org/v2/gh/bird-house/twitcher.git/master?filepath=notebooks\n   :alt: Binder Launcher\n   :height: 20\n\n.. image:: https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg\n   :target: https://nbviewer.jupyter.org/github/bird-house/twitcher/tree/master/notebooks/\n   :alt: NBViewer\n   :height: 20\n\nTwitcher is part of the _ project. The documentation is on .\n\nTwitcher  images are also available for most recent tagged versions.\n\n.. _Birdhouse: http://birdhouse.readthedocs.io/en/latest/\n.. _Pyramid: http://www.pylonsproject.org\n.. _ReadTheDocs: http://twitcher.readthedocs.io/en/latest/\n.. _Magpie: https://github.com/Ouranosinc/Magpie\n.. _PAVICS: https://ouranosinc.github.io/pavics-sdi/index.html\n.. _Weaver: https://github.com/crim-ca/weaver\n.. _CRIM: https://www.crim.ca/en\n.. _Swagger: https://swagger.io/\n.. _Docker: https://cloud.docker.com/u/birdhouse/repository/docker/birdhouse/twitcher/general\n'"
https://github.com/opentrv/ors,OpenTRV REST Server,"b""# ors\nOpenTRV REST Server\n\nyou may need to set up the virtual python env\n\nsudo apt-get install python-virtualenv\n\nvirtualenv \nsource /bin/activate\npip install django (I think I'm using 1.8.3)\npip install selenium\nthen python manage test\n\n"""
https://github.com/LibraryOfCongress/data-exploration,Tutorials for working with Library of Congress collections data,"b'# Library of Congress Data Exploration\n\n## About this repository\n\nThe data-exploration repository includes Jupyter notebooks and example scripts using openly available Library of Congress Digital Collections or records. Nearly all of these notebooks use the APIs available through https://www.loc.gov. \n\nFor more information on how to use the loc.gov APIs, see https://www.loc.gov/apis/ .  For more information about how to use the Jupyter notebooks available in this repository, see https://libraryofcongress.github.io/data-exploration/all-tutorials.html \n\nContact LC-labs@loc.gov for questions about these notebooks, or to suggest a new addition. \n'"
https://github.com/Databingo/Databingo,Databooking examples,b'# Databingo\ndatabooking examples\n'
https://github.com/mat-esp-2015/funcoes-douglas-paulo-julyana,funcoes-douglas-paulo-julyana created by Classroom for GitHub,"b'# Fun\xc3\xa7\xc3\xb5es e programa\xc3\xa7\xc3\xa3o defensiva\n\nParte do curso\n\nda .\n\nMinistrado por .\n\n## Objetivos\n\n* Aprender a utilizar fun\xc3\xa7\xc3\xb5es para manejar seu c\xc3\xb3digo\n* Desenvolver t\xc3\xa9cnicas para produzir c\xc3\xb3digo mais correto\n\n## Leitura recomendada\n\n* Li\xc3\xa7\xc3\xa3o de fun\xc3\xa7\xc3\xb5es do \n  (vers\xc3\xa3o 4)\n* Li\xc3\xa7\xc3\xa3o de programa\xc3\xa7\xc3\xa3o defensiva do Software Carpentry\n  :http://swcarpentry.github.io/python-novice-inflammation/08-defensive.html\n* http://www.scipy-lectures.org/intro/language/functions.html\n\n## Prepara\xc3\xa7\xc3\xa3o\n\nUtilize o link enviado por e-mail para criar um reposit\xc3\xb3rio para seu grupo.\nCada membro do grupo deve clicar no link e selecionar o nome do grupo criado na\npr\xc3\xa1tica passada.\nN\xc3\xa3o se esque\xc3\xa7a de sair de sua conta no github.com e no\nclassroom.github.com.\n\nCrie um arquivo chamado  com os nomes completos de todos os\nintegrantes do grupo. Inclua tamb\xc3\xa9m a qual turma pertencem.\n\nAs tarefas para serem feitas est\xc3\xa3o em um .\nPara utilizar o Jupyter, voc\xc3\xaa precisa iniciar um servidor de notebook\nno seu computador.\nAbra o bash e digite:\n\n    $ jupyter notebook\n\nEspere um pouco at\xc3\xa9 aparecer algo como:\n\n    [I 10:50:47.370 NotebookApp] Serving notebooks from local directory: /home/leo\n    [I 10:50:47.370 NotebookApp] 0 active kernels\n    [I 10:50:47.370 NotebookApp] The IPython Notebook is running at: http://localhost:8888/\n    [I 10:50:47.370 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n\nIsso deve abrir o seu navegador padr\xc3\xa3o tamb\xc3\xa9m em uma p\xc3\xa1gina no endere\xc3\xa7o\n.\nEssa p\xc3\xa1gina ir\xc3\xa1 te mostrar as pastas que est\xc3\xa3o em seu computador\n(a partir da pasta onde voc\xc3\xaa rodou ).\n\n## Tarefas\n\nSiga as instru\xc3\xa7\xc3\xb5es em no notebook disponibilizado no reposit\xc3\xb3rio.\nAs tarefas e suas solu\xc3\xa7\xc3\xb5es devem estar contidas nesse notebook.\nPor isso, fa\xc3\xa7a commits de suas mudan\xc3\xa7as ao notebook.\n\nAVISO: N\xc3\xa3o esque\xc3\xa7a de verificar se abriu o notebook no clone correto!\n\nAVISO 2: Ap\xc3\xb3s cada mudan\xc3\xa7a,  +  + .\n\nAVISO 3: ANTES de come\xc3\xa7ar: \n\n## Dicas\n\n* Fa\xc3\xa7am muitos commits. Quanto mais melhor.\n* N\xc3\xa3o se esque\xc3\xa7a do push.\n* Utilize mensagens de commit descritivas. ""Completei a tarefa 1"" \xc3\xa9 melhor que\n  ""mudan\xc3\xa7a"".\n* Escolha nomes descritivos para vari\xc3\xa1veis. ""temperatura"" \xc3\xa9 melhor que ""a"".\n* Descreva o que (e por que) voc\xc3\xaa fez em coment\xc3\xa1rios e c\xc3\xa9lulas de texto.\n  Isso ser\xc3\xa1 muito \xc3\xbatil quando voc\xc3\xaa voltar a essa tarefa depois.\n* Preste aten\xc3\xa7\xc3\xa3o aos detalhes. Leia as instru\xc3\xa7\xc3\xb5es com aten\xc3\xa7\xc3\xa3o.\n\n## Checklist da avalia\xc3\xa7\xc3\xa3o\n\nUtilizaremos a lista abaixo para avaliar a sua solu\xc3\xa7\xc3\xa3o. Cada item poder\xc3\xa1\nreceber a nota ""total"" se atender perfeitamente ao crit\xc3\xa9rio, ""parcial"" (metade\nda nota) se atender parcialmente ao crit\xc3\xa9rio, ou ""zero"" se falhar ao crit\xc3\xa9rio.\nNote que a nota m\xc3\xa1xima (incluindo a b\xc3\xb4nus) \xc3\xa9 10.\n\n    - [] Mensagens de commit que explicam claramente a mudan\xc3\xa7a que foi feita\n      [total|parcial|zero] 0.5 pt\n    - [] Formata\xc3\xa7\xc3\xa3o do c\xc3\xb3digo apropriada.\n      Ex: ,\n      ,  == BOM.\n      , ,\n       == RUIM [total|parcial|zero] 0.5 pt\n    - [] Utilizar vari\xc3\xa1veis ao inv\xc3\xa9s de colocar n\xc3\xbamero ""na m\xc3\xa3o"".\n      Ex: ,  == BOM.\n      ,  == RUIM. [total|parcial|zero] 1 pt\n    - [] C\xc3\xb3digo com coment\xc3\xa1rios que explicam ""por que"" algo foi feito, n\xc3\xa3o s\xc3\xb3\n      ""o que"" foi feito [total|parcial|zero] 1 pt\n    - [] Nomes de vari\xc3\xa1veis descritivos. Ex: , ,\n      ,  == bom. , , ,  == ruim.\n      [total|parcial|zero] 2 pt\n    - [] C\xc3\xb3digo produz a solu\xc3\xa7\xc3\xa3o correta (exatamente como deveria ser\n      impresso) [total|parcial|zero] 5 pt\n    - [] Tarefa b\xc3\xb4nus [total|parcial|zero] 1 pt extra (n\xc3\xa3o ser\xc3\xa1 considerado\n      caso a nota j\xc3\xa1 seja 10)\n\n## License\n\nAll content can be freely used and adapted under the terms of the\n.\n\n\n'"
https://github.com/elmasria/titanic-survival-exploration,Titanic Survival Exploration,"b""# Titanic Survival Exploration\n\n## Project Overview\n\nCreate decision functions that attempt to Accurately predict survival outcomes, for at least 80%, from the 1912 Titanic disaster based on each passenger's features, such as sex and age.\n\n### Install\n\n#### Software Requirements\n\n- \n- \n- \n- \n- \n- \n\n### Run\n\nIn a terminal or command window, navigate to the top-level project directory , run one of the following commands:\n\n\nor\n\n\nThis will open the Jupyter Notebook software and project file in your web browser.\n\n### Data\n\nThe dataset used in this project is included as . This dataset is provided by Udacity and contains the following attributes:\n\nFeatures\n-  : Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n-  : Name\n-  : Sex\n-  : Age\n-  : Number of Siblings/Spouses Aboard\n-  : Number of Parents/Children Aboard\n-  : Ticket Number\n-  : Passenger Fare\n-  : Cabin\n-  : Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n\nTarget Variable\n-  : Survival (0 = No; 1 = Yes)\n\n\n"""
https://github.com/ducha-aiki/pytorch-sift,PyTorch implementation of SIFT descriptor,"b""This is an differentiable  implementation of SIFT patch descriptor. It is very slow for describing one patch, but quite fast for batch. It can be used for descriptop-based learning shape of affine feature.\n\nUPD 08/2019 : pytorch-sift is added to  and available by \n\nThere are different implementations of the SIFT on the web. I tried to match , which gives high quality features for image retrieval . However, on planar datasets, it is inferior to .\nThe main difference is gaussian weighting window parameters, so I have made a vlfeat-like version too.  MP version weights patch center much more (see image below, left) and additionally crops everything outside the circular region. Right is vlfeat version\n\n\n\n\n\n\n\n\nResults:\n\n\n\n\n\n    \nSpeed: \n- 0.00246 s per 65x65 patch - \n- 0.00028 s per 65x65 patch - \n- 0.00074 s per 65x65 patch - CPU, 256 patches per batch\n- 0.00038 s per 65x65 patch - GPU (GM940, mobile), 256 patches per batch\n- 0.00038 s per 65x65 patch - GPU (GM940, mobile), 256 patches per batch\n\n\n\n\nIf you use this code for academic purposes, please cite the following paper:\n\n\n\n"""
https://github.com/sarineb/lebanon-refugee-data,Analyzing data from informal settlements of Syrian refugees in Lebanon,b'# lebanon-refugee-data\nAnalyzing data from informal settlements of Syrian refugees in Lebanon\n'
https://github.com/bohare/MNIST-Classification,Classifying standard MNIST dataset with NNs.,b'# MNIST-Classification\nClassifying standard MNIST dataset with NNs.\n\nTrying Neural Networks out on Git.\n'
https://github.com/ccopelan/ubcs3-exercises,Exercises for the UBC Scientific Software Seminar,"b'# ubcs3-exercises\n\nExercises for the UBC Scientific Software Seminar\n\n## Exercise 1. \n\nWrite a Jupyter notebook describing a project that youre interested in.\n\nMy notebook is called ""Exercise1Notebook""\n'"
https://github.com/matthiaskoenig/sbmlutils,Python utilities for SBML,"b'.. image:: https://github.com/matthiaskoenig/sbmlutils/raw/develop/docs_builder/images/sbmlutils-logo-60.png\n   :align: left\n   :alt: sbmlutils logo\n\nsbmlutils: python utilities for SBML\n====================================\n\n.. image:: https://github.com/matthiaskoenig/sbmlutils/workflows/CI-CD/badge.svg\n   :target: https://github.com/matthiaskoenig/sbmlutils/workflows/CI-CD\n   :alt: GitHub Actions CI/CD Status\n\n.. image:: https://img.shields.io/pypi/v/sbmlutils.svg\n   :target: https://pypi.org/project/sbmlutils/\n   :alt: Current PyPI Version\n\n.. image:: https://img.shields.io/pypi/pyversions/sbmlutils.svg\n   :target: https://pypi.org/project/sbmlutils/\n   :alt: Supported Python Versions\n\n.. image:: https://img.shields.io/pypi/l/sbmlutils.svg\n   :target: http://opensource.org/licenses/LGPL-3.0\n   :alt: GNU Lesser General Public License 3\n\n.. image:: https://codecov.io/gh/matthiaskoenig/sbmlutils/branch/develop/graph/badge.svg\n   :target: https://codecov.io/gh/matthiaskoenig/sbmlutils\n   :alt: Codecov\n\n.. image:: https://readthedocs.org/projects/sbmlutils/badge/?version=latest\n   :target: https://sbmlutils.readthedocs.io/en/latest/?badge=latest\n   :alt: Documentation Status\n\n.. image:: https://zenodo.org/badge/55952847.svg\n   :target: https://zenodo.org/badge/latestdoi/55952847\n   :alt: Zenodo DOI\n\n.. image:: https://img.shields.io/badge/code%20style-black-000000.svg\n   :target: https://github.com/ambv/black\n   :alt: Black\n\n.. image:: http://www.mypy-lang.org/static/mypy_badge.svg\n   :target: http://mypy-lang.org/\n   :alt: mypy\n\nsbmlutils is a collection of python utilities for working with\n__ models implemented on top of\n__ and other libraries\nwith source code available from .\n\nFeatures include among others\n\n-  helper functions for model creation, manipulation, and annotation\n-  HTML reports of SBML models .\n-  file converters (XPP)\n\nThe documentation is available on . \nIf you have any questions or issues please .\n\n.. image:: docs/presentations/reproducibility_center_2021/screenshot.png\n   :target: https://youtu.be/SxIq8qeXxD0?t=1261\n   :alt: sbmlutils introduction\n   :height: 200px\n\nHow to cite\n===========\n.. image:: https://zenodo.org/badge/55952847.svg\n   :target: https://zenodo.org/badge/latestdoi/55952847\n   :alt: Zenodo DOI\n\nContributing\n============\n\nContributions are always welcome! Please read the __ to\nget started.\n\nLicense\n=======\n\n* Source Code: \n* Documentation: .\nMatthias K\xc3\xb6nig has received funding from the EOSCsecretariat.eu which has received funding \nfrom the European Unions Horizon Programme call H2020-INFRAEOSC-05-2018-2019, grant Agreement number 831644.\n\nInstallation\n============\n is available from __ and \ncan be installed via:: \n\n    pip install sbmlutils\n\nRequirements\n------------\n is required which can be installed on linux via::\n\n    apt-get install python-tk\n    apt-get install python3-tk\n\nPlease see the respective installation instructions for your operating system.\n\nDevelop version\n---------------\nThe latest develop version can be installed via::\n\n    pip install git+https://github.com/matthiaskoenig/sbmlutils.git@develop\n\nOr via cloning the repository and installing via::\n\n    git clone https://github.com/matthiaskoenig/sbmlutils.git\n    cd sbmlutils\n    pip install -e .\n\nTo install for development use::\n\n    pip install -e .[development]\n\n\xc2\xa9 2017-2023 Matthias K\xc3\xb6nig\n'"
https://github.com/richard-alexander/language-translation,Deep learning: Simple model for language translation using encoder/decoder recurrent neural network (RNN),b'# language-translation\nDeep learning: Simple model for language translation using encoder/decoder recurrent neural network (RNN)\n'
https://github.com/chrishokamp/constrained_decoding,Lexically constrained decoding for sequence generation using Grid Beam Search,"b'## Lexically Constrained Decoding with Grid Beam Search\n\nThis project is a reference implementation of Grid Beam Search (GBS) as presented in .\n\nWe provide two sample implementations of translation models -- one using our framework for\nNeural Interactive Machine Translation, \nand another for models trained with .\n\nNMT models trained with Nematus model work out of the box. This project can also be used as a general-purpose \nensembled decoder for Nematus models with or without constraints. \n\n### Quick Start\n\n\n\n\n#### Translating with a Nematus Model: A Full Example \n\nWe assume youve already installed  \n\nYou need to install the theano branch of  \n \n\n\nNow download assets and run constrained translation\n\n\n\n### Citing\n\nIf you use code or ideas from this project, please cite:\n\n\n\n\n### Running Experiments\n\nFor PRIMT and Domain Adaptation experiments, the lexical constraints are stored in  files. \nThe format is : \nEach constraint is a list of tokens, and each segment has a list of constraints. The length of the \nouter list in the  should be the same as number of segments in the source data. If there are no constraints for a\nsegment, there should be an empty list. \n\n\n### Performance\n\nThe current implementation is pretty slow, and it gets slower the more constraints you add :disappointed:. \nThe GBS algorithm can be easily parallelized, because each cell in a column is independent of the others (see paper). \nHowever, implementing this requires us to make some assumptions about the underlying model, and would thus\nlimit the generality of the code base. If you have ideas about how to make things faster, please create an issue. \n\n### Features\n\nEnsembling and weighted decoding for Nematus models\n\n\n### Using the Prototype server\n\nWe provide a  for convenience while prototyping. \n\n\n\n\n\n'"
https://github.com/daedaluschan/HKAiportSchedule,HKAiportSchedule,b'# HKAiportSchedule\nHKAiportSchedule\n\nI am trying to write a show scrip to capture the arrival / departure schedule & actually delay for all the flight using HKIA (Hong Kong International Airport).\n\nThis is still in progress and feel free to provide your comment. :)\n\nLibraries used are quite common \n* lxml (for html parsing)\n* requests (for making HTTP calls)\n* pandas (for data manipulation)\n'
https://github.com/nicolasalvarez/titanic_survival_exploration,Project 0: Titanic Survival Exploration,"b'# Project 0: Introduction and Fundamentals\n## Titanic Survival Exploration\n\n### Install\n\nThis project requires Python 2.7 and the following Python libraries installed:\n\n- \n- \n- \n- \n\nYou will also need to have software installed to run and execute an \n\nUdacity recommends our students install , a pre-packaged Python distribution that contains all of the necessary libraries and software for this project.\n\n### Code\n\nTemplate code is provided in the notebook  notebook file. Additional supporting code can be found in . While some code has already been implemented to get you started, you will need to implement additional functionality when requested to successfully complete the project.\n\n### Run\n\nIn a terminal or command window, navigate to the top-level project directory  (that contains this README) and run one of the following commands:\n\n\nor\n\n\nThis will open the iPython Notebook software and project file in your web browser.\n\n## Data\n\nThe dataset used in this project is included as . This dataset is provided by Udacity and contains the following attributes:\n\n-  ? Survival (0 = No; 1 = Yes)\n-  ? Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n-  ? Name\n-  ? Sex\n-  ? Age\n-  ? Number of Siblings/Spouses Aboard\n-  ? Number of Parents/Children Aboard\n-  ? Ticket Number\n-  ? Passenger Fare\n-  ? Cabin\n-  ? Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n'"
https://github.com/NIEHS/orso,ORSO (Online Resource for Social Omics) is an web application that leverages a social network to connect life scientists to NGS data of interest.,"b'# ORSO: Online Resource for Social Omics\n\nORSO (Online Resource for Social Omics) is a web application designed to help users find next generation sequencing (NGS) datasets relevant to their research interests. ORSO performs this task by creating and maintaining a network of datasets connected based on similarity in primary read coverage values and annotated metadata. An ORSO user can follow these connections to discover datasets with similar characteristics. ORSO is also a social network. Users may favorite datasets and follow other users, creating new connections that influence how datasets are recommended.\n\nFor instructions on how to use the ORSO web application, please see the . We also provide documentation for installation and administration in the .\n'"
https://github.com/1501213456/2016.M3.TQF-ML.FundPerformanceAttribution,Fund Performance Attribution and Prediction,"b'# 2016.M3.TQF-ML.FundPerformanceAttribution\n### Data:\n* \n* \n### Final Report:\n* \n### Code:\n* \n\nThank you for this class, Prof. Choi! ^_^\n'"
https://github.com/EricDoug/tensorflowDev,tensorflowDev主要记录我的一些学习tensorflow的代码。,b'# tensorflowDev\ntensorflowDev\xe4\xb8\xbb\xe8\xa6\x81\xe8\xae\xb0\xe5\xbd\x95\xe6\x88\x91\xe7\x9a\x84\xe4\xb8\x80\xe4\xba\x9b\xe5\xad\xa6\xe4\xb9\xa0tensorflow\xe7\x9a\x84\xe4\xbb\xa3\xe7\xa0\x81\xe3\x80\x82\n\n## RNN\n'
https://github.com/Himanshu141/StreetSignRecognition,ML Hackathon by MapMyIndia and BITS ACM on Kaggle,"b""# Street-Sign-Detection\n\nThe readme will be updated soon\n\nTo run the code, you'll have to change path names in the files.\n\nThe subfolders have the CNN models and the Cascades.\n\nThe csv files are the submissions.\n\nThe code may not be currently well documented. We will work on proper documentation soon.\n\n## Authors:\n\n\n\n\n\n\n"""
https://github.com/chembl/autoencoder_ipython,Ipython notebook for blog post entry,b'# autoencoder_ipython\nIpython notebook for blog post entry\n\nBased in:\n\nLink to the paper\narXiv\n\nLink to the repository\ngithub\n'
https://github.com/jwilber/python_nlp,Python nlp stuff with NLTK,b'init\n'
https://github.com/openhealthcare/openspirometer,Better open data with the open spirometer,"b""Open-spirometer\n====\nA spirometer (http://en.wikipedia.org/wiki/Spirometry) is a device that measures your breathing. This is particularly helpful medically for the managment of chronic lung diseases such as asthma, pulmonary fibrosis, cystic fibrosis, and COPD.\n\nOpen-spirometer is special because it's design will be open and because it will capture and share, with user consent, high quality open spirometry data from day one. \n\nWe'd love you to get involved by using what we make, reporting bugs/suggesting improvements, and fixing bugs/making improvements.  Please refer to the CONTRIBUTING file.\n\nOpen-spirometer is an open (open governance/design/hardware/source) commercial health product by Open Health Care UK that we've not made yet.\n\n\nWhat problem does Open-spirometer solve?\n======\nOpen-spirometer aims to solve the paucity of high quality independant open spirometry data problem. We think patients and health care professionals would be empowered to make better decisions together if they had better data. At present spirometery data often lies locked up in proprietary spirometry devices and non-digital data formats. This is suboptimal. Why we need an open spirometer is also described in a blog post here http://www.openhealthcare.org.uk/?p=449\n\n\nOpen governance\n======\n\nRoadmap: \n\nDiscussion List: \n\nAccess: availability of the latest source code, developer\nsupport mechanisms, public roadmap, and transparency of\ndecision-making\n\nDevelopment: the ability of developers to influence the content\nand direction of the project\n\nDerivatives: the ability for developers to create and distribute\nderivatives of the source code in the form of spin-off\x0b projects,\nhandsets or applications.\n\nCommunity: a community structure that does not discriminate\nbetween developers\n\n(from http://www.visionmobile.com/blog/2011/07/the-open-governance-index-measuring-openness-from-android-to-webkit/)\n\nOpen source\n======\nGNU Affero GPLv3\n\nCommunications\n======\nhello@openhealthcare.org.uk\n\nhttp://www.openhealthcare.org.uk\n\nhttps://twitter.com/ohcuk\n\nchannel #ohc_dev on freenode\n\nhttps://docs.google.com/document/d/1gvlvsJGYwKLe5FtpEOpRt48q_SDEKZP6m3eb6Pu2tlc/edit\n\nDemo\n======\nnot yet\n"""
https://github.com/dvm-shlee/PyNIT,Python based NeuroImaging Toolkit,"b'# PyNIT (Python NeuroImaging Toolkit)\n### Version: 0.2.1\n\nDeprecation Warning: \nPyNIT module is not maintained anymore, please use PyNIPT (https://github.com/dvm-shlee/pynipt) instead.'"
https://github.com/wilkens/edinburgh-masterclass,"Materials for masterclass in quantitative humanities at Edinburgh, Sept 2016","b""# Edinburgh Master Class \n\nMaterials for masterclass in quantitative humanities at Edinburgh, September 2016.\n\nNB. If you're reading these files on GitHub (rather than locally on your machine), you won't be able to change or run any of the code. To run your own copy, download this project from GitHub, make sure you have working Python 3 environment, then start a notebook server via the Anaconda Navigator or at the command line with . More details in the  notebook.\n\nDirectories are named according to contents:\n\n* Data contains input data used for analysis. It has two subfolders, Texts and Other, containing, respectively, plain text versions of literary texts and other types of input data.\n* Notebooks contains code for the course in the form of Jupyter notebooks\n* Readings contains supplemental articles for discussion\n* Results contains the output of various analyses\n* Slides contains a PDF of the opening lecture slides\n"""
https://github.com/donkey-hotei/neuraldata,Exploring Neural Data @ Brown,b'Exploring Neural Data\n---------------------\nProblem set solutions and notes for the Exploring Neural data course offered by Brown Univerity.\n\nThis course is an opprotunity to learn about neuroscience research and explore questions related to how brains work. It is an intoductory-level course that help the student understand real-life challenges faced by  neuroscientists as they work with the large amount of data that they collect from the brain.\n\n'
https://github.com/sujitpal/intro-dl-talk-code,Jupyter notebooks and code for Intro to DL talk at Genesys,b'# intro-dl-talk-code\nJupyter notebooks and code for Intro to DL talk at Genesys\n'
https://github.com/Pmcmanus02/hello-world,Starting my Git hub.,b'# hello-world\nStarting my Git hub.\nWass up peeps? This is the begining of your great adventure!\n'
https://github.com/cabaf/udacity-face-generation,Generating celebrity faces with GANs,b'# Generation of Face of Celibrities with GANs\nGenerating celebrity faces with GANs as final project for the udacity deep learning nanodegree.\n'
https://github.com/yeshg/AI_Art,Deepdream approach to generate high-resolution visualizations of Convolutional Neural Networks.,"b""# AI_Art\n\nTo run the code: need an installation of Caffe with built pycaffe libraries, as well as the python libraries numpy, scipy and PIL. For instructions on how to install Caffe and pycaffe, refer to the installation guide . Before running the ipython notebooks, you'll also need to download the , and insert the path of the pycaffe installation into  and the model path to the googlenet model into .\n\nThis code was based on the  shared by Google, as well as the  by Kyle McDonald and Auduno's article and  on visualizations with GoogleNet. The idea of using bilateral filtering comes from Mike Tyka.\n\n - image net class id to class\n"""
https://github.com/glouppe/tutorials-scikit-learn,Scikit-Learn tutorials,"b'# Scikit-Learn tutorials\n\n1. Tutorial on machine learning and Scikit-Learn (beginner level).\n2. Tutorial on robust and calibrated estimators with Scikit-Learn (mid level)\n\nContact: @glouppe | BSD 3-clause license\n\n## Installation instructions\n\n1)  and install the latest Anaconda distribution, coming with Python 3.5 and the full scientific Python stack. \n\n2) Install dependencies:\n\n\n3) Clone this repository and start Jupyter\n\n\n## Launch on Binder without installing anything!\n\n\n\n'"
https://github.com/spm2164/data-homework,homework for allison!,b'# data-homework\n'
https://github.com/Biodun/Data-Science-Projects,Personal data science projects I've done,b'# Data-Science-Projects\nCollection of data analyses on topics I find interesting\n\nBay Area Bike Share data analysis\n---\n\n'
https://github.com/jorghyq/Gwyddion-Utils,Some modules for Gwyddions,"b'# Gwyddion-Utils\nThis project is to provide a SPM data browser for fast image reviewing and saving (like Nanonis Scan Inspector). It is based on , using the Gwyddion python API.\n\nIt can be used independently as GUI to browse SPM image data, or started as a\nfunction in Gwyddion.\n\nIts biggest advantage is the fast review of the SPM data and fast saving\nfunction.\n\nCurrently, it can handle Nanonis .sxm file and Omicron .mtrx file.\n\n## Usage\n### Use as independent GUI\nTo use it independently, you need to have Gwyddion installed with pygwy.\n\nYou need to be able to use the gwyddion outside Gwyddion software by .\n\nThen you can simply run the SPMBrowser.py in GwyBrowser/gwybrowser.\n\n### Use within the Gwyddion\nTo use it within the Gwyddion, you need to have Gwyddion installed with pygwy.\n\nFor the python part, you need pygtk, numpy, matplotlib and re.\n\nPut the pygwy, ui and icon folders inside the GwyBrowser to the ~/.gwyddion file.\n'"
https://github.com/esmatanis/Advanced_Lane_Lines_Finding,"Developed a pipeline to process a video stream from a car driving on a highway in order to robustly identify individual lane lines from the road irrespective of curvature.  Primary languages/libraries: Python, OpenCV","b'## Advanced Lane Finding\nIn this project, our goal is to write a software pipeline to identify the lane boundaries in a video. Following steps were implemented to acheive the goal :\n\n* Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n* Apply a distortion correction to raw images.\n* Use color transforms, gradients, etc., to create a thresholded binary image.\n* Apply a perspective transform to rectify binary image (""birds-eye view"").\n* Detect lane pixels and fit to find the lane boundary.\n* Determine the curvature of the lane and vehicle position with respect to center.\n* Warp the detected lane boundaries back onto the original image.\n* Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.\n\nThe project consists of following files : \n\n*  : The script for camera calibration.\n*  : The script for applying color and gradient thresholds, applying perspective transformation and detect lane pixels.\n*  : The main script which performs lane detection analysis on video frames.    \n*  : IPython notebook with step-by-step description and execution of entire code. \n\n--\n\n### Camera calibration\nThe code for this step is contained in the file  wherein the relevant class to handle all operations is called . \nI start by reading all the chess board images using  function. The ""object points"", which will be the (x, y, z) coordinates of the chessboard corners in the world, is fixed on the (x, y) plane at z=0, such that the object points are the same for each calibration image. Thus,  is appended with a copy of the same coordinates every time I successfully detect all chessboard corners in a test image using the function .  will be appended with the (x, y) pixel position of each of the corners in the image plane with each successful chessboard detection. Corners for all but three of the images were detected, here are four examples : \n\n\n\nNote that three of the images did not have all the 9x6 corners which will be used to test calibration.\n\nI then used the output  and  to compute the camera calibration and distortion coefficients in the function using the  function (please see  for details). I applied this distortion correction to the test image using the  function (please see  for details) and obtained the following result for the three test images : \n\n\n\n--\n\n### Thresholding\nI used a combination of color and gradient thresholds to split image into 7 binary channels (please see  function on line 35 of the file ). Images are first converted to HSV color space and 7 channels are selected : 4 of these channels are obtained by applying Sobel gradient along x-direction on S and V of HSV keeping the sign of threshold instead of taking absolute values i.e. positive and negative gradient thresholds. The S gradient allows us to pick up yellow line edges while V gradient thresholds pick both white and yellow edges. The other 3 channels are obtained by applying color thresholds, H color threshold for picking yellow and V threshold for picking white color (I used two thresholds for white in case one fails). Here are the results of applying gradient thresholds on V channel and white color selection: \n\n     \n \n\nFrom the above images, it can be seen that using positive and negative gradients allows us to differentiate between lane lines from shadow lines and other irrelevant road markings. Concretely, for lane lines, positive and negative threshold lines always appear in vicinity.\nHere are the results for applying gradient thresholds on S channel and yellow color selection: \n\n     \n\n\n--\n\n\n### Perspective transform\nThe code for my perspective transform includes a function called , which appears on line 12 in the file . The source and destination points were chosen as follows:\n\n| Source        | Destination   |\n| ------------- |:-------------:| \n| (225,700)     | (320,720)     |\n| (590,450)     | (320,-100)    |\n| (690,450)     | (960,-100)    |\n| (1055,700)    | (960,720)     |\n\nI verified that my perspective transform was working as expected by drawing the source and destination points onto test images and its warped counterpart to verify that the lines appear parallel in the warped image. \n\n \n\n--\n\n### Lane detection\nTwo approaches were implemented for finding lane lines. The first approach is the window sliding method which is used when prior lane information does not exist or missing. Although this appraoch is often more robust, it is also computationally time consuming. The other approach is searching for lane pixels in a target region determined by the previous window frame. The second approach is less compuationally intensive and is used for most of the video frames. \n\nThe window sliding method is implemented in  function on line 231 of the file  using 15 windows for each side of the lane. Information from all 7 channels of the thresholding step is utilized. \nFor example, I require that positive gradient edges be accompanied by negative gradient edges, which allows for differentiating lane markings from shadows. For full selection criteria, see the function  on line 107 of .  Here are the results on a test image : \n\n \nAs seen above, a more targeted search window is used when relevant pixels are detected. To demonstrate the working of our thresholding, here is one from a more challenging test image: \n\n\n\nIn order to fit curves to ""good"" pixels, I have used  function to fit a second order polynomial, implementation of which can be found in the function  on line 348 of . Concretely, the best fit curves are parametrized as follows :\n$$x = A y^2 + B y + C$$\nOnce the lane pixels have been detected using sliding window method, a targeted search is performed in subsequent video frames by focusing on region around the best fit curves from the previous frame (see  on line 405 of  ). Here is an example : \n\n\n\n\n--\n\n### Lane parameters\nTo calculate lane parameters, measurement units have to be changed from pixels to real world units such as meter. I use the following conversion : $k_x=3.7/700$ m/pixel in the x-direction and $k_y=30/720$ m/pixel.\n\n* Radius of curvature : The radius of curvature is implemented in  function on line 72 of . Since, the original fit was performed in the pixel space, I use the following formula for conversion to real world space :\n\t$$ R = frac{(k_x^2 + k_y^2(2Ay+B)^2)^{3/2}}{2 k_x k_y |A|}$$ \n\t\n* To find the position of the vehicle with respect to the center, I calculate the x-coordinate of both the left and right lane lines with repect to the center at the base of the image. The average of the two x-coordinates is the position of the center of the road with respect to the camera center. \n\n-- \n\n### Final output\nTo store the history of video frames, I have implemented a class  which can be found in  on line 17. The final pipeline is implemented in the function  (also to be found in  on line 132) which takes as input video frame . In this function, I toggle between two methods for finding lane line pixels : window sliding method is used for the first few frames and every 10th frame while a targetted search is used when lane markings were found in the previous video frame. Several checks are performed to check if the best fit curves actually make sense :\n\n* Fit parameters are required to not be significantly different from previous frame.  \n* Left and right lane lines should not be diverging.\n* The base gap between left and right lines should not be too large or too small. \n* The residuals from curve fit (normalized by the number of pixels found) should not be too big. \n\n Further, I have implemented averaging of the fit parameters over last few iterations so as to avoid jittery lines. \n\nHere are a couple sample output images :   \n\n\nHere is the final output video : \n\n   \n\nand here is one on a more challenging road : \n\n \n\n--\n\n### Discussion\nThe most challenging aspect of this project was the shadows and road markings, specially on the challenging video. I was able to deal with this issue by requiring that positive and negative gradients be in vicinity. The shadows and irrelavant road markings usually have either positive gradients or negative gardient edges but not both unlike the lane markings which have both. \n\nHowever, there is a lot of scope for improvement which I hope to address in the future (currently not implemented due to lack of time) :\n\n* Use of convolution to select hot pixels can help in removing outliers.\n* The consistency checks can be made more robust. In particular, currently they do not work well when roads are curvy as in the harder challenge video.\n* It might be worth exploring more thresholds such as gradient directions.  \n* Information about yellow and white markings can be easily included.     \n '"
https://github.com/TheGoldenRatio/thegoldenratio.github.com,blog,b'## This is my blog. ##\n\nDesign is heavily based off of the [<marko.inline.RawText object at 0x000002CBAF3A66C8>] theme.\n'
https://github.com/kingsman142/Projects,Some of my ongoing and finished projects,"b'# Projects\n\n## Greyscaler\n\nThis repo contains my greyscale program, bitmaps.java - BinaryOutput.png and GreyOutput.png are the two output files associated with the program for the user to see sample output.  Also associated with the greyscale project are: cat.jpg, city.jpg, sunset.jpg, up.jpg, leaf.jpg, all of which are sample images for the user to test with.  However, any image can be used as long as its in the same directory as bitmaps.java. The last two files greyValues.bmp and bitmap.bmp store the grey values and binary values respectively of the images.\n\n## Racing Game\n\nA game where you can drive around a racetrack with a racecar.  Written in C++, it utilizes the SFML 2.3.2 library for graphics.  In order to be ran, the respective files must be put into a project and linked correctly. NOT FINISHED\n\n## Random Terrain\n\nGenerates random terrain and displays it to the user in a JFrame.  Currently, the RandomTerrain.java file generates terrain using the midpoint displacement algorithm, so the terrain is one-dimensional. Its corresponding examples can be seen through MidpointDisplacementExamples.png.  In DiamondSquare.java, terrain is generated using the diamond-square algorithm, so the terrain is outputted to a two-dimensional bitmap but can easily be translated to 3 dimensions.  Its corresponding examples are stored in DiamondSquareExamples.png, with each heightmap being 512x512.\n\n## Networking\n\nContains two files, IRCClient.java and IRCServer.java.  First, run the server, and then the client.  A one way connection works where the client can send messages to the server and it will display the messages but the server wont respond to the client.  Make sure theyre both on the same point using command-line arguments.  I was trying to make an IRC (Internet Relay Chat), but I cant figure out MultiCastSockets right now so Im delaying this project.\n\n## LWJGL-PC\n\nThis project is perhaps the most satisfying of the ones here.  It utilizes LWJGL (a java binding for OpenGL), with OpenGL shaders v3.30 Core to generate 3D random terrain.  In order to generate the terrain, I used my DiamondSquare.java file from the ""Random Terrain"" project to generate the values for each point.  Currently, using the diamond-square algorithm, the terrain is size 512x512.  With a few modifications, the terrain can not only show the grey value of each point, but can also show if the value should be at grass/ground level, hill level, or mountain top level (0-.33 = ground, .33-.66 = hill, .66-1.00 = mountain top) using green, grey, and white.\n\n## LWJGL\n\nMy 3D random terrain project; similar to the early stages of LWJGL-PC.  However, I believe OSX and Windows have different OpenGL drivers so they dont run identically.  I might have to double check that.\n\n## Videos\n\nMy archive of videos from projects.  Currently, there is only one video, and it is for the LWJGL-PC project to show users how the program looks with multiple tests and a couple of modifications.\n'"
https://github.com/habinez/Google-ML-Recipes,A guide to a a friend following Machine Learning Recipes with Josh Gordon,b'# Google-ML-Recipes\nA guide to a friend following Machine Learning [Recipes] (https://www.youtube.com/watch?v=tNa99PG8hR8&list=PLOU2XLYxmsIIuiBfYad6rFYQU_jL2ryal&index=1) with Josh Gordon\n\n1. Week 0 : \n2. Week 1 :  - Machine Learning Recipes #1\n3. Week 1 :  - Machine Learning Recipes #2\n'
https://github.com/mmalmrose/Continuum_flux_demo,read in and plot some astronomical spectra while fitting components to the continuum,b'# Continuum_flux_demo\nread in and plot some astronomical spectra while fitting components to the continuum\n'
https://github.com/nicolasfauchereau/SAM,Calculates the Southern Annular Mode index (SAM) from NCEP/NCAR monthly anomalies,b'# Southern Annular Mode calculation\n\nThis notebook (calculates_SAM_index.ipynb) calculates the Southern Annular Mode index (SAM) as the principal \ncomponent associated with the first EOF coming from an EOF decomposition of the 850hPa\nmonthly anomalies (1981-2010 climatology) from the NCEP/NCAR (aka NCEP1) reanalysis dataset\n\nYou can visualize the notebook by following  \n'
https://github.com/VeerpalBrar/ML-Algorithms-from-Scratch,Implement algorithms from scratch,b'### Implemented So far:\n- Linear Regression on Titanic Dataset as seen on Kaggle\n- 3-layer Neural Network on randomly generated data\n'
https://github.com/toddhodes/ipynb,"iphython note example, checking out the github integration",b'\nThis is from a music class utilizing ipython jupyter notebooks; looking into the extent of github integration with jupyter\n'
https://github.com/hatshex/text-mining,Clase de minería de texto del ITAM,b'# Miner\xc3\xada de texto | ITAM | Prof. V\xc3\xadctor Mijangos\n\n\xc2\xbfDe qu\xc3\xa9 va?\n* Introducci\xc3\xb3n\n  - Miner\xc3\xada de texto\n  - Datos estructurados y no estructurados\n* Lenguaje Natural\n  - Herramientas Ling\n  - Comportamiento estad\xc3\xadstico\n  - Representaci\xc3\xb3n textual\n  - Clasificaci\xc3\xb3n y agrupamiento de textos\n  - Recuperaci\xc3\xb3n de informaci\xc3\xb3n\n  - Modelado de t\xc3\xb3picos\n  - Aplicaciones\n\nBibliograf\xc3\xada\n* Foundations of Statistical Natural Language Processing. Christopher Manning and Hinrich Schuetze.\n* Natural Language Processing and Text Mining. Anne Kao and Stephen R. Poteet\n* https://sites.google.com/site/victormijangoscruz/cursos/metodos-analiticos-para-texto\n\nArt\xc3\xadculos interesantes\n* http://es.slideshare.net/ChristopherMoody3/word2vec-lda-and-introducing-a-new-hybrid-algorithm-lda2vec-57135994\n* http://nlpx.net/archives/330\n* https://datawarrior.wordpress.com/2016/02/15/lda2vec-a-hybrid-of-lda-and-word2vec/\n* http://multithreaded.stitchfix.com/blog/2015/03/11/word-is-worth-a-thousand-vectors/\n* http://insightdatascience.com/blog/thisplusthat_a_search_engine_that_lets_you_add_words_as_vectors.html\n* https://www.cs.bgu.ac.il/~elhadad/nlp16.html\n* http://rare-technologies.com/word2vec-tutorial/\n* http://mghassem.mit.edu/insights-word2vec/\n* https://www.youtube.com/watch?v=tdLmf8t4oqM\n* https://www.youtube.com/watch?v=T8tQZChniMk\n* https://www.youtube.com/watch?v=S75EdAcXHKk&nohtml5=False\n\n* Welcome to the 6th Lisbon Machine Learning School! http://lxmls.it.pt/2016/\n'
https://github.com/GGYIMAH1031/LPinPython,Linear Programming in Python (Using PuLP),b'# LPinPython\nLinear Programming in Python (Using PuLP)\n'
https://github.com/DmitryUlyanov/AGE,"Code for the paper ""Adversarial Generator-Encoder Networks""","b'This repository contains code for the paper\n\n**** (AAAI18) by Dmitry Ulyanov, Andrea Vedaldi, Victor Lempitsky.\n\n\n\n## Pretrained models\n\nThis is how you can access the models used to generate figures in the paper.\n\n1) First install dev version of  and make sure you have  ready.\n\n2) Then download the models with the script:\n\n\n3) Run  and go through .\n\nHere is an example of samples and reconstructions for ,  and  datasets generated with .\n\n#### Celeba\n\n|Samples    |Reconstructions|\n|:---------:|:-------------:|\n| |  |\n\n#### Cifar10\n\n|Samples    |Reconstructions|\n|:---------:|:-------------:|\n| |  |\n\n#### Tiny ImageNet\n\n|Samples    |Reconstructions|\n|:---------:|:-------------:|\n| |  |\n\n\n# Training\n\nUse  script to train a model. Here are the most important parameters:\n\n* : one of [celeba, cifar10, imagenet, svhn, mnist]\n* : for datasets included in  it is a directory where everything will be downloaded to; for imagenet, celeba datasets it is a path to a directory with folders  and  inside.\n* :\n* : path to a folder, where checkpoints will be stored\n* : dimensionality of latent space\n* : Batch size. Default 64.\n* :  file with generator definition. Searched in  directory\n* :  file with generator definition. Searched in  directory\n* : path to a generator checkpoint to load from\n* : path to an encoder checkpoint to load from\n* : number of epoch to run\n* : epoch number to start from. Useful for finetuning.\n* : Update plan for encoder. .\n* : Update plan for generator. .\n\nAnd misc arguments:\n* : number of dataloader workers.\n* : controlles number of channels in generator\n* : controlles number of channels in encoder\n* : parameter for ADAM optimizer\n* : do not use GPU\n* : Parametric  or non-parametric  way to compute KL. Parametric fits  Gaussian into data, non-parametric is based on nearest neighbors. Default: .\n* : What KL to compute:  or . Default is .\n* :  for uniform on sphere or . Default .\n* : loss to use as reconstruction loss in latent space. . Default .\n* : loss to use as reconstruction loss in data space. . Default .\n* : each  epochs a learning rate is dropped.\n* : controls how often intermediate results are stored. Default .\n* : random seed. Default .\n\n\nHere is  you can start with:\n\n### Celeba\nLet  to be a directory with two folders , , each with the images for corresponding split.\n\n\n\nIt is beneficial to finetune the model with larger  and stronger matching weight then:\n\n\n### Imagenet\n\n\n\nIt can be beneficial to switch to  batch size after several epochs.\n\n### Cifar10\n\n\n\n---------------------\n\nTested with python 2.7.\n\nImplementation is based on pyTorch .\n\n# Citation\n\nIf you found this code useful please cite our paper\n\n\n'"
https://github.com/cornetj2/Star-1-Progress,Current work on star 6224062.,b'# Star-1-Progress\nCurrent work on star 6224062.\n'
https://github.com/Zoha131/learning_pandas,This is a practice repository. I have learnt git. So I am practicing it. Also learning pandas.,"b'## Welcome to GitHub Pages\n\nYou can use the  to maintain and preview the content for your website in Markdown files.\n\nWhenever you commit to this repository, GitHub Pages will run  to rebuild the pages in your site, from the content in your Markdown files.\n\n### Markdown\n\nMarkdown is a lightweight and easy-to-use syntax for styling your writing. It includes conventions for\n\n\n\nFor more details see .\n\n### Jekyll Themes\n\nYour Pages site will use the layout and styles from the Jekyll theme you have selected in your . The name of this theme is saved in the Jekyll  configuration file.\n\n### Support or Contact\n\nHaving trouble with Pages? Check out our  or  and we\xe2\x80\x99ll help you sort it out.\n'"
https://github.com/Nathaniel-Rodriguez/relcat,Implements Randall Beer's relational categorization task,"b'Implements the relational categorization task from (Williams, 2008) and (Williams, 2013)\nUses Python 3.5.2+ (recommend using Anaconda)\n(optional) vpython 2\n(optional) jupyter notebook\nRequires CMA-ES library\n\nWilliams, P. L., Beer, R. D., & Gasser, M. (2008). An Embodied Dynamical Approach to Relational Categorization. Proceedings of the 30th Annual Conference of the Cognitive Science Society, 1, 223\xe2\x80\x93228.\n\nWilliams, P., & Beer, R. (2013). Environmental Feedback Drives Multiple Behaviors from the Same Neural Circuit. Advances in Artificial Life, ECAL 2013, 12, 268\xe2\x80\x93275. https://doi.org/10.7551/978-0-262-31709-2-ch041'"
https://github.com/lveronese/capstone,Udacity Machine Learning Nanodegree Capstone Project,"b'# Machine Learning Engineer Nanodegree\n## Specializations\n## Project: Capstone Proposal and Capstone Project\n\nNote\n\nWelcome to my Capstone Project.\nIn order to correctly review the material, this README provides a few instructions about the documentation and code.\n\nThe proposal I submitted is available in the proposal.pdf file.\nI have provided both reviews I had, named: Proposal Udacity Review First.pdf and Proposal Udacity Review Final.pdf.\nThe url for the last proposal is https://review.udacity.com/#!/reviews/398679\n\nThe capstone project report is available as capstone_report.pdf. The report does NOT contain any code and has been written to provide a complete guide to the steps I followed in order to complete my project. The report follows the suggested capstone project structure as of the provided template.\nCompanion of the report is a jupyter notebook capstone_report.ipynb that contains all python code inside a version of the content in the report in order to be able to understand the context the code has been written in. The jupyter notebook is sufficient to have access to all results. So just run all the cells and the data will be loaded (from the data directory) and all code executed. The jupyter notebook is also available in exported html format for reader convenience.\n\nI used only standard libraries (I used the Anaconda installation with python 2.7), in particular: numpy, pandas, matplotlib, scipy.stats (for norm).\n\nThe reference folder contains additional papers I read in order to document myself about possibile solutions. They are referenced inside the report.\n\nThe data folder contains both JSON and CSV versions of the data samples I used for the report.\n\nI proofreaded only the capstone PDF report. Please forgive me for any typos you may encounter in the ipython notebook.\n\n'"
https://github.com/samjfalk/GA-DSI,Projects from General Assembly's Data Science Immersive Program,"b""Within this folder you will find 7 weekly projects and a Capstone from General Assembly's Data Science Immersive. The projects are a representation of the work I completed while in the program\n"""
https://github.com/Prandtl/Bach,i want to get my degree.,b'# Bach\n## How to use\nTo look at example results:\n\nto run hogwild:\n\nor to run and show:\n\n## Files\n* example.cpp - simple gradient descent realization puts gradient descent path to example.out\n* hogwild.cpp - hogwild parallel SGD. Puts output to hogwild.out\n* ariadna.py - python script that takes filename as argument (example.out for example) and shows its contents\n* toast.cpp - One of PETSc examples copied from the source without changes\n* makefile - makefile to compile and run toast with PETSc\n'
https://github.com/manujeevanprakash/Webscraping1,Web scraping mini project -1 (countries),b'Webscraping1\n============\n\nWeb scraping mini project -1 (countries)\n'
https://github.com/datalabgit/datascience_cycle,The Data Science Cycle Workshop,"b""# Data Science Cycle\nHere you'll find the materials for the Data Science Cycle Workshop of September 2016. The topics covered in this workshop are:\n+ Feature Selection\n+ Feature Generation\n+ Unsupervised Learning: K-Means\n+ Supervised Learning: Classification\n\nThis workshop was made by: PhD. Andr\xc3\xa9s M\xc3\xa9ndez-V\xc3\xa1zquez \n\nThe DataLab Notebook (DataLab.ipynb) has the basic code that you can start working on. The voice.csv file contains the data you'll work with during the workshop and the DataLab.py file has all the code solved.\n\nAny further questions, you can contact DataLab Community on facebook: https://www.facebook.com/datalabmx \n"""
https://github.com/Zurga/bearded-dangerzone,Infovis git repo,b'# bearded-dangerzone\nInfovis git repo\n'
https://github.com/coreysery/reposcrape,Class scrape project,"b""# Repository scrape\n\nClass project to track moving data.\nI'm using github and hopefully more sites, to track changes in popular repositories over time.\nChanges including, but not limited to:\n - Repo name\n - Author\n - Contributors\n - Stars\n - Forks\n - Watches\n - Languages used\n - Commits\n\n \n"""
https://github.com/alonsopg/automatic_summarization,Some tests of Automatic summarization libraries,b'# automatic_summarization\nSome tests and notes of how to use Automatic summarization libraries.\n'
https://github.com/PatrickMockridge/SpringBoard-Data-Science-Intensive,SpringBoard Data Science Intensive Course Project Repo,b'# SpringBoard Data Science Intensive\nSpringBoard Data Science Intensive Course Project Repo\n\nThis repository represents the work completed for my Intensive introduction to Data Science at  in early 2017\n\nThe course contains a variety of miniprojects on data munging and machine learning algorithms.\n\nMy Capstone report focussed on data related to the 2016 US Presidential Election:\n\n\n\n\n\n\n\n\n\nThe Introduction to my Final Project submission can be found on my personal blog \n\nThe final project slide deck can be viewed \n\n\n\n\n'
https://github.com/DaveBackus/MFAP,"Macroeconomic Foundations for Asset Prices, an undergrad course at NYU","b""###MFAP\n\nMaterials for Macroeconomic Foundations for Asset Prices (ECON-UB-233), a course at NYU's Stern School of Business about the mathematics of asset prices and the macroeconomic foundations for them.   The repository includes TeX, Matlab, and Python files for notes, assignments, quizzes, etc.  The outputs (mostly pdf files) and a more complete description of the course are posted on the [course site]  (https://sites.google.com/site/nyusternmacrofoundations/home).\n\nSend comments and questions to Dave Backus at NYU:  db3@nyu.edu. \n\nPart of the #nyuecon collection.\n"""
https://github.com/Eehenhyu/DeepLearning,Deep Learning using Python,b'# DeepLearning\nDeep Learning implemented using Python & Tensorflow.\n\nDifferent Neural Net Architectures are used in different projects:\n\nBasicNeuralNet: plain Neural Network.\n\nSentiment plain Neural Network.\n\nImage Convolutional Neural Network.\n\nGAN Generative Adversarial Network.\n\nLanguage Recurrent Neural Netowork.\n'
https://github.com/niharikabalachandra/TimeSeries-MiniProject,Dealing with time series data using pandas,b'# TimeSeries-MiniProject\nDealing with time series data using pandas\n'
https://github.com/anaulin/scratch,Scratch pad,b'Code scratch repo.\n'
https://github.com/bgarcia7/ai_trainer,Your next personal trainer,b'# ai_trainer\nYour next personal trainer\n'
https://github.com/LoLab-MSM/pydyno,Tool that uses tropical algebra concepts to 'decompose' species trajectories in the protein-protein interactions that drive changes of concentration in time,"b'\n\n\n\n# PyDyNo\n\nPython Dynamic analysis of biochemical NetwOrks (PyDyNo) is an open source python library for the analysis of \nsignal execution in network-driven biological processes. PyDyNo supports the analysis of \nand  models.\n\n## Publications\n\nPrePrint: Probability-based mechanisms in biological networks with parameter uncertainty  \nOscar O. Ortega, Blake A. Wilson, James C. Pino, Michael W. Irvin, Geena V. Ildefonso, Shawn P. Garbett, Carlos F. Lopez\n\nbioRxiv 2021.01.26.428266; doi: https://doi.org/10.1101/2021.01.26.428266 \n\nThe preprint paper can be found \n\nJupyter notebooks with the code to reproduce the paper figures can be found \n\n\n## Installation\n\n### From PyPI\n\n\n\n### Installing the latest unreleased version\n\n\n\n### Installing from source folder\n\n- Download and extract pydyno\n- Navigate into the pydyno directory\n- Install (Python is necessary for this step):\n\n\n\n## How to use PyDyNo\n\n# Import libraries\n\n\n\n\n# Load the calibrated parameters and simulate the model with 100 different parameter sets\n\n\n\n\n\n\n\n# Visualize the dynamics of the model\n\n\n\n\n# Obtain the dominant paths for each of the simulations\xc2\xb6\n\n\n\n\n\n# Obtain distance matrix and optimal number of clusters (execution modes)\n\n\n\n\n\n\n\n\n\n\n\n    {2: [OrderedDict([(s5, [[s3], [s4]])]),\n      OrderedDict([(s3, [[s0, s1]]), (s4, [[s0, s2]])])],\n     1: [OrderedDict([(s5, [[s4]])]), OrderedDict([(s4, [[s0, s2]])])],\n     0: [OrderedDict([(s5, [[s3]])]), OrderedDict([(s3, [[s0, s1]])])]}\n\n# Visualize execution modes\n is necessary to obtain these visualizations\n\n\n\n\n\n\n\n\n\n\n\n'"
https://github.com/adriansarno/autoencoder,A simple autoencoder in tensorflow.,b'# autoencoder\nA simple autoencoder in tensorflow.\n'
https://github.com/virgodi/plda,Python Package to run Latent Dirichlet Allocation in Parallel,"b'Parallelized Python/Cython implementation of Latent Dirichlet allocation\nFinal project for CS205 at Harvard University\nWritten by Charles Liu, Nicolas Drizard, and Virgile Audi\n\n# System Requirements:\n\nThis package was tested on OSX. We ran experiments on Python 2.7 with needed packages:\n\n- Numpy\n- Threading\n- Cython\n\nThe execution of the Cython scripts require a C compiler.\n\n# Installation:\n\nTo install the package, download the zip folder from the git repository. We are working to have a pip install link soon.\n\n# Documentation:\n\nThis Python package can be used to perform efficient topic modeling using Latent Dirichlet Allocation. More details on LDA can be found in the IPython notebook below. \n\nThe organisation of the package is as follow:\n\n - Two classes: \n    \n    * The oviLDA class to perform Online Variational Inference and the cgsLDA class to perform Collapsed Gibbs Sampling\n \n    * These 2 classes have identical methods, and only a few specific attributes change for inference purposes:\n \n        - Common attributes include:\n        \n       |    Attribute    |                        Type                       |                                                        Details                                                        |\n       |:---------------:|:-------------------------------------------------:|:---------------------------------------------------------------------------------------------------------------------:|\n       |    num_topics   |                        Int                        |                                                Number of topics desired                                               |\n       |   num_threads   |                        Int                        |                                      Number of threads needed for parallelisation                                     |\n       |      topics     | Array of dimensions: num_topics x len(vocabulary) | Each row representing a particular topic, after normalisation these can be treated as multinomial over the vocabulary |\n       |      gamma      |   Array of dimensions: len(corpus) x num_topics   |                               Each row representing the topic assignment for a document                               |\n       | _log_likelihood |                       Float                       |                                       Perplexity evaluated on the training data                                       |\n        \n        - OVI specific attributes:\n        \n        |    Attribute   |      Type      |                                Details                                |\n        |:--------------:|:--------------:|:---------------------------------------------------------------------:|\n        |   batch_size   |       Int      |             Number of document to consider in every batch             |\n        |       tau      |       Int      |    Parameter used to weight the first iterations of the algorithm     |\n        |      kappa     | Float: (0.5,1] | Parameter controlling the rate at which we forget previous iterations |\n        | max_iterations |       Int      |        Maximum number of iterations on one particular document        |\n        \n        - CGS specific attributes:\n        \n        |   Attribute   |   Type  |                                       Details                                        |\n        |:-------------:|:-------:|:------------------------------------------------------------------------------------:|\n        |   iterations  |   Int   |                          Number of sampling iterations                               |\n        |    damping    |   Int   | Likelihood full number of occurrences will be sampled. See notebook for more details |\n        | sync_interval |   Int   |      Parameter controlling how often threads aggregate topic distributions           |\n        |     alpha     |  Float  |                     Dirichlet prior parameter for document/topics                    |\n        |      beta     |  Float  |                      Dirichlet prior parameter for topics/words                      |\n        |  split_words  | Boolean |            Parallelization method used. See notebook for more details                |\n        \n        - Methods:\n        \n            * fit(dtm): fits the model for a particular corpus\n            \n            | Parameters |                    Type                   |        Details       |\n            |------------|:-----------------------------------------:|----------------------|\n            |     dtm    | array of dimensions: len(docs) x len(voc) | document term matrix |\n           \n            * transform(dtm): Transform new documents into a topic assignment matrix according to a previously trained model\n            \n            | Parameters |                    Type                   |                        Details                        |\n            |------------|:-----------------------------------------:|-------------------------------------------------------|\n            |     dtm    | array of dimensions: len(docs) x len(voc) | document term matrix (NO ZERO COLUMNS FOR CGS METHOD) |\n            \n            |   Return  |                     Type                    |        Details       |\n            |-----------|:-------------------------------------------:|----------------------|\n            |   gamma   | array of dimensions: len(docs) x num_topics |  Topic assignments   |\n        \n    \n -  Useful functions related to the LDA model in the LDAutil folder:\n    \n    * print_topic(model,vocabulary,num_top_words): prints the topics for a fitted LDA model\n    \n    |   Parameters  |                   Type                   |                              Details                              |\n    |:-------------:|:----------------------------------------:|:-----------------------------------------------------------------:|\n    |     model     |             cgsLDA or oviLDA             |                   A previously fitted LDA model                   |\n    |   vocabulary  | array of dimensions: 1 x len(vocabulary) | An array of strings ordered in the same way as the columns of DTM |\n    | num_top_words |                    Int                   |                  Number of wanted words per topic                 |\n    \n    * perplexity(model,dtm_test): computes the log-likelihood of the documents in dtm_test based on the\n    topic distribution already learned by the model\n    \n    | Parameters |                       Type                       |                                          Details                                         |\n    |:----------:|:------------------------------------------------:|:----------------------------------------------------------------------------------------:|\n    |    model   |                 cgsLDA or oviLDA                 |                               A previously fitted LDA model                              |\n    |   dtm_new  | array of dimensions: len(docs) x len(vocabulary) | A new DTM corresponding to the new documents on which we want to evaluate the perplexity |\n\n    |   Return   |  Type |                Details                |\n    |:----------:|:-----:|:-------------------------------------:| \n    | perplexity | float | Perplexity evaluated on new documents |\n\nMore details on these functions and what they actually evaluate are present in the Ipython notebook.\n    \n - A subset of the Reuters news dataset in the form of a document term matrix and the associated vocabulary.\n    \n\n# Test to run:\n\nFor you to test if your system is up to the requirements and to showcase the package in action, we included a Python test.py file.\n\nYou can run both versions of LDA by commenting and uncommenting respectively lines 36 and 39.\n\n# References:\n\n- The OVI code is based on Hoffmans 2010 paper \n- The CGS code relies on:\n    *  by Han Xiao and Thomas Stibor\n    *  by Feng Yan, Ningyi Xu and Yuan (Alan) Qi\n'"
https://github.com/brookisme/gitnb,git tracking for python notebooks,"b'## GITNB \n\nGIT TRACKING FOR PYTHON NOTEBOOKS\n\nA simple idea: GitNB doesnt actually track python notebooks. Instead, GitNB creates and updates python versions of your notebooks which are in turn tracked by git.\n\n1. \n2. \n3. \n4. \n5. \n\n_____\n<a name=quick>\n### QUICK START:\n\nThis quick-start is just an example. It looks long (due to bash-output) but its quick: 1-2 minutes tops.\n\nA. INITIALIZE GIT REPO\n\n\n\n\nB. INITIALIZE GitNB, ADD NOTEBOOKS TO GitNB TO BE TRACKED\n\n\n\n\nC. QUICK LOOK AT A ""NBPY.PY"" VERSION OF A NOTEBOOK\n\n\n\n\nD. UPDATE NBPY.PY FILE AFTER EDITING YOUR NOTEBOOK\n\nThat notebook is buggy ...[updating python notebook]... I just went to the python-notebook and fixed the bugs. Lets see what happened:\n\n\n\nE. CREATE PYTHON-NOTEBOOK FROM NBPY.PY FILE\n\nFinally, lets say we actually need that buggy notebook after all\n\n\n\nMy bugs are back!\n\n\n\n_____\n<a name=lazy>\n### LAZY CONFIG:\n\nIf the  seemed like too much how about this...\n\n\n\nHow in the what? Two things are going on here\n\n1. We are -ing with  instead of \n2. Ive  the user config and set\n\n\n\nNow each time I :\n\n* All new notebooks are -ed to be tracked by gitnb\n* All notebooks are -ed\n* All changes are added to the git repo\n*  is \n\nNote: the  flag is there because the at the time of the commit (before the nbpy.py files are generated there may or may not be changes to commit)\n\nHeres the super-quick-quick-start-example:\n\n\n... go update gitnb.config.yaml ...\n\n\n_____\n<a name=install>\n### INSTALL:\n\n###### pip:\n\n\n###### github:\n\n\n_____\n<a name=docs>\n### DOCS:\n\n\n\n<a name=methods>\n###### methods:\n\n1. : initialize gitnb for project\n2. : install gitnb.config.yaml for user config\n3. : add ipynb & gitnb files to gitignore\n4. : list tracked notebooks or nbpy.py files\n5. : begin tracking notebook\n6. : stop tracking notebook\n7. : update nbpy.py files with recent notebook edits\n8. : update, followed by add, followed by \n9. : perform diff between current notebook version and last -ed version\n10. : convert notebook to nbpy.py file (without -ing)\n11. : convert nbpy.py file to python notebook\n\n_____\n<a name=init>\n\n###### init:\nInitialize Project:\n\n*  required before \n* installs .gitnb directory at the project root\n* creates or appends .git/hooks/pre-commit for auto-tracking config\n\n\n()\n\n_____\n<a name=configure>\n###### configure:\nInstall Config:\n\n* optional: only necesary if you want to change the \n* installs gitnb.config.yaml directory at the project root\n\n\n()\n\n_____\n<a name=gitignore>\n###### gitignore:\nUpdate .gitignore:\n\n\n\nAppends (or creates) gitignore with the recommended settings. Namely,\n\n* notebooks: .ipynb, .ipynb_checkpoints\n gitnb: nbpy_nb/\n\n\n()\n\n_____\n<a name=list>\n###### list:\nList Project Notebooks, or nbpy.py files\n\npositional arg (type):\n\n* (default) all: list tracked and untracked notebooks\n* tracked: list tracked notebooks\n* untracked: list untracked notebooks\n* nbpy: list nbpy.py files\n\n\n()\n\n_____\n<a name=add>\n###### add:\nAdd notebook to gitnb:\n\n* converts notebook(s) to nbpy.py file(s)\n* adds notebook-nbpy pair to gitnb tracking list\n* performs a  on nbpy.py file(s)\n* path: path to file or directory \n* destination_path: (optional) \n    * if path is a file path nbpy file will be at destination_path\n    * if destination_path is falsey (recommended) default path is used\n    * default path can be changed with \n    * if path is a direcotry path, default config is always used\n\n\n()\n\n_____\n<a name=remove>\n###### remove:\nRemove notebook from gitnb:\n\n* notebook will no longer be tracked\n* nbpy.py file will not be deleted\n* ipynb file will not be deleted\n\n\n()\n\n_____\n<a name=update>\n###### update:\nUpdate nbpy files:\n\n* will update nbpy files with current content from your tracked notebooks\n* make sure your notebook has been saved!\n\n\n()\n\n_____\n<a name=commit>\n###### commit:\nUpdate and Commit:\n\n* if (UPDATE_ON_GITNB_COMMIT) perform gitnb update\n* call  with optional flags [a|m]:\n  - -a flag (add all - same as git commit -a)\n  - -m flag (add all - same as git commit -m)\n  - note:  because the nbpy files are not yet updated\n\n\n()\n\n_____\n<a name=diff>\n###### diff:\nDiff for recent changes.\n\nCreates a diff between the most recent nbpy.py version of the noteboook\nand the nbpy.py version of the notebook in its current state (the working copy).\n\n\n()\n\n_____\n<a name=topy>\n###### topy:\nTo-Python:\n\n* converts notebook(s) to nbpy.py file(s)\n* similar to  but does not gitnb or git track\n\n\n()\n\n_____\n<a name=tonb>\n###### tonb:\nTo-Notebook:\n\n* creates new notebook from nbpy.py python file\n* great for collaborators!\n* great for recovering lost work!\n\n\n()\n\n\n_____\n<a name=config>\n### USER CONFIG:\n\nThe  method installs  in your root directory.  This is a copy of the . Note at anytime you can go back to the default configuration by simply deleting the user config file ().\n\nThere are comment-docs in the config file that should explain what each configuration control. However I thought Id touch a couple of the perhaps more interesting configurations here.\n\n##### LAZY INSTALL\n\nsee \n\n##### GIT_ADD_ON_GITNB_ADD (defaults to True):\n\nIf True the  method will perform a  after creating the nbpy file and adding it to the gitnb tracking list.  You can set this to False if you want to explicity call  yourself after looking over the file.\n\n##### UPDATE_ON_COMMIT (defaults to True):\n\nIf True, the gitnb  method will automatically be called when performing a  (during pre-commit hook).\n\n##### AUTO_TRACK_ALL_NOTEBOOKS (defaults to False):\n\nIf True,  (see  method) will automatically be called when performing a  (during pre-commit hook). This will add all notebooks in your project to gitnb.\n\n_Note if the only thing that has changed is your notebooks, youll still need to explicity call  or add the  flag to your ._\n\n##### EXCLUDE_DIRS:\n\nA list of directories not to include when searching for notebooks\n\n##### OTHER:\n\nYou can also configure, default location for new files, if they include an indentifier (like nbpy in ), spacing in nbpy files and more. Check the comment-docs for more info.\n\n\n'"
https://github.com/sebastianrosales/Rosales_Mendez_hw6,Tarea 6 Presa-Depredador y Particula en el Campo Magnetico,b'Rosales_Mendez_hw6\n==================\n\nTarea 6 Presa-Depredador y Particula en el Campo Magnetico\n'
https://github.com/dncn123/WineNLPRecommender,NLP on descriptions of wines from online retailer.,b'# WineNLPRecommender\nNLP on descriptions of wines from online retailer.\n\ndata_collection.ipynb\n  - notebook used to scrap data for exercise\n  \npreprocessing.py \n  - class that can be used to preprocess data\n  \nWineNet.py\n  - class that can be used to create graph of wines\n  \nrecommendations.ipynb\n  - notebook in which preprocessing and network are used\n  \nWine t-SNE.ipynb \n  - notebook using t-SNE clustering - which worked like a dream...\n'
https://github.com/wangjiahong/Titanic-Kaggle,"A tutorial for Kaggle's Titanic: Machine Learning from Disaster competition. Demonstartes basic data munging, analysis, and visualization techniques. Shows examples of supervised machine learning techniques.","b""##Kaggle competition: Titanic Machine learning\n\nBest solution ranks top 4% on leaderboard. (187th out of 5535 teams)\n\n\n\n\nplay_a_random_music.py will play a music randomly from my local disk when my main algorithm is completed. This is a kind of notification for me. \n\n\n'\n"""
https://github.com/xujin1982/Two-Sigma-Connect-Rental-Listing-Inquiries,Kaggle Two-Sigma-Connect-Rental-Listing-Inquiries,b'# Two-Sigma-Connect-Rental-Listing-Inquiries\nCode for Kaggle Two Sigma Connect Rental Listing Inquiries\n'
https://github.com/deepankverma/dlnd_1,First Project of Deep Learning,b'# dlnd_1\nFirst Project of Deep Learning\n'
https://github.com/leonardoaraujosantos/LearningTorch,Doing deep learning experiments with torch,b'# LearningTorch\nPlace where I save all my experiments with torch\n'
https://github.com/sketchychen/ga_datasci_cw,Coursework for GA Seattle's Data Science 2016 Mar 15 - May 19,"b""# ga_datasci_cw\nthis is coursework, belonging to Rachel C., for GA Seattle's Data Science, 2016 Mar 15 - May 19\n"""
https://github.com/mlayeghi/activation-cost-functions,Activation vs. Cost functions,b'# Activation_Cost_Functions\n\nBrief descriptions and TensorFlow examples of activation and cost functions and their applications in Neural Networks.\n\n'
https://github.com/stuti-madaan/Advanced-Predictive-Modeling,Codes for deeper insights in Predictive Modeling,b'# Advanced-Predictive-Modeling\nCodes for deeper insights in Predictive Modeling\n'
https://github.com/akshay-sr/CarND-FindingLaneLines-P1,FindingLaneLines,"b'#Finding Lane Lines on the Road \n\n\nWhen we drive, we use our eyes to decide where to go.  The lines on the road that show us where the lanes are act as our constant reference for where to steer the vehicle.  Naturally, one of the first things we would like to do in developing a self-driving car is to automatically detect lane lines using an algorithm.\n\nIn this project you will detect lane lines in images using Python and OpenCV.  OpenCV means ""Open-Source Computer Vision"", which is a package that has many useful tools for analyzing images.  \n\nStep 1: Getting setup with Python\n\nTo do this project, you will need Python 3 along with the numpy, matplotlib, and OpenCV libraries, as well as Jupyter Notebook installed. \n\nWe recommend downloading and installing the Anaconda Python 3 distribution from Continuum Analytics because it comes prepackaged with many of the Python dependencies you will need for this and future projects, makes it easy to install OpenCV, and includes Jupyter Notebook.  Beyond that, it is one of the most common Python distributions used in data analytics and machine learning, so a great choice if youre getting started in the field.\n\nChoose the appropriate Python 3 Anaconda install package for your operating system here.   Download and install the package.\n\nIf you already have Anaconda for Python 2 installed, you can create a separate environment for Python 3 and all the appropriate dependencies with the following command:\n\n\n\n\n\nStep 2: Installing OpenCV\n\nOnce you have Anaconda installed, first double check you are in your Python 3 environment:\n\n    \n  \n  \n  \n   \n(Ctrl-d to exit Python)\n\nrun the following commands at the terminal prompt to get OpenCV:\n\n\n\n\nthen to test if OpenCV is installed correctly:\n\n  \n  \n  \n(Ctrl-d to exit Python)\n\nStep 3: Installing moviepy  \n\nWe recommend the ""moviepy"" package for processing video in this project (though youre welcome to use other packages if you prefer).  \n\nTo install moviepy run:\n\n  \n\nand check that the install worked:\n\n  \n  \n  \n(Ctrl-d to exit Python)\n\nStep 4: Opening the code in a Jupyter Notebook\n\nYou will complete this project in a Jupyter notebook.  If you are unfamiliar with Jupyter Notebooks, check out this link to get started.\n\nJupyter is an ipython notebook where you can run blocks of code and see results interactively.  All the code for this project is contained in a Jupyter notebook. To start Jupyter in your browser, run the following command at the terminal prompt (be sure youre in your Python 3 environment!):\n\n\n\nA browser window will appear showing the contents of the current directory.  Click on the file called ""P1.ipynb"".  Another browser window will appear displaying the notebook.  Follow the instructions in the notebook to complete the project.  \n'"
https://github.com/Davidjohnwilson/notes,Various notes from meetups and conferences.,b'# Notes\n\nFor notes from meetups and conferences. Most of the time these will be saved as ipython/Jupyter notebooks.'
https://github.com/twhyntie/bokeh_time_series,Code and tools for plotting time series using Bokeh.,"b'# Plotting time series with Bokeh\nThis repository contains code, tools and Jupyter notebooks\nfor making time series plot using Bokeh.\n\nSee this  for\na read-only (i.e. non-interactive) version of the\n.\n'"
https://github.com/RobertPRanney/Blog_Exploration,"Analysis to identify features that lead to highly liked and commented posts, as well as highly subscribed blogs","b""# The Successful Blogger\n\n#### Data Science Immersive Capstone project\n---\n\n#### 5 Minute Presentation Slides\n\n\n\n#### Long Version\n###### Project Motivation\nNear the beginning of the galvanize DSI program I started a blog for myself. I\nwanted a place to help promote myself as I try to transition careers. A place to\ncontribute my data science thoughts, and trip reports. Plus it also just seemed\nlike it could be fun. My fledging blog still needs lots of work, but I do have a\ndecent start on it.\n\n\n\n\n###### Is it possible to identify elements that contribute to a successful blog?\nWhile in the DSI program I was wondering if I could use some of my new data\nscience skills to identify elements of a blog that can lead to its success. Of\ncourse I would never be able to to identify everything that makes a blog do\nwell. It would be hard to capture the visual appeal of a blog, and it would be\nalmost impossible to capture aspects like self promotion. That being said I\nthought it was possible to capture some aspects.\n\n---\n##### Project Overview and Workflow\nTo do this analysis a single topic of blogs was chosen to limit the effect of\nbetter topics doing significantly differently. The executive overview is shown\nbelow. The project involved gathering as many blogs as I felt like, for this I\ngathered 2500 blogs. Then much cleaning and some topic filtering took place.\nThen the porblem is split into two sub problems, 1 to deal with identifying\nelements of successful posts, and one to identify elements of successful blogs.\n\n\n\nAll of the code to run this a analysis, or a similar analysis is contained in\nthe code folder of this repository. But there are quite a few steps that go into\nthe analysis, and the executive overview does not serve as good enough road map.\nThe current code some needs some reorganization, but in the meantime I will\nplace the current roadmap that I used will doing this project. This should serve\nas a passable guide understand how the functions work together.\n\n\n\n\n##### Data Collection\nData was collected from the wordpress api, which is super friendly and easy to\nuse. Allows pulling of a public blogs information (most information), plus all\nof blogs posts, and its comments. This can be done by knowing a blogs url to get\nthe unique ID. Amazingly there is no rate limiting or quota, and doesn't even\nrequire api keys. The less robust part of the data collection currently is how\nto collect the url links. I was going to scrape them from google search results,\nbut quickly abandoned this idea due to the massive problems of that approach.\nInstead I found a fairly efficient manual method to gather about 10,000 google\nsearch result links in about an hour using the chrome 'link clump' add on. This\ndeficiency is something I would like to address later given time. These 10,000\nyield 2500 blogs.\n\n##### Data storage\nA blog and all of its posts were pulled into a mongo db. This was done on an ec2\ninstance, but with only 2500 blogs it was only about 3 Gb of data so it could\nhave been done locally, but just for efficiency almost all analysis stayed up on\nthe instance.\n\n##### Filtering\nBlogs were filtered for having multiples authors, too many post (>2500) and not\nseeming relevant based on not a high enough precent of posts containing fitness\nkey words.\n\n##### Splitting\nFrom here posts are stripped out of blogs to model the things that contribute to\nsuccessful posts.\n\n##### Natural Language Processing\n\nBeautiful Soup was used to pull text from html, the text is cleaned, stripped,\nand lemmatized. The lemmatization metod chosen was the nltk wordnet lemmatizer.\nThis needs to be re-examined though due to the odd words that made it through.\nThe tokenized documents were converted to a tfidf matrix. The resulting matrix\nwas then reduced with NMF, for this analysis 30 was chosen. A longer anaylsis,\nor one done with more post, or no topic division could use more, as it was 30\nseemed to give reasonable topics, but the reconstruction accuracy shows that it\ncould have easier had more topics.\n\n\n##### Topic analysis\nThe resulting topics can be looked at by running helpers.py and giving the topic\nnumber for example topic 5 gives the picture below.\n\n\n\nSome of the posts that made it throuhg topic filtering were obviously not within\nthe realm of fitness though. For example one was very about german.\n\n\n\nlooking at the number of posts that fall into topic shows that most did fit into\nthe fitness topic\n\n\n\n##### Machine Learning: posts\n---\nLots of different models to try and predict the success of posts were done.\nframe work to gridsearch to gridsearch over and then save the best model is\nstored in grid_search_and_save.py. The best models were always the ensemble\nmodels. To deal with the 'skewed' success metric (i.e. 50% of posts have zero\nlikes and zero comments) the continuous variable was turned into a 4 categories.\nThe best model was a random forest, but the adaboost and gradient boost really\nwere very similar. The best one has a confusion matrix below.\n\nThis is obviously far from perfect but it does identify the great traction class\nfairly well. So some things can be learned from this. Individually the features\ncan be examined to see there individual effects, not everything is important\nthough. Looking at the built in feature importance shows this.\n\nSo we can ignore most of these, but lets look at the contribution of some more\ninteresting ones.\n\n\nThis is pretty nice to see trends, but It would be nice to see a few of these in\nterms of the actually probability of being in a particular class, specifically\nthe probability of being in the most successful class. This is accomplished with\na more custom script. Lets look at the effect of changing the number of tags\nsince it is the most interesting.\n\n\nJust by judicious use of the number of tags a post can have a 10% higher change\nof making it into the most successful class of posts. Tags are blogger\ndesignated keywords that identify the topic that a post belongs in. These are\nthen used in the internal topic listings for people to read posts in a certain\ntopic. Wordpress allows up to 20 but says if you use 'too many' then the post\nwill be listed less. This is to avoid users trying to abuse the topic listings\nsince a post can't really be in every topic. So it seems that 'too many' is\nabout 10. This was a nice point in the project, since the effect of this\nfeature is explainable. This shows that the model is most likely picking up on\nreal effects rather than trying to interpret noise.\n\n###### Summary of stuff a blogger can do to be more successful in posts\n* Always use 10 tags\n* Write longer posts (>1000 words) (might not generalize to other topics)\n* Add more images\n* Add more links to other sites\n* Use max number of categories\n* Post whenever (no noticable difference on weekends vs weekdays)\n\n\n#### Machine Learning: Blogs\n---\nIdentifying successful blogs is more straight forward, just try to create a\nmodel to predict number of subscribers. After investigating a number of options\nthe best one turned out to be gradient boosting regressor, although like posts\nthe ensemble models on a whole did about the same, and did far better than other\nthings such as linear regression. Before fitting models outliers were tossed out\nsince the median was about 25 but the max was up at 25,000 subscribers. The\noveral accuracy of the model was not great, about .55 r2, and rmse of 14.\nVisually this looks quite bad.\n\nWell this is pretty bad, but lets see what can be learned anyways. First lets\nlook at feature importance.\n\nMost of these make sense, if you have access to the number of likes for posts\nand number of comments for posts then making a guess at the number of\nsubscribers doesn't seem like a far stretch. Again lets look at individual\ncontributions.\n\nmost of these seem pretty sensible, or might just be noise. Only thing\nI found interesting was the effect of the gap between posts (but it is a minor\neffect). Shown on a more interpretable scale this is seen below.\n\nSo maybe putting more thought into a post a posting less than everyday is\nbeneficial, hard to tell, might just be some noise. Another worth while note is\nthat no topic really did any better for blogs once they were assigned a main\ntopic.\n\n\n##### Thanks For Reading\nIf you made it this far then thanks for reading, feel free to look at the code\nor anything else in the repo\n\nRR\n"""
https://github.com/mjbrodzik/ExploringCETB,Examples for reading and understanding Calibrated Enhanced-Resolution Brightness Temperature (CETB) files,b'# ExploringCETB\nExamples for reading and understanding Calibrated Enhanced-Resolution Brightness Temperature (CETB) files\n'
https://github.com/MohamedDabo/Pokemon-Data-Exploration,Exploring the world of Pokemons!,"b'# Pokemon-Catch-Em-All\nIn this project, I explored the world of Pokemon. Having grown up with the series, I thought it would be a good idea to analyze this dataset found on Kaggle.\n\nFirst I took a look at the different types of pokemon, then I digged a little deeper into battle performances by answering the following questions:\n\n    1. Who has the highest Combat Power (CP)?\n    2. Who has the lowest Combat Power (CP)?\n    3. Who has the highest Hit Points (HP)?\n    4. Who has the lowest Hit Points (HP)?\n    5. Who is the strongest Pokemon?\n'"
https://github.com/shannonxfiles/scrape-fed-salaries,scrape publicly available federal employee salaries into a postgres database,b'# scrape-fed-salaries\nscrape publicly available federal employee salaries into a postgres database\n'
https://github.com/lmarti/jupyterday-philly-19.05.2017,"Slides for my talk ""Jupyter Notebooks in a Computational Intelligence/Machine Learning Class""",b'\n# Jupyter Notebooks in a Computational Intelligence Class\n\n\n\n\n\n\n\nSlides of my presentation at .\n'
https://github.com/sandiegopython/geekgirl,GeekGirl Conference 2014,"b'geekgirl\n========\n\nSan Diego PyLadies hosted a workshop at the Geek Girl Tech Conference on\nSaturday June 21, 2014.\n\n\nIPython Notebook Files\n----------------------\n\nYou can view each of the files online using _.\n\n* _: numbers, strings, variables, booleans, ""if statements""\n* _: lists and loops\n* _: defining functions and using modules\n\n\nGiving the talk\n---------------\n\nBefore giving the talk, IPython Notebook must be installed:\n\n.. code-block:: bash\n\n    $ pip install ipython[notebook]\n\nHow to run an IPython Notebook:\n\n.. code-block:: bash\n\n    $ ipython notebook part-1.ipynb\n\nAt the workshop we gave the talk like this:\n\n* We opened a Python shell on the projector monitor and an IPython notebook on the laptop monitor\n* One person read the notebook and typed into the Python shell\n* Another person stood in front of the audience and explained each step\n* Questions were answered ad-hoc, occasionally by Googling answers or typing at the terminal\n\n\nCopying\n-------\n\nYou can give this talk too!\n\nThis workshop is licensed under the GNU General Public License v2.  See LICENSE file for more details.\n\n\n.. _nbviewer: http://nbviewer.ipython.org/\n.. _part-1.ipynb: http://nbviewer.ipython.org/github/pythonsd/geekgirl/blob/master/part-1.ipynb\n.. _part-2.ipynb: http://nbviewer.ipython.org/github/pythonsd/geekgirl/blob/master/part-2.ipynb\n.. _part-3.ipynb: http://nbviewer.ipython.org/github/pythonsd/geekgirl/blob/master/part-3.ipynb\n'"
https://github.com/mehdi2407/CarND-LaneLines-P1,This repo consists of the first project from the udacit self driving car nanodegree,"b'# CarND-LaneLines-P1\nThis repository consists of the first project from the Udacity Self Driving Car Nanodegree. The pipeline of the algorithm and detailed reflection about the project could be viewed from writeup.md.\nAs perquisites to run this project, you need to install OpenCV package. \n\n'"
https://github.com/abitofalchemy/DmChallenge,SCI Challenge Project,b'Social Topic Analyzer\n=====================\n\nData Mining\nFall 2014 - CSE 40647/60647\n\nProject Team\n------------\n\n* Salvador Aguinaga\n* Aastha Nigam\n\nFolder Contents\n---------------\n\n* Docs- Final project report and PPT Presenteation \n* Data - Datasets used\n* Scripts - Source code\n\n\n\niPython Notebook (Mac OS)\n-------------------------\n* ipython notebook --pylab inline\n\n\nProject Notes\n---------------------\n* Workin on Scripts/Salvador/D_GetTweetsWithUrls.py Data/toy.json\n* Working with Python Twitter and \n* Testing out Twython\n* Working with twython to get all users of wsbt in a loop that waits to collect more info in x amount of time intervals\n\nReferences:\n-----------\n* http://social-metrics.org/python-tutorial-1/\n* http://heuristically.wordpress.com/2011/04/08/text-data-mining-twitter-r/\n* http://stackoverflow.com/questions/19432202/twython-get-followers-list\n* http://stackoverflow.com/questions/11439164/storing-json-dictionaries-from-twitter-streaming-api-using-pymongo\n* http://unsupervisedlearning.wordpress.com/2014/07/06/scraping-your-twitter-homepage-with-python-and-mongodb/\n* http://nbviewer.ipython.org/github/EnricoGiampieri/EnricoGiampieri.github.io/blob/master/_ipython_notebooks/twitter.ipynb\n\t- limits \n* json\n-- https://freepythontips.wordpress.com/2013/08/08/storing-and-loading-data-with-json/\n\n* Visualization\n-- https://www.zotero.org/jwbaker/items/BPHDT5GT?fullsite=0\n\n\n'
https://github.com/feici02/cookbook,:notebook_with_decorative_cover: My code snippets.,b'# cookbook\nMy code snippets.\n'
https://github.com/spiningup/udacity-ml-student_intervention,Build a student intervention system by predicting whether a student will pass or fail an exam,"b'# Project 2: Supervised Learning\n## Building a Student Intervention System\n\n### Install\n\nThis project requires Python 2.7 and the following Python libraries installed:\n\n- \n- \n- \n\nYou will also need to have software installed to run and execute an \n\nUdacity recommends our students install , a pre-packaged Python distribution that contains all of the necessary libraries and software for this project. \n\n### Run\n\nIn a terminal or command window, navigate to the top-level project directory  (that contains this README) and run one of the following commands:\n\n  \n\n\nThis will open the iPython Notebook software and project file in your browser.\n\n## Data\n\nThe dataset used in this project is included as . This dataset has the following attributes:\n\n-  : students school (binary: ""GP"" or ""MS"")\n-  : students sex (binary: ""F"" - female or ""M"" - male)\n-  : students age (numeric: from 15 to 22)\n-  : students home address type (binary: ""U"" - urban or ""R"" - rural)\n-  : family size (binary: ""LE3"" - less or equal to 3 or ""GT3"" - greater than 3)\n-  : parents cohabitation status (binary: ""T"" - living together or ""A"" - apart)\n-  : mothers education (numeric: 0 - none,  1 - primary education (4th grade), 2 - 5th to 9th grade, 3 - secondary education or 4 - higher education)\n-  : fathers education (numeric: 0 - none,  1 - primary education (4th grade), 2 - 5th to 9th grade, 3 - secondary education or 4 - higher education)\n-  : mothers job (nominal: ""teacher"", ""health"" care related, civil ""services"" (e.g. administrative or police), ""at_home"" or ""other"")\n-  : fathers job (nominal: ""teacher"", ""health"" care related, civil ""services"" (e.g. administrative or police), ""at_home"" or ""other"")\n-  : reason to choose this school (nominal: close to ""home"", school ""reputation"", ""course"" preference or ""other"")\n-  : students guardian (nominal: ""mother"", ""father"" or ""other"")\n-  : home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)\n-  : weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)\n-  : number of past class failures (numeric: n if 1<=n<3, else 4)\n-  : extra educational support (binary: yes or no)\n-  : family educational support (binary: yes or no)\n-  : extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)\n-  : extra-curricular activities (binary: yes or no)\n-  : attended nursery school (binary: yes or no)\n-  : wants to take higher education (binary: yes or no)\n-  : Internet access at home (binary: yes or no)\n-  : with a romantic relationship (binary: yes or no)\n-  : quality of family relationships (numeric: from 1 - very bad to 5 - excellent)\n-  : free time after school (numeric: from 1 - very low to 5 - very high)\n-  : going out with friends (numeric: from 1 - very low to 5 - very high)\n-  : workday alcohol consumption (numeric: from 1 - very low to 5 - very high)\n-  : weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)\n-  : current health status (numeric: from 1 - very bad to 5 - very good)\n-  : number of school absences (numeric: from 0 to 93)\n-  : did the student pass the final exam (binary: yes or no)\n'"
https://github.com/deercoder/0-PhD,"Research notes, codes, deep learning","b'# PhD\nResearch notes, codes, misc for my PhD study\n'"
https://github.com/sanjeevbadgeville/Spark2-H2O-R-Zeppelin,"A stack for data mining using Spark2, H2O, R and Zeppelin running on Cloudera Hadoop Distribution","b'# Spark2-H2O-R-Zeppelin\nA stack for data mining using Spark2, H2O, R and Zeppelin running on Cloudera Hadoop Distribution\n\n# Spark2 Setup\n\n## Hadoop Version (tested with CDH5.8)\n$hadoop version\n  Hadoop 2.6.0-cdh5.11.0\n\n## Download Spark \nhttp://spark.apache.org/downloads.html\nDownload Spark: spark-2.2.0-bin-hadoop2.6.tgz\nwget http://apache.mirrors.ionfish.org/spark/spark-2.2.0/spark-2.2.0-bin-hadoop2.6.tgz\n\n## Extract Spark2 downloaded file\n- sudo mkdir /opt/spark\n- sudo chown -R cloudera:cloudera /opt/spark\n- cp /mnt/working/spark-2.2.0-bin-hadoop2.6.tgz /opt/spark\n- tar xvzf /opt/spark/spark-2.2.0-bin-hadoop2.6.tgz\n-  ln -s /opt/spark/spark-2.2.0-bin-hadoop2.6.tgz /opt/spark/current\n\n## Update conf/spark-env.sh\n- SPARK_HOME=/opt/spark/current\n- HADOOP_CONF_DIR=/etc/hive/conf\n\n## Update conf/spark-defaults.conf\n- spark.master                       yarn\n- spark.yarn.jars                    hdfs://localhost:8020/user/cloudera/spark-2.2.0-bin-hadoop2.6/\n\n## Create HDFS folder /user/cloudera (if not present)\n- sudo -u hdfs hdfs dfs -mkdir /user/cloudera\n- sudo -u hdfs hdfs dfs -chown -R cloudera /user/cloudera\n- hdfs dfs -mkdir spark-2.2.0-bin-hadoop2.6\n- hdfs dfs -copyFromLocal jars/  spark-2.2.0-bin-hadoop2.6\n\n## Test Spark2 Installation\n- $ ./bin/run-example SparkPi 10 --master yarn\n- $ ./bin/spark-shell --master yarn\n- $ ./bin/pyspark\nYarn cluster mode\n- $ ./bin/spark-submit --class org.apache.spark.examples.SparkPi     --master yarn     --deploy-mode cluster     --driver-memory 4g     --executor-memory 2g     --executor-cores 1     --queue thequeue     examples/jars/spark-examples*.jar     10\nYarn client mode\n- ./bin/spark-submit --class org.apache.spark.examples.SparkPi     --master yarn     --deploy-mode client     --driver-memory 4g     --executor-memory 2g     --executor-cores 1     --queue thequeue     examples/jars/spark-examples*.jar     10\n\n# H2O Sparkling-Water Setup\n\n## Download Sparkling-Water\nhttp://h2o-release.s3.amazonaws.com/sparkling-water/rel-2.0/0/sparkling-water-2.0.0.zip\n\n## Update sparkling-env.sh\n- SPARK_HOME=/opt/spark/current\n- MASTER=yarn\n\n## Copy Sparkling-Water Fat jar\n- cp /opt/sparkling-water/current/assembly/build/libs/sparkling-water-assembly_2.11-2.0.0-all.jar  /opt/spark/current/jars\n\n## Test Sparkling-Water Installation\n- /opt/spark/current/bin/spark-submit --master=yarn --class water.SparklingWaterDriver --conf ""spark.yarn.am.extraJavaOptions=-XX:MaxPermSize=384m -Dhdp.version=current""  --driver-memory=8G --num-executors=3 --executor-memory=3G --conf ""spark.executor.extraClassPath=-XX:MaxPermSize=384m -Dhdp.version=current""  /opt/spark/current/jars/sparkling-water-assembly_2.11-2.2.2-all.jar\n\n# Install R\n\n$ sudo yum install R\n$ sudo yum install libxml2-devel\n$ sudo yum install libcurl-devel\n\n- install.packages(knitr, repos=""http://cran.rstudio.com/"",dependencies = TRUE)\n- install.packages(data.table, repos=""http://cran.rstudio.com/"",dependencies = TRUE)\n- install.packages(curl, repos=""http://cran.rstudio.com/"",dependencies=TRUE)\n- install.packages(""httr"", repos=""http://cran.rstudio.com/"", dependencies=TRUE)\n- install.packages(""plotly"", repos=""http://cran.rstudio.com/"", dependencies=TRUE)\n- install.packages(""devtools"", repos=""http://cran.rstudio.com/"", dependencies=TRUE)\n- devtools::install_github(""ropensci/plotly"")\n- devtools::install_github(ramnathv/rCharts)\n\n# Apache Zeppelin\n\n## Download Zeppelin \nhttps://zeppelin.apache.org/download.html\nzeppelin-0.7.3-bin-all.tgz\nwget http://mirror.reverse.net/pub/apache/zeppelin/zeppelin-0.7.3/zeppelin-0.7.3-bin-all.tgz\n\n## Update /opt/zeppelin/current/conf/zeppelin-env.sh\n- export MASTER=yarn\n- export SPARK_HOME=/opt/spark/current\n- export SPARK_APP_NAME=zeppelin-cdh\n- export HADOOP_CONF_DIR=/etc/hive/conf\n- export SPARK_SUBMIT_OPTIONS=""--jars /opt/spark/current/jars/sparkling-water-assembly_2.11-2.2.2-all.jar""\n\n## Run Zeppelin with Spark2, Sparkling-water, and R\n- /opt/zeppelin/current/bin/zeppelin.sh -Pspark-2.2\n\n## Test Zeppelin Installation\nhttp://localhost:8080/#/\n\n- %spark\n- import org.apache.spark.sql.\n- val sqlContext = new SQLContext(sc)\n- import sqlContext.implicits.\n- import org.apache.spark.h2o.\n- val h2oContext = H2OContext.getOrCreate(sc) \n- import h2oContext. \n\n- val df: DataFrame = sc.parallelize(1 to 1000, 100).map(v => IntHolder(Some(v))).toDF\n- val hf = h2oContext.asH2OFrame(df)\n- val newRdd = h2oContext.asDataFrame(hf)(sqlContext)\n\n# Oracle Access\n\n## Use the ojdbc7.jar in the lib folder as it has the file defaultConnectionProperties.properties file updated with oracle.jdbc.timezoneAsRegion=false\n\n## Use the Postgres Interpreter\n- postgresql.driver.name\toracle.jdbc.driver.OracleDriver\n- postgresql.max.result\t1000\n- postgresql.password\t    [PASSWORD]\n- postgresql.url\t        jdbc:oracle:thin:@[HOST_IP]:[HOST_PORT]:[SID]\n- postgresql.user\t        [USERNAME]\n\nDependencies\n/opt/zeppelin/current/lib/ojdbc7.jar\n\n## Add Oracle jdbc driver to Spark Interpreter\n\nDependencies\n/opt/zeppelin/current/lib/ojdbc7.jar\n\n## Test Zeppelin to access Oracle using %psql\n- %psql\n- SELECT * FROM DUAL\n\n## Test Zeppelin to access Oracle using %spark\n- %spark\n- sc.version\n-- val pdf = sqlContext.load(""jdbc"", Map(""url"" -> ""jdbc:oracle:thin:[USERNAME]/[PASSWORD]@[HOST_IP]:[HOST_PORT]:[SID]"", ""driver"" -> ""oracle.jdbc.driver.OracleDriver"", ""dbtable"" -> ""dual"") )\n- pdf.printSchema()\n- pdf.registerTempTable(""pdf"")\n\n- %sql SELECT count() FROM pdf\n\n# Vertica\n\n# Use the vertica-jdbc-8.0.0-1.jar in the lib folder\n\n## Use JDBC Interpreter\n- default.driver\tcom.vertica.jdbc.Driver\n- default.password\t[PASSWD]\n- default.url\tjdbc:vertica://[HOST]:[HOST_PORT]/[DB]?user=[USERNAME]&password=[PASSWD]\n- default.user\t[USERNAME]\n\nDependencies\n/opt/zeppelin/current/lib/vertica-jdbc-8.0.0-1.jar\n\n## Test Zeppelin to access Vertica using %jdbc\n- %jdbc\n- SELECT count() FROM [SCHEMA].[TABLE]\n\n## Test Zeppelin to access Vertica using %spark\n- %spark\n- sc.version \n- val pdfv = sqlContext.load(""jdbc"", Map(""url"" -> ""jdbc:vertica://[HOST]:[PORT]/[DB]?user=[USERNAME]&password=[PASSWD]"", ""driver"" -> ""com.vertica.jdbc.Driver"", ""dbtable"" -> ""[SCHEMA].[TABLE]"", ""fetchsize"" -> ""100"") )\n- pdfv.printSchema()\n- pdfv.registerTempTable(""pdfv"")\n\n- %sql SELECT * FROM pdfv\n\n## Impala\n/opt/zeppelin/current/lib/ojdbc7.jar\t\n/tmp/toSratch/2.5.36.1056/ImpalaJDBC41.jar\t\n/opt/zeppelin/current/lib/vertica-jdbc-8.0.0-1.jar\t\n/tmp/toSratch/2.5.36.1056/commons-logging-1.1.1.jar\t\n/tmp/toSratch/2.5.36.1056/hive_metastore.jar\t\n/tmp/toSratch/2.5.36.1056/hive_service.jar\t\n/tmp/toSratch/2.5.36.1056/httpclient-4.1.3.jar\t\n/tmp/toSratch/2.5.36.1056/httpcore-4.1.3.jar\t\n/tmp/toSratch/2.5.36.1056/libfb303-0.9.0.jar\t\n/tmp/toSratch/2.5.36.1056/libthrift-0.9.0.jar\t\n/tmp/toSratch/2.5.36.1056/log4j-1.2.14.jar\t\n/tmp/toSratch/2.5.36.1056/ql.jar\t\n/tmp/toSratch/2.5.36.1056/slf4j-api-1.5.11.jar\t\n/tmp/toSratch/2.5.36.1056/slf4j-log4j12-1.5.11.jar\t\n/tmp/toSratch/2.5.36.1056/TCLIServiceClient.jar\t\n/tmp/toSratch/2.5.36.1056/zookeeper-3.4.6.jar\n\n# References\nhttps://www.linkedin.com/pulse/running-spark-2xx-cloudera-hadoop-distro-cdh-deenar-toraskar-cfa\nhttps://github.com/h2oai/sparkling-water/blob/master/DEVEL.md#SparklingWaterZeppelin\nhttp://www.cloudera.com/documentation/enterprise/5-8-x/topics/cdh_ig_running_spark_on_yarn.html\n- find IPAddress: docker inspect [container_id] | grep IPAddress\n- sudo iptables -t nat -A DOCKER -p tcp --dport 8080 -j DNAT --to-destination [container_ip]:8080\n- sudo iptables -t nat -A POSTROUTING -s [container_ip] -j MASQUERADE -p tcp --dport 8080 -d [container_ip]\nRedhat 6.5 if yum install R fails\n""wget http://mirror.centos.org/centos/6/os/x86_64/Packages/lapack-devel-3.2.1-4.el6.x86_64.rpm\nwget http://mirror.centos.org/centos/6/os/x86_64/Packages/blas-devel-3.2.1-4.el6.x86_64.rpm\nwget http://mirror.centos.org/centos/6/os/x86_64/Packages/texinfo-tex-4.13a-8.el6.x86_64.rpm\nwget http://vault.centos.org/6.2/updates/x86_64/Packages/libicu-devel-4.2.1-9.1.el6_2.x86_64.rpm\nsudo yum localinstall *.rpm\n\n\n'"
https://github.com/data-henrik/idug2016_interactive_db2_reports,How to have fun with DB2 / dashDB and Jupyter Notebooks - interactive presentation and reports,"b'# IDUG EMEA 2016 Conference, Brussels: Interactive DB2 Reports and Presentations\nConference Website: \n\n### How to have fun with DB2 / dashDB and Jupyter Notebooks - interactive presentation and reports\nMy IDUG session E08 is about to following topics\n   * Getting Started with Jupyter Notebooks\n   * Interfaces to DB2 / dashDB\n   * Presentations with RISE\n   * The FUN I had and more...\n\nThe draft for my IDUG EMEA presentation is in the file \n\n\n\n### Stuff I used\nFor my Jupyter Notebook-based presentation I made use of several products, libraries and technologies. Here is a list of those:\n* DB2 and dashDB\n* Jupyter Notebook: http://jupyter.org/\n* SQL Magic / ipython-sql: https://github.com/catherinedevlin/ipython-sql\n* RISE - Reveal.js Jupyter/IPython Slideshow Extension: https://github.com/damianavila/RISE\n* ibmdbpy: http://pythonhosted.org/ibmdbpy/\n* ibmdby sample notebooks: https://github.com/ibmdbanalytics/ibmdbpy-notebooks\n* Python interface to DB2 - ibm_db: https://github.com/ibmdb/python-ibmdb\n* Bokeh Visualization Library: http://bokeh.pydata.org/en/latest/\n* ...\n\nI started my journey with Jupyter Notebooks using the  which includes notebooks.\n\n### Links:\nIn the past I already used and wrote about notebooks and DB2, here are some related links as well as to samples you might find useful:\n* CeBIT Weather: https://github.com/data-henrik/CeBIT-Weather and related blog at http://blog.4loeser.net/2016/03/coincidence-cebit-visitors-and-weather.html\n* Notes on Notebooks: http://blog.4loeser.net/2016/08/notes-on-notebooks-data-db2-and-bluemix.html\n* Sample notebooks provided for Apache Spark service on Bluemix: https://developer.ibm.com/clouddataservices/docs/spark/sample-notebooks/\n* Simple graphing with pandas: http://pbpython.com/simple-graphing-pandas.html\n'"
https://github.com/Sapphirine/twitter_based_eod_stock_market_price_predictor,Twitter Based EOD Stock Market Price Predictor,"b'E6893 Big Data Analytics  \nColumbia University  \n12/22/2016  \nNan Zhao, Ben Zhu, Sabina Smajlaj  \nProfessor Chin-Yung Lin  \n\n#    EoD Price Predictor\n\n1. Data Source: \n      1. Twitter Dumps\n\n            Full Archive: https://archive.org/details/twitterstream\n\n            July, 2016: https://archive.org/details/archiveteam-twitter-stream-2016-07  \n\n            June, 2016: https://archive.org/details/archiveteam-twitter-stream-2016-06 \n\n            Auguest, 2016: https://archive.org/details/archiveteam-twitter-stream-2016-05\n\n      2. Stock Price\n\n            Quandl API: https://www.quandl.com/docs/api\n\n\n2. Tools Used: \n\n      Python, Sk-learn, Hadoop, Pandas, Numpy, JSON, Regular Expression\n\n3. User guide:\n\n      1. Data Fetching\n         get_rel_entries_bash and unzip_dir_bash are responsible for unzipping all data from different directories as well as sub directories. Each archive file is in form of Month-Day-Hour for subdirectories within the file. User shall run unzip_dir_bash and get_rel_entries_bash under Month folder to get 8 different csv for different company mentions.\n\n      2. Data Parsing, Cleaning and ML model\n        pivot_data.py, Quandl_data_parse.ipynb, Join_data_and_ML_model.ipynb\n        These files are responsible for cleaning, arregating data from multiple csv files and generate ML models for servers to provide predictions.\n\n      3. Web Hosting\n        All source code is within web_server folder. LINK to web app: http://e6893stockmarketpredictor.herokuapp.com \n        Web is written in Python with Flask infrastructure. \n  \n  \n'"
https://github.com/xpgeng/du4dama,a script of scratching comments,b'# du4dama\na script of scratching comments\n'
https://github.com/jay-mahadeokar/deeplab-public-ver2,This repo was mirrored from here: https://bitbucket.org/aquariusjay/deeplab-public-ver2,"b'## DeepLab v2\n\n### Introduction\n\nDeepLab is a state-of-art deep learning system for semantic image segmentation built on top of .\n\nIt combines (1) atrous convolution to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks, (2) atrous spatial pyramid pooling to robustly segment objects at multiple scales with filters at multiple sampling rates and effective fields-of-views, and (3) densely connected conditional random fields (CRF) as post processing.\n\nThis distribution provides a publicly available implementation for the key model ingredients reported in our latest .\nThis version also supports the experiments (DeepLab v1) in our ICLR15. You only need to modify the old prototxt files. For example, our proposed atrous convolution is called dilated convolution in CAFFE framework, and you need to change the convolution parameter ""hole"" to ""dilation"" (the usage is exactly the same). For the experiments in ICCV15, there are some differences between our argmax and softmax_loss layers and Caffes. Please refer to  for details.\n\nPlease consult and consider citing the following papers:\n\n    @article{CP2016Deeplab,\n      title={DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs},\n      author={Liang-Chieh Chen and George Papandreou and Iasonas Kokkinos and Kevin Murphy and Alan L Yuille},\n      journal={arXiv:1606.00915},\n      year={2016}\n    }\n\n    @inproceedings{CY2016Attention,\n      title={Attention to Scale: Scale-aware Semantic Image Segmentation},\n      author={Liang-Chieh Chen and Yi Yang and Jiang Wang and Wei Xu and Alan L Yuille},\n      booktitle={CVPR},\n      year={2016}\n    }\n\n    @inproceedings{CB2016Semantic,\n      title={Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs and a Discriminatively Trained Domain Transform},\n      author={Liang-Chieh Chen and Jonathan T Barron and George Papandreou and Kevin Murphy and Alan L Yuille},\n      booktitle={CVPR},\n      year={2016}\n    }\n\n    @inproceedings{PC2015Weak,\n      title={Weakly- and Semi-Supervised Learning of a DCNN for Semantic Image Segmentation},\n      author={George Papandreou and Liang-Chieh Chen and Kevin Murphy and Alan L Yuille},\n      booktitle={ICCV},\n      year={2015}\n    }\n\n    @inproceedings{CP2015Semantic,\n      title={Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs},\n      author={Liang-Chieh Chen and George Papandreou and Iasonas Kokkinos and Kevin Murphy and Alan L Yuille},\n      booktitle={ICLR},\n      year={2015}\n    }\n\n\nNote that if you use the densecrf implementation, please consult and cite the following paper:\n\n    @inproceedings{KrahenbuhlK11,\n      title={Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials},\n      author={Philipp Kr{""{a}}henb{""{u}}hl and Vladlen Koltun},\n      booktitle={NIPS},\n      year={2011}\n    }\n\n### Performance\n\nDeepLabv2 currently achieves 79.7% on the challenging PASCAL VOC 2012 semantic image segmentation task -- see the . \n\nPlease refer to our project  for details.\n\n### Pre-trained models\n\nWe have released several trained models and corresponding prototxt files at . Please check it for more model details.\n\n### Experimental set-up\n\n1. The scripts we used for our experiments can be downloaded from this :\n    1. run_pascal.sh: the script for training/testing on the PASCAL VOC 2012 dataset. Note You also need to download sub.sed script.\n    2. run_densecrf.sh and run_densecrf_grid_search.sh: the scripts we used for post-processing the DCNN computed results by DenseCRF.\n2. The image list files used in our experiments can be downloaded from this :\n    * The zip file stores the list files for the PASCAL VOC 2012 dataset.\n3. To use the mat_read_layer and mat_write_layer, please download and install .\n\n### FAQ\n\nCheck  if you have some problems while using the code.\n\n### How to run DeepLab\n\nThere are several variants of DeepLab. To begin with, we suggest DeepLab-LargeFOV, which has good performance and faster training time.\n\nSuppose the codes are located at deeplab/code\n\n1. mkdir deeplab/exper (Create a folder for experiments)\n2. mkdir deeplab/exper/voc12 (Create a folder for your specific experiment. Lets take PASCAL VOC 2012 for example.)\n3. Create folders for config files and so on.\n    1. mkdir deeplab/exper/voc12/config  (where network config files are saved.)\n    2. mkdir deeplab/exper/voc12/features  (where the computed features will be saved (when train on train))\n    3. mkdir deeplab/exper/voc12/features2 (where the computed features will be saved (when train on trainval))\n    4. mkdir deeplab/exper/voc12/list (where you save the train, val, and test file lists)\n    5. mkdir deeplab/exper/voc12/log (where the training/test logs will be saved)\n    6. mkdir deeplab/exper/voc12/model (where the trained models will be saved)\n    7. mkdir deeplab/exper/voc12/res (where the evaluation results will be saved)\n4. mkdir deeplab/exper/voc12/config/deeplab_largeFOV (test your own network. Create a folder under config. For example, deeplab_largeFOV is the network you want to experiment with. Add your train.prototxt and test.prototxt in that folder (you can check some provided examples for reference).)\n5. Set up your init.caffemodel at deeplab/exper/voc12/model/deeplab_largeFOV. You may want to soft link init.caffemodel to the modified VGG-16 net. For example, run ""ln -s vgg16.caffemodel init.caffemodel"" at voc12/model/deeplab_largeFOV.\n6. Modify the provided script, run_pascal.sh, for experiments. You should change the paths according to your setting. For example, you should specify where the caffe is by changing CAFFE_DIR. Note You may need to modify sub.sed, if you want to replace some variables with your desired values in train.prototxt or test.prototxt.\n7. The computed features are saved at folders features or features2, and you can run provided MATLAB scripts to evaluate the results (e.g., check the script at code/matlab/my_script/EvalSegResults).\n\n### Python\n\nSeyed Ali Mousavi has implemented a python version of run_pascal.sh (Thanks, Ali!). If you are more familiar with Python, you may want to take a look at .'"
https://github.com/mparker2/seaborn_sinaplot,A python implementation of the sinaplot using matplotlib and seaborn,b'# Sinaplot for /\nA python  implementation of the sinaplot\nA kind of hybrid between  and .\nidea is from \n'
https://github.com/gundamkeroro/DLND,This is all my project of Udacity Deep Learning Nanodegree starts in May 25,b'# DLND\nThis is all my project of Udacity Deep Learning Nanodegree starts in May 25\n\n\n##Projects:\n* Project 01: \n* Project 02: \n* Project 03: \n* Project 04: \n* Project 05: '
https://github.com/ProfessorBrunner/rp-pds15,"Practical Data Science course notes offered at the University of Illinois, Research Park in Spring 2015","b'# Practical Data Science\n\nUniversity of Illinois, Research Park    \nInstructor: Robert J. Brunner    \nSpring 2015  \n\n-----\n\nThe  page provides links to each weeks lessons.\n\n-----\n\n\nNote: This is a draft version that will be revised as we progress through the course.\n\n\n### Week 1: Introduction to Practical Data Science \n\nReview the course schedule and learning goals before posting a welcome\nmessage on the course Piazza. Next, learn about virtualization and the\nDocker engine and Docker container concept. In a breakout session,\ninstall the  and the course Docker image\nbuilt for this Practical Data Science course. Next, learn about source\ncode version control, and how to accomplish this by using the git tool.\nIn a breakout session, learn to work with git at the command line, and\nnavigate the github site. Review basic Unix concepts and gain experience\nworking at the Unix command prompt.\n\n### Week 2: Practical Command Line Data Science:  \n\nLearn about Unix, the Unix shell, and the Unix process model and\nfilesystem. Use the Docker technology by working at the Unix command\nprompt within the course Docker container in interactive mode. This will\nfocus on using Unix command line tools and techniques to work with data\nin the BASH shell\n\n### Week 3: Introduction to Python programming and the IPython Notebook\n\nLearn how to use the IPython notebook by using the course Docker\ncontainer in server mode. Also learn basic Python programming, python\ndata types, and file I/O, before finishing with a quick overview of the\nnumpy and scipy libraries.\n\n### Week 4: Exploring Data Through Visualizations:  \n \nLearn how to make data visualization by using Python, primarily from\nwithin the IPython notebook by using matplotlib and seaborn. This will\ninclude a discussion of scatter plots, linear regression and plotting,\nhistograms, box plots, and other advanced visualization concepts.\n\n\n\n### Week 5: Using Databases:  \n\nLearn about database technology, before specifically focusing on\nrelational database management systems. This will include learning how\nto create database, and SQL DDL and DML to create, insert, update and\ndelete data. This will conclude with a discussion of accessing a\ndatabase from Python.\n\n### Week 6: Data Acquisition:  \n\nLearn about acquiring data from diverse sources including webpages,\nonline repositories, and social media. This will require a discussion of\nweb scraping, DOM tress, and JSON.\n\n### Week 7: Statistical & Machine Learning:  \n \nReview basic statistics and probability and learn how to compute\ndifferent random distributions  by using numpy and scipy routines. Next,\nlearn about machine learning and basic approaches to perform machine\nlearning by using the scikit_learn library in Python.\n\n### Week 8: Data Intensive Computing:  \n\nLearn about basic concepts in high performance computing and how to\nperform them in Python. Next, learn about cloud computing, including how\nDocker technology integrates into commercial clouds. Finally, a\ndiscussion of the standard Hadoop platform and its capabilities.\n\n-----\n\nThe Practical Data Science course \n\n-----\n'"
https://github.com/rdemaria/pytimber,Python Wrapping of CALS API,b'# Project now maintained in\n\nhttps://gitlab.cern.ch/scripting-tools/pytimber.git\n'
https://github.com/luizgh/intro_to_cnns,Introdução à redes neurais convolucionais usando Theano + Lasagne,"b'# Introdu\xc3\xa7\xc3\xa3o \xc3\xa0 redes neurais convolucionais\n\nEsse reposit\xc3\xb3rio cont\xc3\xa9m um tutorial de redes neurais convolucionais usando as bibliotecas Theano e Lasagne. O foco \xc3\xa9 na implementa\xc3\xa7\xc3\xa3o de redes neurais usando essas bibliotecas.\n\nO tutorial \xc3\xa9 dividido em tr\xc3\xaas partes:\n* Introdu\xc3\xa7\xc3\xa3o \xc3\xa0 aprendizagem de m\xc3\xa1quina e ao Theano\n* Redes neurais convolucionais (CNNs)\n* Transfer Learning usando CNNs\n\nOs slides podem se consultados na pasta /slides. Exerc\xc3\xadcios para implementar os modelos s\xc3\xa3o propostos em Python, usando Ipython \n\n## Configura\xc3\xa7\xc3\xa3o necess\xc3\xa1ria\n\nOs exemplos foram feitos para serem executados em CPU, e requerem os seguintes programas/bibliotecas:\n* Python 2 (recomendo a distribui\xc3\xa7\xc3\xa3o anaconda: https://www.continuum.io/downloads)\n* Pacotes de python:\n   * scipy, jupyter, notebook, PIL, matplotlib (podem ser instalados usando ""conda"" (se estiver usando a distribui\xc3\xa7\xc3\xa3o anaconda) ou ""pip""\n   * Theano e Lasagne (ler instru\xc3\xa7\xc3\xb5es de instala\xc3\xa7\xc3\xa3o nesse link: https://lasagne.readthedocs.io/en/latest/user/installation.html)\n   \n'"
https://github.com/chusiang/ansible-jupyter.dockerfile,Building the Docker image with Ansible and Jupyter.,"b'# Docker image: Ansible on Jupyter Notebook\n\n \n\nA Docker image for run [Ansible][ansible_official] 2.x on [Jupyter Notebook][jupyter_official] 4.x (ipython notebook) with Browsers.\n\n[ansible_official]: https://www.ansible.com/\n[jupyter_official]: http://jupyter.org/\n\n## Supported tags and respective  links\n\n- ,  [(alpine-3/Dockerfile)][dockerfile_alpine-3]\n-  [(archlinux/Dockerfile)][dockerfile_archlinux]\n-  [(centos-7/Dockerfile)][dockerfile_centos-7]\n-  [(debian-9/Dockerfile)][dockerfile_debian-9]\n- ~~~~ [(gentoo/Dockerfile)][dockerfile_gentoo]\n-  [(opensuse-42.3/Dockerfile)][dockerfile_opensuse-42.3]\n-  [(ubuntu-18.04/Dockerfile)][dockerfile_ubuntu-18.04]\n\n[dockerfile_alpine-3]:      https://github.com/chusiang/ansible-jupyter.dockerfile/blob/master/alpine-3/Dockerfile\n[dockerfile_archlinux]:     https://github.com/chusiang/ansible-jupyter.dockerfile/blob/master/archlinux/Dockerfile\n[dockerfile_centos-7]:      https://github.com/chusiang/ansible-jupyter.dockerfile/blob/master/centos-7/Dockerfile\n[dockerfile_debian-9]:      https://github.com/chusiang/ansible-jupyter.dockerfile/blob/master/debian-9/Dockerfile\n[dockerfile_gentoo]:        https://github.com/chusiang/ansible-jupyter.dockerfile/blob/master/gentoo/Dockerfile\n[dockerfile_opensuse-42.3]: https://github.com/chusiang/ansible-jupyter.dockerfile/blob/master/opensuse-42.3/Dockerfile\n[dockerfile_ubuntu-18.04]:  https://github.com/chusiang/ansible-jupyter.dockerfile/blob/master/ubuntu-18.04/Dockerfile\n\n## Build image\n\n1. Get this project.\n\n    \n\n1. Go to workspace.\n\n    \n\n1. Bunild the image.\n\n    \n\n## Run container\n\n1. Get image.\n\n    \n\n1. Run the container with daemon mode.\n\n    \n\n1. Check container process.\n\n    \n\n1. Enter container with command line.\n\n    \n\n## Play Ansible on Jupyter\n\nNow, you can play the Ansible on Jupyter.\n\n1. Go jupyter web.\n\n    \n\n    ![2016-11-20-ansible-jupyter1]\n\n1. Attach my example ==> [][ansible_on_jupyter.ipynb].\n\n    ![2016-11-20-ansible-jupyter2]\n\n1. Remember use the  prefix to trigger system command.\n\nYou can see more detail at  .\n\nEnjoy it !\n\n[ansible_on_jupyter.ipynb]: https://github.com/chusiang/ansible-jupyter.dockerfile/blob/master/ipynb/ansible_on_jupyter.ipynb\n[2016-11-20-ansible-jupyter1]: https://cloud.githubusercontent.com/assets/219066/20463322/218f0c4a-af6b-11e6-9a95-2411ec7acb5f.png\n[2016-11-20-ansible-jupyter2]: https://cloud.githubusercontent.com/assets/219066/20463319/fa8c047c-af6a-11e6-96d6-f985096c9c8c.png\n\n## History\n\n### 2020\n\n* 12/12 Fixed Python 3 dependency problem on Alpine Linux v3.12, and stop support some EOL images.\n\n### 2018\n\n* 07/11 Add new images of . Stop automated build image of ,  and .\n* 06/18 Add new images of , . Stop automated build image of .\n* 01/10 Stop automated build images of ,  and .\n\n### 2017\n\n* ??/?? Stop automated build images of ,  and .\n\n## License\n\nCopyright (c) chusiang from 2016-2020 under the MIT license.\n'"
https://github.com/srhrshr/word2vec-presentation,"Slidedeck for the presentation at the Big Data Conclave, VIT Chennai",b'# word2vec-presentation\n'
https://github.com/Syps/boston_data_react,Boston data visualization redone in React,b'# boston_data_react\n redone with React\n***\nhttps://www.nicksypteras.com/data/boston\n\n\n'
https://github.com/ilanman/STA561,Probability Machine Learning final project,b'This repo holds my STA561 Machine Learning code and project. The project highlights randomized singular value decomposition methods compared to traditional SVD. There are some computational comparisons implemented in Python. The results show that RSVD methods are faster than SVD and power iterations significantly increase time to completion at a low accuracy cost. \n'
https://github.com/linetools/linetools,"A package for the analysis of 1d astronomical spectra, especially quasar and galaxy spectra.","b'linetools\n=========\n\n\n\nThis is a package for the analysis of 1d astronomical spectra,\nespecially quasar and galaxy spectra.\n\nCheck out the documentation at https://linetools.readthedocs.org/en/latest\n\nDevelopment status\n------------------\n\n\n\nDOI\n---\n\n'"
https://github.com/MollyZhang/Reinforcement_learning_NLP,Implementing Reinforcement Learning to find the best dialogue strategy for a conversation agent (chatbot) by searching for maximum award.,"b'# Reinforcement_learning_NLP\nImplementing Reinforcement Learning to find the best dialogue strategy for a conversation agent (chatbot) by search for maximum award.\n\n\n# To record a converstaion, do:\n1. \n2. \n3. \n4.   \n\n\nIf you want to train and populate reward table based on the 300 conversations recorded,type f,if you want to try a new dialogue,type s,if you want to view the accuracy of the evaluation model,type e,if you want to view the reward table,type r,if you want to view the Q_table type q\n\n### Future improvements\n- Dealing with user saying gibberish like ""dfkjlskdfj""\n- Dealing user repeat itself\n- Dealing with user insult\n- Having strategy to mickmick user input. e.g. When user says ""yay!!!"", bot says""wow!!!!"". \n\n\n### A brief overview of the code\nWe have learnt currently from the 300 odd conversations and populated the Reward table based on the user evaluation metrics.\nThe first block initializes the variables and the Q_table and the R_table.We have 6 strategies and 18 state variables based on the 4 state metrics like (If the user utterance is a question or not,the length of the utterance,the sentiment of the uttterance and whether the utterance is at the beginning(first utterance of the user),we have thus created 18 combinations of these states.\n\nThe second block are all the utility functions used and called by the later blocks.The most prominent amongst them being the training() where we train and populate the Q_table.The logic of Q_learning is implemented here.\n\nThe third block populates the reward table according to whether the utterance is at the beginning(in this case,it is calculated according to 0.8start+0.2 overall,while for the rest utterances,it is 0.4engaging+04interrupt+0.2*overall.\n\nThe fourth block is used for training where it calls the training() method.\n\nThe fifth block records the new conversations and poplates the strategies based on the Q_table and updates the Q_table.\nWork on evaluation is still in progress.\n'"
https://github.com/andy1li/udacity-dlnd,My Projects for the Udacity Deep Learning Nanodegree Foundation Program,b'# My Projects for the Udacity Deep Learning Nanodegree Foundation Program\nhttps://profiles.udacity.com/u/andyli\n\n'
https://github.com/sgandavarapu/FakeNews,W210 Capstone Project,"b'# FakeNews\nW210 Capstone Project\n\n__data.zip__ contains html from the home pages of 14 fake news sources, extracted once per day from 02-18-2017 -> 03/07/2017. It also contains text files with JSON for all of the articles posted 02-18-2017 -> 02-28-2017 - for now only 7 of 14 sources are included.\n  \n__data_credible.zip__ contains txt file of full txt of 5 top articles extracted daily from 02-27-2017 -> 03-05-2017 from 8 credible sources.\n'"
https://github.com/GiorgioBondi/SmartPillow,"Giorgio Bondì, Marta Brunetti, Giulia Crocioni and Francesca Cunsolo's repository for our project during XPH2016 Hackaton @NECSTLab Politecnico di Milano sponsored by Xilinx","b""# SmartPillow\nGiorgio Bond\xc3\xac, Marta Brunetti, Giulia Crocioni and Francesca Cunsolo's repository for our project during XPH2016 Hackaton @NECSTLab Politecnico di Milano sponsored by Xilinx\n"""
https://github.com/pynxton/course,Python course,b'Python course\n===============\n\nSee notebooks in \n\nBooked room for the course on \n 23 Oct     Meadow        \n 30 Oct     Meadow        \n 6  Nov     Meadow        _  \n 13 Nov     Ickleton      \n 20 Nov     Ickleton      \n 27 Nov     Meadow        \n 4 Dec      Ickleton      _\n========== ============ ====================================================================================\n\n'
https://github.com/kristijensen/OOP,I'm playing at defining classes,"b""# OOP\nI'm playing at defining classes\n"""
https://github.com/bwallace/Deep-PICO,Deep-ish learning for PICO extraction from abstracts,"b'# Deep-PICO\n\nExperiments in deep (OK, shallow, but using embeddings) for PICO identification.\n\n##Requirements\n\npython2.7\n\nKeras\n\n\nscikit-learn\n\ngensim\n\ntheano\n\nnltk\n\ngeniatagger\n\nsklearn_crfsuite\n\npycrfsuite\n\n\n\nInstalling tensorflow\n\n\n##Usage\n\n###Running the Conditional Random Field Model\n\n\n###Command line arguments \n\n\n\n\n\n###Running the Convolutional or Standard Neural Network \nTo use the  Convolutional Neural Network or Standard Feed forward Neural Network\n\n\n\n###Command line arguments\n\n\n'"
https://github.com/SimonTong22/Baseball-Data-Explore,Data Exploration of Baseball Stats,b'# Baseball-Data-Explore\n\nJupyter Notebook of Python Exploration of Baseball Data using statistics in numpy and pandas to understand the value of defense \n'
https://github.com/theideasmith/neuralmanifolds,My experimentation with neural networks as approximations of continuous high dimensional surfaces and the dynamics of particles therein. ,"b'---\ntitle: Differential Equations and Modelling of Point Charges\ndate: 2017-2-2\nauthor: Akiva Lipshitz\nlayout: post\n---\n\nThis post continues my \n\nParticles and their dynamics are incredibly fascinating, even wondrous. Give me some particles and some simple equations describing their interactions \xe2\x80\x93\xc2\xa0some very interesting things can start happening. \n\nCurrently studying electrostatics in my physics class, I am interested in not only the static force and field distributions but also in the dynamics of particles in such fields. To study the dynamics of electric particles is not an easy endeavor \xe2\x80\x93\xc2\xa0in fact the differential equations governing their dynamics are quite complex and not easily solved manually, especially by someone who lacks a background in differential equations. \n\nInstead of relying on our analytical abilities, we may rely on our computational abilities and numerically solve the differential equations. Herein I will develop a scheme for computing the dynamics of $n$ electric particles en masse. It will not be computationally easy \xe2\x80\x93\xc2\xa0the number of operations grows proportionally to $n^2$. For less than $10^4$ you should be able to simulate the particle dynamics for long enough time intervals to be useful. But for something like $10^6$ particles the problem is intractable. Youll need to do more than $10^12$ operations per iteration and a degree in numerical analysis. \n\n\n\n## Governing Equations \n\nGiven $n$ charges $q_1, q_2, ..., q_n$, with masses $m_1, m_2, ..., m_n$ located at positions $vec{r}1, {j to i}}$$\n\nAnd then the net acceleration of particle $q_i$ just normalizes the force by the mass of the particle:\n\n$$vec{a}i = {N, i}}{m_i}$$\n\nTo implement this at scale, were going to need to figure out a scheme for vectorizing all these operations, demonstrated below. \n\nWell be using  for our numerical integration. Below, the function  is a function that returns the derivatives for all our variables at each iteration. We pass it to  and then do the integration. \n\n\n\n\nLets define our time intervals, so that odeint knows which time stamps to iterate over. \n\n\n\n\nSome other constants\n\n\n\n\nWe get to choose the initial positions and velocities of our particles. For our initial tests, well set up 3 particles to collide with eachother. \n\n\n\n\nAnd pack them into an initial state variable we can pass to odeint. \n\n\n\n\n## The Fun Part \xe2\x80\x93 Doing the Integration\n\nNow, well actually do the integration\n\n\n\n\n    /Library/Python/2.7/site-packages/ipykernel/main.py:60: RuntimeWarning: divide by zero encountered in divide\n\n\n\n\n\n\n\n## Videos\n\n\n\n\n## Path Plot\n\n'"
https://github.com/olemke/arts,The Atmospheric Radiative Transfer Simulator,"b""\n\n\nWelcome to ARTS\n===============\n\nARTS is free software. Please see the file COPYING for details.\n\nIf you use data generated by ARTS in a scientific publication, then please\nmention this and cite the most appropriate of the ARTS publications that are\n\nsummarized on http://www.radiativetransfer.org/docs/\n\n provides information on contributing\nto ARTS on GitHub.\n \nFor documentation, please see the files in the doc subdirectory.\n\nFor building and installation instructions please read below.\n\n\nDependencies\n------------\n\nBuild Prerequisites (provided by mambaforge):\n\n- gcc/g++ >=11 (or llvm/clang >=13)\n- cmake (>=3.18)\n- zlib\n- openblas\n- libc++ (only for clang)\n- libmicrohttpd (>=0.9, optional, for documentation server)\n- netcdf (optional)\n- Python3 (>=3.9)\n  - required modules:\n    docutils\n    lark-parser\n    matplotlib\n    netCDF4\n    numpy\n    pytest\n    scipy\n    setuptools\n    xarray\n- GUI (optional)\n    glfw\n    glew\n\nTo build the documentation you also need:\n\n- pdflatex (optional)\n- doxygen (optional)\n- graphviz (optional)\n\n\nBuilding ARTS\n-------------\n\nThe following instructions assume that you are using mambaforge as a build environment.  The installer is available at\n.\n\nUse the provided  files to create a conda\nenvironment with all required dependencies. The environment will be called\n:\n\nLinux:\n\n\nmacOS:\n\n\nHere are the steps to use  to build ARTS.\n\n\n\nX is the number of parallel build processes.\nX=Number of Cores gives you usually the fastest compilation time.\n\nWARNING: The compilation is very memory intensive. If you have 16GB of RAM,\ndon't use more than 6-8 cores. With 8GB, don't use more than 2-3 cores.\n\nDevelopment install of the PyARTS Python package:\n\n\n\nYou only have to do the python package install once.\nIf the ARTS source has changed, update the PyARTS package by running:\n\n\n\n\nBuild configurations\n--------------------\n\nBy default, ARTS is built in release mode with optimizations enabled and\nassertions and debugging symbols turned off.\n\nWhenever you change the configuration, remove your build directory first:\n\n\n\nTo build with assertions and debugging symbols use:\n\n\n\nThis configuration offers a good balance between performance and debugging\ncapabilities. Since this still optimizes out many variables, it can be\nnecessary for some debugging cases to turn off all optimizations. For those\ncases, the full debug configuration can be enabled. Note that ARTS runs a lot\nslower in this configuration:\n\n\n\n\nInstalling PyARTS\n-----------------\n\nTo install the PyARTS Python package, you need to build it and install it with\npip. Create your build directory and configure ARTS with cmake as described in\nthe previous sections. Then, run the following commands inside your build\ndirectory:\n\n\n\nThis will not mess with your system's Python installation.\nA link to the pyarts package is created in your home directory, usually\n.\n\nYou don't need to reinstall the package with pip after updating ARTS.\nYou only need to run  again.\n\n\nTests\n-----\n\n'cmake --build build --target check' will run several test cases to ensure that\nARTS is working properly. Use 'check-all' to run all available controlfiles,\nincluding computation time-intensive ones.\n\nSome tests depend on the arts-xml-data package. cmake automatically looks if it\nis available in the same location as ARTS itself. If necessary, a custom path\ncan be specified.\n\n\n\nIf arts-xml-data cannot be found, those tests are ignored.\n\nBy default, 4 tests are executed in parallel.\nIf you change the number of concurrently run test, you can add this option to your  call:\n\n\n\nX is the number of tests that should be started in parallel.\n\nYou can also use the ctest command directly to run the tests:\n\nFirst, change to the  directory:\n\n\nThis runs all test with 4 jobs concurrently:\n\n\nTo run specific tests, use the -R option and specify part of the test case name\nyou want to run. The following command will run all tests that have 'ppath' in\ntheir name, e.g. arts.ctlfile.fast.ppath1d ...:\n\n\n\nTo see the output of ARTS, use the -V option:\n\n\n\nBy default, ctest will not print any output from ARTS to the screen. The option\n--output-on-failure can be passed to ctest to see output in the case an error\noccurs. If you want to always enable this, you can set the environment variable\nCTEST_OUTPUT_ON_FAILURE:\n\n\n\n\nNative build\n------------\n\nTo squeeze out every last drop of performance, you can also build a version\nspecifically optimized for your machine's processor:\n\n\n\nThis option should make the executable slightly faster, more so on better\nsystems, but not portable. Note that since this build-mode is meant for\nfast-but-accurate computations, some IEEE rules will be ignored. For now only\ncomplex computations are IEEE incompatible running this mode of build.\n\n\nOptional features\n-----------------\n\nFeatures that rely on Fortran code located in the 3rdparty\nsubdirectory are enabled by default, but can be disabled by passing the\nfollowing option to the  command:\n\n\n\nThis disables Disort, Fastem and Tmatrix.\n\nIf necessary, certain Fortran modules can be selectively disabled:\n\n\nor\n\n\nIMPORTANT: Only gfortran is currently supported.\nAlso, a 64-bit system is required (size of long type must be 8 bytes).\n\n\nEnable NetCDF: The basic matpack types can be read from NetCDF files, if NetCDF\nsupport is enabled:\n\n\n\nPrecompiled headers: PCH can speed up builds significantly. However, it hampers\nthe ability for ccache to properly skip unnecessary compilations, potentially\nincreasing rebuild times. Tests have shown that it only speeds up the build\nconsiderably for Clang, but not for GCC.\n\n\n\nIf you enable PCH and also use ccache, you need to set the \nenvironment variable properly:\n\n\n\n\nDisabling features\n------------------\n\nDisable assertions: \n\nDisable OpenMP: \n\nDisable the built-in documentation server: \n\n\nccache support\n--------------\n\nThe build utilizes ccache automatically when available, it can be\nturned of with the option \n\nvalgrind --tool=callgrind --separate-callers=10 --separate-recs=3 arts -n1 ...\n\ncmake --preset=perf-gcc-mamba\n\nperf record -g src/arts MYCONTROLFILE.arts\n\nperf record -g ctest -R TestDOIT$\n\nperf report -g graph,0.5,callees\n```\n\nThis will show a reverse call tree with the percentage of time spent in each\nfunction. The function tree can be expanded to expose the calling functions.\n\n"""
https://github.com/sumitbinnani/AIND-Planning,Deterministic logistics planning problems for an Air Cargo transport system using a planning search agent,"b'\n# Implement a Planning Search\n\n# .\n\n## Synopsis\n\nThis project includes skeletons for the classes and functions needed to solve deterministic logistics planning problems for an Air Cargo transport system using a planning search agent. \nWith progression search algorithms like those in the navigation problem from lecture, optimal plans for each \nproblem will be computed.  Unlike the navigation problem, there is no simple distance heuristic to aid the agent. \nInstead, you will implement domain-independent heuristics.\n\n\n- Part 1 - Planning problems:\n\t- READ: applicable portions of the Russel/Norvig AIMA text\n\t- GIVEN: problems defined in classical PDDL (Planning Domain Definition Language)\n\t- TODO: Implement the Python methods and functions as marked in \n\t- TODO: Experiment and document metrics\n- Part 2 - Domain-independent heuristics:\n\t- READ: applicable portions of the Russel/Norvig AIMA text\n\t- TODO: Implement relaxed problem heuristic in \n\t- TODO: Implement Planning Graph and automatic heuristic in \n\t- TODO: Experiment and document metrics\n- Part 3 - Written Analysis\n\n## Environment requirements\n- Python 3.4 or higher\n- Starter code includes a copy of  \nfor the Stuart Russel/Norvig AIMA text.  \n\n\n## Project Details\n### Part 1 - Planning problems\n#### READ: Stuart Russel and Peter Norvig text:\n\n""Artificial Intelligence: A Modern Approach"" 3rd edition chapter 10 or 2nd edition Chapter 11 on Planning, available  sections: \n\n- The Planning Problem\n- Planning with State-space Search\n\n#### GIVEN: classical PDDL problems\n\nAll problems are in the Air Cargo domain.  They have the same action schema defined, but different initial states and goals.\n\n- Air Cargo Action Schema:\n\n\n- Problem 1 initial state and goal:\n\n- Problem 2 initial state and goal:\n\n- Problem 3 initial state and goal:\n\n\n#### TODO: Implement methods and functions in \n-  method including  and  sub-functions\n-  method\n-  method\n-  function\n-  function\n\n#### TODO: Experiment and document metrics for non-heuristic planning solution searches\n* Run uninformed planning searches for , , and ; provide metrics on number of node expansions required, number of goal tests, time elapsed, and optimality of solution for each search algorithm. Include the result of at least three of these searches, including breadth-first and depth-first, in your write-up ( and ). \n* If depth-first takes longer than 10 minutes for Problem 3 on your system, stop the search and provide this information in your report.\n* Use the  script for your data collection: from the command line type  to learn more.\n\n>#### Why are we setting the problems up this way?  \n>Progression planning problems can be \nsolved with graph searches such as breadth-first, depth-first, and A*, where the \nnodes of the graph are ""states"" and edges are ""actions"".  A ""state"" is the logical \nconjunction of all boolean ground ""fluents"", or state variables, that are possible \nfor the problem using Propositional Logic. For example, we might have a problem to \nplan the transport of one cargo, C1, on a\nsingle available plane, P1, from one airport to another, SFO to JFK.\n\nIn this simple example, there are five fluents, or state variables, which means our state \nspace could be as large as . Note the following:\n>- While the initial state defines every fluent explicitly, in this case mapped to TTFFF, the goal may \nbe a set of states.  Any state that is  for the fluent  meets the goal.\n>- Even though PDDL uses variable to describe actions as ""action schema"", these problems\nare not solved with First Order Logic.  They are solved with Propositional logic and must\ntherefore be defined with concrete (non-variable) actions\nand literal (non-variable) fluents in state descriptions.\n>- The fluents here are mapped to a simple string representing the boolean value of each fluent\nin the system, e.g. TTFFTT...TTF.  This will be the state representation in \nthe  class and is compatible with the  and  \nclasses, and the search methods in the AIMA library.  \n\n\n### Part 2 - Domain-independent heuristics\n#### READ: Stuart Russel and Peter Norvig text\n""Artificial Intelligence: A Modern Approach"" 3rd edition chapter 10 or 2nd edition Chapter 11 on Planning, available  section: \n\n- Planning Graph\n\n#### TODO: Implement heuristic method in \n-  method\n\n#### TODO: Implement a Planning Graph with automatic heuristics in \n-  method\n-  method\n-  method\n-  method\n-  method\n-  method\n-  method\n-  method\n\n\n#### TODO: Experiment and document: metrics of A* searches with these heuristics\n* Run A* planning searches using the heuristics you have implemented on ,  and . Provide metrics on number of node expansions required, number of goal tests, time elapsed, and optimality of solution for each search algorithm and include the results in your report. \n* Use the  script for this purpose: from the command line type  to learn more.\n\n>#### Why a Planning Graph?\n>The planning graph is somewhat complex, but is useful in planning because it is a polynomial-size approximation of the exponential tree that represents all possible paths. The planning graph can be used to provide automated admissible heuristics for any domain.  It can also be used as the first step in implementing GRAPHPLAN, a direct planning algorithm that you may wish to learn more about on your own (but we will not address it here).\n\n>Planning Graph example from the AIMA book\n>\n\n### Part 3: Written Analysis\n#### TODO: Include the following in your written analysis.  \n- Provide an optimal plan for Problems 1, 2, and 3.\n- Compare and contrast non-heuristic search result metrics (optimality, time elapsed, number of node expansions) for Problems 1,2, and 3. Include breadth-first, depth-first, and at least one other uninformed non-heuristic search in your comparison; Your third choice of non-heuristic search may be skipped for Problem 3 if it takes longer than 10 minutes to run, but a note in this case should be included.\n- Compare and contrast heuristic search result metrics using A* with the ""ignore preconditions"" and ""level-sum"" heuristics for Problems 1, 2, and 3.\n- What was the best heuristic used in these problems?  Was it better than non-heuristic search planning methods for all problems?  Why or why not?\n- Provide tables or other visual aids as needed for clarity in your discussion.\n\n## Examples and Testing:\n- The planning problem for the ""Have Cake and Eat it Too"" problem in the book has been\nimplemented in the  module as an example.\n- The  directory includes  test cases to evaluate your implementations. All tests should pass before you submit your project for review. From the AIND-Planning directory command line:\n    - \n    - \n- The  script is provided for gathering metrics for various search methods on any or all of the problems and should be used for this purpose.\n\n\n'"
https://github.com/codebasics/py,Repository to store sample python programs for python learning,"b'# py\nRepository to store sample Python programs.\n\nThis repository is meant for beginners to assist them in their learning of Python. The repository covers a wide range of algorithms and other programs, and would prove immensely helpful for everybody interested in Python programming.\n\nIf this is your first time coding in Python, I would love to suggest you begin from the . They are simple to understand and hopefully will prove fun to you.\n\nYou can also pay a visit to my very own .\n\nContributions to the repository are welcome.\n\n.\n\n#### Happy coding!\n'"
https://github.com/lisaleemcb/ouroboros,Associated code for active feedback resonator axion detector prototype,b'ouroboros\n=========\n\nAssociated code for Active Feedback Resonator axion detector prototype\n'
https://github.com/saverkamp/measure-metadata-workshop,Workshop materials for Code4Lib 2016 pre-conference: http://2016.code4lib.org/workshops/Measuring-Your-Metadata,"b""## Measuring Your Metadata -- Code4Lib 2016 Pre-Conference Workshop\nMonday, March 7  \n1:30-4:30pm  \nChemical Heritage Foundation  \nRoom: Ullyot S  \n\n\nCoordinators: Shawn Averkamp, Sara Rubinow, Matt Miller, Josh Hadro   \n\nTools and standards abound for creating and enriching metadata, but measuring, monitoring, and managing metadata for the long haul can be a daunting task. What tools are out there to assess the shape of our metadata? How can visualizations show us the gaps or flaws in our description? What can web traffic analytics tell us about the value of our metadata? What is quality, really? We certainly don\xe2\x80\x99t have all the answers, but together we can workshop the questions. Specific topics will be driven by the interest of attendees. The organizers will bring examples of their own work at NYPL in visualization, data analysis with Python, and Google analytics assessment and invite participants to bring their own tools and strategies to share in group discussion, short demos, and hands-on breakout sessions. Takeaways will include: exposure to approaches and tools in use in the field and an expanded network of commiserators to help you through your next metadata audit.\n\nSchedule:  \n1:30 - 1:40 Introductions  \n1:40 - 3:00   \n3:00 - 3:10 Break  \n3:10 - 4:10   \n4:10 - 4:30 Reporting back and discussion  \n\n## Before you attend\nYou'll get the most out of your hands-on sessions if you install the necessary applications ahead of time. If you're planning to participate in the following hands-on sessions, please try to come prepared!  \n\n### Using Python to assess metadata quality in MODS\nFor this hands-on session, we'll be using  to walk through some simple functions and scripts. We'll also be working with , a third-party Python library for parsing and manipulating XML. Fortunately, both of these are already included in the . We strongly recommend  for this workshop. It also comes bundled with  and all of its dependencies, so it will be useful to have it you're interested in learning more about data analysis.  \n\n### A beginner's guide to metadata analysis in Python with pandas\nFor this hands-on session, we'll be using  to explore basic data analysis with , a Python data analysis library. Fortunately, both IPython notebook and pandas (as well as two additionally-necessary packages,  and ) are already included in the . We strongly recommend  for this session. It also comes bundled with , a third-party Python library for parsing and manipulating XML, so it will be useful to have if you're interested in learning more about using Python to parse your XML data.\n\n### Visualizing your metadata with d3\nThis hands-on session will be a beginner's intro to the d3 visualization library. We will use it to try to render metadata quality results which allows quick visual analysis. All you will need for this session are the examples and data provided in the d3_viz folder, a text editor and a web browser.\n\n## Notes and Resources\nDuring the workshop, we'll take collaborative notes and share favorite resources in . We invite you to contribute!  \n"""
https://github.com/weihuayi/fealpy,Finite Element Analysis Library in Python,"b'# FEALPy: Finite Element Analysis Library in Python\n\n\n\n\n\n\n\nWe want to develop an efficient and easy to use finite element software\npackage to support our teach and research work. \n\nWe still have lot work to do. \n\n\xe5\x85\xb3\xe4\xba\x8e FEALPy \xe7\x9a\x84\xe4\xb8\xad\xe6\x96\x87\xe5\xb8\xae\xe5\x8a\xa9\xe4\xb8\x8e\xe5\xae\x89\xe8\xa3\x85\xe4\xbf\xa1\xe6\x81\xaf\xe8\xaf\xb7\xe6\x9f\xa5\xe7\x9c\x8b\xef\xbc\x9a\n\n\n# Installation\n\n## Common\n\nTo install the latest release from PyPi, use\n \n\nIf you have no  access on Linux/MacOS, please try \n\n\nUsers in China can install FEALPy from mirrors such as:\n- \n- \n- \n\n## From Source\n\n\n\nFor developers, please use  to install it in develop mode.\n\nOn Linux system such as Ubuntu or Fedora, or MacOS, maybe you should use  to install it in\ndevelop mode.\n\n## Uninstallation\n\n\n\n## Warning \nThe sparse pattern of the matrix  generated by  may not be the same as the theoretical pattern, since there exists nonzero values that are close to machine precision due to rounding. If you care about the sparse pattern of the matrix, you can use the following commands to eliminate them\n\n\n## Docker\n\nTo be added.\n\n## Reference and Acknowledgement\n\nWe thank Dr. Long Chen for the guidance and compiling a systematic documentation for programming finite element methods.\n* http://www.math.uci.edu/~chenlong/programming.html\n* https://github.com/lyc102/ifem\n\n\n## Citation\n\nPlease cite  if you use it in your paper\n\nH. Wei and Y. Huang, FEALPy: Finite Element Analysis Library in Python, https://github.com/weihuayi/fealpy, Xiangtan University, 2017-2023.\n\n\n\n\n\n\n\n\n\n\n\n'"
https://github.com/docathon/sphinx-template,A template for a fully-functioning sphinx deployment,"b""# sphinx_template\n\nMaterials to help you go from zero to full documentation as quickly as possible.\n\nThis repository contains the following directories:\n\n* docs/\n  * The Sphinx source of the documentation.\n* my_package/\n  * An example Python package.  We'll grab docstrings from this package and\n  generate documentation for it.\n* examples/\n  * Visual examples that illustrate the use of .  These examples\n  will be turned into a gallery.\n\nOf particular interest may be the  file, which is used to\nconfigure Sphinx.\n\nThe documentation may be built by doing:\n\n\n"""
https://github.com/ubcs3/2016-Fall,UBC Scientific Software Seminar: Machine Learning in Python with scikit-learn,"b""## UBC Scientific Software Seminar\n\nThe UBC Scientific Software Seminar is inspired by  and its goal is to help students, graduates, fellows and faculty at UBC develop software skills for science.\n\n### Fall 2016: Machine Learning in Python with scikit-learn\n\n#### OUTLINE\n\n* What are the learning goals?\n  * To learn how to use  to solve machine learning problems\n  * To master  programming for scientific computing\n  * To learn mathematics and statistics applied to data science and machine learning\n  * To meet and collaborate with other students and faculty interested in scientific computing\n* What software tools are we going to use?\n  * : machine learning in Python\n  * : scientific computing with , ,  and \n  * \n  * : execute code with accompanying text, markdown and LaTeX all in the browser\n  * : manage projects locally from the command line with Git and collaborate online with GitHub\n* What scientific topics will we study?\n  * Machine learning fundamentals (following  provided by ):\n    * Regression, classification, clustering, dimensionality reduction\n  * Special topics:\n    * \n* Where do we start? What are the prerequisites?\n  * UBCS3 Fall 2016 is a continuation of  which included:\n    * Bash shell\n    * Git/GitHub\n    * Python programming\n    * SciPy stack: NumPy, Scipy, matplotlib and pandas\n    * Basic examples using scikit-learn\n  * Calculus, linear algebra, probability and statistics\n* Who is the target audience?\n  * Everyone is invited!\n  * If the outline above is at your level, perfect! Get ready to write a lot of code!\n  * If the outline above seems too intimidating, come anyway! You'll learn things just by being exposed to new tools and ideas, and meeting new people!\n  * If you have experience with all the topics outlined above, come anyway! You'll become more of an expert by participating as a helper/instructor!\n\n#### SCHEDULE\n\nFall 2016 will consist of weekly 1-hour meetings held from October until mid-December. The regular scheduled time is Friday 1-2pm (with additional hour 3-4pm for those who cannot attend 1-2pm).\n\n* Week 1 - Friday October 7 - 1-2pm - LSK 121 []\n  * Overview of machine learning problems\n  * Exploring the scikit-learn documentation\n  * Getting to know the scikit-learn API\n  * First examples with builtin example datasets\n* Week 2 - Friday October 14 - 1-2pm - LSK 121 []\n  * Regression Example: Diabetes dataset\n    * A closer look at least squares linear regression calculations\n    * Can we improve R2? Let's create more features\n    * Splitting the dataset: Training data and testing data\n  * Classification Example: Hand-written digits dataset\n    * K-nearest neighbors classifier\n    * Evaluating the model\n* Week 3 - Friday October 21 - 1-2pm - LSK 121 []\n  * Dimensionality reduction\n  * Principal component analysis\n  * Visualizing the digits dataset\n  * Linear algebra behind principal component analysis\n* Week 4 - Friday October 28 - 1-2pm - LSK 121 []\n  * PCA revisted\n    * Visualizing principal components\n  * Unsupervised learning\n    * Clustering with K-means\n    * Digits dataset: How many different kinds of 1s are there?\n    * Combining KMeans with PCA\n* Week 5 - Friday November 4 - 1-2pm - LSK 121 []\n  * Kernel density estimation and Gaussian processes - Presented by \n* Remembrance Day - No meeting November 11\n* Week 6 - Friday November 18 - 1-2pm - UCLL 109\n  * Natural Language Processing with nltk: Movie Review Classification - Presented by \n* Week 7 - Friday November 25 - 1-2pm - UCLL 109 []\n  * Natural Language Processing with nltk: Movie Review Classification (Continued)\n    * Working with nltk movie review dataset\n    * Using regular expressions to remove punctuation and stopwords\n    * Creating feature vectors from movie reviews\n    * Applying a Naive Bayes classifier"""
https://github.com/rtlee9/SIC-list,List of SIC codes and descriptions from authoritative sources,"b""# SIC codes for download -- open source edition\n\n\n\n\n\n\n\nThis repo provides lists of four-digit SIC codes scraped from the websites of two government agencies: the  and . The cleaned lists can be downloaded  and , respectively, and refresh instructions can be found below.\n\n## Background\nThe Standard Industrial Classification (SIC) is a system used to classify businesses by their primary business activity, or industry. The SIC system was created in the 1930's and has since been  as the industry classification system for Federal statistical agencies; however, it is still widely used by many businesses and by some government agencies.\n\n## Authoritative sources\n\nSIC codes were once maintained and assigned by the US government. I've found that only two government agencies currently publish a list of SIC codes and descriptions:\n\n| Source | Version | Use case |\n| ------ | ------- | -------- |\n|  | 1987 SIC manual | Unknown |\n|  | No version provided, but the SEC website indicates the webpage was last modified January 25, 2015 | Used in  electronic filings |\n\nThe SIC codes provided by the SEC generally align with those provided by OSHA; however, OSHA's SIC manual is more comprehensive -- it contains many more SIC codes than does the SEC's list.\n\n## Other sources\n\nThere are a number of online sources that provide SIC codes and descriptions, though I've found none that provide all of the following:\n* The source of their data\n* Their code, if relevant\n* Machine readable data\n\nTaken together, these are important for assessing data quality and reliability. The purpose of this repository is to provide SIC codes in adherence with these standards.\n\n## Usage\n\nThe latest data can be found in the root directory. To refresh:\n\n1. Install Python 2.7\n1. Install python requirements: \n1. From the command line run \n\n## License\n\n"""
https://github.com/t-lanigan/CarND-LaneLines-P1,Finding lanes lines on the road in Video Streaming,"b'#Finding Lane Lines on the Road \n\n\nWhen we drive, we use our eyes to decide where to go.  The lines on the road that show us where the lanes are act as our constant reference for where to steer the vehicle.  Naturally, one of the first things we would like to do in developing a self-driving car is to automatically detect lane lines using an algorithm.\n\nIn this project you will detect lane lines in images using Python and OpenCV.  OpenCV means ""Open-Source Computer Vision"", which is a package that has many useful tools for analyzing images.  \n\nStep 1: Getting setup with Python\n\nTo do this project, you will need Python 3 along with the numpy, matplotlib, and OpenCV libraries, as well as Jupyter Notebook installed. \n\nWe recommend downloading and installing the Anaconda Python 3 distribution from Continuum Analytics because it comes prepackaged with many of the Python dependencies you will need for this and future projects, makes it easy to install OpenCV, and includes Jupyter Notebook.  Beyond that, it is one of the most common Python distributions used in data analytics and machine learning, so a great choice if youre getting started in the field.\n\nChoose the appropriate Python 3 Anaconda install package for your operating system here.   Download and install the package.\n\nIf you already have Anaconda for Python 2 installed, you can create a separate environment for Python 3 and all the appropriate dependencies with the following command:\n\n\n\n\n\nStep 2: Installing OpenCV\n\nOnce you have Anaconda installed, first double check you are in your Python 3 environment:\n\n    \n  \n  \n  \n   \n(Ctrl-d to exit Python)\n\nrun the following commands at the terminal prompt to get OpenCV:\n\n  \n\n\nthen to test if OpenCV is installed correctly:\n\n  \n  \n  \n(Ctrl-d to exit Python)\n\nStep 3: Installing moviepy  \n\nWe recommend the ""moviepy"" package for processing video in this project (though youre welcome to use other packages if you prefer).  \n\nTo install moviepy run:\n\n  \n\nand check that the install worked:\n\n  \n  \n  \n(Ctrl-d to exit Python)\n\nStep 4: Opening the code in a Jupyter Notebook\n\nYou will complete this project in a Jupyter notebook.  If you are unfamiliar with Jupyter Notebooks, check out Cyrille Rossants Basics of Jupyter Notebook and Python to get started.\n\nJupyter is an ipython notebook where you can run blocks of code and see results interactively.  All the code for this project is contained in a Jupyter notebook. To start Jupyter in your browser, run the following command at the terminal prompt (be sure youre in your Python 3 environment!):\n\n\n\nA browser window will appear showing the contents of the current directory.  Click on the file called ""P1.ipynb"".  Another browser window will appear displaying the notebook.  Follow the instructions in the notebook to complete the project.  \n'"
https://github.com/jimod/deeplearning-meetup-dublin,Repository for any code for the deep learning meetups,b'# deeplearning-meetup-dublin\nRepository for any code for the deep learning meetups currently contains \n\n# Meetup @ Intercom\n\n## Intro to Neural Nets -- from shallow to deep \n\n\n\n\n'
https://github.com/sofianhw/spark-sql,data ingestion with spark,b'# spark-sql\ndata ingestion with spark\n\nWill update soon\n\nhttp://spark.apache.org/docs/latest/\n'
https://github.com/JonathanReeve/dataviz-workshop,"Materials for a workshop in text analysis and visualization, originally given at Columbia University in April 2016. ","b'# dataviz-workshop\nMaterials for a workshop in text analysis and visualization, originally given at Columbia University in April 2016. The \n'"
https://github.com/srikanth261/sentiment140,Twitter sentiment analysis,b'# sentiment140\nTwitter sentiment analysis\n'
https://github.com/xn8812/caffe,for testing,"b'# Caffe\n\nCaffe is a deep learning framework made with expression, speed, and modularity in mind.\nIt is developed by the Berkeley Vision and Learning Center () and community contributors.\n\nCheck out the  for all the details like\n\n- \n- \n-  and the \n- \n\nand step-by-step examples.\n\n\n\nPlease join the  or  to ask questions and talk about methods and models.\nFramework development discussions and thorough bug reports are collected on .\n\nHappy brewing!\n\n## License and Citation\n\nCaffe is released under the .\nThe BVLC reference models are released for unrestricted use.\n\nPlease cite Caffe in your publications if it helps your research:\n\n    @article{jia2014caffe,\n      Author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},\n      Journal = {arXiv preprint arXiv:1408.5093},\n      Title = {Caffe: Convolutional Architecture for Fast Feature Embedding},\n      Year = {2014}\n    }\n'"
https://github.com/nkern/Astro_9,"Python Programming in Astronomy, Summer 2017",b'### Course Repository for Astro 9\n\nSee the course webpage for more details.\n'
https://github.com/andrew-reece/cs171-final-project,Evolution of human social networks (MIT Reality Mining data),"b'This link contains our entire project, including screencast and process book.\n\nhttp://people.fas.harvard.edu/~reece/171/seenet/'"
https://github.com/pottava/mxnet-char-lstm,A multilayer LSTM network with MXNet.,b'# \xe8\xa4\x87\xe6\x95\xb0\xe3\x83\xac\xe3\x82\xa4\xe3\x83\xa4 LSTM \xe3\x83\x8d\xe3\x83\x83\xe3\x83\x88\xe3\x83\xaf\xe3\x83\xbc\xe3\x82\xaf\xef\xbc\x88MXNet \xe5\xae\x9f\xe8\xa3\x85\xef\xbc\x89\n\n## \xe3\x83\x81\xe3\x83\xa5\xe3\x83\xbc\xe3\x83\x88\xe3\x83\xaa\xe3\x82\xa2\xe3\x83\xab\n\n\xe3\x81\xbe\xe3\x81\x9a\xe3\x81\xaf\xe6\x89\x8b\xe8\xa8\xb1\xe3\x81\xae\xe7\x92\xb0\xe5\xa2\x83\xe3\x81\xa7\xe3\x81\xa9\xe3\x82\x93\xe3\x81\xaa\xe5\xad\xa6\xe7\xbf\x92\xe3\x83\xbb\xe6\x8e\xa8\xe8\xab\x96\xe3\x81\x8c\xe3\x81\xa7\xe3\x81\x8d\xe3\x82\x8b\xe3\x81\xae\xe3\x81\x8b\xe3\x82\x92\xe8\xa9\xa6\xe3\x81\x97\xe3\x81\xa6\xe3\x81\xbf\xe3\x81\xbe\xe3\x81\x97\xe3\x82\x87\xe3\x81\x86\xe3\x80\x82  \n\n### 1. docker-compose \xe3\x81\xae\xe3\x82\xa4\xe3\x83\xb3\xe3\x82\xb9\xe3\x83\x88\xe3\x83\xbc\xe3\x83\xab\n\n\xe4\xbb\xa5\xe4\xb8\x8b\xe3\x81\xae\xe3\x83\xaa\xe3\x83\xb3\xe3\x82\xaf\xe3\x81\x8b\xe3\x82\x89\xe3\x80\x81Docker \xe3\x82\x92\xe3\x82\xa4\xe3\x83\xb3\xe3\x82\xb9\xe3\x83\x88\xe3\x83\xbc\xe3\x83\xab\xe3\x81\x97\xe3\x81\xa6\xe3\x81\x8f\xe3\x81\xa0\xe3\x81\x95\xe3\x81\x84\xe3\x80\x82\n\n- \n- \n\n\xe3\x82\xbf\xe3\x83\xbc\xe3\x83\x9f\xe3\x83\x8a\xe3\x83\xab\xe3\x81\xa7\xe3\x80\x81docker-compose \xe3\x82\x82\xe5\x90\x8c\xe6\x99\x82\xe3\x81\xab\xe3\x82\xa4\xe3\x83\xb3\xe3\x82\xb9\xe3\x83\x88\xe3\x83\xbc\xe3\x83\xab\xe3\x81\x95\xe3\x82\x8c\xe3\x81\x9f\xe3\x81\x93\xe3\x81\xa8\xe3\x82\x92\xe7\xa2\xba\xe8\xaa\x8d\xe3\x81\x97\xe3\x81\xbe\xe3\x81\x99\xe3\x80\x82\n\n\n\n### 2. docker-compose \xe3\x81\xa7 Jupyter \xe3\x82\x92\xe8\xb5\xb7\xe5\x8b\x95\n\n\xe4\xbb\xa5\xe4\xb8\x8b\xe3\x81\xae\xe3\x82\xb3\xe3\x83\x9e\xe3\x83\xb3\xe3\x83\x89\xe3\x81\xa7 Jupyter \xe3\x82\x92\xe8\xb5\xb7\xe5\x8b\x95\xe3\x81\x97\xe3\x81\xbe\xe3\x81\x99\n\n\n\n### 3. \xe3\x83\x96\xe3\x83\xa9\xe3\x82\xa6\xe3\x82\xb6\xe3\x81\xa7 Juputer \xe3\x82\x92\xe9\x96\x8b\xe3\x81\x8d\xe3\x81\xbe\xe3\x81\x99\n\n\n\n\xe3\x83\x88\xe3\x83\xbc\xe3\x82\xaf\xe3\x83\xb3\xe3\x82\x92\xe8\xa6\x81\xe6\xb1\x82\xe3\x81\x95\xe3\x82\x8c\xe3\x82\x8b\xe3\x81\xae\xe3\x81\xa7\xe3\x80\x81\xe3\x82\xbf\xe3\x83\xbc\xe3\x83\x9f\xe3\x83\x8a\xe3\x83\xab\xe3\x81\xab\xe8\xa1\xa8\xe7\xa4\xba\xe3\x81\x95\xe3\x82\x8c\xe3\x81\xa6\xe3\x81\x84\xe3\x82\x8b\xe3\x83\x88\xe3\x83\xbc\xe3\x82\xaf\xe3\x83\xb3\xe3\x82\x92\xe5\x85\xa5\xe5\x8a\x9b\xe3\x81\x97\xe3\x81\xbe\xe3\x81\x99\xe3\x80\x82  \n \xe3\x82\x92\xe3\x82\xaf\xe3\x83\xaa\xe3\x83\x83\xe3\x82\xaf\xe3\x81\x97\xe3\x81\xa6\xe9\x96\x8b\xe3\x81\x8d\xe3\x80\x81\xe3\x83\x81\xe3\x83\xa5\xe3\x83\xbc\xe3\x83\x88\xe3\x83\xaa\xe3\x82\xa2\xe3\x83\xab\xe3\x82\x92\xe9\x96\x8b\xe5\xa7\x8b\xe3\x81\x97\xe3\x81\xbe\xe3\x81\x99\xe3\x80\x82\n\n### 4. \xe7\x92\xb0\xe5\xa2\x83\xe3\x81\xae\xe7\xa0\xb4\xe6\xa3\x84\n\n\xe3\x83\x81\xe3\x83\xa5\xe3\x83\xbc\xe3\x83\x88\xe3\x83\xaa\xe3\x82\xa2\xe3\x83\xab\xe3\x81\x8c\xe7\xb5\x82\xe3\x82\x8f\xe3\x81\xa3\xe3\x81\x9f\xe3\x82\x89\xe3\x80\x81\xe3\x82\xbf\xe3\x83\xbc\xe3\x83\x9f\xe3\x83\x8a\xe3\x83\xab\xe3\x81\xab\xe3\x82\x82\xe3\x81\xa9\xe3\x82\x8a  \xe3\x81\xa7\xe3\x83\xad\xe3\x82\xb0\xe7\x9b\xa3\xe8\xa6\x96\xe3\x81\x8b\xe3\x82\x89\xe6\x8a\x9c\xe3\x81\x91  \n\xe4\xbb\xa5\xe4\xb8\x8b\xe3\x81\xae\xe3\x82\xb3\xe3\x83\x9e\xe3\x83\xb3\xe3\x83\x89\xe3\x81\xa7\xe5\xae\x8c\xe5\x85\xa8\xe3\x81\xab Jupyter \xe3\x82\x92\xe5\x81\x9c\xe6\xad\xa2\xe3\x81\x97\xe3\x81\xbe\xe3\x81\x97\xe3\x82\x87\xe3\x81\x86\xe3\x80\x82\n\n\n'
https://github.com/SujathaSubramanian/Machine-Learning--UW,Coursework ML Specialization,b'\n### Machine Learning Specialization\n\nThese are course related working and projects for the 6 course specialization on Machine Learning\nfrom University of Washigton\n\n\n'
https://github.com/harshadss/my-presentations,"Contains codes, presentation files for all the workshops, presentations that I conduct",b'# Presentations\n\nContains code files and presentation files for all the workshops and \npresentations that I conduct.\n'
https://github.com/reubano/lambdaconf-tutorial,"LambdaConf ""A Functional Programming Approach To Data Processing In Python"" tutorial materials ","b'\n\n# lambdaconf-tutorial\n\nMaterials for the LambdaConf tutorial, ""A Functional Programming approach to data  processing in Python"".\nPresentation slides can be found at https://speakerdeck.com/reubano/a-functional-programming-approach-to-data-processing-in-python\n\n## Preparation\n\nThis is an interactive (hands-on) tutorial. As such, you can choose to follow along  on your laptop, or  in a .\n\n### Local\n\nClone this repo\n\n\n\nMake sure you have a recent version of \n\n\n\n(Optional) Setup and activate a \n\n\n\n(Optional) install , an enhanced Python shell\n\n\n\nInstall the packages to be used during the workshop\n\n\n\nStart the interactive Python\n\n\n\nYou should now be in an interactive shell that looks something like this:\n\n\n\nIn this interactive shell, you can enter any valid Python and immediately see the result.\n\n\n\nTo play around with the code, view the following files in your text editor of your choice.\n\n- : use to follow along as I talk, and reference during the exercises\n- : test your own code by typing  in the terminal (NOT a python shell)\n- : see how the solution should look by typing  in the terminal (NOT a python shell)\n\n### Remote\n\nTo play around with the code,  and select the appropriate notebook.\n\n- Presentation (): use to follow along as I talk, and reference during the exercises\n- Exercises (): test your own code by typing directly into your browser\n- Solutions (): interact with the solution and view the intermediate results\n\n## A few tips\n\n- if you are new to Python, browse through the \n- if you are new to meza, checkout the \n'"
https://github.com/jacobeisenstein/gt-nlp-class,"Course materials for Georgia Tech CS 4650 and 7650, ""Natural Language""","b'CS 4650 and 7650\n==========\n\n(Note about registration: registration is currently restricted to students pursuing CS degrees for which this course is an essential requirement. Unfortunately, the enrollment is already at the limit of the classroom space, so this restriction is unlikely to be lifted.)\n\n- Course: Natural Language Understanding\n- Instructor: Jacob Eisenstein\n- Semester: Spring 2018\n- Time: Mondays and Wednesdays, 3:00-4:15pm\n- TAs: Murali Raghu Babu, James Mullenbach, Yuval Pinter, Zhewei Sun\n- \n-  from previous classes\n\nThis course gives an overview of modern data-driven techniques for natural language processing. The course moves from shallow bag-of-words models to richer structural representations of how words interact to create meaning. At each level, we will discuss the salient linguistic phemonena and most successful computational models. Along the way we will cover machine learning techniques which\nare especially relevant to natural language processing.\n\n- \n- \n- \n- \n\n# Learning goals\n\n\n- Acquire the fundamental linguistic concepts that are relevant to language technology. This goal will be assessed in the short homework assignments and the exams.\n- Analyze and understand state-of-the-art algorithms and statistical techniques for reasoning about linguistic data. This goal will be assessed in the exams and the assigned projects.\n- Implement state-of-the-art algorithms and statistical techniques for reasoning about linguistic data. This goal will be assessed in the assigned projects.\n- Adapt and apply state-of-the-art language technology to new problems and settings. This goal will be assessed in assigned projects.\n- (7650 only) Read and understand current research on natural language processing. This goal will be assessed in assigned projects.\n\n# Readings #\n\n\nReadings will be drawn mainly from my . Additional readings may be assigned from published papers, blogposts, and tutorials.\n\n## Supplemental textbooks ##\n\nThese are completely optional, but might deepen your understanding of the material.\n\n-  is the textbook most often used in NLP courses. Its a great reference for both the linguistics and algorithms well encounter in this course. Several chapters from the upcoming  are free online.\n- \nshows how to do hands-on work with Pythons Natural Language Toolkit (NLTK), and also brings a strong linguistic perspective.\n-  can help you review the probability and statistics that we use in this course.\n\n# Grading\n\n\nThe graded material for the course will consist of:\n\n- Seven short homework assignments, of which you must do six. Most of these involve performing linguistic annotation on some text of your choice. The purpose is to get a basic understanding of key linguistic concepts. Each assignment should take less than an hour. Each homework is worth 2 points (12 total). (Many of these homeworks are implemented at quizzes on Canvas.)\n- Four assigned problem sets. These involve building and using NLP techniques which are at or near the state-of-the-art. The purpose is to learn how to implement natural language processing software, and to have fun. These assignments must be done individually. Each problem set is worth ten points (48 total). Students enrolled in CS 7650 will have an additional, research-oriented component to the problem sets.\n- An in-class midterm exam, worth 20 points, and a final exam, worth 20 points. The purpose of these exams is to assess understanding of the core theoretical concepts, and to encourage you to review and synthesize your understanding of these concepts. \n\nBarring a personal emergency or an institute-approved absence, you must take each exam on the day indicated in the schedule. Job interviews and travel plans are generally not a reason for an institute-approved absence. See  for more information on GT policy about absences.\n\n## Late policy\n\nProblem sets will be accepted up to 72 hours late, at a penalty of 2 points per 24 hours. (Maximum score after missing the deadline: 10/12; maximum score 24 hours after the deadline: 8/12, etc.)  It is usually best just to turn in what you have at the due date. Late homeworks will not be accepted. This late policy is intended to ensure fair and timely evaluation.\n\n# Getting help\n\n\n## Office hours\n\nMy office hours follow Wednesday classes (4:15-5:15PM) and take place in class when available.\n\nTA office hours are in CCB commons (1st floor) unless otherwise announced on Piazza.\n- Murali: Friday   10AM-11AM\n- James:  Thursday 11AM-12PM\n- Yuval:  Tuesday  3PM-4PM\n- Zhewei: Monday   1PM-2PM\n\n## Online help\n\nPlease use Piazza rather than personal email to ask questions. This helps other students, who may have the same question. Personal emails may not be answered. If you cannot make it to office hours, please use Piazza to make an appointment. It is unlikely that I will be able to chat if you make an unscheduled visit to my office. The same is true for the TAs.\n\n# Class policies\n\n\nAttendance will not be taken, but you are responsible for knowing what happens in every class. If you cannot attend class, make sure you check up with someone who was there.\n\nRespect your classmates and your instructor by preventing distractions. This means be on time, turn off your cellphone, and save side conversations for after class. If you cant read something I wrote on the board, or if you think I made a mistake in a derivation, please raise your hand and tell me!\n\nUsing a laptop in class is likely to reduce your education attainment. This has been documented by multiple studies, which are nicely summarized in the following article:\n\n- https://www.nytimes.com/2017/11/22/business/laptops-not-during-lecture-or-meeting.html\n\nI am not going to ban laptops, as long as they are not a distraction to anyone but the user. But I suggest you try pen and paper for a few weeks, and see if it helps.\n\n## Prerequisites\n\n\nThe official prerequisite for CS 4650 is CS 3510/3511, ""Design and Analysis of Algorithms."" This prerequisite is essential because understanding natural language processing algorithms requires familiarity with dynamic programming, as well as automata and formal language theory: finite-state and context-free languages, NP-completeness, etc. While course prerequisites are not enforced for graduate students, prior exposure to analysis of algorithms is very strongly recommended.\n\nFurthermore, this course assumes:\n\n- Good coding ability, corresponding to at least a third or fourth-year undergraduate CS major. Assignments will be in Python.\n- Background in basic probability, linear algebra, and calculus.\n\nPeople sometimes want to take the course without having all of these\nprerequisites. Frequent cases are:\n\n- Junior CS students with strong programming skills but limited theoretical and mathematical background,\n- Non-CS students with strong mathematical background but limited programming experience.\n\nStudents in the first group suffer in the exam and dont understand the lectures, and students in the second group suffer in the problem sets. My advice is to get the background material first, and\nthen take this course.\n\n## Collaboration policy\n\nOne of the goals of the assigned work is to assess your individual progress in meeting the learning objectives of the course. You may discuss the homework and projects with other students, but your work must be your own -- particularly all coding and writing. For example:\n\n### Examples of acceptable collaboration\n\n- Alice and Bob discuss alternatives for storing large, sparse vectors of feature counts, as required by a problem set.\n- Bob is confused about how to implement the Viterbi algorithm, and asks Alice for a conceptual description of her strategy.\n- Alice asks Bob if he encountered a failure condition at a ""sanity check"" in a coding assignment, and Bob explains at a conceptual level how he overcame that failure condition.\n- Alice is having trouble getting adequate performance from her part-of-speech tagger. She finds a blog page or research paper that gives her some new ideas, which she implements.\n\n### Examples of unacceptable collaboration\n\n- Alice and Bob work together to write code for storing feature counts.\n- Alice and Bob divide the assignment into parts, and each write the code for their part, and then share their solutions with each other to complete the assignment.\n- Alice or Bob obtain a solution to a previous years assignment or to a related assignment in another class, and use it as the starting point for their own solutions.\n- Bob is having trouble getting adequate performance from his part-of-speech tagger. He finds source code online, and copies it into his own submission.\n- Alice wants to win the Kaggle competition for a problem set. She finds the test set online, and customizes her submission to do well on it.\n\nSome assignments will involve written responses. Using other people\xe2\x80\x99s text or figures without attribution is plagiarism, and is never acceptable.\n\nSuspected cases of academic misconduct will be (and have been!) referred to the Honor Advisory Council. For any questions involving these or any other Academic Honor Code issues, please consult me, my teaching assistants, or http://www.honor.gatech.edu.\n'"
https://github.com/psi4/psi4,Open-Source Quantum Chemistry – an electronic structure package in C++ driven by Python,"b'# \n\n| Status |   |\n| :------ | :------- |\n| Latest Release |     |\n| Communication |     |\n| Foundation |    |\n| Installation |    |\n| Demo |  |\n\n<!--  -->\n<!--  -->\n<!--  -->\n\n<!-- \n  -->\n\n<!-- \n -->\n\n<!--\n| PR Activity | \n \n\n-->\n\nPsi4 is an open-source suite of ab initio quantum chemistry programs\ndesigned for efficient, high-accuracy simulations of\nmolecular properties. We routinely perform computations with >2500 basis functions on multi-core machines.\n\nWith computationally demanding portions written in C++, exports\nof many C++ classes into Python via Pybind11, and a flexible Python driver, Psi4\nstrives to be friendly to both users and developers.\n\n* Users  www.psicode.org\n\n* Downloading and Installing Psi4 https://psicode.org/psi4manual/master/build_faq.html (for the CMake adept, see \n\n* Manual   (built nightly from master branch) or https://psicode.org/psi4manual/1.4.0/index.html (last release)\n\n* Tutorial https://psicode.org/psi4manual/master/tutorial.html for Psithon (), https://psicode.org/psi4manual/master/psiapi.html for PsiAPI ()\n\n* Forum http://forum.psicode.org\n\n* Communication & Support https://psicode.org/psi4manual/master/introduction.html#technical-support\n\n* GitHub  https://github.com/psi4/psi4 (authoritative repository)\n\n* Continuous Integration Status  on Linux and Windows\n\n* Anaconda  https://anaconda.org/psi4 (binary available for Linux, Mac, Mac Silicon, Windows, and WSL Windows  )  ) \n\n* Coverage Python and C++ source code lines hit by running most of the test suite. \n\n* Interested Developers  https://psicode.org/developers.php (replacement page needed) (welcome to fork psi4/psi4 and follow ) \n\n* Sample Inputs  http://www.psicode.org/psi4manual/master/testsuite.html (also in )\n\n* Download Tarball https://github.com/psi4/psi4/releases \n\n\n\n\nLicense \n=======\n\nPsi4: an open-source quantum chemistry software package\n\nCopyright (c) 2007-2023 The Psi4 Developers.\n\nThe copyrights for code used from other parties are included in\nthe corresponding files.\n\nPsi4 is free software; you can redistribute it and/or modify\nit under the terms of the GNU Lesser General Public License as published by\nthe Free Software Foundation, version 3.\n\nPsi4 is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU Lesser General Public License for more details.\n\nYou should have received a copy of the GNU Lesser General Public License along\nwith Psi4; if not, write to the Free Software Foundation, Inc.,\n51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n\nThe full text of the GNU Lesser General Public License (version 3) is included in the\nCOPYING.LESSER file of this repository, and can also be found\n.\n\n\nCitation \n========\n\nThe journal article reference describing Psi4 is:\n\nD. G. A. Smith, L. A. Burns, A. C. Simmonett, R. M. Parrish,\nM. C. Schieber, R. Galvelis, P. Kraus, H. Kruse, R. Di Remigio,\nA. Alenaizan, A. M. James, S. Lehtola, J. P. Misiewicz, M. Scheurer,\nR. A. Shaw, J. B. Schriber, Y. Xie, Z. L. Glick, D. A. Sirianni,\nJ. S. OBrien, J. M. Waldrop, A. Kumar, E. G. Hohenstein,\nB. P. Pritchard, B. R. Brooks, H. F. Schaefer III, A. Yu. Sokolov,\nK. Patkowski, A. E. DePrince III, U. Bozkaya, R. A. King,\nF. A. Evangelista, J. M. Turney, T. D. Crawford, C. D. Sherrill,\n""Psi4 1.4: Open-Source Software for High-Throughput Quantum Chemistry"",\nJ. Chem. Phys. 152(18) 184108 (2020).\n\n*  for Psi4 v1.1\n*  for Psi4NumPy\n*  for Psi4 alpha releases\n*  for Psi3\n'"
https://github.com/sanghaisubham/Tic-Tac-Toe-Game,A Common Tic Tac Toe Game Played Between the User and the System with the System Starting the game,b'# Tic-Tac-Toe-Game\nA Common Tic Tac Toe Game Played Between the User and the System with the System Starting the game\n'
https://github.com/Cyberdog52/Project,UIE,"b""\nBefore you start with the project:\n\n1. \tDownload validation, test and train sets and put them in this directory\n2. \tRun tf_record_to_numpy 3 times for each set (train, test, validation)\n\tThis will create pkl files in the same directory\n\tYou can delete the tf_record files now, don't need them anymore\n3.\tMerge the 18 validation pkl files into train folder. Rename them by hand as if they were train files (from 59 to 76)\n\tThere should now be 76 dataTrain pkl files in the train directory\n\tYou can delete the validation folder now, don't need them anymore\n4. \tRun produce_masked_inputs.py twice for train and test\n\tYou can delete dataTrain and dataTest, from now use newTrain and newTest\n\n\nImportant things:\n\nSome videos are shorter than 50 frames, interpolate them to 50\nIt is better to delete the first and last images of a video than keeping all\nSome segmentation images are blank, do not segment if they are blank\n"""
https://github.com/mingot/lung_cancer_ds_bowl,Data Science Bowl 2017 for lung cancer prediction with Keras,"b'# Lung Cancer Data Science Bowl 2017\n\n## Introduction\nRepository for the Vila del Pingui team for the Data Science Bowl 2017 (Feb2017 to Apr2017). The competetition ($1M in prizes) was about predicting early stage lung cancer from CT Scan images. The training set was 1397 + 200 patients and the test 500 patients. The result is an ensemble of 3 convolutional neural networks (resnet) for feature generation and xgboost for final ensemble.  \n\nThe team ended in 34th position of 2000 teams (top 2%) with the best model scoring in the 17th position.\n\n## Index\nAccess to latest results of each team and to documentation\n\n  1. Preprocessing and datasets (README TBD)\n    1.  \n    2. \n    3. Preprocessed v3 (AWS): /mnt/hd2/preprocessed3 \n  2. DL ()\n    1. Slices: TBD\n    2. Segmentation: TBD\n  3. Final model (README TBD)\n    1. New features: TBD\n    2. XGBoost: TBD\n    3. Final learner - submission: TBD\n  4. Literature: \n    1. Preprocessing ()\n    2. DL ()\n    3. Features ()\n\n## References quick start\nBasic references to understand the problem and the data:\n\n 1. [Video] (https://www.youtube.com/watch?v=-XUKq3B4sdw) how to detect a lung cancer from a physician perspective (15 min).\n 2. Notebooks (Kaggle Kernels) Understand the data set and dealing with DICOM files. \n   1. : understanding DICOM files, pixel values, standarization, ...\n   2.  : basic exploration of the given data set\n 3. [Kaggle tutorial] (https://www.kaggle.com/c/data-science-bowl-2017/details/tutorial) with code for training a CNN using the U-net network for medical image segmentation. Based on the external LUNA data set (annotated).\n 4. [TensorFlow ppt] (https://docs.google.com/presentation/d/1TVixw6ItiZ8igjp6U17tcgoFrLSaHWQmMOwjlgQY9co/edit#slide=id.p) for quickstart (focused on convnets) and code included. After it, you can take the  as the sample code.\n\n## Quickstart\n [TBD]\n 1 - Download the repo:\n \n 2 - Create  (see ) and install python requirements\n\n\n## Jupyter\n - Estan ja instalats els paquets de  amb el kernel de python2.\n - Cada usuari pot fer git pull/commit/push desde un ssh o amb  .. desde la consola de jupyter. No demana contrasenya, el usuari queda identificat amb el email\n - Cada usuari t\xc3\xa9 el seu directori  privat per ell excepte la carpeta  que es compartida per tots.\n - Tots els usuaris tenen perm\xc3\xads de sudo aix\xc3\xad que si cal instalar paquets poden fer servir  paquet desde jupyter i aix\xc3\xad ser\xc3\xa0n accesibles per tots.\n\n## Available datasets\nSee docs/ \n\n## Preprocessing\nThe preprocessed images are stored at . To open the compressed files from python use the following instruction:\n. There is one file per patient. Eah file is a numpy array of 4 dimensions: . The dimension  contains the preprocessed image at index 0, the lung segmentation at index 1, and when available (luna dataset) the nodules segmentation at index 2. All the images have dimensions  and  dimensions of 512x512.\n\n## General guidelines\n - The analysis files should start with the author initials.\n - Avoid storing files >50Mb in Git. In particular, images from data folder should be outside the git repository.\n\n## File structure\n\n\n\n\n## Troubleshoot\n\n### ""Could not find a version that satisfies the requirement SimpleITK==0.10.0""\n\nThe solution is to manually download the egg from the  and install it with .\n\n### ""Fatal Python error: PyThreadState_Get: no current thread""\n\nThe solution is to relink the :\n\n'"
https://github.com/NYC-OPCCR/2014-07-15,DREAM about solving AML,b'2014-07-15\n==========\n\nDREAM about solving AML\n'
https://github.com/rodgzilla/fractal_GAN,A deep learning project where the goal is to train a generator to produce plausible fractal images.,b'# fractal_GAN\nA deep learning project where the goal is to train a generator to produce plausible fractal images.\n'
https://github.com/akalathil/Eng100D_Project,This project is our Eng100D project for onestep,b'# GET REQUESTS #\n\nList out types of affliction\nhttps://eng100d-project.herokuapp.com/list\n  >>\n  \nList all afflictions\nhttps://eng100d-project.herokuapp.com/list/all\n  >>\n  \nList out afflictions with underneath type of affliction\nhttps://eng100d-project.herokuapp.com/list/{type of affliction}\n\n  ex. https://eng100d-project.herokuapp.com/list/disease\n  >>\n  \nSee Full Affliction Profile\nhttps://eng100d-project.herokuapp.com/list/{type of affliction}/{affliction}\n\n  ex. https://eng100d-project.herokuapp.com/list/disease/HIV%2FAIDS\n  >>\n  \nGet info of affliction\nhttps://eng100d-project.herokuapp.com/info/{type of affliction}/{affliction}\n\n  ex.  https://eng100d-project.herokuapp.com/info/disease/HIV%2FAIDS\n>>\n\n\nGet both rows and cols of affliction\nhttps://eng100d-project.herokuapp.com/data/{type of affliction}/{affliction}\n\n  ex. https://eng100d-project.herokuapp.com/data/disease/HIV%2FAIDS\n>>\n\n\nGet cols of affliction\nhttps://eng100d-project.herokuapp.com/data/{type of affliction}/{affliction}/cols\n\n  ex. https://eng100d-project.herokuapp.com/data/disease/HIV%2FAIDS/cols\n  >>\n  \n\nGet rows of affliction\nhttps://eng100d-project.herokuapp.com/data/{type of affliction}/{affliction}/rows\n  \n  ex. https://eng100d-project.herokuapp.com/data/disease/HIV%2FAIDS/rows\n  >>\n\n\n# POST REQUEST #\nEdit Column Labels\nhttps://eng100d-project.herokuapp.com/edit/cols/{type of affliction}/{affliction}\n>> Send JSON in this format:\n\n\nEdit Rows of an Affliction\nhttps://eng100d-project.herokuapp.com/edit/rows/{type of affliction}/{affliction}\n>> Send JSON in this format:\n\n\nEdit Info of Affliction\nhttps://eng100d-project.herokuapp.com/edit/info/{type of affliction}/{affliction}\n>> Send JSON in this format:\n\n\n\nEdit the type of an Affliction\nhttps://eng100d-project.herokuapp.com/edit/type/{type of affliction}/{affliction}\n>> Send JSON in this format:\n\n\nAdd Row to the Table of an Affliction\nhttps://eng100d-project.herokuapp.com/add/row/{type of affliction}/{affliction}\n>> Send JSON in this format:\n\nedit Data field\nhttps://eng100d-project.herokuapp.com/edit/data{type of affliction}/{affliction}\n>> Send JSON in this format:\n\nAdd a new affliction\nhttps://eng100d-project.herokuapp.com/add/affliction/{type of affliction}\n>> Send JSON in this format:\n\n\nDelete Aflliction\nhttps://eng100d-project.herokuapp.com/delete/affliction/{type of affliction}\n>> Send JSON in this format:\n\n'
https://github.com/beckernick/wikipedia_pageviews,The Election's Effect on Candidate's Wikipedia Page Views,"b""# wikipedia_pageviews\nThe Election's Effect on Candidates's Wikipedia Page Views\n"""
https://github.com/darshan-b/Machine-Learning-Regression,"Concepts implemented from scratch include: Gradient Descent, Lasso Regression and Ridge Regression","b'# Machine-Learning-Regression\nConcepts implemented from scratch include: Gradient Descent, Lasso Regression and Ridge Regression\n'"
https://github.com/UCRclyman/P177_Homework04,Homework 04,b'# P177_Homework04\nHomework 04\n'
https://github.com/parantapag/IBD4Health2017,Lecture materials for IBD4Health 2017 summer school,b'# IBD4Health2017\nLecture materials for IBD4Health 2017 summer school\n'
https://github.com/Clique-CS109/project,The main repository for Team Clique CS109 project,"b""# Read me - Flowchart\n\nTo fully enjoy this repository, we recommend you to read files in the following order:\n\n 1.  - this file\n 2.  - a summary of our method and result\n 3.  - data exploration and refinement\n \n\t##### The following three notebooks use files in  folder.\n 4.  - codes for baseline method\n 5.  - codes for kNN method with book similarity\n 6.   - codes for kNN method with user similarity\n\t- In order to run this notebook (faster) you need files in  folder.\n\nAlso, please visit our cool ! Don't forget to watch our cool  too."""
https://github.com/mwaskom/nipype_concepts,Tutorial notebooks for Nipype,b'Nipype Concepts\n===============\n\nThis collection of \nnotebooks should provide an introduction to some of the \nmain concepts central to using \nfor neuroimaging analysis. \n\nStatic HTML Links\n-----------------\n\n- \n- \n- \n\nRequirements\n------------\n\n- \n- \n- \n-  (Optional)\n\nLicense\n-------\n\nSimplified BSD\n'
https://github.com/toshikurauchi/eyeswipe2-analysis,Results and analysis from eyeswipe2 experiment,b'# eyeswipe2-analysis\nResults and analysis from eyeswipe2 experiment\n'
https://github.com/miroli/ML-training,Machine learning training with Jupyter,b'##Machine learning training\n\nJupyter notebooks for teaching myself machine learning.\n'
https://github.com/antingithub/JupyterWorkFlow,Jupyter Work Flow Example,b'# JupyterWorkFlow\nJupyter Work Flow Example\n'
https://github.com/AlexandreCaron/12_steps_to_NS,CFD Python: 12 steps to Navier-Stokes,b'This repository contains source code and files (text/images) regarding my progress in the CFD Python: 12 steps to Navier-Stokes blog by Lorena A. Barba.\n\nURL of blog is : http://lorenabarba.com/blog/cfd-python-12-steps-to-navier-stokes/\n'
https://github.com/0xLiso/Introduccion_ML,Una pequeña introduccion al Machine Learning,b'# Introduccion_ML\nUna peque\xc3\xb1a introduccion al Machine Learning\n'
https://github.com/ashu2012/udacitycCarND,https://classroom.udacity.com/nanodegrees/nd013,b'# udacitycCarND\nhttps://classroom.udacity.com/nanodegrees/nd013\n\nProject 1\n'
https://github.com/jenyquist/FLASH,An Ipython notebook implementation of A Computer Program for Flow-Log Analysis of Single Holes (FLASH),b'# FLASH\nAn Ipython notebook implementation of A Computer Program for Flow-Log Analysis of Single Holes (FLASH)\n'
https://github.com/PacktPublishing/Apache-Spark-2-for-Beginners,"Apache Spark 2 for Beginners, published by Packt","b'#Apache Spark 2 for Beginners\r\nThis is the code repository for , published by Packt. It contains all the supporting project files necessary to work through the book from start to finish.\r\n##Instructions and Navigations\r\nAll of the code is organized into folders. Each folder starts with a number followed by the application name. For example, Chapter02.\r\n\r\n## Software and Hardware List\r\n| Chapter number | Software required (with version) | Free/Proprietary | If proprietary, can code testing be performed using a trial version | If proprietary, then cost of the software | Download links to the software | Hardware specifications | OS required |\r\n| -------- | -------- | -------- | -------- | -------- | -------- | -------- | -------- |\r\n| All | Apache Spark 2.0.0 | Free | NA | NA | http://spark.apache.org/downloads.html | X86 | UNIX or MacOSX |\r\n| 6 | Apache Kafka 0.9.0.0 | Free | NA | NA | http://www.sublimetext.com/3 | X86 | UNIX or MacOSX |\r\n\r\n\r\n## Detailed installation steps (software-wise)\r\nThe steps should be listed in a way that it prepares the system environment to be able to test the codes of the book.\r\n###1. Apache Spark:\r\na. Download Spark version mentioned in the table\r\nb. Build Spark from source or use the binary download and follow the detailed instructions given in the page http://spark.apache.org/docs/latest/building-spark.html\r\nc. If building Spark from source, make sure that the R profile is also built and the instructions to do that is given in the link given inthe step b.\r\n###2. Apache Kafka\r\na. Download Kafka version mentioned in the table\r\nb. The \xe2\x80\x9cquick start\xe2\x80\x9d section of the Kafka documentation gives the instructions to setup Kafka.\r\nhttp://kafka.apache.org/documentation.html#quickstart\r\nc. Apart from the installation instructions, the topic creation and the other Kafka setup pre-requisites have been covered in detail in the chapter of the book\r\n\r\n\r\nThe code will look like the following:\r\n\r\n\r\nSpark 2.0.0 or above is to be installed on at least a standalone machine to run the code samples and do further activities to learn more about the subject. For Spark Stream Processing, Kafka needs to be installed and configured as a message broker with its command line producer producing messages and the application developed using Spark as a consumer of those messages.\r\n\r\n##Related Products\r\n* \r\n\r\n* \r\n\r\n* \r\n###Suggestions and Feedback\r\n if you have any feedback or suggestions.\r\n'"
https://github.com/justinmklam/python-resources,Python resources so I don't forget everything after not using it for 6 months.,"b""# Python Resources\nExample code snippets in python so I don't forget everything after not using it for 6 months.\n\n# How To Use\n+ Open the desired iPython notebook\n+ Look through examples\n+ ???\n+ Profit\n"""
https://github.com/akshat7/Kaggle-Digit-Recognizer,Kaggle submission of the Digit Recognizer challenge using multiple ML Algorithms and their result comparisons.,"b'# Kaggle-Digit-Recognizer\nKaggle submission of the Digit Recognizer challenge using multiple ML Algorithms and their result comparisons.\n\n## Analysis of Different Algorithms\n\nMy first submission was using K-Nearest Neighbour Algorithm (taking K = 3), which gave an accuracy of 96.857%. The accuracy went up to 96.943% when applying PCA before KNN.\n\nThe next submission was made by using PCA with SVM, which gave an accuracy of 98.057%, much better than the previous predictions.\n\nNext prediction was made using decision tree, which brought the accuracy down to 85.286%.\n\n## Dataset\nThe Training and Testing Dataset can be downloaded from Kaggle itself:\n\nhttps://www.kaggle.com/c/digit-recognizer \n\n## Python Libraries Used\n* Scikit-Learn\n* Pandas\n* Numpy\n'"
https://github.com/BibMartin/crossfolium,A plugin to add crossfilters in folium,"b""# crossfolium\n\nA plugin to add  in\n.\n\nThere's no documentation, but you can browse the\n.\n\n"""
https://github.com/birlrobotics/bnpy,bnpy trains nonparameteric markov switching process in python through a simple and useful api.,"b'## bnpy : Bayesian nonparametric machine learning for python.\r\n\r\n* \r\n* \r\n* \r\n* \r\n* \r\n* \r\n* \r\n* * \r\n* * \r\n* * \r\n\r\n# About\r\nThis python module provides code for training popular clustering models on large datasets. We focus on Bayesian nonparametric models based on the Dirichlet process, but also provide parametric counterparts. \r\n\r\nbnpy supports the latest online learning algorithms as well as standard offline methods. Our aim is to provide an inference platform that makes it easy for researchers and practitioners to compare models and algorithms.\r\n\r\n### Supported probabilistic models (aka allocation models)\r\n\r\n* Mixture models\r\n    *  : fixed number of clusters\r\n    *  : infinite number of clusters, via the Dirichlet process\r\n\r\n* Topic models (aka admixtures models)\r\n    *  : fixed number of topics. This is Latent Dirichlet allocation.\r\n    *  : infinite number of topics, via the hierarchical Dirichlet process\r\n    \r\n* Hidden Markov models (HMMs)\r\n    *  : Markov sequence model with a fixture number of states\r\n    *   : Markov sequence models with an infinite number of states\r\n\r\n* COMING SOON\r\n    * grammar models\r\n    * relational models\r\n\r\n### Supported data observation models (aka likelihoods)\r\n\r\n* Multinomial for bag-of-words data\r\n    * \r\n* Gaussian for real-valued vector data\r\n    *  : Full-covariance \r\n    *  : Diagonal-covariance\r\n    *  : Zero-mean, full-covariance\r\n* Auto-regressive Gaussian\r\n    * \r\n\r\n### Supported learning algorithms:\r\n\r\n* Expectation-maximization (offline)\r\n    * \r\n* Full-dataset variational Bayes (offline)\r\n    * \r\n* Memoized variational (online)\r\n    * \r\n* Stochastic variational (online)\r\n    * \r\n\r\nThese are all variants of variational inference, a family of optimization algorithms. We plan to eventually support sampling methods (Markov chain Monte Carlo) too.\r\n\r\n# Example Gallery\r\n\r\nYou can find many examples of bnpy in action in our curated .\r\n\r\nThese same demos are also directly available as Python scrips inside the .\r\n\r\n# Quick Start\r\n\r\nYou can use bnpy from a command line/terminal, or from within Python. Both options require specifying a dataset, an allocation model, an observation model (likelihood), and an algorithm. Optional keyword arguments with reasonable defaults allow control of specific model hyperparameters, algorithm parameters, etc.\r\n\r\nBelow, we show how to call bnpy to train a 8 component Gaussian mixture model on a default toy dataset stored in a .csv file on disk. In both cases, log information is printed to stdout, and all learned model parameters are saved to disk.\r\n\r\n## Calling from the terminal/command-line\r\n\r\n\r\n\r\n## Calling directly from Python\r\n\r\n\r\n\r\n## Advanced examples\r\n\r\nTrain Dirichlet-process Gaussian mixture model (DP-GMM) via full-dataset variational algorithm (aka ""VB"" for variational Bayes).\r\n\r\n\r\n\r\nTrain DP-GMM via memoized variational, with birth and merge moves, with data divided into 10 batches.\r\n\r\n\r\n\r\n## Quick help\r\n\r\n\r\n# Installation\r\n\r\nTo use bnpy for the first time, follow the documentations .\r\n\r\n# Team\r\n\r\n### Lead developer\r\n\r\nMike Hughes  \r\nWebsite: \r\n\r\nPost-doctoral researcher (Aug. 2016 - present)  \r\nSchool of Engineering and Applied Sciences  \r\nHarvard University  \r\n\r\n### Faculty adviser\r\n\r\nErik Sudderth  \r\nAssistant Professor  \r\nBrown University, Dept. of Computer Science  \r\nWebsite: \r\n\r\n### Contributors \r\n\r\n* Soumya Ghosh\r\n* Dae Il Kim\r\n* Geng Ji\r\n* William Stephenson\r\n* Sonia Phene\r\n* Gabe Hope\r\n* Leah Weiner\r\n* Alexis Cook\r\n* Mert Terzihan\r\n* Mengrui Ni\r\n* Jincheng Li\r\n\r\n# Academic References\r\n\r\n## Conference publications based on BNPy\r\n\r\n#### NIPS 2015 HDP-HMM paper\r\n\r\n> Our NIPS 2015 paper describes inference algorithms that can add or remove clusters for the sticky HDP-HMM.\r\n\r\n* ""Scalable adaptation of state complexity for nonparametric hidden Markov models."" Michael C. Hughes, William Stephenson, and Erik B. Sudderth. NIPS 2015.\r\n\r\n\r\n\r\n\r\n#### AISTATS 2015 HDP topic model paper\r\n\r\n> Our AISTATS 2015 paper describes our algorithms for HDP topic models.\r\n\r\n* ""Reliable and scalable variational inference for the hierarchical Dirichlet process."" Michael C. Hughes, Dae Il Kim, and Erik B. Sudderth. AISTATS 2015.\r\n\r\n\r\n\r\n\r\n#### NIPS 2013 DP mixtures paper\r\n\r\n> Our NIPS 2013 paper introduced memoized variational inference algorithm, and applied it to Dirichlet process mixture models.\r\n\r\n* ""Memoized online variational inference for Dirichlet process mixture models."" Michael C. Hughes and Erik B. Sudderth. NIPS 2013.\r\n\r\n\r\n\r\n\r\n## Workshop papers\r\n\r\n> Our short paper from a workshop at NIPS 2014 describes the vision for bnpy as a general purpose inference engine.\r\n\r\n* ""bnpy: Reliable and scalable variational inference for Bayesian nonparametric models.""\r\nMichael C. Hughes and Erik B. Sudderth. Probabilistic Programming Workshop at NIPS 2014.\r\n\r\n\r\n\r\n# Target Audience\r\n\r\nPrimarly, we intend bnpy to be a platform for researchers. \r\nBy gathering many learning algorithms and popular models in one convenient, modular repository, we hope to make it easier to compare and contrast approaches. We also hope that the modular organization of bnpy enables researchers to try out new modeling ideas without reinventing the wheel.\r\n'"
https://github.com/niudd/dps,dps = Deep Player Score (Not Damage Per Second!),b'dps = Deep Player Score \xef\xbc\x88\xe4\xb8\x8d\xe6\x98\xafdamage per second\xef\xbc\x81\xef\xbc\x89\n\xe8\xb6\xb3\xe7\x90\x83\xe7\x90\x83\xe5\x91\x98\xe8\xaf\x84\xe5\x88\x86\xe7\xb3\xbb\xe7\xbb\x9f\xef\xbc\x8c\xe5\xbb\xba\xe7\xab\x8b\xe5\x9c\xa8\xe6\xb7\xb1\xe5\xba\xa6\xe7\x9a\x84\xe8\xb6\xb3\xe7\x90\x83\xe6\x95\xb0\xe6\x8d\xae\xe7\xbb\x93\xe6\x9e\x84\xe4\xb8\x8a\xe3\x80\x82\n\n\nmodel.py\xe5\xae\x9e\xe7\x8e\xb0\xe4\xba\x86\xe8\xaf\xa5\xe6\x96\xb9\xe6\xb3\x95\xe3\x80\x82\n'
https://github.com/tgvaughan/bacter,Bacterial phylogenetics in BEAST 2.,"b'bacter\n======\n\n\n\nBacter is a   package which facilitates\ninference of a (restricted kind of) ancestral recombination graph (ARG) and\nrelated parameters from a sequence alignment.  It is based on the model\ndescribed in [Didelot et al.s 2010 Genetics paper][1].\n\nThis archive contains the source code of the package and is therefore of\nprimary interest to programmers.  For installation and usage instructions, as\nwell as links to tutorials and other documentation, please visit the project\nhome page hosted at http://tgvaughan.github.io/bacter.\n\nArchive Contents\n----------------\n\n*  : Example XML files, simulated data for the tutorial and a\n  Jupyter notebook with implementation validation details. (You can view this\n  notebook [online][2].) \n*  : Required libraries.\n*  : Java source code.\n*  : Java source code (unit tests).\n*  : BEAUti templates.\n*  : BEAST package version file.\n*  : Ant build script.\n*  : Causes SCM to ignore certain files\n*  : Control file for Travis CI server\n* : Used by Travis to build a reproducible test environment.\n*  : Software license.\n*  : This file.\n\nBuilding package from source\n----------------------------\n\nTo build this package from source, ensure you have the following installed:\n\n* Java JDK v1.8 \n* Apache Ant v1.9 or later\n* An internet connection\n\nThe internet connection is required since the build script downloads the most\nrecent version of the BEAST 2 source to build the package against.\nAssuming both Java and Ant are on your execution path and your CWD is the root of\nthis archive, simply type ""ant"" from the command line to build the package.\nThis may take up to a minute due to the script fetching the BEAST source, and\nthe resulting binary will be left in the  directory.\nTo run the unit tests, use ""ant test"".\n\nLicense\n-------\n\nThis software is free (as in freedom). You are welcome to use it, modify it,\nand distribute your modified versions provided you extend the same courtesy to\nusers of your modified version.  Specifically, it is made available under the\nterms of the GNU General Public License version 3, which is contained in his\ndirectory in the file named COPYING.\n\nAcknowledgements\n----------------\n\nWork on this project is made possible by the support of the following institutions:\n\n* \n\n* \n\n* \n\n* \n\n[1]: http://www.genetics.org/content/186/4/1435\n[2]: http://nbviewer.jupyter.org/github/tgvaughan/bacter/blob/master/examples/Validation.ipynb\n'"
https://github.com/jpdeleon/ircs,Basic data reduction of near infrared images with polarimetry specific for IRCS--the infrared camera and spectrograph instrument in the Subaru telescope,"b'# IRCS\nCommand-line implementation of basic data reduction of near-infrared images with polarimetry.\nThe data is from the infrared camera and spectrograph (IRCS) instrument on board the 8-m Subaru telescope in Hawaii.\n\n## Progress\n* 2017/03/20: basic scripting\n* 2017/04/24: created setup.py\n* 2017/04/26: added crop and bgsub\n* 2017/04/28: added calflat.py\n\n## Installation\nThis assumes that you have an environment with at least python 2.7 installed.\nIf not, use conda to create an environment called :\n\n\n\nNow, clone and then install   inside the environment:\n\n1. Clone this repository\n\n\n\n2.  into the proper directory and install\n\n\n\n## Sample run\n\n### Part 1 ircs-imaging\n\n\n\n### Part 2 ircs-polarimetry\n\n\n\nSee also other plotting helper functions in /ircs/utils.py.\n\nTO DO: \n1. define input/output directories using a .yaml\n2. implement low-level control\n3. upgrade to classes\n'"
https://github.com/wstrinz/infopipes,"Use existing services to feed, tag, organize and read news (wip)","b""#'Infopipes' prototyping\n\nCheckout \n"""
https://github.com/mat-esp-2016/integracao-numerica-oceano_saq,integracao-numerica-oceano_saq created by GitHub Classroom,"b'# Pr\xc3\xa1tica de Integra\xc3\xa7\xc3\xa3o Num\xc3\xa9rica\n\nParte do curso\n\nda .\n\nMinistrado por .\n\n## Objetivos\n\n* Aprender as regras de ret\xc3\xa2ngulos e trap\xc3\xa9zios para integra\xc3\xa7\xc3\xa3o num\xc3\xa9rica\n* Aplicar os conceitos de programa\xc3\xa7\xc3\xa3o em Python aprendidos at\xc3\xa9 agora\n\n## Prepara\xc3\xa7\xc3\xa3o\n\nUtilize o link enviado por e-mail para criar um reposit\xc3\xb3rio para seu grupo.\nUtilizaremos os mesmos grupos da pr\xc3\xa1tica passada.\nCada membro do grupo deve clicar no link e selecionar o nome do grupo criado na\npr\xc3\xa1tica passada.\nN\xc3\xa3o se esque\xc3\xa7a de sair de sua conta no github.com e no\nclassroom.github.com.\n\nCrie um arquivo chamado  com os nomes completos de todos os\nintegrantes do grupo. Inclua tamb\xc3\xa9m a qual turma pertencem.\n\nAs tarefas para serem feitas est\xc3\xa3o em um .\nPara utilizar o Jupyter, voc\xc3\xaa precisa iniciar um servidor de notebook\nno seu computador.\nAbra o bash e digite:\n\n    $ jupyter notebook\n\nEspere um pouco at\xc3\xa9 aparecer algo como:\n\n    [I 10:50:47.370 NotebookApp] Serving notebooks from local directory: /home/leo\n    [I 10:50:47.370 NotebookApp] 0 active kernels\n    [I 10:50:47.370 NotebookApp] The IPython Notebook is running at: http://localhost:8888/\n    [I 10:50:47.370 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n\nIsso deve abrir o seu navegador padr\xc3\xa3o tamb\xc3\xa9m em uma p\xc3\xa1gina no endere\xc3\xa7o\n.\nEssa p\xc3\xa1gina ir\xc3\xa1 te mostrar as pastas que est\xc3\xa3o em seu computador\n(a partir da pasta onde voc\xc3\xaa rodou ).\n\n## Tarefas\n\nSiga as instru\xc3\xa7\xc3\xb5es em .\nAs tarefas e suas solu\xc3\xa7\xc3\xb5es devem estar contidas nesse notebook.\nPor isso, fa\xc3\xa7a commits de suas mudan\xc3\xa7as ao notebook.\n\nAVISO: N\xc3\xa3o esque\xc3\xa7a de verificar se abriu o notebook no clone correto!\n\nAVISO 2: Ap\xc3\xb3s cada mudan\xc3\xa7a,  +  + .\n\nAVISO 3: ANTES de come\xc3\xa7ar: \n\n## Dicas\n\n* Fa\xc3\xa7am muitos commits. Quanto mais melhor.\n* N\xc3\xa3o se esque\xc3\xa7a do push.\n* Utilize mensagens de commit descritivas. ""Completei a tarefa 1"" \xc3\xa9 melhor que\n  ""mudan\xc3\xa7a"".\n* Escolha nomes descritivos para vari\xc3\xa1veis. ""temperatura"" \xc3\xa9 melhor que ""a"".\n* Descreva o que (e por que) voc\xc3\xaa fez em coment\xc3\xa1rios e c\xc3\xa9lulas de texto.\n  Isso ser\xc3\xa1 muito \xc3\xbatil quando voc\xc3\xaa voltar a essa tarefa depois.\n* Preste aten\xc3\xa7\xc3\xa3o aos detalhes. Leia as instru\xc3\xa7\xc3\xb5es com aten\xc3\xa7\xc3\xa3o.\n\n## Leitura recomendada\n\n* https://en.wikipedia.org/wiki/Numerical_integration\n* http://www.if.ufrj.br/~sandra/MetComp/2012-2/\n* https://en.wikibooks.org/wiki/Numerical_Methods/Numerical_Integration\n'"
https://github.com/Obeyed/udacity,Exercises from Udacity courses,b'Various exercises from the online courses on udacity.com\n\n# Content\n\n- Deep learning course (https://classroom.udacity.com/courses/ud730)\n'
https://github.com/LuisM78/python_machine_scripts,a place to store my python machine learning scripts,b'# python_machine_scripts\na place to store my python machine learning scripts\n'
https://github.com/pmorissette/klink,A Simple and Clean Sphinx Docs Theme,"b"".. image:: http://pmorissette.github.io/klink/static/logo.png\n\n.. image:: https://github.com/pmorissette/klink/workflows/Build%20Status/badge.svg\n    :target: https://github.com/pmorissette/klink/actions/\n\n.. image:: https://codecov.io/gh/pmorissette/klink/branch/master/graph/badge.svg\n    :target: https://codecov.io/pmorissette/klink\n\n.. image:: https://img.shields.io/pypi/v/klink\n    :alt: PyPI\n    :target: https://pypi.org/project/klink/\n\n.. image:: https://img.shields.io/pypi/l/klink\n    :alt: PyPI - License\n    :target: https://pypi.org/project/klink/\n\n\nklink - A Simple & Clean Sphinx Theme\n=====================================\n\nKlink is a _ which can be mighty useful.\n\nFor a live demo, please visit .\n\nOptions\n-------\n\nHere are the theme options. They should be added to the html_theme_options in\nyour .\n\nYou will also need to change your conf.py file. The following settings should\nwork::\n\n    html_theme = 'klink'\n    html_theme_path = ['_themes']\n    html_theme_options = {\n        'github': 'yourname/yourrepo',\n        'analytics_id': 'UA-your-number-here',\n        'logo': 'logo.png'\n    }\n"""
https://github.com/batra-mlp-lab/VT-F15-ECE6504-HW0,ECE6504 Homework 0,"b""# \n\n## Homework 0\n\n### Part 1: Getting Started with ECE6504\n\nIn this course, we will be using python considerably (most assignments will need a good amount of python).\n\n#### Anaconda\n\nAlthough many distributions of python are available, we recommend that you use the . Here are the advantages of using Anaconda:\n\n- Easy seamless install of  (most come standard)\n- It does not need root access to install new packages\n- Supported by Linux, OS X and Windows\n- Free!\n\nWe suggest that you use either Linux (preferably Ubuntu) or OS X.\nFollow the instructions  to install Anaconda python.\nRemember to make Anaconda python the default python on your computer.\nCommon issues are addressed here in the  .\n\nTODO\n\nInstall Anaconda python.\n\n#### Python\nIf you are comfortable with python, you can skip this section!\n\nIf you are new to python and have sufficient programming experience in using languages like C/C++, MATLAB, etc., you should be able to grasp the basic workings of python necessary for this course easily.\n\nWe will be using the  package extensively as it is the fundamental package for scientific computing providing support for array operations, linear algebra, etc. A good tutorial to get you started is . For those comfortable with the operations of MATLAB,  might prove useful.\n\nFor some assignments, we will be using the . IPython is a command shell for interactive computing developed primarily for python. The notebook is a useful environment where text can be embedded with code enabling us to set a flow while you do the assignments.\nIf you have installed Anaconda and made it your default python, you should be able to start the IPython notebook environment with:\n\nTODO\n\n\n\nThe IPython notebook files have  extension which you should be able to open now by navigating to the right directory.\n\n### Part 2: Starting homework 0\n\nThis homework is a warm up for the rest of the course. As part of this homework you will:\n\n- Implement a Multi-Class Support Vector Machine (SVM)\n    - vectorized loss function 4 points\n    - vectorized gradient computation 4 points\n- Implement Softmax Regression (SR)\n    - vectorized loss function 4 points\n    - vectorized gradient computation 4 points\n- Implement Stochastic Gradient Descent 2 points\n- Tune the hyper parameters using Spearmint 2 points\n\nYou will train the classifiers on images in the . The CIFAR-10 is a toy dataset with 60000 images of size 32 X 32, belonging to 10 classes. You need to start with  first to implement the SVM and then go ahead with  to implement logistic regression.\n\nThis homework is based on  of the CS231n course at Stanford.\n\nTODO\n\nDownload the starter code .\n\n#### Getting the dataset\n\nMake sure you are connected to the internet. Navigate to the  folder and run the following:\n\nTODO\n\n\n\nThis script will download the python version of the database for you and put it in  folder.\n\n#### Getting Spearmint\n\nAs part of this homework, you will be using Spearmint to tune the hyper-parameters like learning rate, regularization strength, etc. Spearmint is a software package to perform Bayesian optimization. It is designed to automatically run experiments in a manner that iteratively adjusts a number of parameters so as to minimize some objective in as few runs as possible.\n\nIf you have Anaconda installed, setting up Spearmint should be pretty straightforward. You can find installation and usage instructions . You need to use the command line interface to work with Spearmint. To get an idea of how Spearmint works, look up the  provided in the  folder.\n\nTODO\n\nInstall Spearmint. Follow instructions in the IPython notebook.\n\nDeliverables\n\nRun the  script and upload the resulting zip file.\n\n### Part 3: SVM and Logistic Regression\n\nAs you might already know, both SVM and SR classifiers are linear models but they use different loss functions (hinge loss in SVMs vs softmax loss in SR). Here is a brief summary of the classifiers and if you need a detailed tutorial to brush up your knowledge,  is a nice place.\n\nBefore we go into the details of a classifier, let us assume that our training dataset consists of (N) instances (x_i in mathbb{R}^D ) of dimensionality (D). \nCorresponding to each of the training instances,\nwe have labels (y_i in {1,2,dotsc ,K }), where (K) is the number of classes. \nIn this homework, we are using the CIFAR-10 database where (N=50,000), (K=10), (D= 32 times 32 times 3) \n(image of size  (32 times 32) with (3) channels - Red, Green, and Blue).\n\nClassification is the task of assigning a label to the input from a fixed set of categories or classes. A classifier consists of two important components:\n\nScore function: This maps every instance (x_i) to a vector (p_i) of dimensionality (K). Each of these entries represent the class scores for that image. Both SVM and Logistic Regression have a linear score function given by:\n\n[ p_i = f(x_i;W,b) ]\n\nwhere,\n\n[ f(x;W,b) = Wx + b ]\n\nHere, W is a matrix of weights of dimensionality (K times D) and b is a vector of bias terms of dimensionality (K times 1). The process of training is to find the appropriate values for W and b such that the score corresponding to the correct class is high. In order to do this, we need a function that evaluates the performance. Using this evaluation as feedback, the weights can be updated in the right 'direction' to improve the performance of the classifier.\n\nWe make a minor modification to the notation before proceeding further. The bias term can be incorporated within the weight matrix W making it of dimensionality (K times (D+1)). The (i^{th}) row of the weight matrix W is represented as a column vector (w_i) so that (p _i^j = w _j^Tx_i). The superscript j denotes the (j^{th}) element of (pi{j neq y_i} bigg[max big(0, p_i^j - p_i^{yi} + {i=1}^{N}log bigg( frac{e^{p_i^{yi}}}{{k,l}^2 ]\n\nThe regularization term (R(W)) is usually multiplied by the regularization strength (lambda) before adding it to the loss function. (lambda) is a hyper parameter which needs to be tuned so that the classifier generalizes well over the training set.\n\nThe next step is to update the weight parts such that the loss is minimized. This is done by Stochastic Gradient Descent (SGD). The weight update is done as:\n\n[ W := W - eta nabla L ]\n\nHere, (nabla L) is the gradient of the loss function and the factor (eta) is the learning rate. SGD is usually performed by computing the gradient w.r.t. a randomly selected batch from the training set.\nThis method is more efficient than computing the gradient w.r.t the whole training set before each update is performed.\n\nReferences:\n\n1. \n\n---\n\n&#169; 2015 Virginia Tech"""
https://github.com/camillescott/avida-modularity,Study on the evolution of genetic modularity in the absence of recombination,"b'# Evolution of Genetic Modularity in the Absence of Recombination\n\nCode for data generation, analysis, and manuscript for our study of the evolution of modularity in the absence of recombination in digital organisms using the Avida platform.\n'"
https://github.com/mat-esp/minimos-quadrados,Método dos mínimos quadrados,"b'# M\xc3\xa9todo dos M\xc3\xadnimos Quadrados\n\nParte do curso\n\nda .\n\nMinistrado por .\n\n## Objetivos\n\n* Aprender a formula\xc3\xa7\xc3\xa3o matricial do m\xc3\xa9todo dos m\xc3\xadnimos quadrados (MMQ)\n* Treinar o uso de fun\xc3\xa7\xc3\xb5es e programa\xc3\xa7\xc3\xa3o defensiva\n* Utilizar o MMQ para ajustar uma reta a dados ruidosos\n* Generalizar o MMQ para qualquer fun\xc3\xa7\xc3\xa3o linear\n\n## Leitura recomendada\n\n* https://en.wikipedia.org/wiki/Least_squares\n* https://en.wikipedia.org/wiki/Linear_least_squares_%28mathematics%29\n* http://www.mat.ufmg.br/gaal/aplicacoes/quadrados_minimos.pdf\n* https://pt.wikipedia.org/wiki/M%C3%A9todo_dos_m%C3%ADnimos_quadrados\n\n## Prepara\xc3\xa7\xc3\xa3o\n\nUtilize o link enviado por e-mail para criar um reposit\xc3\xb3rio para seu grupo.\nCada membro do grupo deve clicar no link e selecionar o nome do grupo criado na\npr\xc3\xa1tica passada.\nN\xc3\xa3o se esque\xc3\xa7a de sair de sua conta no github.com e no\nclassroom.github.com.\n\nCrie um arquivo chamado  com os nomes completos de todos os\nintegrantes do grupo. Inclua tamb\xc3\xa9m a qual turma pertencem.\n\n## Avalia\xc3\xa7\xc3\xa3o das pr\xc3\xa1ticas\n\nUtilizaremos a lista abaixo para avaliar a solu\xc3\xa7\xc3\xa3o entregue de cada pr\xc3\xa1tica.\n\n- [ ] Mensagens de commit que explicam claramente a mudan\xc3\xa7a que foi feita\n  [total|parcial|zero] 0.5 pt\n- [ ] Formata\xc3\xa7\xc3\xa3o do c\xc3\xb3digo apropriada.\n  Ex: ,\n  ,  == BOM.\n  , ,\n   == RUIM [total|parcial|zero] 0.5 pt\n- [ ] Utilizar vari\xc3\xa1veis ao inv\xc3\xa9s de colocar n\xc3\xbamero ""na m\xc3\xa3o"".\n  Ex: ,  == BOM.\n  ,  == RUIM. [total|parcial|zero] 1 pt\n- [ ] C\xc3\xb3digo com coment\xc3\xa1rios que explicam ""por que"" algo foi feito, n\xc3\xa3o s\xc3\xb3\n  ""o que"" foi feito [total|parcial|zero] 1 pt\n- [ ] Nomes de vari\xc3\xa1veis descritivos. Ex: , ,\n  ,  == BOM. , , ,  == RUIM.\n  [total|parcial|zero] 2 pt\n- [ ] C\xc3\xb3digo produz a solu\xc3\xa7\xc3\xa3o correta (exatamente como especificado em\n  ""Resultado esperado"") [total|parcial|zero] 5 pt\n- [ ] Tarefa b\xc3\xb4nus [total|parcial|zero] 1 pt extra (n\xc3\xa3o ser\xc3\xa1 considerado\n  caso a nota j\xc3\xa1 seja 10)\n\nCada crit\xc3\xa9rio de avalia\xc3\xa7\xc3\xa3o poder\xc3\xa1 receber pontua\xc3\xa7\xc3\xa3o:\n\n* Total: se atender perfeitamente ao crit\xc3\xa9rio\n* Parcial: (metade da nota) se atender parcialmente ao crit\xc3\xa9rio\n* Zero: se falhar ao crit\xc3\xa9rio\n\nNote que a nota m\xc3\xa1xima, incluindo a tarefa b\xc3\xb4nus, \xc3\xa9 10.\nCada grupo ter\xc3\xa1 acesso a corre\xc3\xa7\xc3\xa3o de sua solu\xc3\xa7\xc3\xa3o.\n\n## Entrega das solu\xc3\xa7\xc3\xb5es\n\nA solu\xc3\xa7\xc3\xa3o de cada pr\xc3\xa1tica ser\xc3\xa1 um reposit\xc3\xb3rio no \ncom o c\xc3\xb3digo feito pelos alunos (criado a partir dos templates no\n""Cronograma"").\nA entrega das solu\xc3\xa7\xc3\xb5es ser\xc3\xa1 feita criando uma\n\nno reposit\xc3\xb3rio da disciplina\n.\nUtilize o link abaixo para ir direto para as Issues:\n\nhttps://github.com/leouieda/matematica-especial/issues\n\nCada grupo deve criar uma Issue para entragar cada pr\xc3\xa1tica.\nA issue dever\xc3\xa1 seguir o padr\xc3\xa3o abaixo:\n\n* T\xc3\xadtulo: Deve conter o nome da pr\xc3\xa1tica seguido dos nomes dos integrantes do\n  grupo e a qual turma pertecem (caso haja mais de uma). Ex: ""Pr\xc3\xa1tica\n  Integra\xc3\xa7\xc3\xa3o: Bilbo, John, Arthur - Turma 1""\n* Corpo: Deve conter o link para o reposit\xc3\xb3rio do grupo (ex:\n  ) e qualquer\n  coment\xc3\xa1rio que achar necess\xc3\xa1rio (ex: problemas com os commits, erros que foram\n  arrumados depois, dificuldades, etc).\n\n## License\n\nAll content can be freely used and adapted under the terms of the\n.\n\n\n'"
https://github.com/vyachegrinko/Python_for_Data_Science,training module,"b'# Python for Data Science\n\nThis short primer on  is designed to provide a rapid ""on-ramp"" for computer programmers who are already familiar with basic concepts and constructs in other programming languages to learn enough about Python to effectively use open-source and proprietary Python-based machine learning and data science tools.\n\nThe primer is spread across a collection of , and the easiest way to use the primer is to  on your computer. You can also , and manually copy and paste the pieces of sample code into the Python interpreter, as the primer only makes use of the Python standard libraries.\n\nThere are four versions of the primer. Three versions contain the entire primer in a single notebook:\n\n* Single IPython Notebook (cleared output cells): \n* Single IPython Notebook (filled output cells): \n* Single web page (HTML): \n\nThe other version divides the primer into 5 separate notebooks:\n\n* \n* \n* \n* \n* \n\nThere are several exercises included in the notebooks. Sample solutions to those exercises can be found in two Python source files:\n\n* : a collection of simple machine learning utility functions\n* : a Python class to encapsulate a simplified version of a popular machine learning model\n\nThere are also 2 data files, based on the  in the UCI Machine Learning Repository, used for coding examples, exploratory data analysis and building and evaluating decision trees in Python:\n\n* : a machine-readable list of examples or instances of mushrooms, represented by a comma-separated list of attribute values\n* : a machine-readable list of attribute names and possible attribute values and their abbreviations\n\n## Change Log\n\n2015-07-26\n\n* Updated  and  notebooks with additional cells base on PyData Seattle 2015 tutorial Q&A\n\n2015-07-21\n\n* Updated 5 subnotebooks for Python 3 compatibility\n* Changed file name of  to  (class name is unchanged)\n* Reordered introduction of  and  containers\n* Reordered the values returned by \n* Added more examples of formatted printing via \n* Various and sundry other minor changes to prepare for  at \n\n2015-02-23\n\n* Added attribution for suggested changes to accommodate Python 3 to \n\n2015-02-22\n\n* Added  for Python 3 compatibility\n* Updated  and  to also use Python 3  and \n* Replaced  (Python 2 only) with  (Python 2 or 3)\n* Replaced  (Python 2 only) with  (Python 2 or 3)\n* Changed  to \n* Added  (and reference to duck typing) to section on \n* Added variable for  rather than hard-coding  character\n* Cleaned up various cells'"
https://github.com/dangall/Deep-Learning-course,Contains notebook exercises for a Deep Learning course from Udacity,b'# Deep Learning course content\nContains notebook exercises for a Deep Learning course from Udacity\n'
https://github.com/JuliaX/cajun-talks,Talk materials for the Cambridge Area Julia User Network,b'julia-cajun-talks\n=================\n'
https://github.com/akshaykumarpal/Stock-Market-Movement-Prediction-using-News-Feed-Python-and-R,This project is about Stock Market Movement Prediction using News-Feed in Python and R (Deep Learning and Sentiment Analysis),"b'# Stock Market Movement Prediction using News-Feed in Python and R programming\n\nEnvironment: Python 3.5.2, Anaconda Navigator (1.3.1), R (3.3.1) and R Studio (1.0.136)\n\nWe had taken news feeds from different sources and build a model on a scale of 1 to 4 using sentiment analysis. Also, we took stock data from Dow Jones investors and made the data scaled between 0 to 1 so that we can apply Neural Network. Then, combining these data our final dataset was from past 2008 to 2016 years containing 25 topics for each month. Now we also applied logistic regression to make an indexing of the class label where our objective becomes to predict whether the stock price increases or decreases as \xe2\x80\x980\xe2\x80\x99 and \xe2\x80\x981\xe2\x80\x99 respectively. \n\nWe have applied both Linear Regression and Polynomial Regression to find relation between the change in stock market price vs. the volume as well as sentiment of news articles. We applied our training data on two classification methods such as Logistic Regression and Multi-Layer Perceptron (MLP) to estimate the movement of change in stock market price, volume and the sentiments of news articles. We trained our model on the basis of TF-IDF (Term Frequency \xe2\x80\x93 Inverse Document Frequency) sparse matrix using both unigram and bigram scores.\n\nWe saw Bigram model performs better than Unigram model for both Multi-Layer Perceptron and Logistic Regression. Also, we found that Multi-Layer Perceptron gives us the better accuracy than Logistic Regression.\n'"
https://github.com/jimpala/msci-hep,Code for HEP data science work at UCL.,"b'# msci-hep\nCode for HEP data science work at UCL, Summer 2016.\n'"
https://github.com/danielfrg/coursera-intro-ds,Coursera intro to Data science - Spring 2013,b'coursera-intro-ds\n=================\n\nCoursera intro to Data science - Spring 2013\n'
https://github.com/CedricVallee/pythonKNearestNeighbors,"This repository contains code for the class IEOR265 ""Learning and Optimization"".","b'# pythonKNearestNeighbors\nThis repository contains code for the class IEOR265 ""Learning and Optimization"".\n'"
https://github.com/PythonWorkshop/scalable-sklearn,Demonstrations of scalable sklearn with dask for out-of-core computation.,"b""# Exploring scikit-learn with dask for scaling out computation on large data\n\n## tl;dr\n\nHere, you'll find demonstrations of scalable sklearn with  for out-of-core computation on large and complex datasets.  Dask uses task graphs (which are even modifiable) to scale out computation onto disk (out-of-core).  In this way both the computation and amount of data can be scaled in a big way which is really nice for ML.\n\n\n## Blurb\n\nIt\xe2\x80\x99s becoming increasingly important to scale up machine learning and deep learning computation either using a common solution in a cluster of GPUs or out-of-core computation on a single machine with enough local disk storage, which is rarely a problem these days.  Dask is a new library built on python that through out-of-core processes in task graphs can handle large datasets (gbs - tbs) for resource hungry computation.  It can do all this on a single PC/laptop given enough disk.\n\n## Outline\n\n1. Skimage to convert to numeric\n* Standard scaling of data\n* (Optional) clean up noise\n* Image classification with \n  * MLP setup (using sklearn 0.18.dev0)\n  * use dask and the partial_fit for MLP\n* Visualize task graph\n* Try it with gridsearchcv for hyperparameter tuning\n\n\n"""
https://github.com/abhishekanand/DS4UX,Data Science for User Experience Researchers,"b'# DS4UX\nData Science for User Experience Researchers  \n\nNew MS Course: Data Science for User Experience Researchers\nHCDE 598\nData Science for User Experience Researchers\nSuccess in many UX related roles, particularly user research, require workers to possess an understanding of data science concepts and to have facility with the tools of data analysis. This course introduces students to widely-adopted programming and data science tools to give them the skills to use data to answer questions about the characteristics, behaviors, and needs of people who use a wide variety of products.\n\nThis course has students working with real data from real users. It is built around scenarios that are directly relevant to performing user research in industry, such as:\n\n* identifying user segments (e.g. power users) and popular content\n* manipulating very large datasets (too big for Excel!)\n* performing data visualization and statistical analysis using code (not GUIs)\n* implementing experimental designs such as A/B tests and funnel analysis\n\nThe goal of the course is to provide students with a basic grasp of programming and data science concepts using tools that they can reuse elsewhere. No previous programming experience is required, or even expected.  While the course is framed around user research, the use-cases we will work with are relevant to a wide variety of non-engineering roles in software development and the broader technology industry.\n\nUpon completion of this course, students should be able to:\n\n* Write or modify a program to collect a dataset from Wikipedia or the City of Seattle\xe2\x80\x99s open data portal (Data.Seattle.gov)\n* Effectively read web API documentation and write Python software to parse and understand a new and unfamiliar JSON-based web API.\n* Understand database schemas and use MySQL to extract user data from relational databases.\n* Use web-based data to effectively answer a substantively interesting question and to present this data effectively in the context of both a formal presentation and a written report.\n'"
https://github.com/datakop/rabbitmq-notebook-examples,RabbitMQ example inside iPython Notebook.,b'## RabbitMQ iPython Examples\n\n\n#### Install environment\n\n\n\n#### Ramp up RabbitMQ(in docker)\n\n\n#### How to use\nI Run iPython notebook\n\nII Visit RabbitMQ managment site  to check how MQ works.\n\nTo get docker ip run \n\n##### ToDo:\n- Remote procedure call (RPC) example. \n\n### Links:\n- \n- \n- '
https://github.com/anne4180/my_first_python_rep,my first python repository,b'# my_first_python_rep\nmy first python repository\n2020\n'
https://github.com/yanyanmountainview/student_intervention,"Based on students' performance data,  developed a model that predicted the likelihood that a given student will pass, quantifying whether an intervention is necessary.","b'# Project 2: Supervised Learning\n## Building a Student Intervention System\n\n### Install\n\nThis project requires Python 2.7 and the following Python libraries installed:\n\n- \n- \n- \n\nYou will also need to have software installed to run and execute an \n\nUdacity recommends our students install , i pre-packaged Python distribution that contains all of the necessary libraries and software for this project. \n\n### Code\n\nTemplate code is provided in the notebook  notebook file. While some code has already been implemented to get you started, you will need to implement additional functionality when requested to successfully complete the project.\n\n### Run\n\nIn a terminal or command window, navigate to the top-level project directory  (that contains this README) and run one of the following commands:\n\n  \n\n\nThis will open the iPython Notebook software and project file in your browser.\n\n## Data\n\nThe dataset used in this project is included as . This dataset has the following attributes:\n\n-  ? students school (binary: ""GP"" or ""MS"")\n-  ? students sex (binary: ""F"" - female or ""M"" - male)\n-  ? students age (numeric: from 15 to 22)\n-  ? students home address type (binary: ""U"" - urban or ""R"" - rural)\n-  ? family size (binary: ""LE3"" - less or equal to 3 or ""GT3"" - greater than 3)\n-  ? parents cohabitation status (binary: ""T"" - living together or ""A"" - apart)\n-  ? mothers education (numeric: 0 - none,  1 - primary education (4th grade), 2 -\xe2\x82\xac\xe2\x80\x9c 5th to 9th grade, 3 - secondary education or 4 -\xe2\x82\xac\xe2\x80\x9c higher education)\n-  ? fathers education (numeric: 0 - none,  1 - primary education (4th grade), 2 - 5th to 9th grade, 3 - secondary education or 4 -\xe2\x82\xac\xe2\x80\x9c higher education)\n-  ? mothers job (nominal: ""teacher"", ""health"" care related, civil ""services"" (e.g. administrative or police), ""at_home"" or ""other"")\n-  ? fathers job (nominal: ""teacher"", ""health"" care related, civil ""services"" (e.g. administrative or police), ""at_home"" or ""other"")\n-  ? reason to choose this school (nominal: close to ""home"", school ""reputation"", ""course"" preference or ""other"")\n-  ? students guardian (nominal: ""mother"", ""father"" or ""other"")\n-  ? home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)\n-  ? weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)\n-  ? number of past class failures (numeric: n if 1<=n<3, else 4)\n-  ? extra educational support (binary: yes or no)\n-  ? family educational support (binary: yes or no)\n-  ? extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)\n-  ? extra-curricular activities (binary: yes or no)\n-  ? attended nursery school (binary: yes or no)\n-  ? wants to take higher education (binary: yes or no)\n-  ? Internet access at home (binary: yes or no)\n-  ? with a romantic relationship (binary: yes or no)\n-  ? quality of family relationships (numeric: from 1 - very bad to 5 - excellent)\n-  ? free time after school (numeric: from 1 - very low to 5 - very high)\n-  ? going out with friends (numeric: from 1 - very low to 5 - very high)\n-  ? workday alcohol consumption (numeric: from 1 - very low to 5 - very high)\n-  ? weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)\n-  ? current health status (numeric: from 1 - very bad to 5 - very good)\n-  ? number of school absences (numeric: from 0 to 93)\n-  ? did the student pass the final exam (binary: yes or no)\n'"
https://github.com/pszjmb1/fun_with_metropolis_algorithm,Developing intuition behind MCMC sampling,"b""# fun_with_metropolis_algorithm\nDeveloping intuition behind MCMC sampling. \n\nBased on .\n"""
https://github.com/pbrusco/crystal-learn,Machine Learning in Crystal ,b'## An sklearn-like machine-learning library for Crystal\n\nExample (that can be found in examples folder)\n\n\n### Output:\n\n'
https://github.com/yanyu711141/6_12_2016_SoftwareCarpentry,code from the workshop,b'# 6_12_2016_SoftwareCarpentry\ncode from the workshop\n\n#\n##\n###\n'
https://github.com/jasonamyers/pycon2014,My Intro to SQLAlchemy Core Slides,"b'Slideshow available on \n\nYou can view a PDF version of the slides at\n.\n\nIf you want to run the slides, you must have node installed then:\n* type \n* type \n* open \n\nIf you want to access the ipython notebook for this presentation:\n* create a virtualenv (For example: )\n* type \n* type \n'"
https://github.com/cgerson/bike-share-analysis,Analysis of open bikeshare data,"b'# bike-share-analysis\n\nI used NYC citibike system data to implement a classification system to predict routes of citi-bikers given starting station, gender & age of user, day of the week, and time of day.\n\n\n####Data:\n- Merged NYC boundaries geometry file (source) with station lat/lon data in CartoDB to link each station to its associated neighborhood. (see nyc_neighborhoods.csv)\n- Downloaded Dec 2014, Jan & Feb 2015 citibike data and merged with nyc_neighborhoods to find associated neighborhoods for each bike route. (see citibike_dec.csv, citibike_jan.csv & citibike_feb.csv)\n\n####Models:\n- Data was merged and cleaned (see citibike notebook)\n- Ran different classification algorithms to predict destination neighborhood and scored each on accuracy and precision. Made sure models sampled evenly from each class, given class imbalance. Each destination neighborhood is either correct or incorrect, so accuracy is an effective metric. (see citibike_models notebook)\n\n####Results:\n- See my blog post for a write up of results and a copy of the final presentation'"
https://github.com/toyota790/Twitter_PanamaPapers_Analysis,This project is using the Logstash to get data from Twitter. Then use the PySpark K-Means algorithm to clustering.,"b'# Twitter Panama Papers Analysis\nThis project is using the Logstash to get data from Twitter. Then use the PySpark K-Means algorithm to clustering.\n\n##Architecture\n\n\n##Environment\n* Anaconda version: 4.0.8\n* Python version: 2.7.11 \n* iPython version: 4.1.2\n* Spark version: 1.5.2\n* NLTK version: 3.2\n* Pandas version: 0.18.1\n* Scikit-learn version: 0.17.1\n* Snow Ball Stemmer version: 1.2.1\n* Bokeh version: 0.11.1\n* Logstash version: 2.3.1\n* Elasticsearch version: 2.3.1\n* JAVA Version 8 Update 77\n\n##Data Collection\n* Logstash to Elasticsearch ()\n(Note: You can use the Python to crawl the data, it use the . Reference code is available in my Github .)\n* Data Format: CSV\n* Search Keyword:\n\n\t""#panamapapers""\n\t""panamapapers""\n\t""panama paper""\n\t""the panama paper""\n\n\n##Data Source\n* 514 attributes\n* Data Size\n\t* Total: 200000 (484 MB)\n\t* Training dataset: 20000\n* Time\n\t* Start: Sun Apr 10 16:18:35 +0000 2016\n\t* End: Wed Apr 13 18:32:27 +0000 2016\n\n##Data Cleaning\n* URL\n\t* https, http\n* Emoji\n\t* UCS-4, UCS-2\n* Alphabet\n\t* a, c, l, etc.\n* Stop word\n\t*  NLTK\xe2\x80\x99s list of English stop words\n* Punctuation\n\t* dot, question mark, etc.\n##Feature Selection\n* Stemming\n\t* Panamapapers -> panamapap\n\t* Family -> famili\n\t* Link -> link\n* Tokenizing \n* TF-IDF\n\t* 2000 features\n\n##Data Modeling\n* K-means++ Clustering Algorithm\n\t* K=4\n* The size of each cluster:\n\t* Cluster 0: 5158\n\t* Cluster 1: 964\n\t* Cluster 2: 13233\n\t* Cluster 3: 645\n\n##Visualization\n* Bokeh (Two dimensions)\n![Bokeh Result] (Result/Bokeh_Result.png)\n\n* Word Cloud - Cluster 0\n![Word Cloud Cluster 0] (Result/WordCloud_cluster0.png)\n\n* Word Cloud - Cluster 1\n![Word Cloud Cluster 1] (Result/WordCloud_cluster1.png)\n\n* Word Cloud - Cluster 2\n![Word Cloud Cluster 2] (Result/WordCloud_cluster2.png)\n\n* Word Cloud - Cluster 3\n![Word Cloud Cluster 3] (Result/WordCloud_cluster3.png)\n\n* Plotly (Three dimensions)\n![Plotly Result] (Result/Plotly_Result.png)\n'"
https://github.com/phgcarva/atai-notebooks-machine-learning,A series of homeworks done for the course CK0146 - Advanced Topics in Artificial Inteligence (Machine Learning),b'# atai-notebooks-machine-learning\nA series of homeworks done for the course CK0146 - Advanced Topics in Artificial Inteligence (Machine Learning)\n'
https://github.com/mbmilligan/msi-ipython-nb-ex,Example IPython notebooks for MSI,"b'\n\nPython Examples for MSI\n=======================\n\nThe IPython notebooks in this repository accompany the Python for Scientific Computing workshop hosted by the Minnesota Supercomputing Institute. Visit https://www.msi.umn.edu/tutorials/python-scientific-computing for details and a PDF copy of the slides.\n\nGetting started\n---------------\n\nYou can clone a copy of this repository using git, for example:\n\n    git clone https://github.com/mbmilligan/msi-ipython-nb-ex\n    \nIf you have a web browser and are connected to the UMN network, you can access our JupyterHub server at: \n\nIf you are at a command line at the MSI, the commands\n\n    module load python\n    cd msi-ipython-nb-ex\n    ipython notebook\n\nwill start the Jupyter notebook system in your web browser. From there, you can open the saved IPython notebooks (files with the extension .ipynb) by clicking on them.\n\nIf you do not have access to a Python environment right now, you can click on the Binder badge above to run these examples through  in a temporary environment. \n'"
https://github.com/JonathanToro/Fish_Image_Classification,Kaggle competition code that uses CNNs for image classification. Ranked top 5% at the time of submission,b'# Catching Fish With Neural Nets\n## Goal: Use computer vision and CNNs to accurately detect fish species in an image\n\nMy main approach in tackling this Kaggle competition was by using Convolutional Neural Networks. I used two different pretrained neural networks and architectures provided by Google. My best results was by using the InceptionV3 model and augmenting the test data so that the model had multiple tries to classify each image correctly. I hosted the modeling on Amazon Web Services by using their g2.8x instance which provided me with 4 gpus to work with. The code to parallelize the training is provided above. At the time of the submission of my best model it ranked top 5% on Kaggle.\n\nPlease refer to my [blog][1] for more information.\n\n[1]: https://jonathantoro.github.io/Catching-Fish-With-Neural-Nets/\n'
https://github.com/aprilnovak/coupling-moose,Local MOOSE repo holding common functionalities that in the future should be put in a MOOSE module,"b'MOOSE\n=====\n\n\n\nThe Multiphysics Object-Oriented Simulation Environment (MOOSE) is a finite-element, multiphysics framework primarily developed by . It provides a high-level interface to some of the most sophisticated  on the planet. MOOSE presents a straightforward API that aligns well with the real-world problems scientists and engineers need to tackle. Every detail about how an engineer interacts with MOOSE has been thought through, from the installation process through running your simulation on state of the art supercomputers, the MOOSE system will accelerate your research.\n\nSome of the capability at your fingertips:\n\n* Fully-coupled, fully-implicit multiphysics solver\n* Dimension independent physics\n* Automatically parallel (largest runs >100,000 CPU cores!)\n* Modular development simplifies code reuse\n* Built-in mesh adaptivity\n* Continuous and Discontinuous Galerkin (DG) (at the same time!)\n* Intuitive parallel multiscale solves (see videos below)\n* Dimension agnostic, parallel geometric search (for contact related applications)\n* Flexible, plugable graphical user interface\n* ~30 plugable interfaces allow specialization of every part of the solve\n\nMore Information\n================\n\nFor more information, including installation instructions, please see the official website: \n\nContributing\n============\n\nFor information on how to contribute code changes to MOOSE please .\n'"
https://github.com/andridns/tf-cifar10,Convolutional Neural Network to classify CIFAR-10 image dataset using TensorFlow,"b""# tf-CIFAR10\n\nA Convolutional Neural Network to classify CIFAR-10 image dataset using TensorFlow.\n\n## Files\n\nTo open the main code, simply open  on any desktop browser, or you can download and run the cells in a Python 3 environment. The code is presented in a  / iPython notebook for readability purposes.\n\n\n## Overview\n\nThis project classifies images from the . The dataset consists of airplanes, dogs, cats, and other objects. We will preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.\n\n\n\n## Data\n\nThe dataset is broken into batches to prevent our machine from running out of memory. The CIFAR-10 dataset consists of 5 batches, named data_batch_1, data_batch_2, etc.. Each batch contains the labels and images that are one of the following:\nairplane\nautomobile\nbird\ncat\ndeer\ndog\nfrog\nhorse\nship\ntruck\n\n## Convolutional Neural Network Architecture\n\n\n\nThis code builds combination of convolutional neural network, max pooling, dropout, and fully connected layers.  At the end, there will be a test on the neural network's predictions on sample images.\n\n\n\n## Result\n\nThe final accuracy is about 67%, which is much better than pure guessing (pure guessing on CIFAR-10 dataset will stochastically yield 10% accuracy).\n\n\n\n## Dependencies\n\nThis project requires Python 3 and the following Python libraries installed:\n\n* \n* \n*  - Progress Meter\n* \n* \n\n\n\n\n"""
https://github.com/fsfelix/audio-thumbnailing,Repository for my scientific initiation project on audio thumbnailing. ,"b'# audio-thumbnailing\nRepository for my scientific initiation project on audio thumbnailing. \n\n## Usage Requirements\n        * python 3\n        * jupyter-notebook\n        * numpy\n        * librosa\n\n## References\n* Cooper, Matthew L., and Jonathan Foote. ""Automatic Music Summarization via Similarity Analysis."" ISMIR. 2002.\n* M\xc3\xbcller, Meinard, Peter Grosche, and Nanzhu Jiang. ""A Segment-Based Fitness Measure for Capturing Repetitive Structures of Music Recordings."" ISMIR. 2011.\n* Bartsch, Mark A., and Gregory H. Wakefield. ""Audio thumbnailing of popular music using chroma-based representations."" IEEE Transactions on multimedia 7.1 (2005): 96-104.\n'"
https://github.com/tmhughes81/student_intervention,udacity Project #2: Student Intervention,"b'# Project 2: Supervised Learning\n## Building a Student Intervention System\n\n### Install\n\nThis project requires Python 2.7 and the following Python libraries installed:\n\n- \n- \n- \n\nYou will also need to have software installed to run and execute an \n\nUdacity recommends our students install , a pre-packaged Python distribution that contains all of the necessary libraries and software for this project. \n\n### Code\n\nTemplate code is provided in the notebook  notebook file. While some code has already been implemented to get you started, you will need to implement additional functionality when requested to successfully complete the project.\n\n### Run\n\nIn a terminal or command window, navigate to the top-level project directory  (that contains this README) and run one of the following commands:\n\n  \n\n\nThis will open the iPython Notebook software and project file in your browser.\n\n## Data\n\nThe dataset used in this project is included as . This dataset has the following attributes:\n\n-  : students school (binary: ""GP"" or ""MS"")\n-  : students sex (binary: ""F"" - female or ""M"" - male)\n-  : students age (numeric: from 15 to 22)\n-  : students home address type (binary: ""U"" - urban or ""R"" - rural)\n-  : family size (binary: ""LE3"" - less or equal to 3 or ""GT3"" - greater than 3)\n-  : parents cohabitation status (binary: ""T"" - living together or ""A"" - apart)\n-  : mothers education (numeric: 0 - none,  1 - primary education (4th grade), 2 - 5th to 9th grade, 3 - secondary education or 4 - higher education)\n-  : fathers education (numeric: 0 - none,  1 - primary education (4th grade), 2 - 5th to 9th grade, 3 - secondary education or 4 - higher education)\n-  : mothers job (nominal: ""teacher"", ""health"" care related, civil ""services"" (e.g. administrative or police), ""at_home"" or ""other"")\n-  : fathers job (nominal: ""teacher"", ""health"" care related, civil ""services"" (e.g. administrative or police), ""at_home"" or ""other"")\n-  : reason to choose this school (nominal: close to ""home"", school ""reputation"", ""course"" preference or ""other"")\n-  : students guardian (nominal: ""mother"", ""father"" or ""other"")\n-  : home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)\n-  : weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)\n-  : number of past class failures (numeric: n if 1<=n<3, else 4)\n-  : extra educational support (binary: yes or no)\n-  : family educational support (binary: yes or no)\n-  : extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)\n-  : extra-curricular activities (binary: yes or no)\n-  : attended nursery school (binary: yes or no)\n-  : wants to take higher education (binary: yes or no)\n-  : Internet access at home (binary: yes or no)\n-  : with a romantic relationship (binary: yes or no)\n-  : quality of family relationships (numeric: from 1 - very bad to 5 - excellent)\n-  : free time after school (numeric: from 1 - very low to 5 - very high)\n-  : going out with friends (numeric: from 1 - very low to 5 - very high)\n-  : workday alcohol consumption (numeric: from 1 - very low to 5 - very high)\n-  : weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)\n-  : current health status (numeric: from 1 - very bad to 5 - very good)\n-  : number of school absences (numeric: from 0 to 93)\n-  : did the student pass the final exam (binary: yes or no)\n'"
https://github.com/srv902/MonoDepthCNN,A sample take on the CNN based approach to find the depth map of a monocular image.,"b'# MonoDepthCNN\nA sample take on the CNN based approach to find the depth map of a monocular image.\n\ndatasolver.m\nIt preprocesses the RAW NYUV2 dataset across different classes. The order of execution according to what I have done of the matlab script is given below. The mentioned matlab scripts are available freely under NYUV2-dataset-toolbox. \n\n1. Get the matched depth and rgb images using get_synched_frames.m\n2. After obtaining the images, apply project_depth_map.m . It aligns the depth map onto the rgb image.\n3. crop_image.m to select only those portions of the image which have valid depth data.\n4. fill_depth_cross_bf.m to fill the missing depth values which incorporates cross bilateral filter...\n'"
https://github.com/interactivetech/DeepStyleArtClassifier,CornellTech specialization project,"b'# DeepStyleArtClassifier\n## Specialization Project, Cornell Tech\n\n\nUsing Keras and Tensorflow to develop a classifier that can classify different art styles.\nWe will be leveraging weights from VGG16, VGG19, Inceptionv3, and ResNet50\n\n\n## Installation\n\npip install -r requirements.txt\nipython notebook\n\n\n### NOTE: I would recommend using virtualenv\n\nlinks that helped this project:\n\nLoading the models: http://www.pyimagesearch.com/2017/03/20/imagenet-vggnet-resnet-inception-xception-keras/\nTraining new model with VGG Model weights: https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n'"
https://github.com/DessimozLab/treeCl,Phylogenetic clustering package,"b'#  - Phylogenetic Tree Clustering\n\n\n\n is a python package for clustering gene families by\nphylogenetic similarity. It takes a collection of alignments, infers their phylogenetic trees,\nand clusters them based on a matrix of between-tree distances. Finally, it calculates a single representative tree for each cluster.\n\nYou can read the paper \n\n## Installation\n\n#### Preparing dependencies\n\nIf your system already has python 2.7, cython, numpy and a C++11-capable compiler (e.g. gcc >= 4.7), then youre ready to install.\n\nThe remaining python dependencies will be automatically installed during the build process.\n\n#### External dependencies\n\nTo be able to build trees, treeCl needs to call on some external software. The choices are RAxML, PhyML, FastTree or PLL (using ). If any of these are installed, available in your path, and keep the standard names they were installed with, they should work.\n\n#### Installing \nAll remaining dependencies will be installed automatically using pip\n\n    pip install treeCl\n\n\n\n## Example Analysis\n``` python\n\nimport treeCl\n\n""""""\nThe first point of call is the treeCl.Collection class. \nThis handles loading your data, and calculating the trees \nand distances that will be used later.\n\nThis is how to load your data. This should be a directory\nfull of sequence alignments in fasta .fas.phy formats. These can also be zipped using gzip or \nbzip2, treeCl will load them directly.\n""""""\nc = treeCl.Collection(input_dir=input_dir, file_format=phylip)\n\n""""""\nNow its time to calculate some trees. The simplest way to \ndo this is\n""""""\nc.calc_trees()\n\n""""""\nThis uses RAxML to infer a tree for each alignment. We can \npass arguments to RAxML using keywords.\n""""""\nc.calc_trees(executable=raxmlHPC-PTHREADS-AVX,  # specify raxml binary to use\n             threads=8,  # use multithreaded raxml\n             model=PROTGAMMAWAGX,  # this model of evolution\n             fast_tree=True)  # use raxmls experimental fast tree search option\n\n""""""\nWe can use PhyML instead of RAxML. Switching programs is \ndone using a TaskInterface\n""""""\n\nphyml = treeCl.tasks.PhymlTaskInterface()\nc.calc_trees(task_interface=phyml)\n\n""""""\nPhyML doesnt support multithreading, but treeCl can run \nmultiple instances using JobHandlers\n""""""\n\nthreadpool = treeCl.parutils.ThreadpoolJobHandler(8)  # external software can be run in parallel\n                                              # using a threadpool.\n                                              \nc.calc_trees(jobhandler=threadpool, task_interface=phyml)\n\n""""""\nTrees are expensive to calculate. Results can be cached to disk, \nand reloaded.\n""""""\nc.write_parameters(cache)\nc = treeCl.Collection(input_dir=input_dir, param_dir=cache)\n\n""""""\nOnce trees have been calculated, we can measure all the \ndistances between them. treeCl implements Robinson-Foulds (rf), \nweighted Robinson-Foulds (wrf), Euclidean (euc), and \ngeodesic (geo) distances.\n""""""\ndm = c.get_inter_tree_distances(geo)  \n\n# Alternatively\nprocesses = treeCl.parutils.ProcesspoolJobHandler(8)  # with pure python code, it is better to use processpools to parallelise for speed\ndm = c.get_inter_tree_distances(geo, \n                                jobhandler=processes, \n                                batchsize=100)  # jobs are done in batches to\n                                                # reduce overhead\n\n""""""\nHierarchical Clustering\n""""""\nhclust = treeCl.Hierarchical(dm)\npartition = hclust.cluster(3)  # partition into 3 clusters\n\n# To use different linkage methods\nfrom treeCl.clustering import linkage\npartition = hclust.cluster(3, linkage.AVERAGE)\npartition = hclust.cluster(3, linkage.CENTROID)\npartition = hclust.cluster(3, linkage.COMPLETE)\npartition = hclust.cluster(3, linkage.MEDIAN)\npartition = hclust.cluster(3, linkage.SINGLE)\npartition = hclust.cluster(3, linkage.WARD)  # default, Wards method\npartition = hclust.cluster(3, linkage.WEIGHTED)\n\n""""""\nSpectral Clustering\n""""""\nspclust = treeCl.Spectral(dm)\npartition = spclust.cluster(3)\n\n# Alternative calls\nfrom treeCl.clustering import spectral, methods\nspclust.cluster(3, algo=spectral.SPECTRAL, method=methods.KMEANS) # these are the defaults\nspclust.cluster(3, algo=spectral.KPCA, method=methods.GMM) # alternatives use kernel PCA and a Gaussian Mixture Model\n\n# Getting transformed coordinates\nspclust.spectral_embedding(2) # spectral embedding in 2 dimensions\nspclust.kpca_embedding(3) # kernel PCA embedding in 3 dimensions\n\n""""""\nMultidimensional scaling\n""""""\nmdsclust = treeCl.MultidimensionalScaling(dm)\npartition = mdsclust.cluster(3)\n\n# Alternatives: classical or metric MDS\nfrom treeCl.clustering import mds\npartition = mdsclust.cluster(3, algo=mds.CLASSICAL, method=methods.KMEANS)\npartition = mdsclust.cluster(3, algo=mds.METRIC, method=methods.GMM)\n\n# Getting transformed coordinates\nmdsclust.dm.embedding(3, cmds)  # Classical MDS, 3 dimensions\nmdsclust.dm.embedding(2, mmds)  # Metric MDS, 2 dimensions\n\n""""""\nScore the result via likelihood\n""""""\nraxml = treeCl.tasks.RaxmlTaskInterface()\nsc = treeCl.Scorer(c, cache_dir=scorer, task_interface=raxml) \nsc.write_partition(partition)\nresults = sc.analyse_cache_dir(executable=raxmlHPC-PTHREADS-AVX, threads=8)\n\n""""""\nGet the results\n""""""\n# Get concatenated sequence alignments for each group\nconcats = [c.concatenate(grp) for grp in partition.get_membership()]\nalignments = [conc.alignment for conc in concats]\n\n# Get a list of the loci in each group\nloci = sc.get_partition_members(partition)\n\n# Get trees for each group\ntrees = sc.get_partition_trees(partition)\n\n# Get full model parameters for each group\nfull_results = sc.get_partition_results(partition)  # same as returned by analyse_cache_dir\n\n'"
https://github.com/regreg/regreg,A multi-algorithm Python framework for regularized regression,"b'######\nRegReg\n######\n\nRegReg is a simple multi-algorithm Python framework for prototyping and solving\nregularized regression problems such as the LASSO. The goal is to enable\npractitioners to quickly and easily experiment with a variety of different\nmodels and choices of regularization.  In that spirit, the emphasis is on the\nflexibility of the framework instead of computational speed for any particular\nproblem, though the speed trade-off will generally not be too bad.\n\n****\nCode\n****\n\nSee https://github.com/regreg/regreg\n\nReleased under the BSD two-clause license - see the file  in the\nsource distribution.\n\nThe latest released version is at https://pypi.python.org/pypi/regreg\n\n*******\nSupport\n*******\n\nPlease put up issues on the _.\n'"
https://github.com/shiftshuffle/biomat,biomat,b'# biomat\nbiomat\n'
https://github.com/devrandom/python-blockstack,Python API for blockstack.io,b'\n\nBlockstack API (https://blockstack.io/)\n\n## Examples\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'
https://github.com/demunger/time_series,Techniques and Applications of Time Series Analysis,"b'### Techniques and Applications of Time Series Analysis\n\nIncluded above are samples of notebooks demonstrating random process simulations, as well as implementations of time series analyses.\n\n* : a two-dimensional branching Brownian motion simulation, including functions to compute density autocorrelation and particle clustering\n* : comparison of physical and Fourier space convolution methods on speed and accuracy\n* : a one-dimensional diffusion simulation, with an application of principal component analysis\n\nThis repository draws on work from Spring 2017 Time Series Analysis and Stochastic Processes.\n'"
https://github.com/umer-rasheed/Lane-Detection,Lane Detection for Self-Driving Cars,b'# Lane-Detection\n\nLane Detection for Self-Driving Cars\n\nThe repository aims to provide Canny Edge Detection based Lane Detection for for Self-Driving Cars.      The .ipynb and .html files are in Python-Files folder.                                                                     The input and output images and videos are Test-Images and Output-Images folder.                                           The input and output videos are in Videos folder.\n\n'
https://github.com/a-kravtsov/coolfunc_example,Example of how to use cooling functions tabulated by Gnedin & Hollon (2012),b'# coolfunc_example\nExample of how to use cooling functions tabulated by Gnedin &amp; Hollon (2012)\n'
https://github.com/karenlmasters/ComputationalPhysicsUnit,Jupyter Notebooks and Other Materials Used in Teaching Computational Physics (at Portsmouth Uni in 2017),"b'# Computational Physics Unit\n\nContent developed/modified for Portsmouth University, 2nd year Computational Physics Unit. Six lectures (python). \n\nText Book: Newman, M., Computational Physics - Revised and Expanded, [2013] \nSome Chapters available to download from: http://www-personal.umich.edu/~mejn/computational-physics/\n\n* Tips for getting the most out of Computational Physics unit. Why people chose different languages. (e.g why we\xe2\x80\x99re doing Python). Introduction to Python programming in general, and specifically in Portsmouth. \n* More introduction to python - lists and arrays. User defined functions.   \n* Graphics and visualization with python. \n* Stochastic methods (in python)\n* Stochastic methods (in python)\n* Monte Carlo Simulations (in python; lab not done in 2017)\n\nFor those interested - a really nice Markdown Cheat Sheet can be found at https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet#code'"
https://github.com/dmuralik/datascience,All about data science!,b'# datascience\nAll about data science!\n\n\nUseful resources:\n* http://slendermeans.org/pages/will-it-python.html - data analysis in R ported to python\n* http://greenteapress.com/thinkstats/thinkstats.pdf - probability and stats in python\n* http://www-bcf.usc.edu/~gareth/ISL/ - introduction to statistical learning\n* http://www.dataschool.io/\n* http://radimrehurek.com/data_science_python/\n* https://dnc1994.com/2016/05/rank-10-percent-in-first-kaggle-competition-en/ - Kaggle How to get to the top 10%\n* http://www.karsdorp.io/python-course/ - python course\n* https://www.pg-versus-ms.com/ - postgresql vs microsoft sql\n* https://pushpullfork.com/build-an-instant-twitter-dashboard-with-just-a-little-code/ - hwo to build a twitter dashboard\n* https://github.com/henripal/labnotebook - visualization for deep learning\n* http://serialmentor.com/dataviz/ - fundamentals of visualization in R\n* http://blog.minitab.com/blog/statistics-and-quality-data-analysis/what-is-a-t-test-and-why-is-it-like-telling-a-kid-to-clean-up-that-mess-in-the-kitchen - t statistics'
https://github.com/livenb/LungCancerPrediction,My personal solution of data science bowl 2017,"b'# LungCancerPrediction\nMy personal solution of data science bowl 2017\n### Data Preprocessing\nI have not precess the dicom image before, so I adapt some preprocess procedure from those 2 well written tutorial on kaggle ,\n\n\nSince the image is scanned the in cylinder shape, the outside body part comes with a HU value -2000, which should set to 0.\n\nBefore             |  After\n:-------------------------:|:-------------------------:\n  |  \n\n| Substance             | HU                                              |\n|-----------------------|-------------------------------------------------|\n| Air                   | \xe2\x88\x921000                                           |\n| Lung                  | \xe2\x88\x92500                                            |\n| Fat                   | \xe2\x88\x92100 to \xe2\x88\x9250                                     |\n| Water                 | 0                                               |\n| CSF                   | 15                                              |\n| Kidney                | 30                                              |\n| Blood                 | +30 to +45                                      |\n| Muscle                | +10 to +40                                      |\n| Grey matter           | +37 to +45                                      |\n| White matter          | +20 to +30                                      |\n| Liver                 | +40 to +60                                      |\n| Soft Tissue, Contrast | +100 to +300                                    |\n| Bone                  | +700 (cancellous bone) to +3000 (cortical bone) |\n'"
https://github.com/ChaseRichmond2/Data-Visualization-Project-,My final project for a Data Visualization course working with NCAA basketball statistics,"b'1) Motivation\rCollege Basketball garners the interest of viewers across the country. This interest intensifies each year in March when the Champion is crowned in a 68-team bracket style tournament. This single elimination tournament is known as \xe2\x80\x9cMarch Madness\xe2\x80\x9d. Each year millions of Americans cast their predictions for how the bracket will play out. In an attempt to better understand this tournament and college basketball as a whole. I have analyzed College Basketball data. My analysis seeks to address three main questions: \r\xe2\x80\xa2\t1) Which in-game statistic is the best predictor of team wins?\ro\tDistributional Analyses, Correlational Analysis, and Linear Regression\r\xe2\x80\xa2\t2) Is there a group of influential points when predicting team wins?\ro\tOutlier Detection\r\xe2\x80\xa2\t3) Do certain clusters of teams that perform significantly better than other clusters? How many clusters are optimal?\ro\tClustering\r\xe2\x80\xa2\t4) What are the most important features for classifying a team as Qualified/Not Qualified? How accurately can we classify teams?\ro\tClassification (Random Forest)\r2) Data Source\r\tTo address these research questions, I used a dataset from Sports Reference: http://www.sports-reference.com/cbb/seasons/2017-school-stats.html. This dataset features season long statistics for 351 NCAA division 1 basketball teams. Upon import this data was in a .txt file with a numbered index, to streamline my analysis I reset the index as the name of the team. Additionally I dropped unnecessary columns that detailed a team\xe2\x80\x99s performance within their conference, because this information was already represented through a team\xe2\x80\x99s aggregate total. I also divided in-game statistics by the number of games that team had played to get the statistics on a per-game basis. After these transformations, the data included the following:\r\xe2\x80\xa2\tSchool: String (index)\t\t\xe2\x80\xa2 FT: numpy.float64\t\r\xe2\x80\xa2\tG: numpy.int64\t\t\t\xe2\x80\xa2FTA: numpy.float64\r\xe2\x80\xa2\tW: numpy.int64\t\t\t\xe2\x80\xa2FT%: numpy.float64\r\xe2\x80\xa2\tL: numpy.int64\t\t\t\xe2\x80\xa2ORB: numpy.float64\r\xe2\x80\xa2\tW-L%: numpy.float64\t\t\xe2\x80\xa2TRB: numpy.float64\r\xe2\x80\xa2\tSRS: numpy.float64\t\t\t\xe2\x80\xa2AST: numpy.float64\r\xe2\x80\xa2\tSOS: numpy.float64\t\t\t\xe2\x80\xa2STL: numpy.float64\r\xe2\x80\xa2\tTotal Points: numpy.float64\t\xe2\x80\xa2BLK: numpy.float64\r\xe2\x80\xa2\tOpponent Points: numpy.float64\t\xe2\x80\xa2TOV: numpy.float64\r\xe2\x80\xa2\tFG: numpy.float64\t\t\t\xe2\x80\xa2PF: numpy.float64\r\xe2\x80\xa2\tFGA: numpy.float64\t\t\t\xe2\x80\xa23P%: numpy.float64\r\xe2\x80\xa2\tFG%: numpy.float64\r'"
https://github.com/yuriyi/hackathon2017,Hackathon 2017 project,"b""# hackathon2017\nHackathon 2017 project\n\n### An Agile Scientific and Total Organized Geoscience Hackathon\n\n### Project by ['Yuriy Ivanov', 'Anna Lim', 'Princy Ndong', 'Song Hou', 'Justin Gosses']\n\nJune 2017 Paris & virtually\n\n# 'Seismic Shotgather Interpreter'\nTeamname = 'Classy'\n\n1. Generate synthetic seismic data with labels (direct energy, reflection, multiples, coherent noise)\n2. Train a classifier using SVM\n3. Generate Tests dataset\n4. Draw interpreted lines\n5. Test model prediction\n\n\n\n\n### The final draft notebooks are below. \nThe UI notebook runs the functions notebook.\n- Project_hackathon_functions.ipynb\n- User_Interface_v6_Final"""
https://github.com/elisevmol/Machine-Learning,Course Machine Learning AUC,b'# Machine-Learning\nCourse Machine Learning AUC\n'
https://github.com/slowmotionprojects/icestupa,Hosting code written by Slow Motion Projects in the context of its collaboration with the Ice Stupa Project (http://icestupa.org).,b'# icestupa\nHosting code written by Slow Motion Projects in the context of its collaboration with the .\n'
https://github.com/katanachan/AInetworks,Asynchronous Irregular Networks in Cortical & Thalamic Regions,"b'# AInetworks\n Paper referred to : http://www.ncbi.nlm.nih.gov/pubmed/19499317\n Self-sustained asynchronous irregular states and Up-Down states in thalamic, cortical and thalamocortical networks of nonlinear integrate-and-fire neurons.\n Alain Destexhe et al. J. Computational Neuroscience: 2009\n\n\nFor single neuron model:\n 9 different types of results were recorded in the simulation:\n1. For a hyperpolarising current:\na = 0.001 microSiemens, b = 0.04 nanoAmpere, we get fast spiking\na = 0.001, b = 0.005, we get bursting\na = 0.001, b = 0, we get fast spiking\nIf we consider a moderate value of a=0.02 and b=0, the model also displays spike frequency adaptation. However, this model also generates a rebound burst in response to hyperpolarising pulses. This behaviour is seen in cortical low-threshold spike.\nA further increase in parameter a leads to more robust bursting activity and weaker spike-frequency adaptation and strong rebound bursts. This is observed in a = 0.04 and b = 0.\n With larger values, a = 0.08 and b = 0.03, the model generated bursting activity in response to both depolarising and hyperpolarising stimuli. As seen in thalamus reticular neurons.\n\nFor network model:\nTo initiate activity, a number of randomly-chosen neurons (from 2% to 10% of the network) were stimulated by random excitatory inputs during the first 50 ms of the simulation. The mean frequency of this random activity was high enough (200\xe2\x80\x93400 Hz) to evoke random firing in the recipient neurons. In cases where selfsustained activity appeared to be unstable, different parameters of this initial stimulation were tested.  It is\nimportant to note that after this initial period of 50 ms, no input was given to the network and thus the activity states described here are self-sustained with no external input or added noise. The only source of noise was the random connectivity (also termed \xe2\x80\x9cquenched noise\xe2\x80\x9d).\n\nThe plots for a network of neurons of 20, 40, 60, 80 and 100 neurons in the network are present in the networks/ folder. The Poisson Input was given at 200 ms at a rate of 300 Hz.\nObservations: As we increase the networks size, the mean firing rate increased and the irregulariy and asynchrony became higher.\n'"
https://github.com/ThomasRoca/Lecture-Columbia-Science-Po-2017,This folder contains and information and script for the dataviz workshop,"b'# The Data Driven Lecture & workshop Columbia-SIPA / Science-Po-2017:\nThomas Roca, Phd, Researcher and Data Officer @Agence Fran\xc3\xa7aise de D\xc3\xa9veloppement\n\nStay in touch via  & \n\n\nPart I. June the 1st\n\n##  DataDriven development\n\nWhether they are massive - big data - or more traditional (census, household surveys, administrative data, etc.), we are the witness of the explosion of the use of data for decision and policy making; first within the private sector then in the administration and lately at the crossroads between sectors and public affairs (e.g. Cambridge analytics, project google election, Facebook monitoring fake news etc.)\n\nAlgorithms now ""make decisions"" based on real time data. But these algorithms are mostly black boxes, parsing and computing data that are not open data (which providers have never been so centralized - GAFA, etc.). This raises societal and democratic challenges. \n\nAre we entereing a Post-StatistiK world? What seems obvisous today is that data and statistics production are no longer a State monoply. What are the promises and challenges of the data revolution ?\n\n#### I. Data-driven lecture\nSeminar organization: 1h30: 3x 20 mins presentations + 10 mins QA\nSlide available in this folder: Big_Data_Public_Policy_columbia_univ_SIPA.ipynb, \n\n#### II. Dataviz workshop\nSlide available in this folder: Dataviz_workshop_Columbia_Science_PO_SIPA.ipynb; \n\n---\n\n### 1. Introduction: The Open movement, from accountability to efficiency\n\n- The open movement\n- Towards more accountable States\n- More efficient administration ?\n- Towards new data partenerships with in the private sector?\n- What about Development Assistance ?\n\nConclusion: data is about people\n\t- A new data ecostystem\n\t- A new power distribution\n\t- A new ethic\n### 2. Data, the raw material of the digital revolution:\n- ""Traditional"" sources of data:\n\t- Census and Survey data\n\t- Administrative data\n\t- Africas Statisticall Tragedy\n- Big Data: when its raining information:\n \t- Sensor data (Sat., IoT, Cell-phone)\n \t- Social network data\n   \t- Use case: \n\t    - vulnerability to flood using statellite imagery (use case: )\n- Big Data: big difficulty to get access to it\n    - Privacy, security, business\n    - About Data ethics\n### 3. The Digital Humanitarian movement\n- The history of the digital humanitarian movement\n\t- Ha\xc3\xafti 2010\n\t- Crisis mapping ()\n- Official statistics & Big Data in emergency context\n\t- When saving time saves life, real time data matters\n\t\t- Early assessment use case:\n\t\t\t\t    - Using cell phone to monitor displacement (use case: )\t\t\n\t\t- Coordination of the emergency response ()\n\nConclusion: new tools, new skills and analytics strategy: AI everywhere ?\n\n## Further reading: \n- \n- \n- Data-Pop Alliance, 2015, \xe2\x80\x9cBeyond Data Literacy: Reinventing Community Engagement and Empowerment in the Age of Data.\xe2\x80\x9d . Bhargava, R & al.\n- Data-Pop Alliance, 2015, ""Leveraging Algorithms for Positive Disruption: On data, democracy, society and statistics"", , Letouz\xc3\xa9, E., Sangokoya, D.\n- Flowminder, 2016, Rapid and Near Real-Time Assessments of Population Displacement Using Mobile Phone Data Following Disasters: The 2015 Nepal Earthquake, Plos current disasters - \n- Lanier, J. 2013, Who owns the future ?\n- Meyer, P. 2015, \n- ONeil, C. 2014, On Being a Data Skeptic, \n- ONeil, C. 2016, Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy\n- Roca, T. 2015, \n- Roca, T. 2016, \n- Schartum, D.W., 2016, Law and algorithms in the public domain, Nordic Journal of Applied Ethics \n- Schwarz, B. & al. 2017, Socio-Physical Vulnerability To Flooding In Senegal, AFD, Cloud to Street, \n- Schwarz, B., Roca, T. 2016, Data-driven preparedness for disaster, devex - \n- Slatalla, M., Quittner, J. 1995, Master of Decepetion, the Gang that Ruled the Cyberspace, HarperPerennial\n- UN 2015, Data Revolution Report, \n- UN-OCHA, 2012, humanitarianism in the network age, \n- UN-OCHA, 2014, Humanitarian Innovation:The State of the Art \n- World Bank, 2016 World Development Report: \n\n--- \n\nPart II. June the 2nd\n\n## Workshop: Datavisulisation when statistics meets web & datascience (2h30)\nThis folder contains information and script for the dataviz workshop\n\nFor this workshop, we are going to use Python 3.4.3 and JavaScript.\nI recommend installing  and a code editor (e.g.  or ) we are also going to use \n\nHere the thematic this workshop will cover:\n\n1. Basic instroduction: (1h30)\n-   using packages with R and Python (15 min) \n-\tSIG using  and \n-\tBasic introduction to , \n- \tIntroduction to SVG\n-\tJavaScript & dataviz\n    + quick intro to \n    + dataviz the example of  and \n   \n2. Getting real time data: the API revolution: (1h)\n\n- Introduction: what is an  ?\n    + Intro to \n-  Example: Using \n-  Use cases:\n    + An application leveraging \n    + Buidling an  application \n    + Scrapping the web with (and twitter again) \n\t-  \n## Further reading:\n- Lupi, H., Posavec, S.,2016, ""Dear Data"", http://www.dear-data.com/\n- Roca, T. 2014, ""Web programming and dataviz with Stata"" - \n- Yau, N. 2011, visualize this: the flowingdata guide to design, visualization, and statistics\n- https://www.w3schools.com\n- https://www.highcharts.com/blog/\n'"
https://github.com/GregVial/clue_hackathon_code,Time series analysis and prediction with RNNs,"b'\n\nThis repository contains our contribution to the\n\n\n# 1. Usage\n## 1.1 Dependencies\nThe code was developed with python 3.5 and the following libraries and\nrespective versions :\n- pandas 0.19.2\n- keras 2.0.1\n- tensorflow-gpu 1.0.1 (It can also work with non gpu version)\n- numpy 1.12.0\n- joblib 0.10.3\n\n## 1.2 Training\n   First, you have to train the model using the train.py script.\n\n   While training, the weights are automatically stored each time the\n   validation loss decreases (in the /weights directory).\n   The parameters by default train a simple\n   LSTM model with  1 layer of 128 neurons over 15 epochs. It uses 100000 sequences\n   for training and 50000 for testing. The input and output size is 16\n   (i.e. 16 symptoms in input and 16 symptoms in output), \n   and sequences of 90 consecutive days are used for training and\n   predicting. The parameters can be tweaked from the command line\n   interface :\n\n\n\n\n## 1.3 Prediction\n   After model training, the predictions are made with predict.py.\n\n   It automatically loads the pretrained weights assuming you use the\n   exact same parameters as during training. The parameters can be tweaked from the\n   command line interface :\n\n\n\n## 1.4 Additional checks\n\n   Before starting the training or prediction phase, ensure all csv files are added\n   to the data/ directory. Due to privacy concerns these files have not been uploaded\n   with the source code.\n\n# 2. Preprocessing\n\nThe preprocessing steps are explained in detail on this notebook:\n\n\n# 3. Modeling\nOur solution leverages neural networks, more specifically recurrent neural networks (RNN) with long short term memory (LSTM). RNNs are well suited to deal with time series, which is why we chose this approach.\n\n## 3.1. Benefits of neural networks\nNeural networks offer the benefit of being end-to-end solutions, i.e. if well architectured they deal with feature\nengineering by themselves for the most part. For instance with a RNN there is no need to include whether the user was inactive on a specific day, or whether she was active but didnt experience a specific symptom. RNNs will determine this by themselves, if it is a useful feature for minimizing loss.\n\nThe other benefit is that neural networks often provide better performance than traditional machine learning techniques.\nThis has been observed for image recognition, image caption, speech recognition amongst others.\n\n## 3.2 Drawbacks of neural networks\nThe main drawback of neural networks is the time it takes to train them. Convergence to a minima can be very time consuming,\nin particular for RNNs which consist of many neural networks running in parallel, rapidly growing to millions of parameters to tune.\nThis has been a big challenge for this competition given the amount of data to process and the timeout set to 2 hours on the Statice platform.\n\nOur solution was designed so that the RNN can be trained locally and weights are reused without further training on the Statice platform.\nThis speeds up processing, but this also means that training is performed on synthetic data that doesnt necessarily match well the real data.\nAs a result we observed big discrepancies between local performance and on the Statice platform.\n\nAnother drawback of NNs is the difficulty to interpret them. With millions of parameters and little human feature engineering,\nunderstanding the logic of how the NN learns and predicts can be nearly impossible. With increasing concerns for transparency and new\nEU regulations soon to be effective, the need to explain clearly the automated decisions may prevent the use of NNs in certain situations.\nHowever, this is not an issue in this particular case, as the RNNs are not used to take clinical decisions, only as a suggestion engine of plausible\nexperienced symptoms throughout the cycle.\n\n## 3.3 Architecture\nWe chose to explore two main architectures: 1 LSTM layer with 128 cells and 2 LSTM layers with 256 cells.\n\n\nIllustration of a multi input/single output lstm\n\nOur RNNs are trained with a historical sequence of n days (by default 90) describing symptoms experienced by users (the input X), and the labels are\nthe symptoms experiences by the same users on the n+1 day (the output y)\n\nGiven the amount of users and length of the history, ideally we would like to train our network on all sequences of 90+1 days for every\nwoman, however this would generate too much data. Therefore we limit the number of training sequences to 100000 by default (but\nthis can be increased) and we chose to skip m (by default, STEP_DAYS = 3) step days of history for each woman. In other words, we look at the sequence of days\n1 to 90, then 4 to 93, then 7 to 96 etc.\n\nOther parameters include the number of symptoms to be predicted (by default 16 corresponding to the symptoms to be predicted in the hackathon, and we never modified that value), as\nwell as the number of symptoms to be used as input (by default the same 16 symptoms, but can be increase to\nthe full 81 symptoms). Our code also allows the user to include further data: day in cycle, whether user is experiencing her periods on a given day. We did not include these features for our tests due to training time constraints but we believe they may be useful.\n\nFinally the number of epochs corresponds to the number of times the RNN will see the full set of training sequences. Typically we observe\nthat for a number of epochs the training and validation loss both decrease, then we reach a point where validation loss stagnates, and\nfinally the validation loss increases while the training loss keeps decreasing. This last phase corresponds to overfitting, and at that\npoint it is better to stop the training. We setup the network so the RNN weights are saved only when the validation loss improves, so\ncontinuing training after reaching the overfit phase doesnt harm the model, but it is a pure waste of time.\n\n# 4 Performance\nOn local machines the performance of RNN looked very promising. We used a PC with 16Gb RAM and a GPU GTX 960M to pre-train the models.\nWith the default parameters, the RNN took 21 minutes to train and we achieved a log loss on hold out set (validation)\n of 0.0531 after 15 epochs, as shown in the graph below.\n \nUsing the same weights on the Statice platform the obtained log loss is 0.0761\n\nTrained on the Statice platform with the same parameters, we obtain a log loss of 0.0748\n\nThe training phase can be tested in a more interactive manner using this notebook:\n.\nThe training/validation loss and accuracy evolution over epochs are displayed at the end of the training.\n\nThere may be several reasons why the performance on the remotely trained model is not as good as the performance on the synthetic data.\nOne highly likely reason is that the 100,000 sequences we used to train on the static platform are not representative enough of the full user base. The power of neural nets lies in utilising large datasets, therefore the parameter N_train should be increased to train on more samples (if hardware allows).\n\nIt is also expected that increasing the sequence length from 90 to 120 days and reducing the step to 1 day intead of 3 will lead\nto better performances (if hardware allows).\n\n# 5. Next steps\n## 5.1 Add additional variables\nOur solution didnt take into account several variables made available to us, in particular specifics about the user such as\nage, weight, country etc. These information may be meaningful and could help improve performance.\n\nAlso, our intuition is that adding an additional variable ""last day of cycle"" would greatly help the RNN to improve prediction\non the first few days of the next cycle.\n\n## 5.2 Improve the RNN architecture\nThere are two obvious areas where the RNN can be improved.\n\nThe first one is linked to the regularization technique. We used simple dropout of 50%, but it is know that for RNNs dropout\nshould only be applied to non recurrent layers, as described in this \n\nThe second one is connected to statefulness of RNNs. Our RNN is stateless, however we are processing sequences which are related\nto each other, therefore at training time we could use statefulness to improve network.\n\n## 5.3 Test the solution on a remote platform equipped with GPU\nMost of our attempts to train the RNN on the Statice platform failed due to the the 2 hours timeout, whereas they were\nexecuting successfully locally on a PC equipped with GPU. Having a remote environment running a GPU would allow remote training\nand would very likely lead to performances equivalent to those observed locally.\n\n# 6. Lessons learned\nThis competition was the first hackathon that all members of the team ever attended. It has been a lot of fun, a lot of effort\nand came with numerous teachings. Here are some of them\n\n## 6.1 Neural networks\nWe loved working with RNNs. They are state of the art and the way forward for many applications. Despite the lack of results on the\nStatice platform, the good results we obtained locally give us confidence that they are the right way to deal with the challenge\nproposed by clue.\n\nWe will keep learning about them and experimenting with them in future assignments.\n\n## 6.2 Statice platform\nWorking with the Statice platform was a good challenge. Given this was the first public test of the platform, there are lots of adjustments that can be made. Our main recommendations are the following:\n- enable GPU instances for those using neural networds\n- enable better tracking/logging of errors to avoid to many back and forth between the developer and the platform administrator\n- setup multiple platforms with variable amount of data to enable quicker iterative process during the development phase that requires\nintensive testing\n\nOverall we were proud to be early adoptors of the Statice platform, which will no doubt address important privacy concerns that might have held many companies from sharing their data in similar competitions.\n\n## 6.3 Clue data\nThis readme document referred a lot to the technical approach and little to the data itself. This is in part because the approach we chose required minimal feature engineering of the data itself, although extensive preprocessing was necessary to train/predict using a RNN.\n\nIt is worth mentioning that being purely a team of males, all of us learnt a lot from the initial brief and the data itself. The breadth of symptoms experienced and reported by women surprised us and definitely give us a better understanding of women around us.\n\nThank you Clue for collecting this data and using it for the benefit of all!\n'"
https://github.com/dvu4/AIND-Recognizer, Build word recognizer for American Sign Language video sequences with Probabilistic Model,"b'# Artificial Intelligence Engineer Nanodegree\n## Probabilistic Models\n## Project: Sign Language Recognition System\n\n### Install\n\nThis project requires Python 3 and the following Python libraries installed:\n\n- \n- \n- \n- \n- \n- \n- \n\nNotes: \n1. It is highly recommended that you install the  distribution of Python and load the environment included in the ""Your conda env for AI ND"" lesson.\n2. The most recent development version of hmmlearn, 0.2.1, contains a bugfix related to the log function, which is used in this project.  In order to install this version of hmmearn, install it directly from its repo with the following command from within your activated Anaconda environment:\n\n\n### Code\n\nA template notebook is provided as . The notebook is a combination tutorial and submission document.  Some of the codebase and some of your implementation will be external to the notebook. For submission, complete the Submission sections of each part.  This will include running your implementations in code notebook cells, answering analysis questions, and passing provided unit tests provided in the codebase and called out in the notebook. \n\n### Run\n\nIn a terminal or command window, navigate to the top-level project directory  (that contains this README) and run one of the following command:\n\n\n\nThis will open the Jupyter Notebook software and notebook in your browser. Follow the instructions in the notebook for completing the project.\n\n\n### Additional Information\n##### Provided Raw Data\n\nThe data in the  directory was derived from \nthe . \nThe handpositions () are pulled directly from \nthe database . The three markers are:\n\n*   0  speakers left hand\n*   1  speakers right hand\n*   2  speakers nose\n*   X and Y values of the video frame increase left to right and top to bottom.\n\nTake a look at the sample \nto see how the hand locations are tracked.\n\nThe videos are sentences with translations provided in the database.  \nFor purposes of this project, the sentences have been pre-segmented into words \nbased on slow motion examination of the files.  \nThese segments are provided in the  and  files\nin the form of start and end frames (inclusive).\n\nThe videos in the corpus include recordings from three different ASL speakers.\nThe mappings for the three speakers to video are included in the  \nfile.\n'"
https://github.com/bertrammueller/FBT-analyze,Scrape trades from Facebook channel and execute on IG Markets,b'# FBT-analyze\n\nscraping trades from facebooktrader\n'
https://github.com/amueller/applied_ml_spring_2017,Website and material for the FIXME course on Practical Machine Learning,b'# Applied machine learning\nWebsite and material for the Sprint 2017 COMS W4995 course on Applied Machine Learning.\nYou can find the course website .\n'
https://github.com/kjayashankar/python-exercises,Sample Applications in Python,b'# python-exercises\nSample Applications in Python\n'
https://github.com/dBeker/Faster-RCNN-TensorFlow-Python3,Tensorflow Faster R-CNN for Windows/Linux and Python 3 (3.5/3.6/3.7),"b""# tf-faster-rcnn\nTensorflow Faster R-CNN for Windows and Linux by using Python 3\n\nThis is the branch to compile Faster R-CNN on Windows and Linux. It is heavily inspired by the great work done  and .\n\nCurrently, this repository supports Python 3.5, 3.6 and 3.7. Thanks to @morpheusthewhite\n\n### PLEASE BE AWARE: I don't have the time or intention to fix all the issues for this branch because I don't use it commercially. I created this branch just for fun. If you want to make any commitment, it is more than welcome. Tensorflow has already released an object detection API. Please refer to it. https://github.com/tensorflow/models/tree/master/research/object_detection\n\n# How To Use This Branch\n1. Install tensorflow, preferably GPU version. Follow . If you do not install GPU version, you need to comment out all the GPU calls inside code and replace them with relavent CPU ones.\n\n2. Checkout this branch\n\n3. Install python packages (cython, python-opencv, easydict) by running  \n   \n(if you are using an environment manager system such as  you should follow its instruction)\n\n4. Go to  ./data/coco/PythonAPI  \nRun   \nRun   \nGo to ./lib/utils and run \n\n5. Follow  to download PyCoco database.\nI will be glad if you can contribute with a batch script to automatically download and fetch. The final structure has to look like  \n  \n\n1. Download pre-trained VGG16 from  and place it as .  \nFor rest of the models, please check \n\n7. Run train.py\n\nNotify me if there is any issue found.\n\n"""
https://github.com/pydata/parallel-tutorial,Parallel computing in Python tutorial materials,"b'# Parallel Python: Analyzing Large Datasets\n\n\n\n\n## Student Goals\n\nStudents will walk away with a high-level understanding of both parallel\nproblems and how to reason about parallel computing frameworks.  They will also\nwalk away with hands-on experience using a variety of frameworks easily\naccessible from Python.\n\n\n## Student Level\n\nKnowledge of Python and general familiarity with the Jupyter notebook are\nassumed.  This is generally aimed at a beginning to intermediate audience.\n\n\n## Outline\n\nFor the first half we cover basic ideas and common patterns in parallel\ncomputing, including embarrassingly parallel map, unstructured asynchronous\nsubmit, and large collections.\n\nFor the second half we cover complications arising from distributed memory\ncomputing and exercise the lessons learned in the first section by running\ninformative examples on provided clusters.\n\n- Part one\n    - Parallel Map\n    - Asynchronous Futures\n    - High Level Datasets\n- Part two\n    -  Scaling cross validation parameter search\n    -  Tabular data with map/submit\n    -  Tabular data with dataframes\n\n\n## Installation\n\n1.  Download this repository:\n\n        git clone https://github.com/pydata/parallel-tutorial\n\n    or download as a .\n\n2. Install  (large) or  (small)\n3. Create a new conda environment:\n\n        conda env create -f environment.yml\n        source activate parallel  # Linux OS/X\n        activate parallel         # Windows\n\n4. If you want to use Spark (this is a large download):\n\n        conda install -c conda-forge pyspark\n\nTest your installation:\n\n    python -c ""import concurrent.futures, dask, jupyter""\n\n\n## Dataset Preparation\n\nWe will generate a dataset for use locally.  This will take up about 1GB of\nspace in a new local directory, .\n\n    python prep.py\n\n\n## Part 1: Local Notebooks\n\nPart one of this tutorial takes place on your laptop, using multiple cores.\nRun Jupyter Notebook locally and navigate to the  directory.\n\n    jupyter notebook\n\nThe notebooks are ordered 1, 2, 3, so you can start with \n\n\n## Part 2: Remote Clusters\n\nPart two of this tutorial takes place on a remote cluster.\n\nVisit the following page to start an eight-node cluster:\n\n\nIf at any point your cluster fails you can always start a new one by\nre-visiting this page.\n\nWarning: your cluster will be deleted when you close out.  If you want to save\nyour work you will need to \n\n\n## Slides\n\nBrief, high level slides exist at\n.\n\n\n## Sponsored Cloud Provider\n\nWe thank Google for generously providing compute credits on\n.\n'"
https://github.com/haustre/eurobot-hauptsteuerung,Eurobot 2015 Hauptsteuerung,b'Eurobot 2015 Hauptsteuerung\n===========================\n\nThis software has two parts. One runs on the BeagleBone and controls the robot. The other is running on the laptop.\n\nTo start the programm on the BeagleBone ::\n    Python3 /eurobot/hauptsteuerung_main.py\n\nTo start the programm on the laptop ::\n    Python3 /eurobot/laptop_main.py\n\nThe full documentation for this software is in /docs/_build/html.'
https://github.com/BuzzFeedNews/2017-04-fake-news-ad-trackers,"Data and analysis supporting portions of the BuzzFeed News article, ""Fake News, Real Ads,"" published April 4, 2017.","b'# Fake News Ad Trackers Analysis \xe2\x80\x94\xc2\xa0before Nov. 2016 vs. March 2017\n\nThis repository contains data, analytic code, and findings based on a collaboration between BuzzFeed News and , an author of the forthcoming .\n\nThe findings support portions of the BuzzFeed News article, "","" published April 4, 2017. Please read that article, which contains important context and details, before proceeding.\n\n## Data\n\nThis repository contains the following five CSV files:\n\n- : The data BuzzFeed News received from the researchers behind A Field Guide to Fake News, slightly restructured.\n\n- : For each website-and-tracker combination, whether the website had that tracker (a) before Nov. 2016, and/or (b) in March 2017. (Includes only ""comparable"" sites. See below for details.)\n\n- : Using the data in , counts the number of comparable sites containing each tracker during the two timeframes, and the net change.\n\n- : Using the data in , classifies each website-and-tracker combination into one of four categories: kept, removed, added or never had the tracker.\n\n- : Using the data in , counts the number of sites that kept, removed, added, or never had each tracker.\n\n### A note on ""comparable"" sites\n\nTo make the two time frames \xe2\x80\x94\xc2\xa0before Nov. 2016 and in March 2017 \xe2\x80\x94\xc2\xa0comparable, we removed two types of sites:\n\n- Sites with no observed trackers in the ""before"" period\n\n- Sites that had disappeared by the ""after"" period\n\nAfter doing so, we were left with 51 websites.\n\n## Code\n\nThe Python code that processes the data can be .\n\n## Feedback / Questions?\n\nContact Jeremy Singer-Vine at jeremy.singer-vine@buzzfeed.com.\n\nLooking for more from BuzzFeed News? .\n'"
https://github.com/Automating-GIS-processes/Lesson-7-Automating-Raster-Data-Processing,Basics of raster data processing with Python and Gdal,"b'# Automating Raster Data Processing:\n- Lesson 7: \n- Exercise 7: \n\n### Online Resources:\n - [www.gdal.org/] (http://www.gdal.org/)\n - [Python GDAL/ORG Cookbook] (https://pcjericks.github.io/py-gdalogr-cookbook/)\n - Lawhead 2013, Chapters 6 & 8 [Available as an ebook] (http://site.ebrary.com/lib/helsinki/detail.action?docID=10790286)\n \n'"
https://github.com/4OH4/datascience-python-snippets,Example scripts and code fragments for data analysis and management in Python,b'# dataSciencePython\nExample scripts and code fragments for data analysis and management in Python\n'
https://github.com/fmacrae/AI-Learning,Stuff I build while learning AI,b'# AI-Learning\n# AI-Learning\n'
https://github.com/vinyasmusic/Projects,A collection of Projects undertaken while studying Data Science,"b""# Projects\nA collection of Projects undertaken while studying Data Science.\n\n * Indian Data Set : A few visualizations using Tableau for some data sets I found interesting to look into. (Data from data.gov.in)\n * Kaggle : A script used in Kaggle competition for the Analytics Edge course at edX by MITx. The aim was to predict the voting category for the given test set using a training set. \n * Panama Papers : A visualization in R of Indian addresses mentioned in the latest Panama Papers data set. The necessary data was filtered in R and the script used is also included. It is also accompanied by 2 more visualizations created with the help of BatchGeo and Google's Fusion Tables, source for which is the accompanying CSV file.\n * TextNook Assignment : 2 Assignments part of an Interview. One involves scraping data from a blog. The other was to create a Reddit Clone. \n * \n \n\nNotes : \n * Need to include Rmd for Kaggle detailing the thought process and explaining the code\n * Try out Scrapy for scraping the data.\n"""
https://github.com/msk007/approximate-square-root,folder for square root HW,b''
https://github.com/cal859/odsc_hackathon,iPython Notebook for Challenge 1 of ODSC Hackathon,b'# odsc_hackathon\niPython Notebook for Challenge 1 of ODSC Hackathon\n'
https://github.com/whaley-group-berkeley/qspectra,Quantum simulations of nonlinear spectroscopy and dynamics for molecular aggregates,"b'QSpectra: Nonlinear Spectroscopy for Molecular Aggregates\n=========================================================\n\nQSpectra is a Python package designed for fast and flexible calculations of non-\nlinear spectroscopy signals on molecular aggregates, such as photosynthetic\nlight-harvesting complexes. The focus is on solving approximate models of\nelectronic dynamics under known effective Hamiltonians as open quantum systems.\n\nThe core idea is a dynamical model interface which allows for flexible\ncomposition of different dynamical models with different spectroscopy methods,\nas long as the equation of motion is a linear function of the system\nHamiltonian.\n\nTo enable efficient calculations, all simulations are performed under the\nrotating wave approximation. Furthermore, because the effective Hamiltonians\nconserve the number of electronic excitations, each model (when practical) only\npropagates within the necessary fixed subspaces of Liouville subspace.\n\nAlthough the QSpectra framework is written in Python, we expect that eventually\nsubmodules for new dynamical models may be written in a compiled language such\nas Fortran or C when if necessary for satisfactory performance.\n\nNote (November 12, 2021): qspectra is no longer maintained. If you find it\nuseful in your research, I encourage you to fork it on GitHub.\n\nInstall\n-------\n\nFirst, make sure youre running Python 3 and have recent versions of numpy\nand scipy installed.\n\nThen, clone the git repository and use :\n\n\nI highly recommend using  flag, which keeps the install directory in-place\nfor local development.\n\nTo view the example notebooks, you need . To run the unit tests,\nyou need .\n\nFeatures\n--------\n\n- Hamiltonians:\n    - Electronic systems (under effective Hamiltonians within the Heitler-London\n      approximation)\n    - Vibronic systems (electronic systems with explicit vibrational modes)\n- Dynamical models:\n    - Unitary\n    - Redfield theory (secular/non-secular)\n    - Zeroth order functional expansion (ZOFE)\n    - Hierarchical equations of motion (HEOM).\n    - Time non-local ME (special case of HEOM), on the roadmap\n- Spectroscopy/simulation:\n    - Free evolution (no field)\n    - Equations of motion including fields, such as:\n        + The density matrix following pump pulses\n        + Equation-of-motion phase-matched-approach (EOM-PMA), in progress\n    - Linear response based methods, such as:\n        + Absorption and emission spectra\n        + Impulsive probe pulses\n    - Third-order response functions (in particular, for 2D spectroscopy)\n\nExamples\n--------\n\nExample notebooks demonstrating the features of QSpectra are included in the\n""examples"" directory.\n'"
https://github.com/dataewan/dlnd-your-first-network,First neural net from the deep learning nanodegree,"b""# DLND your first network\n\nFirst udacity project, implementing a neural net in numpy.\nPredicting ride sharing usage,\nbased on actual data.\n\n## My Notebook\n\nThis builds on the homework lessons, where solutions were provided.\nCombining all of them together.\nThis was fiddly.\n\nOnce the neural net is coded, the main bit of work is picking the hyperparameters.\nA few observations there.\nYou can graphically see when the training rate is too high or too low by looking at the loss functions over epochs.\nIf it slopes down too slowly, increase the training rate.\nIf it jumps about too much, then the training rate is too high.\n\nI don't have an intuition for the number of nodes in the hidden layer yet, \nthis is something that I'll have to do more reading on.\nIt isn't just a case that putting more hidden nodes in is always a good thing.\n"""
https://github.com/ub247/CrimesResearch,Crimes Reserach Masters Thesis,"b'# CrimesResearch\n\nI will be sharing my Crimes research related code here but without out sharing the data.\nIts for my own personal reference, it will be good if can be helpful for others as well, just basic data science stuff.\n'"
https://github.com/smy5/Sandy---KimLab,Sandy's Jupyter Notebooks ,"b""# Sandy---KimLab\nSandy's Jupyter Notebooks \nI love pizza\n"""
https://github.com/smallnamespace/fireplace-profiling,Profile fireplace to look for speed gains,b'# fireplace-profiling\nProfile fireplace to look for speed gains\n'
